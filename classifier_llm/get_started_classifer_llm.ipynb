{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model_name_openai = \"gpt-4o-2024-08-06\" # Garante 100% do output em JSON\n",
    "\n",
    "llm_openai = ChatOpenAI(\n",
    "    model=model_name_openai, # 100% json output\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Você é um assistente de IA muito prestativo que vai auxiliar um cozinheiro a classificar textos que falam especificamente de receitas de BOLOS. Você precisa classificar o texto como YES ou NO, dependendo se o texto fala de um bolo ou não.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt), \n",
    "            (\"human\", \"query do usuário: \\n\\n {query}\")\n",
    "        ]\n",
    ")\n",
    "\n",
    "class GetSchema(BaseModel):\n",
    "    \"\"\"Schema de bolo\"\"\"\n",
    "    \n",
    "    resultado: str = Field(description=\"YES caso o texto fale de BOLOS e NO caso contrário\", examples=['YES', \n",
    "                                                                                                       'NO'])\n",
    "    \n",
    "\n",
    "llm_openai_with_tools_extraction = llm_openai.bind_tools([GetSchema]) #, strict=True)\n",
    "chain_openai_structured_extraction = prompt | llm_openai_with_tools_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"Olá, eu gostaria de saber como fazer um bolo de cenoura com cobertura de chocolate. Pode me ajudar?\"\n",
    "query_2 = \"Olá, eu queria saber qual foi o placar do jogo de ontem. Pode me informar?\"\n",
    "response_1 = chain_openai_structured_extraction.invoke({\"query\": query_1})\n",
    "response_2 = chain_openai_structured_extraction.invoke({\"query\": query_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetSchema',\n",
       "  'args': {'resultado': 'YES'},\n",
       "  'id': 'call_rDlrL6zqJuMTJrYdg0HbcjWw',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'GetSchema',\n",
       "  'args': {'resultado': 'NO'},\n",
       "  'id': 'call_5xYU0mcvA52pxlGRwO3GWa4q',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2.tool_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
