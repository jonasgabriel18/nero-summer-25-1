{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from pytube import Playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://www.youtube.com/watch?v=Mz8-LTednt4&list=PLofp2YXfp7TZZ5c7HEChs0_wfEfewLDs7\"\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(test_url)\n",
    "transcript = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"[Music] thank you hi my name is Greg durat I'm really excited to introduce you to my course natural language processing I'm an assistant professor in the department of computer science here at UT Austin and I've been working in the field of natural language processing for about 12 years now and it's been a really exciting 12 years and we've seen a progression all the way from linear models uh things like classifiers up through the latest and greatest systems like Chachi BT so if you've played around with Chachi PT you know that it can do things like this we can ask a question like how many years after the start of the Revolutionary War did George Washington step down as president and the system will do its computation and it will produce this answer that has to look up these two dates and take the difference between them and figure out this number of years so Chachi BT is a large neural network language model but what's a language model what kind of neural network is being used here what can it be used for how is it trained these are all kinds of questions that you might have that are going to get answered in this course so the goal is to start all the way from ml fundamentals like classification and then build up to the tools that we need to understand systems like Chachi BT we're going to cover all of the methods that we need here and we're also going to understand the applications that modern NLP research is used for and we're going to give you the tools to understand how to take these systems and apply them to real world problems so when you're faced with something down the road you know what to use the lectures for this course are going to walk you through a lot of these different modeling details and show you how these are actually built from the ground up here's an example of what one of the lectures is going to look like so their formula involves q k and V which are computed based on the kind of input embeddings like we've been doing multiplying by these parameter matrices WQ WK and WV and we take this soft Max of Q times K transpose so that we've all seen and then there's one extra little step that they do here which is they divide and rescale everything by square root of DK which is the basically Vector Dimension that all of these uh inner products are happening in it basically just has the effect of making the softmax less peaked it's not too important and then they multiply by V here in order to get the output so they're taking their uh attention Matrix a and multiplying that by V and so what they're getting is the output is a kind of weighted combination of V in addition to the lectures we also have a number of programming assignments which are designed to give you hands-on experience you're going to be actually building these models basically from scratch in pi torch so you're going to understand all the pieces from the ground up and you're also going to understand the data and how important that is in these processes so a big emphasis of the course is going to be thinking about data you can look at the syllabus in the course website for more details I'm really excited to teach this course and I hope to have you as a student thank you\", metadata={'source': 'Mz8-LTednt4'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcrição de um curso de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_playlist = Playlist(\"https://www.youtube.com/playlist?list=PLofp2YXfp7TZZ5c7HEChs0_wfEfewLDs7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp_playlist.video_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_course_transcripts = []\n",
    "\n",
    "for video_url in nlp_playlist.video_urls:\n",
    "    # print(video_url)\n",
    "    video_loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "    video_transcript = video_loader.load()\n",
    "\n",
    "    nlp_course_transcripts.append(video_transcript[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"[Music] thank you hi my name is Greg durat I'm really excited to introduce you to my course natural language processing I'm an assistant professor in the department of computer science here at UT Austin and I've been working in the field of natural language processing for about 12 years now and it's been a really exciting 12 years and we've seen a progression all the way from linear models uh things like classifiers up through the latest and greatest systems like Chachi BT so if you've played around with Chachi PT you know that it can do things like this we can ask a question like how many years after the start of the Revolutionary War did George Washington step down as president and the system will do its computation and it will produce this answer that has to look up these two dates and take the difference between them and figure out this number of years so Chachi BT is a large neural network language model but what's a language model what kind of neural network is being used here what can it be used for how is it trained these are all kinds of questions that you might have that are going to get answered in this course so the goal is to start all the way from ml fundamentals like classification and then build up to the tools that we need to understand systems like Chachi BT we're going to cover all of the methods that we need here and we're also going to understand the applications that modern NLP research is used for and we're going to give you the tools to understand how to take these systems and apply them to real world problems so when you're faced with something down the road you know what to use the lectures for this course are going to walk you through a lot of these different modeling details and show you how these are actually built from the ground up here's an example of what one of the lectures is going to look like so their formula involves q k and V which are computed based on the kind of input embeddings like we've been doing multiplying by these parameter matrices WQ WK and WV and we take this soft Max of Q times K transpose so that we've all seen and then there's one extra little step that they do here which is they divide and rescale everything by square root of DK which is the basically Vector Dimension that all of these uh inner products are happening in it basically just has the effect of making the softmax less peaked it's not too important and then they multiply by V here in order to get the output so they're taking their uh attention Matrix a and multiplying that by V and so what they're getting is the output is a kind of weighted combination of V in addition to the lectures we also have a number of programming assignments which are designed to give you hands-on experience you're going to be actually building these models basically from scratch in pi torch so you're going to understand all the pieces from the ground up and you're also going to understand the data and how important that is in these processes so a big emphasis of the course is going to be thinking about data you can look at the syllabus in the course website for more details I'm really excited to teach this course and I hope to have you as a student thank you\", metadata={'source': 'Mz8-LTednt4'}),\n",
       " Document(page_content=\"[Music] hello my name is greg durat welcome to natural language processing in this segment we're going to talk about the overall outline of the course and the broad goals so let's start with what the goal of natural language processing or nlp is we're going to call it going forward is so nlp is about solving problems that require deep understanding of text and i'm going to talk about deep understanding in contrast to shallow understanding things like string matching or regular expressions that you might have learned in earlier programming courses uh and we want to understand a little bit about the kinds of things that we want to build that require this deep understanding and why they want to then why they require that so for example we want to build systems that can talk to us like dialog systems machine translation systems automatic summarization systems etc and so you know we'd like to be able to have a conversation with our phone that goes like this hey siri what's your favorite kind of movie just sort of conversational right um and siri says i like superhero movies and then you say okay what's come out recently and siri says the avengers so we'd like to be able to have a conversation with a personal assistant like this be able to have a back and forth and get some information out of it and in order to do this this system needs to be able to understand what we're saying respond appropriately and then in some cases be able to retrieve information like for example a list of movies that have come out recently so why does this require deep understanding well you're never going to be able to write down a whole big set of rules that are going to encapsulate all the possible flows that the dialogue might take right the system needs some way of uh recognizing what we're saying beyond just some kind of rules or templates and then knowing how to respond to it so translation is another great example of this so here's a sentence in chinese and a corresponding translation from google translate so the interesting thing here is that you know you cannot just go along and translate the sentence word by word in particular uh in the first chunk here the political bureau of the cpc central committee that's this first segment which is followed by july 30th which is followed by hold a meeting in chinese that's roughly what we call glosses of those uh those little phrases and that's out of order from with respect to how it's ordered on the english side and so in this translation process we actually need to uh look at the syntactic structure here understand the you know the kind of concepts being communicated and be able to render them in the right surface order we can't just apply a dictionary and go along kind of word by word so again we need this deep understanding here and a final example i'll give is systems that extract information and answer questions if you type when was abraham lincoln born into google something like the following is going to happen we recognize that born means okay we've got a big database of people and names and we need to map the idea of someone being born to like this birthday field in the database that doesn't sound all that complicated but you know once you get a large database with a whole bunch of different columns and many many ways of asking about them it actually becomes a fairly involved problem and then you know we need to retrieve the right birthday from this database with some sort of computation it gets even trickier when you ask about things that aren't in a database for example how many visitors center our centers are there in rocky mountain national park okay you could theoretically have a database that stores this information but i guarantee you google does not and so instead what we need to do is retrieve some information from the web using something like google search and then try to find a relevant snippet and answer the question based on that so in this case the third paragraph of the wikipedia article has a sentence that tells us the park has a total of five visitor centers and so um we're gonna learn techniques in this course that are gonna allow us to understand this pipeline how to retrieve this information and then answer questions based on these snippets okay so this kind of tells you the sorts of things that we want to build and i think gives you an idea of why these problems might be hard so let's talk a little bit about the standard nlp methodology for doing this so our starting point is text in this course we're always going to start with text rather than speech input there's a whole ton of interesting work that does use speech and in general you know we're gonna assume that if you wanted to use speech you could run something like an off-the-shelf automatic speech recognition system first all right for the first part of the class we're going to talk about a set of tools for producing text what we call text analysis uh and this is going to be layers of annotation on top of this text like syntactic parses understanding the syntactic structure of the text co-reference resolution understanding which pieces of the text refer to the same real world concepts entity disambiguation figuring out what those concepts are understanding the discourse structure and so in general we have a whole set of tools available that are going to allow us to understand a little bit more of this deep structure of text so we can start to get at some of these things that we want to do and then that's going to enable us to do all these cool applications like summarize things extract information answer questions identify sentiment translate the text into another language etc and so throughout this course we're going to tackle all of these problems with the you know under the kind of broad umbrella of statistical approaches using machine learning all of these things require dealing with complex open domain text and the only way we know to reliably do that is to learn models from large-scale data all right so let's talk about how we represent language so in this kind of first text analysis piece what do these representations look like so we're going to start off the course by thinking about labels so for example if we say the movie was good it's going to have a discrete sentiment label that's just going to say this is positive sentiment we might have a statement like beyonce had one of the best videos of all time this is subjective we might classify statements as subjective versus objective all right and there's a whole bunch of other different types of of labeling schemes we might use another layer of annotation looks like either sequences or tags and this is where we're going to go after labels we're going to look at start to look at text as a structured object and we want to be able to recognize that in this case the span tom cruise is a person and mission impossible is a work of art and this is a task called named entity recognition we're also going to look at things like part of speech tagging that analyze text sequentially and from there we're going to move on to tree structure so this is really one of the distinguishing features of language is the fact that it has this recursive hierarchical syntactic structure and so we're going to understand what parse trees like the one here look like and how to understand them and how to predict them we're also going to look at semantic representations like lambda calculus here this is another sort of tree structure that builds an expression to represent this idea of flights to miami which could be executed against a database so taken together these are this is going to give us a set of tools for understanding these different structures of language and for each tool we're going to see what machine learning techniques allow us to predict it and deal with it all right so now we've got these representations how do we actually use them for these downstream applications so we're going to zero in here on the case of trees and i'm just going to give you a brief flavor for how this kind of thing is going to come up so early in the course when we think about feature-based linear models one thing that trees might allow us to do is to extract some sort of syntactic feature for classification later when we think about neural networks a cool thing you can do with trees is actually build your neural network to have some sort of tree structure and when we think about machine translation one thing we might do is actually model the translation process as a mapping from a syntactic parse in one language to a syntactic parse in another language and this is the the model to do that is something called a tree transducer so there's a whole bunch of ways that these analyses of text can impact our modeling of course the the elephant in the room uh is that with the rise of deep learning and neural networks there's an increasing trend towards building purely end-to-end models and so there's going to be this tension throughout the course of you know do we use these intermediate pieces of text analysis or do we build models strictly end to end and the answer is not going to be clear-cut there have been huge successes from purely end-to-end models but there's also a lot of interesting kind of cutting-edge work right now that is starting to move them back towards uh incorporating this kind of discrete structure for reasons of interpretability or controllability and so this is a topic that's going to come up again and again throughout the course so we have to think about also why any of these predictions is hard so uh you know i mentioned that we're going to need to build some sort of statistical models for mapping both from text to these analysis layers and then also going from the analysis layers to the applications and the reason this is hard is because language is complicated and language is ambiguous and so in order to understand that a little bit more we're going to talk about what ambiguities we need to resolve and what ambiguity means in this context all right so we can look at a a sort of apocryphal headline here teacher strikes idle kids all right so this is this is kind of funny at least it's it's funny to me uh because there's two interpretations of it and despite the fact that we when we look at this we probably know what the author meant uh we still get this other reading so what the author probably meant was that teacher is a noun and strikes as a noun idol is a verb this is a sort of unused unusual usage of it and then kids is a noun okay but you could also read it as teacher as a noun strikes as a verb idol is an adjective and kids is a noun and so you know the second interpretation of course is that the teachers are are really fed up with these kids who are idle and just pow and so i talked about sequential analysis and part of speech tagging which we're going to see you know early on in this course and that's essentially about resolving the which interpretation of the sentence we want to get all right uh ban on new dancing on governor's desk all right so we can think about this in the context of the last example like what are the parts of speech here it turns out that actually the parts of speech aren't really that ambiguous in this case what's ambiguous is the syntactic structure what the author intended is that we have two prepositional phrases here um we have a band that is on new dancing and that ban on new dancing is on the governor's desk but the sort of funny interpretation is that we actually have this noun phrase concept of new dancing taking place on the governor's desk okay they would probably want to ban that too but that's neither here nor there so the ambiguity in this case is a higher level syntactic one of how these different pieces compose it's not a low level part of speech ambiguity but instead one that we're going to have to understand syntactic parsing in order to deal with all right and let's look at a third example iraqi head seeks arms actually in this case we don't have either of these ambiguities the the parts of speech are totally clear and the syntactic structure turns out is clear as well what is ambiguous is the is the sense of the word uh is the sense of the word head is it someone's head or is it like a head of state and then similarly for arms uh is it you know someone's arms or are we talking about weapons here so in this case we have a more of a semantic ambiguity that arises from the different senses of these these words okay so what we've what we've seen here is that there are a bunch of these sort of binary ambiguities right there's two possible interpretations of something and we need to pick between them and this is what statistical models are going to allow us to do and the reason nlp is hard is not because there's two possibilities but actually because there's an exponentially large number of them and so we're going to need really powerful models in order to reject all of those bad possibilities um so we're going to think about a translation example here i'll remove which roughly means it's really nice out or it's really nice or the weather is beautiful there's actually lots of different valid translations of this but if we think about even for this four word sentence what could happen you know it is really beautiful outside that's another possibility um if you go word by word and translate this he makes truly beautiful this is actually something that you know is totally valid translations of each of those four words of course it absolutely does not make sense and is not what uh the speaker or writer of this intended or in fact actually handsome another case so a lot of times when we look at these these examples of ambiguity and we say oh haha there's two interpretations no in fact there's you know billions of interpretations and uh are we're going to need structured models that can reason about them and again are very statistically powerful so they can pick the right one out of all of these possibilities okay so in order to understand those kind of models and uh situate what we're going to be looking at with respect to history we're just going to take a very quick uh cruise through the timeline of nlp so our timeline is going to start in the 80s late 80s with the so-called ai winter and so systems at this time were largely non-statistical they were either rule-based or expert systems and so you know the the history of ai has been about there's progress and then there's periods where we sort of hit a wall and then uh progress happens again and so this was one of those periods of cooling off where there was less interest sort of a wall was hit in the early 90s researchers at ibm did some of the earliest work on statistical machine translation using uh the proceedings of the canadian parliament which was a whole bunch of aligned data so sentences that are were in in this case in french and english uh that corresponded to each other and they were able to build a statistical translation system out of that so this as well as the development of the pen tree bank in 1993 which was one of the first large-scale corpora annotated with syntactic information are two of the sort of first instances of well-known successful statistical modeling uh efforts and so uh the pen tree bank once that data set existed uh started leading to people building things like part of speech taggers and syntactic parsers and saying oh hey wait it looks like having all this data is actually useful and so then this led to an explosion of interest in things like supervised learning support vector machines uh conditional random fields applied to tasks like named entity recognition or sentiment analysis so we started to understand that data was really useful and machine learning could help us harness the power of that data to solve some of these problems there was a lot of interest also in unsupervised learning things like topic models and grammar induction we're not going to talk about these so much in the course they sort of ended up kind of falling off in terms of the the overall interest in them people started becoming more interested in okay how do we combine labeled and unlabeled data and then this led to uh in 2014 2015 the the neural revolution and then following that the kind of revolution and pre-trained models we've seen with things like uh bert and gbt so those models are great at this task of of harnessing all the data on the web as well as whatever supervised corpus you have for the task that you want to solve and so this uh what's here in this box is the techniques that are going to be the focus of this course so we are going to focus on supervised learning semi-supervised methods and neural network models and the kind of common thread of all these models is that they are going to be able to handle ambiguity by learning how to map from input text to linguistic representations using data and that's going to allow us to start to tackle these issues that we saw on the previous few slides so to just give a brief sense of where we're going and the outline of the rest of the course uh we're going to start by talking about classification we'll start with linear classification and then very quickly move into neural and talk about word representations which is a really important kind of ingredient for how to actually take string valued language data and convert it into something that machine learning can use then we're going to talk about these text analysis tasks like tagging and parsing and we're going to talk about the structured models that can deal with these tasks then we'll talk about language modeling and pre-training so once we've seen this we really need to kind of build up the idea of language models which are going to establish the idea of recurrent neural networks and things like that for sequentially predicting text and then pre-trained models are going to inform how we think about building models for the course going forward we'll then go into question answering and semantics machine translation and other applications we're going to kind of see how the those core tools and models can then be built out to solve all these different problems and a sort of brief tour of what the state of the art looks like in each of these areas so that's where we're headed and that's it for this segment thank you\", metadata={'source': 'k5p8teUNHX4'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the simplest machine learning models that we're going to see in this course which are linear binary classifiers so uh this these may be something that you've come across in a previous course so uh you know consider this a way to kind of establish notation for this and just to make sure we're all on the same page so what a classifier is roughly going to do is it's going to take as input a point x in r to the d this means a some sort of uh numeric data in a d dimensional real valued feature space uh and we are going to also use the notation f of x extensively in this course where f is what we call a feature extractor all right and so the reason we think about this way we think about it this way is that x in a lot of our problems is going to be a string it's not going to be a numeric valued thing and then uh f x is going to be the set of features uh in the d dimensional space so um we might move back and forth between these two different notations at various points throughout the course um but generally there does need to be some process of mapping from again a string input which most natural language is into some sort of numeric representation that can be used in machine learning all right and then each point has a label y and in binary classification there's only going to be two possibilities and we are going to refer to these as minus one and plus one or negative and positive class so roughly the way we can think about this is that we've got a set of points this is going to be a two-dimensional feature space and we're going to draw some pluses up here and we're going to draw some minuses down here and so each of these again represents one of these x's and then whether it's plus or minus represents the label all right and then we're going to represent a classifier as a weight vector w um and you might see theta in some places um such as you know texts or uh academic papers um we're gonna try to stick with w in this course but again these these mean the same thing and so the way that this weight vector leads to a classification decision is the following um we compute the dot product or the kind of vector transpose here of the weights and the features associated with the point and then we ask if that's greater than zero and essentially the way we can think about this is that uh there's the weight vector points in some direction in this feature space and then we've got this decision boundary that's perpendicular to the direction that w is pointing so the way to think about this is any point on the top right side of this decision boundary is going to be classified as positive because the dot product with w is going to be positive any point uh on the other side of the boundary is going to be negative um and and you know we're not going to worry too much about what happens if you exactly land on the boundary you need to break ties somehow right so that's the basics of uh the kind of formulation of linear binary classification um one thing i will note is that uh we often see in kind of other settings here we see a a bias term b and we are going to we are going to not use that um and the reason is because we can just take f of x um which you know let's say the features for a particular point are something like this and we transform this into an f of x that has a 1 on the end of it so by adding a one to the end we've basically folded this bias term into uh you know into the actual feature vector and so that's what's going to allow us to stick with this representation of a classifier going forward and we're not going to have to juggle these bias terms so this is the this is the kind of basic setup of binary classification going forward over the next few segments we're going to talk about uh the perceptron and logistic regression which are two different ways of thinking about learning these weights that we're going to see are functionally quite similar so that's it for this segment\", metadata={'source': 'DVxR3AwdxoA'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about an example of doing classification for sentiment analysis and that's going to help us start to build up our pipeline for how to go from text to feature vectors which uh we're then going to be able to use in a classifier and you know take labeled data and train a classification model so let's start with a simple example the movie was great um so this is uh you know going to be a positive uh sentiment example so um we've talked about positive and negative uh labels in binary classification and for sentiment analysis that's going to correspond to positive sentiment uh meaning you like something or negative sentiment meaning you don't like something let me make this example a little bit more complicated [Music] would watch again all right so why do we think that uh this example is positive well it has this word great in it which uh is going to be a word that typically conveys positive sentiment and we have this idea of watching something again right that sounds like you know probably you liked it enough in order to do that so another example the film was awful i'll never watch again all right so uh in this case this example is conveying negative sentiment we see the word awful and interestingly we see uh this this uh you know this pair of words watch again uh but this case we're saying we'll never watch again so i you know this kind of indicates a little bit about why this task is gonna be complicated because we're going to have to think about factors like negation and kind of higher level structure and how they impact things but the basic kinds of models we're going to look at are going to use what are called bag of words features and we're going to get to those in a second but those features are essentially just going to look at what words or short sequences of words are present in this this data okay so there's going to be roughly two steps so we're going to map from text to these feature vectors and this is a step called feature extraction and then given a date you know a data set of d labeled examples um we're going to train a classifier uh so in this segment we're going to talk about the first step of this process just how to go through this this feature extraction step and then we'll come back to the machine learning aspects of this okay so for feature extraction then we're going to think about how we can turn one of these examples into a feature vector all right so we're just going to simplify it to the movie was great all right so the basic uh the kind of most basic version of features we are going to use is what we call bag of words and roughly what this means is assume we have 10 000 words in our vocabulary so we have a list of let's say the 10 000 most common words in whatever language we're doing sentiment in um in this case that'll be english and we lay them out in a big vector um so you know we have the a of at you know etc somewhere down the line we have movie somewhere down the line was somewhere down the line great um etcetera so there's 10 000 of these and the way this is going to work is we're going to have a one in each position that uh we that the word is present in the input example and a zero in all of the other positions um so that is going to lead to a vector that looks like this so we are going to have four ones um and you know 9996 zeros in this vector all right and so there's a there's kind of an additional uh variable here which we haven't talked which you doesn't come up in this example but um these values could either be counts um so how many the are present or it can just be presence absence um so this is going to be either 0 or 1. and so in that in that count case if we had multiple does in the sentence we would get you know a count of 2 or a count of 3 in this feature vector all right so there's a so so so this is the representation that we're going to use uh kind of going forward and it's a fairly effective representation for uh for sentiment analysis um i'll talk about two small extensions of it um one is what we call bag of n-grams so an n-gram is a is just a sequence of n consecutive words and so uh if we think about the two grams of the example we have the movie i'm just going to underline them so you know that they're a unit movie was and was great i mean if you want you can add kind of separate start and end tokens that get kind of rolled up with these as well but we're just going to stick with these with these three for now and so you can build a similar kind of representation where instead of 10 thousand words now you maybe have a hundred thousand bi-grams uh and you know that can be your feature space now and so that's gonna allow you to capture things like if you said the movie was not great you're gonna have a bi-gram there for not great and you're going to be able to use that all right and one other uh modification of this uh is what's called tf idf term frequency inverse document frequency so we're not going to use this a ton in the course going forward but i will briefly mention it term frequency is just the count of the term um so you know again this is just the count representation from above so if you have the twice the term frequency of the is going to be two and then idf is what's called inverse document frequency and there's there's a lot of there's a lot of ways to do this but roughly it's written as the log of the total number of documents over the number of documents with word w in them and so the reason you use a representation like this uh is because what this what this does is suppose that you have a common word like the then the is going to be in almost every document in some kind of collection that you have and this log term is going to be very close to zero and so the tf idf score for a word is just going to be the term frequency times the inverse document frequency and so then the is going to receive a value in this vector that's very close to zero whereas if uh instead you're you have a word that's very rare like mango or something like that maybe mango shows up 10 times in your document and actually only shows up in 1 out of 10 000 other documents that you might have and in this case this log term uh is going to be reasonably large if you're taking log base 10 it'll be four and so you know now you're going to end up with a much higher value for that and so it's going to emphasize words that are kind of characteristic of the particular document that we're talking about while ignoring other ones all right like i said we're not going to make you make too much use of that but i wanted to introduce the concept all right so the last thing i'm going to talk about here is pre-processing so we talked about mapping from either words or n-grams into this feature space but what we haven't talked about is how to go from a raw string to those words or n-grams uh and so this is a product process called tokenization and in english we often take tokenization for granted because we have spaces between words and so so-called white space tokenization works pretty well but there are a lot of languages in the world where that assumption is not good and identifying what units correspond to words is actually a fairly tricky task in and of itself but we can illustrate the challenges of tokenization or the benefits of it even with english so let's say we have i'm just going to look at these kind of snippets was great versus was great all right if you just use white space tokenization you end up with two different words in your vocabulary so in that bag of words vocabulary we're going to have great and then we're also going to have great with an exclamation point after it and so this is not great in that uh you know when we train a machine learning model we are going to treat these as different concepts uh but that doesn't seem right i mean really what we're doing is we're combining a piece of punctuation with the word great and we really just shouldn't be doing that the punctuation is really a kind of separate thing but it just gets merged together when we actually write it down so what a tokenizer does um is it's going to take uh a string like this and turn it into a string with possibly some additional spaces another example of what these do is if you have the word wasn't it will turn this into was and then and apostrophe t so it won't actually rewrite the an apostrophe t as uh it's as as not but uh it will break these up um and and tokenizers will also do things like handle hyphenated noun compounds and a few other things but primarily what you're going to see them be useful for is for kind of breaking up these contractions and handling punctuation correctly all right so uh that's one thing that we need to think about as pre-processing um another thing uh and this is something we only do sometimes is stop word removal so stop words are generally function words uh in english and the reason we might want to remove these is the same motivation as tf idf these are words that sort of skew our skew our bag of words vectors and don't actually contribute a lot to a task like sentiment analysis you know generally the notion of sentiment comes from content words rather than these function words so uh we are not always going to want to do this but sometimes and particularly for sentiment this can be a good thing to do another thing you might think of doing sometimes is casing your data so this might be lower casing um again if you're doing something like sentiment analysis that could be a good idea it could be true casing if you're doing something that works from raw speech data and maybe your speech recognizer you know returns just uh data that's all you know that doesn't have any case associated with it all right and then uh the fourth thing is handling unknown words um so if you have a rare word like direct this is not going to be accounted for in the bag of words space you know it's not one of the 10 000 words in the english language so 10 000 most common words and so uh what we're going to do is we're often going to replace this with some sort of unk placeholder now for sentiment analysis you can often just drop this word but for in other cases we're actually going to want to make sure that we have these unk tokens in here and then the final the final piece here is indexing and this is simply the process of taking your string once you've done all these things to it the tokenization the stop word removal the casing the unknown word replacement and map each either word or engram whatever you're using um into uh the space of natural numbers and so we that basically this just just looks like uh use a map we need to keep track of oh okay the is at position five in the feature space and great is at position 247. so if you go through all these steps you should be able to convince yourself that if i give you a raw string then you can apply these transformations these five transformations and turn it into a one of these count-based feature vectors or or presence-based feature vectors either bag of words or tf idf and that's going to be the first step in our pipeline towards building a sentiment analysis system and that's it for this segment you\", metadata={'source': '0jSElGFUxro'}),\n",
       " Document(page_content=\"in this segment we're going to talk about the basic overall machine learning framework that we're going to be applying in this course and we're going to introduce the idea of gradient descent so the high level idea is that we have a set of parameters w to optimize and these might be the uh weights associated with a linear binary classifier or they might be something like parameters of a big neural network we're going to assume when we're doing supervised learning which nearly everything in this course is that we have access to labeled data which is going to be in this form so what this means is that there's d points and each one consists of an input x which would be something like a sentence and a label y and the superscript i means that that's the uh you know input or the label associated with the i training example so suppose we're doing classification with 10 000 features and so uh w is a vector in r to the ten thousand and the way we can think about machine learning is we're searching for an optimal w um we want to find a weight vector that is going to allow us to do well at our classification task that we've got and so we think about this uh as an optimization problem we have some sort of training objective which is going to be linear over examples and then we are going to optimize it and this linearity is going to be important for this idea of stochastic gradient descent so what does this objective look like again to keep things pretty general we'll get concrete you know when we actually see algorithms that fall into this paradigm the objective looks like a sum from i equals one to d so a sum over the training examples of of a loss defined with respect to the ith data point and the weights or parameters that we are optimizing and so roughly the way you could think of this as uh okay for these current weights how well is it doing at fitting this particular example and by summing up those losses over the training data we get an overall sense of how well does this uh this weight vector this current weight vector w fit the training data and what we want to do is we want we want to find a w that fits this training data very well um and so this leads to the idea of stochastic gradient descent and there's a whole space of algorithms here i'm basically just presenting the general framework so that we have the basics of the machine learning and you know we can get on to the nlp stuff uh but obviously there's a rich set of techniques here in optimization that we can take advantage of so uh basically what this looks like is the following so for a variable t up to you know sum num epochs for i up to d what we do is we sample j from the interval 1 to d so basically we are picking a data point and then we are applying an update to our weight vector which looks like the following all right and so what do we have here we have the gradient of the loss with respect to w so what this basically is is this points towards w that give higher loss and so because this gradient tells us okay if we move the current value of w in this direction we're going to get we're going to get a higher amount of loss what we want to do is we want to subtract that we want to move in the other direction and reduce the loss multiplied by some step size alpha and what this step size does is it allows us to to kind of look at how far we want to move and adjust that accordingly and so we'll be talking about the step size you know a little bit down the road when we um actually talk more about implementing this and then for deep neural networks this step size parameter is fairly important but what this gives you is the kind of overall picture of where nearly all of the methods in this course are going to fall into and so when we talk about perceptron and logistic regression they're both going to turn out to be instances of this framework and so we're going to be able to understand them in that vein and then even training super complicated deep neural networks uh is also fundamentally going to look like this and so this is the framework that we're going to be taking advantage of going forward and that's the end of this segment you\", metadata={'source': '_We4tlPkaj0'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the perceptron this is going to be a basic algorithm for training a well in this case we're going to look at binary classification but training a linear binary classifier that we're going to be able to apply to our sentiment analysis example so again recall that a binary classifier we were defining to have the following decision rule and in this case i'm going to use uh whether the weights dotted with the features is strictly greater than zero and this is going to give us a decision y which is either going to be minus 1 if it's you know less than or equal to 0 or plus 1 if it's greater than 0. all right and so what we are doing is we are defining an algorithm to learn the set of weights w from a training set so here's the perceptron algorithm so we are going to run this for a certain number of epochs this is typically how we use it in practice you can also run it until it converges but that's not going to be something we're doing much in this course we're going to iterate through our data and then the first step is we take we basically make a prediction using the weights applied to the current example and so again this is going to return either plus 1 or minus 1. and then we're going to update our weight vector based on this prediction so if y pred equals y i so if we predicted the example correctly we are going to do nothing we are just going to keep the weight vector the same otherwise we are going to uh add alpha times f of x i um if y i is plus one and we are going to subtract alpha times f of x i if if y i equals minus 1. so essentially if we predict the example incorrectly and it should have been a positive prediction but it was negative that's this uh you know that's this first else line here then we are going to add the weight vector into sorry the feature vector times uh a constant alpha into the weight vector and so recall that the features and the weights are in the same space so if we have a ten thousand dimensional feature space we also have a ten thousand dimensional weight vector and so these things can be added together and roughly what this is going to do is it's going to encourage the dot product of the weights and the features to be more positive on uh future iterations so in machine learning course you can prove that this will converge if the data are eventually separable so in the other case the other else line we subtract alpha times f of x if y is negative and that's just because we want to down essentially down weight the value of the prediction in the case where uh the true label was negative all right so let's see how this actually works out on an example so we're going to look at some even more simplified sentiment examples now movie good movie bad and not good okay and so uh we've got a value of y for each one so plus one minus one minus one um and then we have our features f x which uh we are going to represent by these columns mgb and n um which stands for movie good bad and not so again we have a bag of words feature space with four features here uh and the value of each is just going to be the count of that uh word in the sentence it's the the silliest sentence ever um and so uh that is going to give the following feature vectors all right so you should be able to verify that by counting the uh you know just counting the the number of occurrences of each word we get something like this all right so now we're going to start off with a weight vector w which is going to be all zeros all right and now we're gonna follow the algorithm so uh we're gonna come to this first example we're just gonna go through these examples in order so uh for the first example why pred is going to be minus one and the reason is that you know we have an all zero weight vector so when we dot it with the feature vector we are going to end up with a value of zero and recall that we said that our decision rule in this case was is the weights transpose the feature strictly greater than zero in order for it to be positive um so again the ties don't really matter but we have to define it for the purposes of this uh you know of this uh execution so what happens then is we update w according to the first else line here so we are going to add alpha times the weight times the feature vector to w and so let's just say that alpha is one for simplicity and so after this first example after dealing with this first example we are going to have one one zero zero as our weight vector all right we come to the second example so why pred i'll give you a few seconds to think about it but then i'll spoil the answer which is that why pred in this case is going to be plus one and roughly this is because uh we've learned that movie is associated with positive sentiment okay that's not actually right in this case but it was a reasonable thing to learn from the first example we don't know whether it's movie or good that's making it positive so why pred is one the actual y is minus one this is bad uh and so we go to that second else line down there and we're going to subtract off the uh the the features and so that's going to give us the following uh the following weight vector now all right we'll go through the we'll just you know the third line uh again you know i i cooked this up so it would it would always be wrong um and uh so we again make the wrong prediction we're again going to subtract off and so we end up with 0 1 minus 1 minus 1. okay and so this is uh you know this oh wait um no 0 0 minus 1 minus 1. uh so what we've learned at the end of this first epoch this first pass through the data is that uh bad has a negative weight here and not also has negative weight okay now we're going to circ we're going to kind of come back around to the first example so i'm just going to reproduce that down here um this so so this was this was kind of epoch 1 and this is going to be epoch 2. um you know again i'm just gonna rewrite the example uh and it turns out that here we again screw up this first one um we've got y prime as minus one uh and it should have been positive so we add you know we add in the example and then we get 1 1 minus 1 minus 1. um all right and now it turns out we're actually good when we go through the rest of the examples we're going to classify them as positive and it turns out you know we could keep doing this as long as we want and we'll always classify them as positive so the algorithm is converged so hopefully this kind of shows you uh you know sort of how this works and how from this kind of data we can learn some reasonable weights here all right we're going to look at one more example uh and then to to kind of understand one of the shortcomings of this algorithm so we're going to use the following four again quote-unquote sentences so good bad not good not bad again y plus 1 minus 1 minus 1 plus 1 and then here are the uh the f of x's one zero zero zero one zero one zero one and zero one one all right so i'm just gonna draw this using my three-dimensional drawing skills which are non-existent but and we end up with the points arranged like this um so again this is a good kind of test for visualizing what these feature spaces look like um so because we have three features we're in a three-dimensional space and so uh if we think about you know this point for example corresponds to not bad because uh we're kind of out at position one on the bad access position one on the not axis and then position zero on the good axis uh yeah all right and so uh what happens when we execute the algorithm so i encourage you to go off and do this as an exercise uh but you might already kind of see the problem here which is that the algorithm will actually loop infinitely and this is because the data are not separable um the perceptron is guaranteed to find a solution uh or a classification boundary that separates the positive and negative examples if one is possible in this case you can't do it they're they're kind of on this uh they're sort of on this sheet that's exactly you know aligned with your screen and there's no way to there's no way to cut them up so this this kind of illustrates i mean this sort of illustrates a fundamental issue with the perceptron and unigram features which is that they can't model these interactions between words that are somewhat important and so the solution to this ends up being that we want to add bi-grams and so what we could do is we can expand our feature space with not good and not bad and it turns out if we do this you know i'm not going to draw a five-dimensional space because i can't but if we do this the data end up being separable and now we uh now we can classify them and so separability is not something that we're going to think about all that much because uh a lot of times the data you know is separable because you just end up with having a bunch of features but um what this does illustrate is that by changing the underlying feature set uh we're going to go from you know something that doesn't work to something that does work and that is going to be a pattern that we see fairly frequently as we think about feature design problems for the first part of this course uh so essentially this shows how the kind of how the perceptron works you've seen an example of it and we can think a little bit about how it interacts with the underlying set of features that's it for this segment you\", metadata={'source': 'tMGv5ZcuVP4'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to look at the perceptron from a different perspective rather than kind of defining it as this algor as this kind of algorithm that iterates through the data and uses this error driven update procedure we're instead going to reframe it as an instance of stochastic gradient descent where it's minimizing a loss function and so the place we're going to start with that is looking at what that loss actually is so we are going to define a loss function in the vein of what we saw with sgd that is going to have a little bit of a complicated structure but we're going to unpack it um okay so we have this uh yeah we have we have these two cases here that depend on this term w transpose f of x i times y i this is a little bit of a weird thing uh the way i want you to think about this is that uh basically this is zero if w transpose f of x i is greater than or equal to or greater than or equal to zero and y i is one um or w transpose f of x i is less than zero and y i is minus 1. all right so what we're going to do is we're we're going to we're just going to focus on the case where y i equals 1. it ends up being kind of symmetric for positive and negative examples but this is just a little bit easier to wrap our heads around so it turns out we end up with a loss function that looks like this so what i'm going to put on the x-axis here is w transpose f of x i so i'm not defining this in terms of like some normal thing like x we're instead looking at the dot product between w and the feature vector on this example and the loss ends up having this structure um so the y-axis here is loss all right so you should be able to convince yourself that when y i is one um again that's what that's the case that we're looking at here um you know you can kind of plug this into the equation up above and we get these two terms so it's zero when this w transpose f of x thing is greater than or equal to zero so that's the that's the kind of flat part on the on the right and then we have negative of that quantity when it's less than zero so um that gives the slanted part on the left here all right so i've just defined this loss function and i guess what i'm asserting to you is that this loss function plus stochastic gradient descent is going to give you the perceptron algorithm as we saw it before um so we could do a bit of a sanity check here so so first of all when the loss is zero we don't make an update so uh in the perceptron algorithm we check if the example was classified correctly and if it was we don't make an update in this case when we're on the kind of right part of this graph we've classified the example correctly and the gradient uh of the loss is with respect to w is going to be zero so that that kind of tracks um and what is that gradient um so as we said it's zero when wtf of x i greater than zero and it's minus f of x i otherwise uh and so the computing the great like thinking about the gradient of this is is a little bit tricky but um this ends up just being a linear function of you know you could think about it just as essentially scalars w times times f of x you can actually write out the dot product if that's easier for you to think about and when you yeah and you should be able to confirm that when you're on the left part of this graph um you know and you have this minus w transpose f x thing um taking the gradient with respect to each particular weight w uh gives you that corresponding feature value and so we can just kind of batch all those up as um you know the gradient with respect to the vector w is uh minus f of uh you know the the whole the whole feature vector uh f of x um if you're not familiar with uh vector derivatives that's okay we're not going to use them too much throughout this class all right so this is kind of encouraging in that uh it also lines up with the other case that we saw so again we're looking at a positive example here and in the stochastic gradient descent algorithm when we come to an example we multiply the gradient we multiply the gradient by this minus alpha thing and so uh let's say alpha is one what that's going to do is that's going to take our weight vector and when we apply that gradient update we're going to add f of x into that into the weight vector w and that exactly lines up with what the perceptron algorithm was doing so we have a bit of a we have sort of an alignment here between this view of the perceptron and our stochastic gradient descent algorithm when we define the loss function in this particular way um so we can actually look at what this looks like on an example so the uh what i have on the right here is i've cooked up a small example with um basically two uh two coordinates here um so uh we have four data points two are positive and two are negative and they have the uh the coordinate values as shown um and we can plot the loss value with respect to the weight vector so what we're showing here on the left is as you change the values of x and y which are the the two components of the weight vector here um you incur different amounts of loss uh and so that's that's colored with this heat map here and the thing you want to note is that there's this seg there's this kind of wedge shape shaped segment here where the loss is zero and what does this mean well if we have a weight vector that's right here at the edge of the wedge what that does is that gives you this decision boundary um i'm actually going to draw it slightly above the axis just so you can um you know just so you can see that it kind of exactly just you know sort of barely gets everything correct depending on how you break ties and then when you have a vector that's uh kind of pointing out here along okay if i can draw a reasonable diagonal line pointing out here along the diagonal um the the kind of perpendicular to this looks like this and so um again you're going to kind of just barely get the the sort of positives on the right side of the line um and then you know any other vector um in the middle here works as well all of these have lost zero uh and we can look at the you we just kind of rotate this and look at it from another perspective and we can see the sort of landscape here of the lost value with respect to uh the you know with respect to these coordinates of the weight vector and so the way to think about this is that gradient descent wants to move down along this lost surface and so if we initialize the model to some weight point here the and then and then we pick up an example well the gradient on on that example is gonna you know somehow push us you know down down this lost surface or if we accumulated the gradient over all of these examples um you know we would actually we would kind of directly go um down along the direction of steepest descent here uh and so as we as we kind of make our our perceptron updates following this loss we will eventually get to this kind of uh the sort of flat plateau region at the bottom and that's represents convergence of the algorithm uh so you know there's this nice there's this nice kind of visual intuition i think as well for thinking about how this loss function interacts with the the perceptron training i'll just quickly show one more example of this here we've removed uh the data point in the top right and so the loss values are now different because uh you know we we only we have three data points instead of four and so the um the kind of sum of losses over this training set is going to be different however the thing i want to point out is that this this kind of wedge here is still the same and the reason is because these two points were the ones that really kind of governed where the separating decision boundary had to be and so the fact that we remove the other point actually doesn't impact where the optimal solutions are at all uh and so you know i i think it's it's just a nice visualization of again connecting the uh loss formulation of perceptron with the kind of error driven procedure we saw before um and so we're going to move on from from perceptron and and discuss other other learning algorithms but this you know maybe helps connect possibly a version of the perceptron you might have seen before with this sgd formulation that's it for this segment you\", metadata={'source': 'hhTkyP7EzGw'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to be talking about logistic regression logistic regression is going to be an alternative classification model to the perceptron and it's going to be the foundation of a lot of the techniques that we build going forward in this course including things like feed forward neural networks so logistic regression is what we call a discriminative probabilistic model and what this means is that it is it places a distribution p of y given x over labels conditioned on input data points so the alternative to this would be a generative model which looks like placing a joint distribution over p of x y rather than a conditional distribution for classification this would include approaches like naive bayes we're going to come back to generative modeling later in this course particularly when we talk about hidden markov models so for right now we'll stick with this discriminative framework okay so let me write down the definition of binary logistic regression so we are defining here the probability of the positive class the probability that y equals plus one conditioned on the input data point and it takes the following form okay so uh what you should notice is that we still have this dot product of the weight vector and the feature vector which was characteristic of the perceptron algorithm as well but now we've embedded it in this function e to the x over 1 plus e to the x and so this is called the logistic function which is where the name logistic regression comes from and if we plot this for different values of x what we get is the following so this is y equals 1 this is y equals 0.5 and we get a kind of curve that looks like this so it intersects the at the origin it takes the value of 0.5 you can confirm this plug in x equals 0 you get a half and then as it goes out in either direction it approaches either plus 1 or 0. and so the main reason that this is useful is because it allows us to map from real you know any real number into something that's going to be a valid probability okay so uh again what this does is it takes this w transpose f of x quantity and turns it into a probability associated with the positive class okay and so uh we could think about the negative class probability and without me writing it down you should be able to figure out what this is and the reason you should be able to figure that out is we have binary logistic regression so p of y equals minus 1 and p of y equals plus 1 have to sum to 1 and so you can double check that the value that causes that to happen is this one all right and then the other thing we can ask ourselves is what decision boundary does this imply so remember we're doing classification so when i give you a point and you have a logistic regression classifier you should be able to tell me a label for that point okay so you know one kind of plausible way of doing this is you return plus 1 if p of y equals plus 1 given x is greater than 0 right so it turns out that this is equivalent to w transpose f of x is greater than zero sorry um if that probability is greater than .5 we want to return plus one and that happens when w transpose f of x is greater than zero and you know again we you can just look at what happens at the origin and kind of confirm that for yourself okay so basically the decision rule ends up being the same as for the perceptron we're just comparing this dot product with zero but now we have this kind of probabilistic interpretation um and so again you know if you're just doing prediction that doesn't matter but where it impacts things is training so how are we going to train logistic regression so we're going to assume our labeled data set which consists of points x and labels y and we're going to have capital d of these and what we want to do is we want to maximize the following quantity okay so uh what we're maximizing here is what we call the likelihood or the discriminative likelihood of the data meaning that we have a bunch of labels y for data points x and we want to maximize the probability of observing this data and so in this case because we have a discriminative model we're conditioning on x and we're maximizing the product of the probability of each y given x essentially let's set our model parameters w such that the data looks as likely as possible and so we're going to write this as a maximization over w of now we're going to transform this likelihood expression okay so now this is the log likelihood because we've taken the logarithm and so what you should be able to confirm for yourself is that remember that log is a monotonic function and so when we want to maximize the likelihood of something or in general maximize any function we can also maximize the log of that function and as long as that function is going to be positive and the log is defined um it's you know the the maximum remains the same uh and so this is gonna it's gonna turn out that this simplifies things a lot for us uh kind of computationally to deal with log likelihoods uh and so that's what we're gonna do here so we take this we take this log and um you know we can push it through this the product and it becomes a sum and then we get the max of a sum of log probabilities um and then the one one more transformation we're going to do is uh we're just going to put in a negative sign and flip it around and instead of maximizing the log likelihood we're going to minimize the negative log likelihood and the reason we're doing this is so that now we can think of this term as a loss uh in the sense of the basic machine learning principles we saw earlier so we're now we now have uh now we have our training objective which is going which is negative log likelihood gets abbreviated to nll particularly in a lot of the deep learning frameworks um and now in order to deal with this uh we and and optimize it we can call back to that framework that we introduced earlier of stochastic gradient descent and in order to use stochastic gradient descent what we need to be able to do is we need to be able to compute the gradient of each one of these terms of this outer sum so the sgd loop involves picking you know one we're going to say right now one data point to deal with at a time out of our d and compute the gradient of the loss on that data point so that's a term that looks like this all right so remember that for the perceptron we saw you know just kind of defining this algorithm that uh you know checked if you made a mistake and tried to kind of correct the value of the weight vector to make that mistake happen less in the future now instead we've kind of formulated this first principles idea of maximizing the probability of the data and now we're going to compute a gradient on this so we can try to directly optimize this objective function with gradient descent okay so what we are going to do here is we are going to assume y i equals plus one um so we're going to derive this for the case of a positively labeled example it's a good exercise for you if you want to think about this more to try to work it out for a negatively labeled example as well okay so what do we end up with so we're going to plug in the value of the loss here which remember was the negative log of the logistic function so you end up with this um so again the the log kind of breaks up the uh the the division and the numerator was this e to the w transpose f x thing um the log also kills off the exponential and then you negate it uh and then in the denominator we end up with this slightly ugly log of one plus e to the something which cannot be broken up because uh you know we have this this log of something and you can't uh commute log past sum okay so now we need to take the gradient of this expression with respect to w so uh if you're not familiar with vector derivatives again you can just think about a dot product is just going to be um a sum up for each term in w and each term and f of x uh of the product of them and so uh when you take the gradient with respect to w it's it's this linear sum and the w terms just kind of go away and leave you with the f of x terms all right and then the second term gets a little bit more complicated we need to appeal to the chain rule so the the way that log works is you get one over the uh one over the kind of inside stuff of the log um times the derivative of the inside um and so the derivative of exponential is uh exponential um again times the derivative of the inside stuff using the chain rule and uh we end up with this okay and so now let's rewrite this a little bit so we're going to factor out an f of x and then we end up kind of cleaning up the second term here all right and so the second term is kind of cool now because we had this big mess of exponentials and you know one over one plus you know e to the whatever and it turns out that we can simplify this now uh in the following way actually i'll write it in this order so the second term ends up just being the logistic function again it kind of comes back out from the process of differentiation and now we can just replace that with the probability of y equals plus 1 given x under the model so this is a quantity that we kind of already have have defined because this is the definition of our logistic regression model and so we end up with something that we can kind of simplify in this way okay so then finally recall that the the sgd update looks like this you have to subtract off alpha times the gradient so i'm going to write this as adding alpha times negative the gradient and and that gives us this okay so we did a whole lot of work and recall that if we just look at this piece of it we have something which looks very similar to the perceptron in the case of misclassifying a positive example remember when we misclassify a positive example is negative we have to boost up its score and so we add in f of x to the weight vector so here we have the same term but uh this alpha times f of x is now modulated by this one minus p of y equals plus one thing um so then like just to kind of intuitively think about what happens let's think about when this term is very close to one the model is essentially making the right prediction um we are not going to make much of an update because uh you know this second term this y one minus p of y plus one term is going to be very close to zero and sort of cancel out what if instead this is very close to zero then we recover again like i was saying the perceptron update right because this one minus p of y equals plus one term uh essentially takes the value of one and uh essentially takes the value of one and the update looks basically like the perceptron update from before all right so what we're going to do in order to kind of understand this is we're going to let z equal w transpose f of x and now we can rewrite the uh we can rewrite the law the um the the uh nll negative log likelihood for this model as the following log of 1 plus e to the z minus z and so that's just following basically what's on the top of the slide here before we took the gradient and what we could do now is we can plot this quantity z or w transpose f x against uh against the value of the loss and so previously we saw that the perceptron had a kind of structure that looked like this um it basically uh you know it has this sort of hinge-like behavior where uh it's zero it kind of when the when the value of z is is positive when the prediction is correct and otherwise as this y equals kind of minus x sort of structure um and it turns out if you plot this thing you end up with something that looks like this and this is the loss for logistic regression and what you should be able to convince yourself of is that uh you know basically in the kind of limits as you go towards uh you know the basically this z term gets uh very large negative or very large positive you end up with something very similar to the perceptron loss and so the nice thing here is that these two algorithms which were motivated from very different uh intuition ended up doing almost exactly the same thing so this kind of informs a little bit of how we're going to approach classification for the rest of this course we're not going to be too concerned about whether we use perceptron or logistic regression or svm we're going to kind of stick with what ends up essentially being most convenient for us given what we're doing so that that kind of wraps up this segment about uh logistic regression and and closes us off for basic binary classification you\", metadata={'source': '0naHFT07ja8'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about sentiment analysis we've already talked about this as an instance of a binary classification problem and we've seen uh some kind of basics about how we might deal with it with bag of words features um but we can think about it a little bit more uh kind of rigorously here so we've looked at examples like this the movie was great would watch again and we talked about how having these bag of having bag of words features on unigrams or single words lets you capture things like great as positive sentiment um and things like bigrams uh a lot or pairs of words in our feature space allow us to capture pairs like watch again and so if we think about other kind of examples this works out well sometimes but sometimes not so great so for example the movie was gross and overwrought but i liked it uh in this case we have two negative sentiment words gross and overwrought but the person then says but and that sort of negates everything that came before right uh or the movie was not really very enjoyable bag of words model uh especially with unigrams might say oh enjoyable looks great but in fact again this is in a negated context here so bag of words models don't seem sufficient from the perspective of handling this kind of discourse structure and negation phenomena that we're seeing in these examples um but you know getting around this and actually operationalizing this to do a lot better is a little bit tricky um so in the world of bi-gram features you could do things like when you see a not basically broadcast that not to every word that follows it so extract not really not very not enjoyable and now you kind of get all of the uh you know you get the idea that all of these pairs are being negated though without some kind of syntactic analysis like a parser you're going to have a hard time doing this totally reliably okay so turning the clock back to 2002 um some of the first work on using uh classifiers for sentiment analysis like this was was done by a team at cornell and the rough conclusions they found were that these simple feature sets that we've looked at so far can actually do pretty well and so their best performing models here were uh these unigrams and unigram plus bi-gram features so frequency or presence what that column means is are they taking this count based version of bag of words where if a word occurs twice it gets a value of two or just presence or absence where if a word occurs once or twice or five times it only still gets a value of one in the feature vector and so n nb is naive bayes a classification framework we haven't really talked about me maximum entropy is just another term for logistic regression um and then svms again another classifier we haven't talked about much but which is more or less similar to logistic regression from our standpoint so it we see that these uh you know these simple feature sets do fairly well and it's actually kind of hard to beat this um so there was some work uh ten years down the road uh cetawang and chris manning from stanford uh revisited the bag of words classifiers versus other methods and uh the what they found is that uh a modified version of naive bayes uh actually could do basically as well as this raa pr rae pre-train method with an orange rectangle here this was a kind of neural network model this recursive auto-encoder model fairly sophisticated model and they were sort of harkening back to those results on the previous slide from bo pong and lillian lee and found that uh the you know essentially like they were able to take modified versions of those earlier techniques and kind of beat the neural nets from 2012. now of course this was before neural nets really exploded in nlp and so uh you know just a couple of years later yoon kim with his work on using cnns for this was able to beat these results fairly substantially so if we look at where we are today this is from a website called nlp progress which tracks basically state-of-the-art progress on a number of different data sets and this looks at a slightly different data set than those previous two uh works this looks at the stanford sentiment treebank data set for classification the best systems now use these big pre-trained models and so uh you know we have results up here from burt from excel net stuff that we're going to talk about later in the course and these have really pushed the performance pretty substantially so we've gone from at the bottom here by lstms getting a performance of around 90 to uh the kind of best state-of-the-art pre-trained models getting a performance of almost 97. uh so this kind of underscores you know i guess two things one is how far you can get with basic classifiers that we've already learned about so far and two how much this idea of pre-training which again we'll come back to later in the course once we've set up all the neural machinery to deal with it how much that's going to buy us that's it for this segment you\", metadata={'source': 'cKbnEmjxnOY'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to optimize things and this is going to introduce this concept of optimization that's going to come up going forward throughout the course now this is not a course on optimization so this is going to be an extremely abbreviated treatment there's a lot more there to unpack but we're just going to look at what we need to understand what we want to do in nlp so the basic idea of optimization is that we have some kind of loss function and we are finding a setting of weights w to minimize that loss so this is we could think of as a search over the space of parameters w and again our loss which it we're going to start writing as this script l here is defined with respect to a training set like this and with respect to a training set and also our weight vector and what we said was that all of the loss functions we're going to look at basically we are going to think of as uh this linear sum over the training examples and so what we're thinking about this uh right now as is as a function of w right so we're searching for a w that's going to minimize this thing and so we kind of treat the training data uh we're going to treat the training data as fixed and w is sort of the variable that we're going to be changing to try to find a good value of this loss function so so in stochastic gradient descent the idea was we repeatedly pick an example i and then apply this update okay and so i'm abbreviating here this is loss on ith example um i will put in i will put in w here just to make that extra clear so we pick an example i we compute the gradient of uh the loss on that example with respect to the weights and then we subtract off alpha times that and again because we're minimizing things subtracting the gradient is the way to kind of go in the right direction so the part that we haven't kind of talked about how to deal with and which is what makes this whole process pretty tricky is the step size so let's see a little example of why step size matters so suppose this equals w squared and there's just one feature here so again basically w is just a single a vector with a single coordinate w in it and let's suppose further that w equals minus 1 is is sort of our initial starting point here um okay so the gradient of l with respect to w uh is just 2w all right so now the minimum value of w here uh is is zero this is a this is a quadratic function right i guess i'll just draw that little cartoon here um and what happens with sgd if alpha equals 1. so let's apply this so we have our weight current weight w which we said is -1 and then we subtract off alpha which is 1 times the gradient which is 2w which is 2 times -1 and so that gives us an updated value of w which is equal to one now so what happened was we started out here and the gradient uh kind of told us to go towards the origin and then what we did was we ended up over here and if if you keep alpha equal to one what happens is that this oscillates this will never converge to the true minimum which is at w equals zero instead it just kind of bounces back and forth here instead if alpha equals a half um you should be able to convince yourself that w uh you know kind of immediately jumps to the right answer uh of zero and so even with the same gradient as before the fact that we had a step size now the the kind of correct step size allowed us to get to the optimum so that kind of indicates how important the step size parameter might be in that you know it makes the difference between not converging and converging with the same algorithm okay so how to choose step size i mean it's it's not much of an exaggeration to say that there's um you know hundreds of papers published at nurips or icml which basically kind of amount to this question from an optimization standpoint uh so the answers from the perspective of sgd are going to be the following um try them out so you know we a lot of times maybe we want to try uh just a range of different orders of magnitude um if you don't know too much about uh the function that you're optimizing you're not even really sure what scale you should be searching on here um and a lot of neural models you need pretty small step sizes for example uh you know one e minus four one e minus five things like that depending on how big the model is um another thing that's pretty common is to start with a larger step size and go to a smaller step size um so uh so something like one over t for epoch t or one over the square root of t something like that um similar so so that's kind of a fixed schedule um or you can decrease step size when performance stagnates on held out data so you basically kind of you know look look at when you're no longer making progress on your validation set um or a development set uh both terms get used and then you you turn down the step size okay so these are these are sort of course techniques and it's a little bit hard to find exactly the right uh the right value for this thing and it's actually not even clear that this is the right way to go about it because what it does is it treats all of the positions of your weight vector equally but not all the positions of your weight vector are equal when you're dealing with uh you know when you're dealing with neural nets they might reflect different kind of layers of the architecture that might have very different sizes or different behavior and when you're dealing with feature based models they can reflect different like you know one might be a very common feature like uh you know a bag of words feature on said or a common word like that versus you know a feature that occurs one time in your entire training data because it's someone's name so what we want to do is we want to be a little bit smarter about this um so we're not going to discuss it in too much detail um but the the kind of smarter thing to do here is something called newton's method okay so uh basically this is something called the inverse hessian um and the idea here is that we use the curvature of the space to to or of the objective function to figure out what the right step size is um and so one way to think about this is it's like a second order taylor approximation where uh if you are optimizing a quadratic what this will allow you to do is it will allow you to take your gradient which is kind of locally going uphill on the quadratic and it will tell you okay you know based on the gradient here and the the kind of second derivative of the curvature like we can immediately jump to uh the optimum here um the problem is that this is this is very expensive to compute it's quadratic in the number of features and so in big neural models or even in linear models which are going to have you know once you start to get into hundreds of thousands of features um it starts to become you know totally infeasible to do this um so there's a whole bunch of methods uh at a grad at a delta atom these are so-called adaptive methods and a lot of them are motivated as approximations to the inverse hessian but there but they're going to be linear in the number of features so these are going to be techniques that are kind of useful for deep learning and we're not going to talk about them a ton but adam is definitely one that we are going to revisit when we get to deep learning and so we're going to talk about you know we're going to talk about it in its hyper parameters a little bit more there and the last thing i'll say about optimization before we close out is on the subject of regularization which we i've got to say don't really use so in classical statistics what we would say is that actually fully optimizing the value of the loss is bad um there's this notion of a bias variance trade-off where what we want to do is not actually fully optimize it and in doing so reduce the kind of variance of our estimator and do better on new data the idea of regularization is useful but we're typically not going to explicitly add in regularization to our objective instead what we're going to do is we're going to benefit from either early stopping just not running as many iterations the fact that our optimization is not going to be perfect at optimizing this function and other kind of ad hoc tricks for deep learning things like dropout that are going to allow us to get the same benefits of regularization in terms of not overfitting the data without explicitly incorporating it so uh the the kind of main thing you'll need to know for the first portion of this course is simply the step size and the fact that these kind of different optimization techniques exist and then we'll revisit some of these concepts when we come to optimization and deep learning later that's it for this segment you\", metadata={'source': '65ui-GdtY0Q'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about multi-class classification so so far we've talked about binary classification where roughly we kind of looked at pictures that look like this where we have a bunch of points in space that are labeled as either positive or negative and now we're going to think about having you know basically points that might come from multiple classes and so this is going to be a generalization of binary classification it's going to be useful for a lot of problems in nlp where there are more than just two classes so we're going to define uh the set of classes as this thing script y here we're also going to talk about this sometimes as the output space particularly when we get to structured models so there are ways of doing this with binary classification so for example you can have a technique called one versus all where basically you draw a kind of boundary that separates each class from uh you know all of the other ones and so this comes about from having n binary classifiers um but there are times when this doesn't work you should be able to convince yourself that if we've got something that looks like this um it's going to be very difficult in well you would need more than linear classification to uh handle the fact that the kind of threes can't be separated from the ones and two so easily um and so we want to kind of avoid hacks like this instead just do things the right way so instead we're going to think about reformulating the classification techniques we've seen so far uh using uh one of two methods for generalizing them to this multi-class case so uh these two techniques are going to be as follows we're either going to have one weight vector class and we're going to call this the different weights or dw approach or we have different features we're going to call that df per class so what we're going to see is that these are two more or less equivalent ways of formulating things in the multi-class case but different weights is going to look like what we do when we do a lot of neural net stuff and different features is going to be very useful for thinking about structured classification so i do want to set up both of these even though uh for the algorithms that we're about to see and the kind of basic setting they're completely equivalent so uh under different weights different weights you know in some ways you could kind of think of it as a little bit like one versus all though it's not it's not trained as a bunch of binary classifiers but the idea is is kind of similar we say that the classification decision is the arg max over y in this set y so previously we thought about a decision boundary where is it positive where is it negative now we think about okay which value in y lets the following expression takes it take its maximum value and the expression is this so what we have here now is this w is now indexed by y and so what we have is we have a fixed set of features and then we say okay i'm going to hit this with a whole bunch of different weight vectors and whichever weight vector has the highest dot product with it that's my prediction all right the different features version of this looks like the following single weight vector now not indexed by y but now the features that we extract do depend on y um and so the thing i want to emphasize is that this y in here is what we think of as a hypothesized y so it's not the case that we're like cheating and looking at the gold classification decision instead what we're doing here is we're scrolling through all of the possible labels that we can have and saying okay which one of these actually uh or you know instantiate features with each one of these possible labels and then give me the score when i take the dot product here so the reason we're not going to use this as much in neural nets is that basically the way we're going to think about neural networks is is like kind of taking the input x and doing a whole bunch of computation on it and then at an output layer we we kind of determine what class something falls into the different features technique is not so good for that because by injecting the y kind of early into the process you would end up having to re-run your neural net n times for doing n-way classification which is not something you want to do okay so we're going to look at an example of topic classification using these two different frameworks so we're going to have a sentence x too many drug trials too few patients and y in this case is going to be uh health sports or science okay um f of x is going to be bag of words uh let's say bag of unigrams um and we are only going to look at the following unigrams drug patients and baseball okay so uh you should be able to convince yourself that uh f of x for this example is one one zero we have drug and patients there but not baseball all right so that's sort of sufficient to think about the different weights version of this so in the different weights versions version of this we think about um for example the weight vector we have a w health weight vector which maybe i mean i'm just going to make up some values here maybe drug and patients both have high weights under this weight vector um but baseball has a low weight because typically health articles won't talk about baseball and so you know this gives us a score of 7.6 um and just to you know throw out one for uh weights for sports um you know maybe drug shows up there sometimes uh patients not so much and then baseball obviously is highly you know is going to have a high weight here and this gives minus 1.9 okay so uh you know we're not going to show uh for science but basically you compute these three dot products and oh health is going to be the highest so that's going to be the classification decision that we pick so the only difference from before is that we have one weight vector per class and uh you know rather than comparing it against zero we're now saying which of these weight vectors gives us the highest score okay now thinking about the different features perspective different features is a little bit uh is a little bit weirder um so recall that we're going to define this feature vector f of x comma y and what we're going to do is we're going to define this as f x replicated for each class you'll see in a second why this i said that this is equivalent to what we had before let's see um so what we have here is the f of x comma y equals health is going to have this 1 1 0 vector in this first position here and then it's going to have a whole bunch of zeros after that when we have f of x with y equals sports instead what we're going to have is zeros and then ones in the second the the the feature vector now kind of occupies the second position here and then a bunch more zeros so the way to think about this is that previously our features were just an indicator of does the sentence x contain this word so now it's an indicator of sent contains you know word i and y equals um you know in this case sports that's the that's the kind of meaning of the features here and so the reasoning behind this is that uh you know we're defining this conjunctive feature that looks at both x and y and now what we can do is we can define a single weight vector w where it is a linearization of the weights from before so now all the weights can live in a single weight vector and as you kind of toggle between which why you're hypothesizing you end up with different uh you know you end up with different values of this dot product because the features now kind of change in this block structure so the reason i the the reason i kind of belaboring this point about how these things behave as indicators is that when we get into structured classification what we're going to see is that we're going to think about um you know whys that are more complicated they're like set part of speech tags at each point in a sentence or something like that and so there it's not going to be practical to think about having one set of weights per um you know per different class that we're classifying because our classes are going to be these large structured objects like sequences of parts of speech and so we're going to have to think about these more complicated feature structures that can join properties of the input like what words does the sentence contain with properties of the output like what label are we using here for now though you could think about either of these things as equivalent either you know you have this kind of copy paste feature vector structure and one weight vector or you have just a single feature vector and then uh you know different weight vectors for each one that's the end of this segment you\", metadata={'source': 'My6GaGhqxdI'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the multi-class generalizations of perceptron and logistic regression these algorithms are going to look roughly similar as before but now we're going to have to deal with the fact that there are these multiple possible classes that we can [Music] predict and there's sort of many different types of mistakes that we can make so we're going to dive into multi-class perceptron and then we're going to see an example of it so the outer loop we're going to iterate through epochs and data and now the prediction is going to follow the multi-class scheme and so this is going to use the different weights idea and so again we index our weight vectors by y so we have the weight vector associated with the y class and then we take the dot product of that weight vector with the features f x all right and so then again we're only going to make an update if y pred is not equal to y i um and the update we actually make now has two parts what we have to do is we have to tell the model okay why pred was wrong um we want to kind of kick down the value of the dot product of w y pred with uh f of x because that wasn't uh you know that wasn't the right class to be predicting here and then similarly we want to boost uh the value of y i so we add in f of x here and we'll put in the step size alpha here as well okay so the general kind of idea here is the same as before which is that we want to take a step towards making the models dot product with the correct class or the score of the correct class let's say be higher and the incorrect class be lower previously that just amounted to one update which brought us towards positive away from negative or towards negative away from positive um now we have to juggle these two different classes there's kind of the bad one and the good one okay so let's see how this works out in an example so we're going to take our example from before where the sentence here was too many drug trials too few patients and then we're going to throw in another example here with label y equals 2 for sports and just to kind of invent a story for this one we'll say baseball players taking drugs that's the headline now okay so i assume that assume that the default prediction is y equals three and by that i mean assume that when everything is tied the model is going to return y equals three so our output space i guess i'll define up here is uh you know one two three um and uh so what's going to happen when we execute this algorithm well we start off with a set of weight vectors and i'm just going to write them all kind of in a line here so again kind of different features and different weights sort of look similar so the way i'm going to do this is i'm going to write w y 1 w y 2 and w y 3 all here and it's going to start off with all zeros all right so if the so the quote-unquote default prediction is y3 then what happens is we have uh when we come to the first example here um so we're gonna you know update on this one one zero comma one example what happens well so we have y pride equals three and y equals one these are not equal so we need to make an update and that update uh you know again we're going to take alpha to be one that update consists of adding the features for uh the correct value of y so that gives us this and then subtracting the features for the other values of for for the incorrect value of y so that gives us this and then we have zeros in the middle here i'm gonna i'm gonna just gonna put a squiggle in the middle here to indicate that these aren't really in the same vector but i'm just kind of writing them that way graphically all right and then when you go to the second example what happens is that the model prediction we can think about what this should be if you do the computation you could see that it's going to predict why pred equals 1. and the reason for that is because 1 0 1 the dot product with w y 1 is going to be 1 and that's higher than the other two dot products and so in this case y y was equal to 2 i'll write y i here just to make it clear that that's the the gold one and then uh when we make our update uh we have to subtract off f of x from the from the y print weight vector um so that gives us this it sort of kicked it down now by one zero one and we have to boost the correct one so that gives us one zero one in the middle here and then uh minus one minus one zero the last position remains unchanged okay so this kind of gives you a sense i think of how the algorithm goes you're always kind of dealing with two classes the one that you predicted and the the true one unless of course you make the correct prediction um in which case the zip statement's actually not even really necessary because if these things are equal then the two updates just cancel out but i think you know it's a little bit easier to think about it in terms of uh you know making an update whenever you make a mistake okay and i'm not going to bother to write out the uh form of this in uh different features fully um i guess i'll just replace i'll just say uh when you have different features the last line here for example becomes the following actually yeah so in different features there's only one weight vector now so what happens is you what the term that you end up adding is alpha times f of x associated with y i um so that's what this last line would be um and the other lines kind of changed similarly where instead of having uh why pred be associated with the weights um now it kind of lives inside this this feature function but otherwise you know everything kind of proceeds as before okay so that's multi-class perceptron so again the algorithm looks similar as before but with a little bit of modification and multi-class logistic regression uh looks the same way um so we're gonna write just the definition and the gradient um and and kind of not go through the whole update and just for a variety we'll do it in the different features formulation okay so what's going on in this definition so we're computing the probability that y equals y hat and so i'm using y hat here to kind of denote the um you know the class that we're actively computing the probability for um and so the numerator plugs that into f of x comma y again if you were doing different weights you would have w of y hat and then the denominator sums over all of the y's in the output space so this looks different from before what we had before was remember we had 1 plus e to the w transpose f of x now instead we are summing over all of the possible output values and again you know you should be able to motivate this for yourself from the standpoint of this uh probability distribution has to sum to one um so you know with the the numerator kind of being this e to the w transpose f of x comma y hat thing um the only way to get that to normalize is by having the denominator effectively reflect the sum over all those possible values and another way to another way to think about this is in binary logistic regression we kind of do have another class so when you have y equals plus 1 we're basically saying that that's associated with this score and when you have y equals minus one that is you know that we're basically calling uh you know assigning the value of one um which we could think of as e to the w transpose uh times the zero vector right where uh we you know again e to the zero is one and so there's sort of an implicit feature vector for the negative class in binary logistic regression um which uh you know which just kind of cancels out that exponential term and gives you uh and gives you a value of one so that's that's kind of i think a nice way to see this as a generalization of binary logistic regression even though you've we've kind of moved away from the logistic function and now we're doing this um exponential over sum of exponential's thing all right and um again we're not going to kind of belabor what the loss is um but uh we can use we could define uh the log likelihood of the data in the same way as before um so again you know we we're just talking about the probability of y taking on the correct value for each data point um we can log it um you know whatever and uh it turns out the gradient of that has the following form all right and again we're sticking with different features here all right so intuitively this actually has some kind of nice properties so again we can think about when p of you know y i given x is very close to 1 what happens well we have this minus f of x comma y i term over here and then plus something that we're going to say is approximately equal to f of x y i and then this you know gives us zero we don't make much of a gradient update if instead we have p of let's say y i'm just going to call it y bad then we have minus f of x y i plus f of x y uh y bad here all right and then remember that in sgd we subtract off alpha times the gradient and if you uh kind of unpack what's going on here it turns out we're going to subtract alpha times the features of x comma y bad which looks like in multi-class perceptron again kind of kicking down the value of y ba the you know the weights associated with y bad um and then we're going to add uh alpha times f of x y i that's boosting the correct thing um so again we see this kind of close connection here where you know if your probabilities are close to zero or close to one you end up with an update that looks very similar when they're in the middle you get a kind of soft form of the update here um where you're summing over i should have defined this explicitly but we were summing over the uh all the y's in the output space and so um if you're putting a uniform distribution over on them you're kind of uh you know kicking all those other classes down a little bit and then up up voting uh the correct one here so these are the kind of generalizations of perceptron and logistic regression to these multi-class cases here um these are going to be algorithms that we're going to kind of come back to throughout the course going forward and we're going to revisit this when we come to neural networks and you know kind of embrace the different weights version there and kind of embrace different features when we think about structured prediction that's the end of the segment you\", metadata={'source': 'EA627DC7k6M'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about some examples of multi-class classification problems that show up in nlp the first one we're going to start off with is text classification there are a number of different data sets ranging from things like collections of news articles to postings on news groups online and more where the basic task is to figure out which topic or rough category a piece of text falls into and this is a task that is often studied as a benchmark for multi-class classification but it's only kind of sometimes useful a lot of these data sets are were really kind of constructed more for the purpose of driving academic research and serving as benchmarks rather than corresponding to a real world problem uh another data set or setting that people look at is what's called textual entailment this we can think of as a three-class classification task and it deals with sentence pairs so for example if we have the statement a soccer game with multiple males playing and then a second statement which we call a hypothesis some men are playing a sport the first statement about the soccer game entails the hypothesis meaning the hypothesis is implied to be true on the basis of that first statement you can also have relationships like contradiction a black car starts up in front of a crowd of people which contradicts a man is driving down a lonely road these statements are not about the same thing and they actually can't be about the same thing and then you can have other statements like the one on the bottom here which are unrelated or you can't tell the relationship either entailment or contradiction based on these statements alone so this is a task that we're going to return to later right now with the tools that we've built up so far like bag of words features it's very not obvious how to build systems that can do well at this and in the history of textual entailment systems were not doing very well until uh neural networks came along and you know arguably there's still some challenges uh here trying to handle this task in its full generality another example of a task is entity disambiguation or entity linking suppose we see the following chunk of text although he originally won the event the united states anti-doping agency announced in august 2012 that they had disqualified armstrong from his seven consecutive tour de france wins now we know that when we see armstrong here we can infer that this is talking about lance armstrong the cyclist but if we just kind of naively approach this from the perspective of all right i'm going to try to find out which real world entity represented by a wikipedia article this refers to there's lots of other possibilities for example it could refer to armstrong county in pennsylvania right now in this case of course clearly we all can tell that it refers to lance armstrong because of the context it's clearly a person it's talking about cycling etc but in order to actually do this task which consists of linking this so-called mention of armstrong to an entity an article on wikipedia we need to think about what the features are that might help us associate this mention in that context with the article and the challenge here is that it's you could think of it as multi-class classification but there's 4.5 million classes you know it could be any article on wikipedia now in practice we can prune that down to you know anything that has armstrong in the title at least or use heuristics like that but that still leaves a number of options and compared to what we've seen so far we're going to need a very different structure for the features we're not going to be able to get away with this idea of conjoining indicators about the input with an indicator of the class label we need something a little bit more complex the last example which i'll dig into here is authorship attribution so this is an old problem in that people have been using statistical methods for authorship attribution going back to the early 20th century and the idea here is that if you have texts and you don't know who wrote them some of the information which we typically throw out from the perspective of text classification can actually be pretty useful here things like stop word frequencies there's lots of information about uh kind of someone's writing style in their use and frequency of different function words and things like that and the reason this has been a historic task is just because there are some very famous examples of works that we want to attribute to different authors for example there's questions about whether shakespeare wrote all of his plays or in some of the federalist papers their disputes about whether alexander hamilton or james madison wrote them so the form of this we're going to talk a little bit more about is a twitter authorship attribution given a whole bunch of tweets can we figure out who wrote them and so uh roy schwarzedal looked at took a data set of 500 million tweets and they took a thousand users with at least a thousand tweets each so this is again uh you know a sort of artificial task here and that we're assuming we have a fair amount a number of examples of what each of these people is writing and moreover were you know looking at twitter data where if you just had the sort of raw twitter dump you would know uh the at least the kind of username of the user who wrote things but it's an interesting question that if we have you know a thousand different people who could have written something and we have a held out tweet and we don't know who wrote it is actually enough kind of information in that tweet to tell us so uh the model they used is relatively straightforward a support vector machine with character four grams and word two through five grams meaning their features look at pairs of words triples of words quadruplets and quintuplets uh and so i'm going to just show here a snippet of the results from the 50 author classification task so um in rather than one in a thousand it's it's kind of one in 50. but the accuracies here are actually surprisingly high um their fall model which is in the blue line here gets to around 70 accuracy when it's trained on a thousand tweets from each author uh and even when you start out with only 50 or 100 um you're still doing reasonably well and so you know you this might be kind of surprising because it's like okay with 50 people you know you might be hard-pressed to look at a tweet and figure out okay what's actually the kind of distinctive content that might tell me who wrote this and the interesting thing they found was that this notion of what they called a k signature and this is an engram here i'm showing some examples of character engrams that appear in k percent of the author's tweets but not in anyone else's and so for example in the top here we see this particular example of a kind of emoji here the with these this carrot these carrots and this underscore which uh is not common not a lot of people use this but uh this author decided to use it a fair amount and we can do the same analysis at the word level as well and find little kind of distinctive signatures used by different people so this this kind of shows why bag of words methods and basic linear classifiers can do this task which otherwise seems fairly challenging so this is just to give you some examples of what multi-class classification can be used for and we'll be coming back to these ideas throughout the rest of the course uh in particular when we get to part of speech tagging uh that sort of looks at like a word by word multi-class classification task so but this could show you some examples of how to apply what we've talked about already that's the end of the segment you\", metadata={'source': 'va2i7LXt9zI'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about fairness and classification so now that we've built out these i this idea of training classifiers either in the binary or the multi-class case it's worth thinking about how these get used in reality and what ramifications that might have so the reason we might care about fairness is that classifiers can actually be used to make real-world decisions that impact things in life for example who gets an interview we might have a big stack of resumes and and try to run a classifier over those to decide who might be the most successful candidate who should we lend money to based on kind of financial history is this online activity suspicious and people have even done things like train classifiers to say is someone a criminal based on their face now this last case is kind of so cartoonishly wrong in a lot of ways uh it's kind of not well founded in science obviously it's you know rife with possibilities for discriminating and things like that this is a kind of system that we should not even consider building but for the rest of these it's something which apparently machine learning might be able to do and might be able to help us with but there's a fair amount of ethical risk here as well so when humans have to make these decisions often they have to go undergo some kind of training and they're bound by various laws for example anti-discrimination laws and when we want to apply classifiers to these problems we need to ensure that they're fair in the same way and so there are there are a lot of different factors that we might want to think of when deploying machine learning models in the real world and we'll come back to some of these throughout the course um for example how bad is a false positive versus a false negative how do we kind of judge the the error rate but here we're going to look at a particular notion of fairness which is going to illustrate why just looking at accuracy is not going to be sufficient for thinking about how our model performs and whether it's kind of being the most beneficial model that it can for this application so the big idea i want to underscore here is that we need to think about what classifiers are doing beyond just accuracy and so the idea is that if we have different population subgroups we want to think about whether the classifier is handling all of these groups fairly so tn cleary defined fairness as or rather bias as follows a test is biased if prediction on a subgroup makes consistent non-zero prediction errors compared to the aggregate so let me illustrate what that means so we have two populations here pi 1 and pi 2. and on the x-axis we have their results taking some tests and on the y-axis we have their ground truth performance on some task and so let's look at performance level y star here so y star which is on the uh you know this value on the y axis here corresponds to this particular point uh for pi 2 and this particular point for pi 1. and so what that means is that given the same uh ground truth level of performance participants from pi 1 will score higher on the test than people from pi 2. you know someone on pi 2 gets an 85 on the test someone from pi 1 gets a 90 but they actually have the same underlying ability and so the we could think of this test as bias because it's penalizing pi 2 right they're getting a lower score on the test despite having the same underlying abilities all right now this is a kind of regression view of fairness we can distill this down into a kind of sense for classification as well due to thorndyke and peterson and novick and others this idea of fairness and classification being grounded in the ratio of predicted positives to ground truth positives and that must be approximately the same for each group so for example suppose we have two groups and we'll use our movie review data from kind of sentiment examples that we've been looking at let's say in group 1 the reviews are 50 positive and group 2 is 60 positive maybe group 1 is comedies and group 2 is horror movies if we have a classifier that predicts 50 positive in both groups that's not fair regardless of how accurate it is now note that if it predicts 50 positive in both groups it could be getting you know zero percent accuracy on group one by just completely flipping the predictions and it could be getting as high as 90 accuracy on group two but it's still kind of not giving group two a fair shake there should be uh you know a higher number of positives in group two but the classifier is penalizing a group two kind of under predicting the positive rate so this the the the kind of formalism um particularly from peterson and novik allows for using different criteria across groups for example imposing a different classification threshold and kind of counter-intuitively this can actually give a fairer result because we might say all right the classifier on group two actually tends to under predict positive so let's like turn the threshold for uh positive prediction down a little bit so that um you know more things get predicted as positive all right so all these questions of fairness you might wonder well can't we just make our classifiers not depend on certain sensitive features like for example don't look at the genre of the movie don't look at the gender of the person who's writing this text or the you know the person who we're giving a loan to or things like that um and the answer is no we need to be thinking about this because it's very easy to build classifiers that discriminate without even meaning to so the problem is that you have other features that are going to correlate with membership in a certain minority group x and then might learn to penalize that group so for example in the authorship attribution case uh even just throwing simple bag of words features which seem like how could they be biased they they can recognize different dialects of english like aave or the presence of code switching like when you you know write half the sentence in english and half the sentence in spanish and so the model is very easily going to figure out what group someone is a member of and then the classification results could well differ across those groups another famous example is using zip code information you know you're you're building a system to give loans and you don't show information about the race of the person you're making a decision about but you do show it zip code information and then that heavily correlates with race and so suddenly you're kind of back to using race in the equation uh and this is not hypothetical in the least there is a relatively well-known case a few years ago where amazon had this tool for sorting resumes that when they looked into the weights of what it had learned it was learning negative weight features for women's x organizations or having gone to a women's college was also a negative weight feature for a particular few cases and so again accuracy doesn't catch this uh and it's actually hard a little bit hard to evaluate here because uh in the case of amazon it's possible that the humans were being biased as well right so we can't necessarily say that this this criteria about the right number of positives is even is even necessarily the right thing but we absolutely want to be aware of when systems are kind of running away and and doing this without us knowing about it so the kind of takeaways here which we'll touch on throughout the rest of the course to think about are the following whenever you're building something you should be thinking about how is this going to serve the users and what are the minority groups in the population we should be mindful of and then we've established some of these fairness criteria can we check these and then finally you know do we have parts of the system which can correlate with these protected classes or minority groups and are going to cause issues as a result that's the end of this segment you\", metadata={'source': 'N4f2-S19LME'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about a kind of basic introduction to neural networks and why they're something that we're going to want to use here so i'm going to return to this example that we saw in binary classification where we had the following sentences quote unquote sentences good bad not good and not bad and we had the following features uh kind of bag of unigrams features associated with these giving rise to the following feature vectors so this gave us something that geometrically looked like this um and uh all of these points are kind of on the same plane and the kind of i guess checkerboard pattern of pluses and minuses here means that no linear classifier is able to separate these things okay so there's plenty of solutions to this problem so we talked about one which was defining these bigram features which look at pairs of words and then suddenly there were kind of more dimensions in the feature space and we could do it you could also appeal to something like kernel methods like polynomial kernels or rbf kernels allow you to learn a classifier that separates these things the you know there's a lot of reasons we're not going to pursue these methods in general throwing in like all pairs of features is really expensive and uh kernel methods uh have sort of undesirable properties um from the standpoint of nlp so what we're going to do is we're going to think about neural nets as our kind of way of fixing this and so what neural networks are going to do is they're going to transform the data into a latent feature space so rather than kind of operating with our classifier in this raw space uh with uh the features that we have here we're instead going to uh transform it and what we're going to see is that in that transformed space it's going to be not too hard to learn something that allows us to separate these okay so the idea here is that we want to have uh our linear classifier from before again we're just going to think about binary here but it's the same thing for multi-class um we are going to replace f of x with a non-linear function of you know i'll say the original f of x so we're going to define z a vector this is going to be our latent feature vector as the following okay so what are the different pieces here so this is we're going to say an n dimensional feature vector and this is just going to be the same f of x as before so it's what's at the top of the slide here for these data points v is going to be a d by n matrix um and it's going to be dpi d by n so when we multiply f of x by v it's going to map from this n-dimensional feature space into a d-dimensional space all right and then g is going to be some kind of non-linearity all right uh and so uh what do we what do we mean by a non-linearity well there's a whole bunch of different functions we can use here i'm just going to kind of list and draw out a few of the kind of standard ones so hyperbolic tangent is one it kind of goes between minus one and one uh or you know it maps things into the range from minus one to one um and and kind of looks like this where it asymptotes um another common one is what's called rectified linear units which uh look like this sort of hinge thing um where basically it's just the max of the value and zero so it kind of clips and becomes flat at zero uh and there's other choices like sigmoids um various other kind of forms of value that do uh you know kind of funky things where they like dip down and then go up um etc so there's lots of different choices here um and it's going to be very important that we use that we use one of them but exactly which non-linearity we use again is not going to impact too much of what we do all right so now what we need to think about is you know how how can v and g give us useful latent features so we've kind of defined this whole this whole pipeline and what we want to do is we want to somehow transform f of x in a way that's actually going to make the problem easier and the reason is because now what then what we're going to do is we're going to classify with w transpose z i guess i'll write i guess i'll write z of x here just to make it clear that this is still a function of x but we're going to try to learn a classifier with w transpose z and so now we want these points to be separable in this z space right okay so let's see how we can actually make this work so suppose v is the following matrix and g equals tan h and then also suppose uh tan h of zero is zero well okay you don't have to suppose that that's just true tan h of 1 we're going to say is approximately equal to 1 and tan h of 2 is approximately equal to 1 as well so that's it's it's not quite true but um from the purposes of keeping the math simple uh this will let us illustrate uh the kind of point here um and most neural networks by the way do have this uh this you know this property with when you use units like tan h where there's a kind of saturation um and that's kind of why uh that's part of the reason why the term neural networks came about is it's sort of motivated from the idea of action potentials and neurons which is that you know you basically get this kind of buildup and then the neuron fires um but it's this kind of binary thing where um you know it it it sort of fires in a discreet way so here it's like once you get enough activation um you get the value of 1 but then kind of increasing it additionally doesn't do anything all right so now z in this case is going to be uh the okay so uh sorry the example that we're going to use here is going to be a simplified version of the good bad example that's going to look like this and now z is going to be tan h of x1 tan h of x2 and tan h of x1 plus x2 okay where did this come from so remember that uh we're multiplying v by uh f of x in this case f of x is two features uh it's x one and x two and so uh and then we're applying g to it right and then that's going to give us z so when we do all that we get now a three coordinate z that has this property okay so now what we're doing is we're transforming this original feature space into the following one so this point at the origin stays at the origin what happens to the pluses that are along these coordinate axes well the plus that has a value of 1 for x1 gets a value of 1 for 0 z1 0 for z2 and 1 for z3 so that plus ends up here um the other plus uh does exactly the same thing and then uh this is the part that's kind of impossible to draw but the there's a minus that ends up being at uh one one one and i'll kind of draw this from another perspective um you know we have like the negative here uh the two positives like here and here and then this other negative is is kind of out here and it's a little bit hard to see but in this space now these are actually separable um you can kind of imagine cutting away like the the two the two pluses um which are uh you know kind of you know up and and and kind of back closer to the uh origin than than the other minus and so you can um you can kind of get a hyperplane in there to separate these so the overall idea and motivation behind all this is that now that we've introduced these uh tan h's they allow us to transform the space in a meaningful way and learn these conjunctive features that talk about multiple parts of the input so for example this last coordinate of z is effectively an or gate between x1 and x2 at least the way that we've defined tan h here and this is going to allow us to think about you know just again with this machinery of vng here learning these combinations of features that are going to allow us to do a better job at classifying these things and so we're not going to go through the example in detail but you should be able to convince yourself that uh with the right v and g um you can try to write down ones that will be effective here you can get this example of good bad not good not bad um to also be separable and so what neural nets will do is that rather than having to specify okay here you know here are the interactions i want and here's how we're going to broadcast not to these different words we're instead just going to be able to learn all of this in an end-to-end way so we'll talk a little bit more about uh neural net intuition and motivation and definitions but this this gives you the basics and that's the end of this segment you\", metadata={'source': 'DU_p-RBy5gM'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to look at some visualizations of feed forward neural networks so we have here the basic equations which i've rewritten in a slightly different form so we could think about a feed forward network as producing this latent feature representation z um via a combination of a few different operations so we take our input features f of x we multiply them by this matrix v uh which we can think of as warping the underlying feature space then we apply a shift uh or bias which you know we've generally ignored and then put this whole thing through a non-linear transformation g like tan h or relu then the final classification happens via a kind of linear classifier layer on top of this so the way we can think about how this works uh is shown in this animation here which is uh courtesy of chris ola who has a great blog post showing uh in kind of more detail uh unpacking some of the kind of i guess visual intuition besides behind what neural nets are doing and so what we see here is the three stages of this transformation process so first we have this warp space operation then a shift then a nonlinear transformation and so the warping space is happening right now this is the kind of turning it into parallelograms the shift is the translation where these things shift over and then finally the non-linear transformation uh squashes the entire space uh into this minus one to one square and this is using hyperbolic tangent which uh again maps kind of the whole real line into that range so the way the the the kind of interesting uh i guess connection here is that uh we could think of graphically sort of what's happening to the underlying f of x at least when you have a low dimensional space it's kind of you know tractable to think about what these transformations might be doing um and again we're basically ignoring this this shift term because we could just fold that into the underlying feature space which is normally going to be much higher dimensional than just two so you know we wouldn't be able to visualize it anyway so the reason you might want to do this is if you have two uh sets of points which here are shown on these two uh these red and uh blue curves when we want to separate them a linear classifier can't do it but a neural net can and one way to think about this is because once we apply those transformations to the curves we suddenly map them into a space where now these two curves are linearly separable so again this kind of motivates the idea of coming up with this latent feature space z and then just using linear classification in that space so we can kind of stack this up further and take a a number of different layers on top of each other um so this is the kind of classic idea or one of the things that make deep networks so powerful is the fact that we can just iterate these transformations learn the entire thing with back propagation and not have to uh you know try to kind of hand design you know any additional transformations or things like that we can just plug all of these pieces together and so to get an idea of what that looks like we can repeat those operations we saw two slides ago these uh kind of warping shifting and non-linearity and these two spirals that were deeply entangled with each other and kind of not linearly separable at all now through this series of operations we can really kind of pull them apart and you can see how warped the original space gets you know recall that there's a grid there to start with and now uh it ends up being this uh this like totally mashed up you know sort of loon surface thing uh so this indicates kind of how powerful these techniques are and why they're going to enable us to learn such sophisticated classification models and that's the end of this segment you\", metadata={'source': 'rdohzaGa8aE'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to define feed forward neural networks in a little bit more formal mathematical notation and we're going to talk about how back propagation works so we start off with the multi-class logistic regression with different weights formulation that we had before so the probability of a class y is the exponential of weights associated with that class dotted with features divided by the sum over that same kind of class score weight stop features for each possible class y prime in the output space so i just want to make the data types of all of these things pretty clear because we're going to be vectorizing these operations we're going to be going from a computation that returns a single scalar probability to a computation that returns a vector of probabilities this is going to transform this into a form which is a lot closer to what pie torch code looks like so when we have these three classes and let you know let's assume we have three classes here and this different weights formalism each class has a weight vector and the dot product gives sum score which looks like this and then we can transform these into class probabilities as described in the above equation using an operation called the softmax and all softmax means is take these numbers exponentiate them and then normalize them divide by their sum you should be able to confirm for yourself that taking the uh each of these class scores weight stop features and exponentiating and normalizing gives you the class probabilities under this multi-class logistic regression model and the reason we kind of go through all this effort is that we are going to write this uh whole equation at the top in this fairly compacted form soft max of w times f of x so what have we done here we've stacked each of these vectors per of class weights w1 w2 w3 into a single matrix and now we can express this whole operation as multiplying that matrix by the feature vector and then softmaxing it so what we had before was a single scalar probability and now the weight vector each weight vector in each class gets kind of stacked up into this weight matrix which is num classes by number of features and the output now is a vector of probabilities capturing the probabilities for all the classes so that's why we write bold y on the left and the p of y given x instead of uh you know just normal y and then finally we what we can do is we can change f of x or rather we can introduce this hidden layer which computes our latent feature z by multiplying v and applying g as we were doing before and so the nice thing is that now we've gone from something that we've looked at and understand which is multi-class logistic regression um to a feed-forward neural network with one hidden layer just by kind of transforming this formula with a couple of extra mathematical operations so to kind of map out exactly what's going on here and think about it a little bit graphically we start out with our n features in f of x we multiply those by v which is a d by n matrix and put that through our non-linearity g and then that gives us a vector of d hidden units so again this is our latent feature space z it might be the same size as n it might be a lot less who knows and then we multiply by w which is our num classes by d matrix so again the sizes of these matrices have to agree with the sizes of the layers in order for the operations to work out and then we apply a soft max and we get our vector of probabilities which is num classes long and so again if we have a three-class classification problem we want three numbers at the end here and so w needs to have three rows one corresponding to each of the individual classes weight vectors if we're thinking about things in the different weights uh paradigm okay so this is a you know there's a kind of gory detail graphical unpacking of what's going on in this in this formula up here so now we understand how to do this computation if we have all these parameters we can compute this just you know using relatively basic matrix math but the real question is how to train these things um we've talked about you know ways of training multi-class logistic regression taking the gradient and and using stochastic gradient descent and now we need to think about how those generalized to this case all right so recall that we can think about this in terms of two stages one is computing these latent features z and another is uh to actually compute the output probabilities um and so uh we're gonna train with the same basic objective as before which is uh maximizing the log likelihood of training data or if you want to think about it using loss as negative log likelihood and then minimizing that so the log likelihood of a training example uh can be written in this way so it's a little bit of a different notation than before so what's going on here so i star on the right we're using to denote the index of the gold label so for example in a three class problem if the gold label is at position two you call this e2 and so what is e e is basically just a kind of selector that has a one in the position of the subscript and zero elsewhere so in that case it would be a vector zero one zero and the you know the reason we write it out this way is so we can keep using our soft max notation but now we're saying okay the log probability of you know the true class is com is gotten by grabbing the log probabilities of everything and then taking this dot product and then logging in so if we do that we apply the log and everything we get a equation which looks like this which is a little bit different than what we saw in the multi-class logistic regression case but i claim that it's actually the same what we have is this linear term to start with uh which basically looks like the weights of the correct class dotted with z uh and then we subtract off this log sum of exponential term which is tip what has sort of fallen out from the denominator of logistic regression um in the past and uh you know again it's just using this this slightly different notation here all right so the fact that it looks exactly like multi-class logistic regression means that we can think about computing the gradients of w exactly as before um so you know we we once we compute the z's we don't even really have to kind of think about them anymore we can just compute gradients of w and the loss and everything and treat it exactly like multi-class logistic regression even from the standpoint of learning even from the standpoint of computing gradient updates so this is the kind of you know the the top layer of the network right the one closest to the output so it's not too uh you know nothing too crazy is going on uh but now we have to think about uh using so-called back propagation and actually computing gradients for earlier stages of the network um and so this is where things get a little bit more complicated i'm only going to go through a sketch of it there's plenty of resources out there for learning more about you know exactly what this computation looks like and going through all the details with the chain rule the high level idea is that we are going to be able to compute this vector that we call an error signal from based on the output layer and we're going to pass that back to this you know position where we've computed z in the network and then we're going to be able to ignore everything that happens after that well okay we've already computed our gradient updates for w um so you know modulo applying those gradient updates we can now ignore everything that's going on there from the standpoint of the rest of the computation and so this is why you know we're thinking of this as back propagation um you know we compute something and then we're going to kind of work our way backwards through the network and compute gradient updates all right and so just to kind of mathematically motivate this a little bit um really what we're trying to do is compute the gradient of the loss with respect to uh the positions of the you know the different values in the v matrix which is the the kind of first um matrix in the computation the one which helps compute z um and so this is using the chain rule um what we do is we first compute the gradient of the loss with respect to z that's this so-called error term uh and then we multiply that by the gradient of z with respect to uh you know the the matrix v and so this second term isn't too scary um it involves dealing with the gradient of the activation function which you know fortunately we've chosen all of these things like tan h rectified linear units etc are differentiable so you know we can compute that and then this the second term um the gradient of a which we're just using as a kind of intermediate value here uh with respect to v that's just a purely linear function so given this error signal what we can then do is compute gradient updates for v based on that by kind of applying a couple more operations based on the non-linearity and these other linear operations and so the way we can think about this is that for each of these parameter matrices v and w we have some gradient term which comes backward from the loss um which is either either kind of chain rule the like partial derivative of the loss with respect to the output or it's just the the kind of loss itself when you're at the last layer and then we combine that with information from the input to that layer either the features themselves f x or the hidden unit z and so this is the the the idea of back propagation is that we have all these values saved up from the forward pass and then going backwards we you know we look at the first chunk compute the gradient pass an error signal back look at the next chunk compute the gradient pass an error signal back etc so if you want more information about back propagation again there there are other resources that you can look to for it for this course we're not going to assume a kind of deep you know knowledge of exactly how this works we're more going to use this kind of functionally and try to understand it uh from the perspective of what's actually needed in order to train the kinds of models that we're interested in that's the end of this segment you\", metadata={'source': '8WhPYIWyR5g'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about some basics of implementing neural networks in pi torch the basic idea behind frameworks like pytorch and tensorflow and others is that computing gradients is hard and we want to instead be able to offload that work onto a basically onto something like a compiler and so the core idea is that if we have a bunch of mathematical operations written in code uh we can theoretically use a a tool called automatic differentiation to understand basically what's going on in that code and keep track of the derivatives so roughly you can think about this as follows if i write y equals x times x um then a compiler should be able to uh look at that multiplication operation and you know basically generate code which does the following we're going to keep track of this this kind of d y term here which is going to track uh the derivatives here as well and of course y is going to capture x times x as before um and so this is the idea behind so-called computation graphs rather than just doing operations on numbers in our code we want to think about defining and instantiating this object which represents that computation and which then we can do reasoning over like for example differentiation and so the computation is now something that uh we handle symbolically and so you know we have to change the way we write code a little bit right and that we couldn't necessarily just um you know use kind of constants or do all the kinds of operations we want before um but assuming we're willing to kind of play within those you know very mild restrictions we can use libraries like pi torch or tensorflow to express our computation and now we don't need to go about computing derivatives ourselves we can instead get these frameworks to do it for us so essentially this is how we're going to think about pi torch in this course it's that it is a framework for defining computations that provides easy access to derivatives or gradients and so we're going to see a very brief crash course uh in pytorch here just to understand uh the the kind of core pieces that uh that go into pi torch code um from the perspective of what we're going to be building so uh a torch module uh is the kind of basic unit of a neural network um this these things can be recursive or hierarchical so you can wrap other modules which either implement predefined layers or other parts of your network if you wrote them etc so there's two uh kind of critical pieces of functionality here we have forward which takes an example uh or an input and then computes some kind of result so this can represent either again your entire network from feature vector to prediction or uh you know just one layer from a vector of a certain size to another vector of a different size and then we have backward so this typically you do not need to write by hand and so the cool thing about pi torch is that based on what you write in the in forward backward will be automatically instantiated and will do the correct thing uh to compute gradients um the caveat is that if you do some kind of really crazy math in in forward um you know you might have to define backward yourself if there's some kind of extra you know part of the gradient that you need to like compute analytically and you know code that in but otherwise you know if you're using relatively basic mathematical operations or standard neural net tools then uh you know you'll never need to touch backward all right so remember that this was our our kind of basic neural net starting with features uh going to this vector of hidden unit z and then going to an output distribution over classes with these two weight matrices v and w and one non-linearity so one hidden layer here let's see how this looks uh in pi torch so in pi torch it's relatively straightforward to define so we have first these different layers we have our weights v and w uh our non-linearity g which in he which in this case is hyperbolic tangent and we have our soft max at the end which is going to map from uh again the vector of real numbers into a vector of probabilities and so these uh these linear layers uh essentially you know you could think of these as just implementing the matrix multiply and they have associated parameters with them um so we're calling them v and w but basically inside each of these linear layers is the matrix which is actually implementing that operation and the forward then has this fairly simple form uh where you know we don't actually have to call forward on each of these layers we can just use apply so self.v of x you know multiply by v apply g multiply by w apply soft max and this again you know you can look up at the math above and is a very direct translation from the way that we've defined this mathematically into this uh forward function so the input to the network is something that we have to uh we have to kind of massage typically so we've talked about how you know we always need to go from raw text into a feature representation and so as part of that process when you're using pi torch you additionally need to convert things into some kind of tensor and whether this is turning things into integer word indices which can then get embedded using various word embedding layers that's kind of where we're going next in the course uh or whether you're using real valued vectors basically you need to convert things into a format that that torch can understand and so the you know the usual uh i you could translate these things to and from numpy arrays fairly easily so uh if code you have is already structured in terms of numpy getting it to work with pi torch is not too difficult the main caveat is that if you do kind of weird stuff like you're always mapping in and out of torch representations you can break back propagation if you do this inside the network for example if you go all the way to just like you know some bare number um and then convert it back pytorch will not necessarily be able to track derivatives through all of those operations and so you have to be a little bit careful typically you want to you know form your inputs uh to pie torch and then kind of do every you know do everything up to the output uh all within the the torch framework which is fairly flexible and so should not cause too many issues all right so then in terms of training this network so we can instantiate it again this kind of calls back to what was on a few slides ago showing the the the definition of this feed-forward network so we instantiate it with uh sizes for the input layer the hidden layer and the output layer and then we instantiate an optimizer so we're going to talk about optimization a little bit more later here we're going to be using an optimizer called atom which is going to operate over the parameters of this network and use a particular learning rate all right so now our loops look like the basic machine learning framework that we've set up so far so we have a certain number of epochs that we loop over and then here we're just going to loop over uh inputs and labels in the training data uh we'll talk about batching a little bit later as well doing multiple inputs at the same time this gold label here we are going to assume is one of these vectors e i which is a one hot vector so uh it has a one in the position corresponding to the true label which in this case would be the second class and zeros elsewhere so the first thing you do is you have to clear out gradient variables using this zero grad call and the reason is because pi torch caches information about the gradients inside the actual network and if you do not do this you'll kind of keep accumulating gradient information and everything will be messed up and this will be a bug that's very hard to find we do that then we call forward on the input and we get some probabilities as output so again we've defined the forward network to go all the way to softmax and give you a valued probabilities now we need to compute a loss so this is showing the explicit loss computation there's plenty of built-in losses in pi torch nll loss which kind of implements what's here um you have to make sure that you pass in the right thing whether it's log probabilities or logits there's a few different ones uh just make sure you check the documentation for whatever you use but here what we want to do is we take the negative log of these probabilities and then we dot it with the gold label and again we think of that as a selector operation and so that's going to give us the negative log probability of the correct label which is our negative log likelihood loss and then we call dot backward on this loss and so what this does is this computes the gradient of the loss with respect to all of the uh parameters of the network and then we take a step with the optimizer so now the step it knows it's kind of responsible for optimizing all the parameters of this network and when we call optimizer.step it will use the atom optimizer based on the gradients that were just computed as part of this lost dot backward operation and apply our gradient update so the basic you know kind of flow of training looks like what we saw in the previous slide now the thing that we have to be careful of is that we initialize everything correctly so this is another point that we're going to come back to but it's very important that we initialize the network parameters to non-zero values and this is because it's unlike uh in the linear classification case we now have a non-convex optimization problem and so uh gradient descent can find different optima based on what the initialization of the network is and so if you initialize with uh zero weights you kind of get stuck and never learn anything and finding good initializers is very important all right so the kind of final just picture of training i'm going to give which we're showing here is we define our modules we initialize our weights for them and our optimizer and then we loop through epochs and data batches we zero out the gradient we compute the loss and we use backward our automatic differentiation to compute gradients and we take a step on the optimizer and then after each epoch one thing we might consider doing is check performance on our development set so we want to make sure that we're not over fitting that we're still making good progress that you know everything is kind of set up correctly so this is a good thing to get in the habit of doing is to you know just periodically have a small set of validation data and check uh performance on that just to make sure um you know to make sure that things are working as intended and then finally once you've gone through uh this whole process uh you want to compute your final results uh either on your development or test set as appropriate so that kind of gives you an overview of how we take the network that we had converted into pi torch code and and kind of map all these elements of the training phase into code and that's it for this segment you\", metadata={'source': 'IRZCQO18QAI'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about some considerations when training and optimizing neural networks so these are just going to be a few kind of practical techniques that are uh going to be useful for the kinds of networks we're going to see in this course and we're not going to go too too deep on it so the first thing that you know essentially always is something you're going to use is what's called batching so batching is where you process multiple data points in parallel and you know one of the main reasons to think about doing this is it can speed up your uh you know basically the amount it can reduce the amount of time that a single epoch takes pretty substantially because you can do uh more efficient matrix operations and achieve higher parallelism by operating over a group of points of the time and so this does actually require making the computation graph be able to take not just one data point but actually a whole bunch of them however this is not too difficult in pi torch because of the way that everything is structured in terms of tensors so instead of thinking about an input which is just a number of features and a gold label which is just like a number of classes and these are both vectors now we're going to think about these as matrices where we have an extra dimension which reflects the batch size and it turns out that the forward implementation for this network does not need to change at all everything that uh exists in the running example we have so far generalizes seamlessly to this case and what happens now is the probabilities that we get out are now a matrix where um each row is a different example in the batch the one place that things need to change here is that when we're thinking about the loss we need to actually accumulate that down into one number so we do need to sum these negative log probabilities across the batch but that's not too onerous and again because the other operations are all kind of naturally defined in terms of tensors we don't need to do other modification here in order to batch our uh you know in order to implement batching in our network so this is very nice uh in that you should be able to get code working for a single instance and then generalize it without too much difficulty in terms of batch sizes uh it's hard to give a kind of range uh that is universal for all problems uh you know anywhere between one and a hundred usually uh it the story changes pretty substantially when you're dealing with pre-trained models uh and a lot of times when you're training more sophisticated networks batching is very important not just from a standpoint of speed but also from a standpoint of the kind of optimization and getting to a good uh optimum so uh using a batch size a kind of moderate batch size of like 16 or 24 especially when you have a a decent sized gpu is often a good idea but on a lot of simpler cases like training a very basic seek to seek model um batch size one kind of works just fine from the standpoint of uh you know what you actually converge to okay so the basic framework is that we have a batch we compute gradients on it and then we use one of these uh first order optimization techniques that just need access to gradients so the main questions we're going to try to answer here are initialization regularization and optimization and again there is there you know there's such deep theory behind all of these things that there's no way we can cover it so there's entire courses you could take to try to understand these things more but here we're going to talk about this mostly from a practical standpoint of what settings of hyperparameters you'll typically want to use all right so here is our basic feed forward network and so to think about initialization what we want to start out with is thinking about v and w here and how we initialize them and what consequences it has so this is a non-convex problem so the initialization matters in particular if you initialize v here to zero you could think about what might happen what happens is that with either of the choices for g listed here you end up with z that is all zero all the time and what happens then is that the model never gets off the ground with respect to learning because essentially logistic regression the last layer is just seeing a vector of all zeros it has no idea uh kind of which of these features might be useful because none of them can convey any information and so the gradients just end up being zero and nothing gets learned so we need to initialize to some non-zero value for the early layers of our network in order to be able to break symmetry between different hidden units and and kind of get learning off the ground now how does the fact that this model is non-linear affect things so here we're showing hyperbolic tangent and when we go out to the edges of uh the curve here we start to see that the slope of hyperbolic tangent gets pretty flat and this corresponds to a phenomenon called saturation of gradients so imagine that the inputs to this network are or the inputs to this layer let's say are always going to be very large or very you know very large positive or very large negative so we're always getting five or minus five or something on that order it turns out that then the model doesn't then try to change those inputs very much because the gradient is so flat here it looks at this and says okay well even if i were to change the input it won't actually change the output that much so like why bother right so you're all the way out at five kind of saturated over here and the model doesn't even want to think about bringing it back towards the origin and so it's very hard for learning to happen because you're going to pass back a very small gradient through this layer so this is one reason to consider using rectified linear units or values these kind of hinge shaped things in green here these can produce larger values um but they have a similar sort of problem which is that if everything ends up being negative somehow um you know you can also get a little bit stuck um so again you have to think about the right way to initialize in order to avoid these things happening uh and so you know we don't want to just initialize to all zeros we don't want to initialize too large so there's been a lot of thought put into what the right technique for initializing is both from a distribution standpoint and also the scale so one initializer is due to xavier aguero and it looks like a uniform distribution between this funky expression here that is based on the so-called fan in and fan out of a layer that's just a fancy term for the number of inputs to that layer and the number of outputs and the reason we we kind of scale things and do this funky thing is that we want the variance of the inputs and then the gradients for each layer to be about the same so you know we might have some layers in our network which are like a hundred dimensions in and 50 dimensions how we might have others which are a thousand dimensions in and 500 dimensions out and these are on very different scales and so we need the right initializer that balances things so a single gradient update with a single step size can can do something reasonable there's a more sophisticated take on this sort of idea not for initialization but during learning something called batch normalization where what we do is we take each layer's activations and we shift and rescale them across a batch so they have mean 0 and variance 1. this is not going to show up that much in this course it's much more of a consideration in computer vision where if you have very deep networks you need to think about again how to kind of balance these basically how to balance the optimization across these different layers all right so that's that's kind of what we're going to say about initialization there's various sort of pre-constructed schemes that you can use but it is important to use something and not just initialize to zero for dropout we can zero out parts of the network during training to prevent overfitting that's the basic idea behind dropout and then at test time we're going to use the entire network so the sort of idea here is that uh we don't want like the one of the ways you can get overfitting in a classification model is if uh you have two features that are highly correlated but maybe differ on one example and then you know those features in aggregate should give a score of like plus one most of the time but what you do is you set one feature to plus ten and another feature to minus nine because now it lets you really fit that one example where they do something different and then most of the time they just cancel out and give you one so this is a kind of stochastic way to enforce that something like that is not happening and the reason is because when you drop out sort of random chunks of your network you prevent these uh different neurons or activation or you know cells in the layers from learning to uh you know this kind of highly co-evolved behavior another way to think about this is connected to ensembling basically what you're saying is i want any random sub network to be able to do well at this problem and so then at test time when i take the whole network that's like having an ensemble of a whole bunch of sub networks and so you know even if some signal is is kind of missing from the input or whatever um you know we've got other parts of the ensemble that can maybe pick up the slack here and uh you know this should roughly still be able to do the right thing um so this is a this is a relatively easy thing to add you can just add a layer that allows you to do it um i will say that setting dropout rates too high like is is a good way of actually getting bad performance um and uh usually this doesn't matter as much as you might think it does so it can be a nice thing to add that can give a little bit of performance increase but is generally not too important all right the last thing we're going to talk about is the choice of optimizer so one good choice is an optimizer called atom this is very widely used especially in nlp and it combines two ideas um one is the notion of an adaptive step size so this looks like basically for each coordinate we think about essentially what the size of the gradient updates we've seen for that coordinate is and we rescale it appropriately and then momentum is the other trick this this incorporates meaning that we basically use an exponentially weighted average of uh the past few gradient updates and you know when you make an update that that update will then inform the next few updates as well that's kind of why it feels like momentum i mean these are both these are both effective techniques there's lots of methods that balance these two things but atom is one that's pretty widely used that seems to work well for a lot of nlp problems there are some results due to aisha wilson at all from nurip's 2017 showing that these methods actually might not be the best thing for test time so on the top uh adam is fitting the data really well but on the bottom uh adam doesn't actually generalize as well as sgd with like carefully tuned learning rates and and momentum and things like that so uh it's sort of an open question um adam is going to work well for a lot of what we do in this course and so that's the that's kind of recommended one that just kind of works out of the box most of the time and the final thing uh that is important from an optimization standpoint especially with more complex models is gradient clipping basically you don't let the gradient exceed some max value and this just kind of ensures that it doesn't go too off the rails during learning so we've talked now about some different considerations for initialization regularization and optimization and so you can refer back to this or use this as a resource for uh thinking about what values of hyper parameters to use etc but you know we've given you here the basic template i guess for most of what we're going to use in this course from an implementation standpoint that's it for this segment you\", metadata={'source': 'KPZb2rYS4BE'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about word embeddings word embeddings are one of the most important ideas in nlp in the last decade and the reason is because they really are what allow us to bridge the gap from raw text to uh representations that can work well for models like neural networks neural networks have been around for a very long time and they're very good at learning functions over continuous data but for a long time it wasn't really clear how to take discrete kind of very discreet data like words and and particularly like bag of words representations and actually get them to work well in neural nets and so uh word embeddings are kind of the first step towards bridging that gap and so they were one of the key pieces to enable the kind of neural revolution in nlp with things actually working a lot better okay so to sort of set up the basic idea we're going to start with uh again very simple sentence sentiment uh example sentence um where uh we can map movie was good into a bag of words representation and so again we're going to use unigrams here and so uh you know roughly we could think about that as some space like this where you know you've got a whole bunch of zeros and then three ones in this vector that correspond to the positions of the words which occur in the sentence one way to think about this is that it is the sum of three vectors um and i'm just gonna write them like this uh to mean that there's only a single one uh in in each of these okay so uh again we can decompose this into uh three vectors each of which has a single one in it sums together and that gives our bag of words representation that has three ones in it okay so this is a way of thinking about going from a word level representation to a sentence level representation so why is bag of words a problem so if we say film is great uh you know we have some other set of ones and this is actually orthogonal to movie was good so there is no apparent connection between these two sentences because they don't overlap in any words so the dot product between these is going to be zero and generally we kind of can't tell that these are related at all and so that's that's okay if we have a ton of training data right like if we have a whole bunch of data we can theoretically see examples of both movie was good and film is great and uh you know learn appropriate weights for all of these things but in some sense this this sort of shows that our input representation doesn't reflect very much about the underlying structure of language because we're not taking advantage of the fact that for example film and movie are you know very closely related terms in this case and so this is where the idea of word embeddings come in so word embeddings are we are going to say low dimensional representations of words um capturing their similarity okay when i say low dimensional uh you know a typical range of values we're going to think of at least for this part of the course is between 50 and 300. so this may not seem all that low dimensional until you think of the fact that uh you know let's assume that our vocabulary size is 10 000 words um and like this vec this 0 1 vector up here was a 10 000 dimensional vector so 300 much less than 10 000 and sort of low dimensional from that standpoint and the rough kind of picture you should have which again we're going to draw a kind of two-dimensional view of this is that uh these embeddings should group similar words near each other right so uh unlike before where every word was as like as far apart from every other word as it was from any other word uh because they're all mutually orthogonal in this 10 000 dimensional space now you know you can expose the idea that movie and film are perhaps more similar to one another than either is to the words was or is so this is a very powerful idea because it's going to start to let us have networks that generalize potentially to words that we haven't seen before we trained something on movie was good and now suddenly film is great comes in as input and even though we've never seen those particular words the vectors are sufficiently close to things that we have seen before that the network's gonna like output the same prediction and in this case that would be the right thing to do so before we get into any specific algorithms we're just going to talk about the rough idea of how we might go about learning these embeddings so this is an idea that goes back to j.r firth in 1957 and it's what's called the distributional hypothesis and it can be succinctly summarized as you shall know a word by the company it keeps all right what does that actually mean concretely here so let's say we have access to a whole bunch of text on the web how are we going to use that to learn what these different words mean here we're going to look at examples of four sentences all right so we have four examples here two each containing movie and film and so the basic idea behind the distributional hypothesis is that movie and film seem to be words that are substitutable or can occur in similar contexts so for example i watched the movie or i watched the film um we're gonna talk about syntactic parsing further down the road but uh movie and film here are both direct objects of watched and so watched has what we call selectional preferences it you know has certain types of arguments that uh you know are things that you can watch right uh and in this case we see that movie and film are both things that can be watched um the film inspired me the movie inspired me so there are also things that in this case are the subjects of the verb inspire and so even just looking at kind of surface word context around each of these words we can get a sense that they might be similar because they get used in similar contexts and so that's the kind of idea behind firth's hypothesis so it's worth asking it's worth saying that like i i'm presenting a very simplified view here um you know if you have another sentence like i developed the film in the dark room um this is going to be a context that's unique to film because we have a different sense of the word we're talking about like the actual like film reel or whatever rather than um you know the idea of a movie basically um so uh you know there's there's going to be complexity in terms of learning these things but basically in aggregate we expect to see movie and film in more similar contexts than we expect to see movie and mango or something like that so it's also worth noting that there have been implementations of this idea going back a long time in nlp so um you know uh there's kind of older algorithms like brown clustering etc that have uh implemented this kind of idea for a long time um so but uh the kind of version of this that really seemed to take off and stick was a version called word to back by tomas mikkelav at all in 2013. and the idea is the following each word is going to be mapped to a a word vector and a context vector and we predict each word's context given that word so the idea here is that we are going to see a whole bunch of examples of words in context and we are going to learn vectors where a word's vector should be predictive of the words that are going to occur in the con or in you know around places where that word is seen in the text and so by doing this uh we're going to talk about the uh exact you know methods for doing so which there's a lot of them um but essentially we are going to learn we're going to kind of operationalize this idea of the distributional hypothesis and be able to learn vectors that capture this kind of similarity that's it for this segment you\", metadata={'source': '8EqQROdVPyM'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to concretely produce word vectors or word embeddings given a particular algorithm that's called the skip gram uh well a model called the skip gram model so the input to this method is a large Corpus of sentences okay and so unlike other techniques for things like sentiment analysis where we've thought about supervised learning of classification uh here all we need is raw data and you'll see why when we Define it a little bit later so you know literally the best thing to do here is to just scrape as much data as you can from the web and uh try to uh you know try to learn your embeddings over that uh and there's some there's some issues with that particularly if you pick up uh kind of sensitive data or things that you don't you know associations that you don't necessarily want your model to have but this is the standard methodology uh in any case which is to try to get as much data of the form that you do want as possible all right and the output is going to be vectors V and C for each word type W uh so I'm going to use uh type here to reflect that for the word movie there's going to be a single V and a single c um even though we're going to see many occurrences of the token move all throughout the text so we'll uh you know there might be thousands of occurrences of movie as a token but there's going to be one vector associated with the word type movie all right the hyper parameters of this method are a word Vector Dimension D um which I said you know maybe we could use 50 or 300 or something in that range these are common values that you see um in uh kind of pre-computed and distributed word embeddings uh as well as what's called a window size K all right so uh we're going to assume k equals 1 for right now and then let's look at how this works so here's one of our example sentences the film inspired what we are going to do is uh take all Neighbors of each word token up to k um you know positions away okay so um in this case if K if K were equal to two you would look two words on either side um you know if you add more words in the sentence and now what we have is we basically have uh what we call our word and our context word and then the skip gram model is a probabilistic model of context given a word so specifically it's defined in the following way all right if we use y to denote the context word and X to denote the word uh then we have something that should look somewhat familiar okay so this looks like multiclass logistic regression in that we have an exponential of some kind of score um divided by a sum of exponentials so things I want to point out are that this bottom sum is over the vocabulary so uh we if we have 10,000 words that we've seen in our data this is going to be a sum over all 10,000 of them all right and then um V and C are model parameters um and you know each of these is going to be V by D and the way to think about this is just that uh you know basically think about stacking up the vectors for these words into one big Matrix and so you're going to have V rows and each of those is a d dimensional uh word embedding and we have two different sets of parameters for uh one you know one for V and one for C so we have 2 * V * d parameters in this model all right so now let's finally think a little bit about what this is actually saying um basically what this model says is if you know VX is similar to Cy um Y is likely to be in X's context right so we are taking this dot product and then exponentiating if you imagine that uh VX and Cy are very closely aligned then their dotproduct is going to be large and then when we exponentiate that it's going to get a high score and then we'll assign that thing a high probability so again we're learning two different sets of vectors here V and C uh but the the idea is that we should be able to capture relationships that say all right you know V uh basically v v for one word being similar to C for another word is going to be indicative of those two things occurring near each other all right let's take a look at an example here so we have a uh relatively simple Corpus here um and then let's assume that we have uh D equal 2 um and we just have the two words I and saw and they look like this so this is VA saw and this is VI so spoiler alert the way this algorithm is going to work is that we are going to randomly initialize all of these parameters and run gradient descent so there's no notion of like having a fixed set of vectors a priori but we are taking this fixed set of vectors here just to understand mathematically what's going on uh you know in this computation and and what is you know what what's kind of Happening Here okay so we get two word and context pairs from this um so in one the word is I and the context is saw and the other the context the word is saw and the context is I all right so the first thing we can do to just kind of understand a little bit more here um if C saw is 1 Z and CI is 01 um what is p of context given uh the word is saw all right so just to just to kind of understand um just to understand kind of what's going on here uh what we are saying is that uh seaw is going to be over here at the same place as VI um and then CI is going to be up here I'm drawing them not totally on top of each other but they are and we could go through the computation that we saw on the the previous slide uh and for this particular example we have to compute X of V uh saw. CI then also xove VA saw. C saw now this is important because remember we have to Loop over the whole vocabulary so even though we we never see like saw and saw next to each other like what we're thinking about when we think about this distribution is we're thinking about the whole vocab and so we need to compute these these values for everything in the vocabulary okay so um VA saww and seaw here are orthogonal if you look up at the the picture and so the x of this is going to be one um and then vs saww and CI are aligned their dotproduct is going to be one and then the EXP of that we're going to say it's roughly equal to three we're going to assume e is three um so in this case P of uh context equals I given word equals saw is uh 34s and then uh for saw given saw um it's going to be 1/4 so this is kind of nice in that it Con sort of we we've set up these word and context vectors and it kind of confirms our intuition um about how this should behave right we should have uh you know saw be more likely or yeah I I'm more likely to happen given saw than the other way around the other thing it shows you is this idea that the the word space V and the context space V are not really like the same right um in fact we kind of need them in this case to be like rotations of each other so that like the words can be close to their contexts um but they're not necessarily close to like those you know word vectors right like VSA and CI need to be close to each other but VSA and VI are typically not okay so that g that gives you a kind of sense of this computation um so now we're going to talk about training this and training it is also going to follow a similar uh kind of schema as we've seen before okay so remember that based on our window size K we extract a certain number of word and context pairs like we saw in the example and what we're going to do in training is we are going to maximize the sum over these pairs of the log probability of that context given that word so again given given the web we just with k equals 1 we're just kind of extracting these adjacent pairs of words we form this big training set of these word context Pairs and now we want to maximize the probability of the the sum of the log probabilities of The observed pairs so the thing I will say that is a bit different from sentiment is that this is an imp I'm going to say quote unquote impossible problem we're never going to be able to uh uh well I'll say we can't we can't drive the probability to one we're never going to be able to fit this distribution perfectly or I guess we're never going to be able to completely optimize this objective from the standpoint of of getting every Pro prediction to be of probability one um because there's going to be uh many words that occur in the context of a single word X so you're going to have these like conflicting training examples so unlike classification where it's totally reasonable to assume that you can fit the data perfectly um and in many cases a big enough neural network will uh here that's not going to happen and uh the the last thing I'll point out here is that uh we are going to initialize our parameters randomly so again we've talked about for neural networks how important it is to have good initialization here we're just going to kind of throw out some random vectors and then iterate over this data and what happens roughly is that you know the model is going to kind of pull you know similar vectors together over time because they're going to be seen in similar contexts and optimizing this objective is going to give us vectors of the sort that we want so this is the basic skip gram model and Skip gram uh training procedure uh we'll talk more about alternatives to this and understanding it uh in a little bit more detail um but this is the foundation for producing word vectors which are going to be useful for lots of different NLP tasks um and it's a kind of nice technique that builds out of a lot of things we've seen so far that's it for this segment\", metadata={'source': 'hznxqCIrzSQ'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about connections between the skip gram method for producing word embeddings and other techniques and sort of related ideas that have come up in the literature so recall that the skipgram method predicts a word of context based on each word we have in some big corpus up to a distance k i'm going to show you another formulation of this which is that we take our kind of word and focus which here is bit and we could think about this prediction as multiplication by a matrix w and then applying a soft max and this returns a distribution over the vocabulary which reflects a distribution over context words given um again this kind of word in focus uh and so you know for this example we have bit dog as one example and bit the would be another example when we take window size k equals one and the parameters here are the same as the other version of it where we just think about it in terms of vectors the only difference is that now this matrix w is what's encoding the context vectors but you should be able to verify for yourself that essentially this embedding of the word bit this word vector then gets kind of multiplied by each each row of w and so we can think about those rows of w as context vectors corresponding to each word in the vocabulary and so why is this technique called the skip gram model well because if we think about window sizes larger than one we would be having pairs in our uh training corpus like bit man and so we're predicting the context word man skipping over this intermediate word the and this is related to an idea from engram language modeling we'll come back to engram language modeling later and the idea was when you want to predict a word you know based on some other words in the context but not kind of immediately adjacent ones that was called a skip grand model so we can think about very similar approaches to tackling this problem and tomas mikulov at all in their work in 2013 looked at a related technique called the continuous bag of words and this almost in a way kind of flips around the skip grand model now instead of thinking about predicting a words context from that word we're thinking about predicting the word based on its context and the only difference here is that now we can think about using multiple words of context so for example if bit is our focus word now it's what we're trying to predict so we have the and dog embeddings from those we just add those up to get a single embedding of size d and then everything kind of proceeds the same as before we have a multiplication by a matrix w soft max and uh now we have a prediction where in this case what we're trying to predict is bit and this gives us a distribution over the vocabulary so mathematically we can write that like this where we have context embeddings for w minus one and w plus one i'm just using those as placeholders for saying the previous word and the next word embed those with these context embeddings multiply by w and softmax the parameters of this are the same as before i've i've kind of flipped the colors here but you can see that the context vectors are now what is kind of at the input layer and this matrix w now reflects the word vectors but again these things are kind of you know symmetric in a way so which one you call word and context is not is not super important it's really just notation this technique performs very similarly to the skip grab model so we're not going to talk about it in much more detail but it goes to show that there's multiple different ways of framing this idea of learning embeddings based on just sort of co-occurrence in a large corpus and there's several different reasonable things you can do and the last thing i'll say about this is that here we show an example again with window size k equals one but you could do the same thing as in the skip gram model where you can go up to progressively larger window sizes and incorporate more words of context in this initial sum so we could look at these two formulations here and the problem with them from an implementation standpoint is that this matrix multiply and softmax over the entire vocabulary is very slow to compute if we think about what's required suppose we have you know a billion words on the web that would give us roughly a billion training examples let's say but then for each one of those what we need to do is we need to multiply by this uh if our vocabulary size is 10 000 which is actually pretty small for one of these vocabularies but if it's 10 000 we have to multiply by a 10 000 uh you know matrix with 10 000 rows in it and take a soft max over that um which starts to get very very slow so the problem here is that we're using the standard softmax where we have to compute all a dot product for each word in the vocabulary so we can leverage this idea called the hierarchical softmax this is one way to make things faster and the idea here is rather than having a single kind of ten thousand way branching decision essentially we instead have a bunch of binary decisions and these form a tree and at the leaves of the tree are the words in our vocabulary so for example here we might have v and a uh at the leaves here and then the tree might kind of continue over here so we can think of this as basically taking our vocabulary doing some kind of encoding scheme like huffman coding to convert it into a sequence of binary decisions and then we can model those binary decisions with binary classifiers so in order to actually get to a leaf of this tree we now only need to make log v binary decisions for each prediction that we want to make so obviously significantly better than order v the number of dot products that we need to do in computing it is much smaller but um it's worth noting that the number of parameters is the same because in order to have a binary tree with uh with v nodes here um we end or with with v leaves to encode the entire vocabulary we end up having v minus 1 decisions in it so essentially the same number of parameters as before this is one technique for taking these methods and speeding them up another one that's a little bit more common is what's called skip gram with negative sampling and the way we could think about this is okay well we formulated the skip gram objective in the continuous bag of words but i mean we had a lot of degrees of freedom when doing this right did we even need to necessarily define our embedding optimization problem in this particular way and the answer is no and here's another way to think about forming formulating this problem which is that we can take word and context pairs and classify them as quote unquote real or not and so real pairs come from our data so for example if we have bit and the and we see those together that constitutes a real pair and then we can create negative examples by taking you know the word bit and saying okay what's a sort of fake context word that i could make up for this and i'm just going to sample some random word maybe i get cat and so we call this a negative example now this is an example that actually could show up in our corpus so in some sense it's you know like it's not a kind of hard negative and that we know that uh you know this could show up elsewhere um but if we if we kind of go through this process we'll we'll sort of get uh you know a collection of uh both positives that we collect and negatives that we sample and some of them will be good and some of them won't be and then we can formulate this as binary classification where we consider the score of a pair to be the dot product of the weight vector in the context vector and we just use standard logistic regression to say is this a positive example or a negative example and so we get the same effects as before which is that a word two words is similar context should select for uh you know similar basically similar other words and if we optimize this objective it turns out we can kind of achieve the same effects and again we don't care about this actual classifier what we care about are the embeddings so from the perspective of learning embeddings this is a viable way to do it so the number of parameters is the same as before we've still got word and context vectors and the objective now looks like this thing where we are maximizing the log probability of uh the positive label associated with real pairs that's what's on the left here and then we have a set of uh k negatives here where the for each negative we want to uh well maximize the probability of it being labeled in the negative class and so what we've written here is as sampling the sampling the word whether you sample the word or the context doesn't really matter all right so this is one of the more common techniques for doing this kind of uh for you know in this uh mykolov style uh contiguous bag of words skip gram formulation this is one of the most common techniques just because it's the most efficient we're now going to connect up these techniques that we've seen with a couple of uh ideas from that encode the co-occurrences as a matrix and are going to avoid our dependence on the size of our corpus altogether so here's how that's going to look like so one thing we can kind of reflect on our current technique is that the skip ground model is really in some sense operating over counts of word pairs right uh you know what we've done is we look at our corpus and we compute these counts of okay bit and the i saw that here and then i saw you know bit cat later whatever and so we can think about boiling down our corpus the entire web whatever into a set of counts like this and then what comes out from this procedure is a set of word vectors and context vectors which have these dimensions now this if we if if we look at this this really kind of looks like matrix factorization and so the question is can we interpret it this way and the answer is yes so uh omar levy at all in 2014 uh kind of worked worked this out where if we assign the cells of this v by v matrix to have the following uh values um the ijt the ijth cell has a value being the pointwise mutual information of wi and cj minus log k where log k is is like the k from negative sampling um then what we recover it turns out is going to be the skip gram with negative sampling objective and so this pmi term looks like this uh it's basically the probability of wi and cj co-occurring which is just the count of those pairs divided by like the total number of pairs in our data set d and then in the denominator here we just have the number of the or the fraction i guess of occurrences of wi and cj each and so the skip gram objective with negative sampling exactly corresponds to factoring this matrix under two assumptions the first is that we sample negative examples from the uniform distribution over words um and uh the reason actually sorry that's a mistake this should be the uh the unigram distribution um meaning the counts of each uh bit you know basically the number the number of times you see that word normalized by the corpus and the reason uh the reason you know connecting this up to the pmi thing above is that uh you know we see each context a certain number of times and then if we sample the negative examples uniformly at random that kind of gives us this independent process for picking a context word and then a word and so that kind of gives you the denominator here for your negatives and then the numerator looks like the positives because it looks like true pairs from the data set uh and the other thing you have to do is make this a weighted factorization problem where the cost of reconstructing each cell in this matrix depends on the uh frequency of uh or basically the kind of weight associated with the uh the word is in that cell so this is kind of a this is kind of a nice connection and it leads us into uh the kind of main technique that gets used here um which is called glove for global vectors this is a technique that also operates on the counts matrix it doesn't view it as a factorization problem but instead just as a regression problem so here what we are doing is we are trying to reconstruct the values in the cells of this matrix where the values here are these log counts and the regression works by using uh the word and context vectors and then also uh some constants here and so the uh you know essentially what essentially the kind of nice thing about this technique is it enables us to break away from uh having to again if we have a billion words in our data set having to iterate over a billion training examples instead we just need to look at this word by word vocabulary matrix and do a regression on it and this ends up being a lot more efficient so it's quadratic in the vocabulary size but ends up being constant in the data set size and this lets it scale more nicely this is probably the most common method for using word vectors today it has a huge number of citations and those started coming soon after it came out so it very quickly became the standard technique in this space all right i'll just mention two last related techniques the first is called fast text this is the same as skip gram with negative sampling but it looks at each word and breaks it down into a set of character engrams for example uh when we have the word where we could decompose that into three grams four grams five grams and six grams like this and then instead of representing a word by a set of or by a single vector we represent that word as a sum over the character engrams within that word so the nice thing about this is it lets us actually deal with words that we've never seen before because we can now embed them by summing up their engrams so that's pretty cool it turns out that this technique uh you know as as nice as it is uh sort of fell by the wayside with the advent of pre-trained models uh so these are models that we're gonna come back to uh later in the course but the way they deal with uh the problem of rare words or uh needing to generalize to things we haven't seen before is by encoding what are called sub words um so for example we have this these snippets and there were no refueling stations anywhere or one of the city's more unprincipled real estate agents and in this case refueling and on principle these are rare words and what happens is they get broken up into these little chunks that end in semicolon sorry that end in underscores and so any word is either in the vocabulary like most of the words we see here or it can be expressed as a sequence of sub words that are in the vocabulary so we don't have to think about kind of summing over all possible engrams instead we have a canonical segmentation for each word all right we're not going to talk more about these embedding techniques now rather than just assigning a fixed embedding to a sub-word the main power of these techniques is that they use uh techniques like recurrent neural networks and transformers to actually compute those embeddings in a context-aware way so it's not just like a big table of numbers like we've got in our other embedding techniques but we actually need to run some sort of heavyweight model in order to compute them uh and these are these techniques are all going to be learned through language modeling it doesn't look so dissimilar from what we're doing with skip gram and continuous bag of words in the sense that we're going to be you know predicting a word based on either the words that came before it or the words around it or something but we're going to come back to uh language modeling ideas later uh in the course and so for here you know the main takeaway is that there's many ways to think about going from a large corpus to a set of word embeddings and we've seen a few of those here and this should give you an idea of what the standard techniques in the space are doing and what their differences are and that's the end of the segment you\", metadata={'source': 'gpP-depOUwg'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about one of the sort of lingering problems with the word embedding technique from the standpoint of bias and fairness so the way that we train word embeddings is we take some very large corpus usually as much data as we can get so you know basically as kind of text from the web in whatever language we want to deal with but the problem is that this text is not neutral or somehow drawn from some uh you know underlying distribution that uh is is just kind of linguistically pure right this data encodes biases about the real world and the concern from the standpoint of word embeddings is that we are at a very early stage in our nlp pipeline encoding those biases into our model so let's talk about what that kind of means one way we can analyze this is we could take the words she and he and look for them in our vector space and take the difference between them and what this finds is this finds an axis that should roughly correspond to notions of binary gender here if we are kind of just looking at these two genders and we can look at words that are most heavily associated with she and most heavily associated with he and we find that the uh the the occupations once we filter these words to just be occupations kind of reflect stereotypes about uh you know what historically men and women have tended to do in society for example homemaker is one of the most extreme she occupations and we can find similar things for uh kind of race and religion as well so these models kind of encode these associations based on the data which you know it shouldn't be surprising if you think about well okay maybe this is just scraping data from newspapers right and you know a lot of newspaper articles are talking about you know what jobs people had in the you know 1980s right if it's data that's a little bit older and it's naturally going to have these kinds of biases so um there's some of this work that uh you know first of all it assumes uh it assumes a gender binary here so it kind of doesn't fully handle this problem certainly but they tried to at least take a crack at de-biasing by looking at this she he axis and other gendered words like woman and man and identifying this uh you know this kind of thick line here that they said roughly corresponded to the um you know the axis of of gendering these things then what they did is they took words like homemaker they projected them onto the subspace and then they subtracted off that projection from uh the words from the original word embedding so find this new embedding homemaker prime and so you notice that this is kind of centered homemaker now along uh along this axis and you know of course this is in much higher dimensional space than two dimensions so the picture looks a little bit different but the idea is to take all these words that we think or we don't want our model to associate with particular gender and kind of bring them back to the middle here this is a nice technique um it doesn't work there's some results from this but the problem is that in these word embedding spaces everything is kind of tied up together right it's not just about where the occupation words lie but it's about you know where homemaker lies with respect to like you know child or caring or something like that and you know when you do the even when you kind of do this process you still end up with other dimensions of of meaning that are are kind of reflecting this uh this bias and these differences and so the bias is not just this kind of local thing that we can get rid of but instead it's a it's a it's a kind of pervasive property of the space that's very hard to scrub from our data and so this is a major ongoing challenge in nlp there's no easy answers for it uh if we need to train on large-scale text corpora there's no way we can get ones that that kind of don't reflect bias somehow and so there's work on both improving this from the standpoint of pre-trained models and things like word embeddings improving it in our kind of final downstream classifiers by thinking about how to evaluate it and how to kind of correct for it even if it does exist in the embeddings but it's a major ongoing area of research that uh we don't have many standard techniques in the repertoire for yet that's the end of this segment you\", metadata={'source': 'J_227g77Jqg'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to take word embeddings and actually apply them in a basic neural net architecture for nlp the deep averaging network so the typically the way that embeddings get used is in the first layer of the network the first step we do is we take uh our indices of words essentially we've we've uh kind of mapped them into our vocabulary and then we embed those words we uh we use our embeddings to convert them into vectors so there's actually a few different ways to do this the first is that we learn these embeddings as parameters from our data meaning we don't use any kind of pre-trained or vectors that are learned over the web instead we just randomly initialize all of them and learn them with back propagation and this actually can work reasonably well if you have enough data in your in your data set you can learn the important relationships between words purely from the data without uh needing to appeal to this kind of external resource of course what it can't do is let you deal with words that aren't seen in your training set and things like that so the second approach is to initialize these word embeddings with glove and then keep them fixed meaning that we take glove vectors and i mean again you can use any of the techniques here continuous bag of words skip gram whatever you take those vectors uh your first layer of the network just looks up that glove embedding and then sticks it into whatever is going to happen from there and so this is uh this is sort of slightly more efficient from a training perspective because we don't need to update the uh we don't need to update the word embedding layer at all um but of course it's a little bit inflexible if the glove objective doesn't actually learn exactly the right structure in the vector space for whatever task you have um it's not necessarily going to work great and this is true for sentiment where a kind of famous problem with embeddings is that antonyms like good and bad often end up kind of closer than you would want because they can occur with a lot of the same words like you know good food bad food right both of those show up so the third technique uh is to initialize with glove but then additionally fine-tune the embeddings during learning meaning uh kind of back propagate your gradients into this embedding layer as well and whether this works better than approach two or approach one like this really depends on the task and the specifics of the amount of data that you have but for tasks like sentiment this is often the the most standard way to do it we can so when we think about applying these word embeddings we can think about evaluating them uh intrinsically so-called on tasks like word similarity meaning we just look at the embeddings and say okay how good is this embedding for as sort of for embedding sake this is not typically how we're going to think about embeddings um typically what we want to do with embeddings and then also with pre-trained models like elmo and burt is do kind of real downstream tasks like sentiment analysis for example so we're not going to think about taking uh you know so much about these intrinsic evaluations but instead we're going to think about which embeddings are best from the perspective of when we plug them into these downstream tasks what works best all right so let's talk about about a method for actually plugging them in so this is a relatively simple technique for sentiment that works surprisingly well um it's a network called the deep averaging network due to mohit ir at all and the idea is quite simple it's a feed-forward neural network based on an average of the word embeddings from the input so we have uh an input predator as a masterpiece uh each word has an embedding you know using one of the approaches from the previous slide and we average those just straight up arithmetic mean and then we pass those through one or more layers of a feed-forward network and then use a softmax to actually make our prediction so the the kind of key thing is that the feed forward network here can learn you know fairly sophisticated interactions between the different components of this green vector here however what it cannot do is learn about complex relationships between the different words in the input so at the time that this came out there was a widely held view that we need to model the syntactic structure in order to uh represent you know and and solve this kind of sentiment task effectively people were building networks that looked like this on the right where we have this compositional structure to it where it builds up uh a representation of the sentence following a syntactic parse tree which we're going to come to a little bit later in the course but what it turned out was that at least in 2015 the simple averaging technique can work as well as this kind of syntactic composition for these problems and i think really what this showed is not that syntax is useless but that at the time these methods were not actually really leveraging it and we're not doing more than uh this more basic approach can do so if we look at the kind of results on this from this model uh the deep averaging network here um no so this was actually not using pre-trained embeddings at all so is approach one from the first slide and this was uh outperforming some of these very strong bag of words methods sometimes and it was also you know about on par with these tree-structured neural networks which had seen a lot of a lot of work in this uh in particular this convolutional neural network approach from kim in 2014 we see that the results are sort of roughly in the same ballpark and so this indicates that like you know in fact a lot of these techniques that were either building up a tree structure using convnets weren't actually achieving much better composition than just simply averaging these vectors together so we could take a look at what this method is is kind of unable to do and you know maybe this this will then surprise us to how good these results are so we have here a few examples and we have that that are all uh ground truth positive and the deep averaging network's predictions and this uh deep recurrent neural network's predictions which uses this tree structured composition we see that for all of these examples the deep averaging network uh or the deep averaging network should be predicting positive and it gets positive for the first one which kind of makes sense given that there's you know a fair amount of sort of positive stuff in here but then for the next two it predicts negative it's not able to uh you know it's not able to deal with this kind of mix of positive and negative things and appropriately judge how these things combine in the final assessment and we can look at a a kind of more targeted case here the movie was not good good bad not bad and uh it's not able to correctly learn this structure either basically not and bad both end up meaning negative and so it predicts negative three in three out of four cases which is not correct um so we'll come back to the ideas of compositionality once we get to some syntax uh and start talking about tree structures and also when we talk about uh recurrent neural networks and long short term memory models or lstms but for now this is a you know this is at least one way of taking the word embeddings we have so far making predictions uh and it works in i would say surprisingly well for this task and that's the end of this segment you\", metadata={'source': '3pwwdHuH0I4'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about engram language modeling language modeling is one way of placing distributions over sentences and there are many ways to place a distribution over sentences we could for example do it with a probabilistic context-free grammar which models a tree but also we can think of as generating a bunch of words in a sentence n-grams are going to do something a little bit simpler we are going to say that the probability of a sequence of words which i'll write out explicitly as w1 through wm i can be written as just the product of each word the probability of each word conditioned on the previous words using the chain rule so essentially each word conditions on all of the passwords so far and this is just true because of the chain rule of probability now an n-gram language model makes a simplifying assumption about this distribution that's going to allow us to actually model it and do parameter estimation etc so an n-gram lm defines p of w in the following way as the product of i equals one to m it generates word wi conditioned on the previous n minus one words okay so we didn't get very far in the expansion above but once once you get up to say w10 it's going to condition on all past nine words and in an engram language model we simplify that and only condition on the past n minus one and n typically has values that are something like you know three through seven or something like that so let me just illustrate what a two gram lm looks like we're going to use this bracket s as a start of sentence symbol here so when we generate the first word i'm just going to write it that we condition on a start of sentence symbol rather than having like a separate distribution over words here this way just everything looks like two grams and so on and so forth so you notice that w3 is conditionally independent of w1 given w2 and so an n-gram lm basically corresponds to a n minus one order markov model and so in this case for example we recover our so-called first order markov model that we saw in hmms for part of speech tagging each in that case tag conditioned on the one before it and here each word conditions on the one before that one all right and so then a three gram language model just to make it completely explicit we're now going to have two start of sentence tokens and so on and so forth so what we see here is that uh we've dramatically simplified the amount of context that each word looks at and so we're going to kind of think about whether statistically this is a good assumption or not from a modeling perspective it throws out a lot of context that we might otherwise find useful we're going to look at some cases later where look being able to look back many words is probably important for knowing what word's about to come next however what it does is it can allow us to model this in a fairly simple way and dramatically reduce the number of parameters that we need so the typical way that we do this i'm just going to take the case of a 2 gram model is we use multinomial distributions so what we have is where we're going to let v denote the size of the vocabulary we have v by v parameters and what do these parameters look like the parameters look like a table of probabilities for each possible word that can follow a context word for so for example if we look at uh what words come after the we're generally going to end up with some very flat distribution over about over the following words because if we only see the word the we have no idea what's coming up next and so we're not gonna have a lot of constraints to tell us oh okay we're in some very specific context that only like one word can come next no we generally these have sort of the same property as skip ground models we're going to draw some additional connections later but these are the same property as skip gram models in that it's a very under constrained problem we don't know exactly what's going to come up next and so even a well-trained model is going to place some very flat distribution like this all right so the way we estimate parameters is by maximum likelihood estimation or mle from a large corpus now like we saw in the case of hmms there's a problem here in that we're not going to necessarily see every pair of words right and once you get up into higher order engrams you're certainly not going to see a large number of you know words that occur after this set of four words in the context so we're going to need to look at smoothing techniques which we'll do in a future segment but for right now we're just going to say that p of dog given the is going to be defined by count of the dog over count of the in some big corpus so it's however many times we see the and then how many times did we see dog continue that if it if it came afterwards uh ten percent of the time then we get this probability being one tenth and that's the maximum likelihood estimate here all right so what we could do now is if i give you a large corpus of data you can compute these counts for various kinds of engram orders and then compute these language modeling probabilities but as we've said this doesn't this isn't some sort of uh magic trick in that it doesn't tell you uh it doesn't tell you what word's going to come up next it's only going to give you a kind of rough sense a rough probability distribution so why is this useful so there are a few things that ngram lms have historically been used for the first is generation and specifically in machine translation typically the way a phrase based machine translation system works is you have a bunch of candidate translations of chunks of the source sentence whatever sentence you're trying to translate into another language and when you put those candidate translations together you're like assembling these pair these like phrases you're kind of gluing them together in this pairwise fashion and what you use an engram language model for is to assess okay the stitching together of all these individual phrases did i do that in a way that is uh kind of gives me plausible language in whatever language i'm translating into in that it should have reasonably high probability under one of these language models another application is grammatical error correction so if we want to build a system that can take some text that might have some grammar errors in it and figure out uh let's say you use the wrong choice of articles somewhere what we can do is we can try checking okay do other articles like dramatically increase the language modeling probability so can it help us find text that's ungrammatical but more like the the or something like that right which should have pretty low probability under one of these models uh and the final application is not of engram language modeling per se but of language modeling more general um it's a way to build uh what i'm going to call word to vect plus plus and so language models basically the task of trying to predict the next word in general requires drawing out a lot of cues from text and just like we saw that skip gram is a useful way of learning word embeddings by just saying okay what other words can occur with this one we can take this language modeling task and turn it into something more sophisticated and then use that to uh and then use that to build a more powerful set of word embeddings later so for now we've seen that we can take a corpus estimate these probabilities from it and build a simple engram language model and that's it for this segment you\", metadata={'source': 'J-yHbD8LYCM'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to smooth engram language models so the reason we might need smoothing is that when we build engram lms we are basically counting uh word counts in a large corpus and so when we have like a 2 gram or a 3 gram model this can work fine we can usually see most of the triples of words that make a lot of sense especially if we have enough data but actually five gram models are more kind of in the range of n that we want these actually can work pretty well and to just kind of illustrate this if we think about the distribution of words after two so this would be this would be a two gram model versus the distribution of words after go to versus the distribution of words after want to go to or after hate to go to then we can see that the distribution of words based on wanting or hating to go somewhere is probably going to be very different maybe we hate to go to class but we want to go to austin and so having this additional context is generally going to be very useful for these models so we're going to see that recurrent neural networks allow us to get this in one fashion but before people were using recurrent neural networks heavily for this task there was this trade-off between how many words of context you used and how difficult it was to estimate your parameters so this is what smoothing uh is supposed to do so suppose that we have the probability of austin given two um you know we're we is assuming we have a big enough data set that contains the word austin we're probably going to see it in some construction of 2 austin and so this is going to be greater than 0 because it's seen in in the data but this only holds for a 2 gram model if we think about the probability of austin given want to go to a lot of times this is going to be zero if the corpus isn't huge and even if you do have a huge corpus uh you know are you necessarily going to see want to go to hate to go to like to go to etc every combination of it with like every place no you're not so the reason for that is just because we don't see uh we don't see this five gram want to go to austin occur with non-zero count so this is going to be the idea behind smoothing so there are many techniques for figuring out how to reserve some probability mass and basically like give a small amount of probability to these unseen instances so we can you know not judge them as totally impossible if they come along in future data so the scheme that we're going to talk about is called absolute discounting i'm just going to give you the high level picture of what's going on here so you understand what this family of techniques looks like the idea is that we reserve mass from seen five grams to allocate to unseen five grams all right and so concretely what this means is if we have p of austin given want to go to it's defined in the following way i'm going to abbreviate this so this is the count of want to go to austin over the count of one to go to so this is just the maximum likelihood estimate of the parameters here get given the counts drawn from a big corpus so what we are going to change here is we are going to subtract a constant k and we are just going to say that 0 is less than k is less than 1 here and then we are going to add lambda so i'm going to write this as p-a-d to indicate that this is this absolute discounting distribution plus lambda times p-a-d of austin given to go to all right so lambda here is set to make this normalize so let me show you an example of this so if we have the context want to go to maybe we observe this four times in our data and we see want to go to maui twice we see want to go to class once and want to go to campus once we're very studious here so what we do is if k equals 0.2 then we are sub essentially think of it as subtracting off from each of these counts and so our kind of denominator here the count of want to go to is four right we've seen it four times with uh you know with these three unique contexts and so lambda here ends up being point six over four it's the number of word types seen in this context times k that's what the that's what the numerator here is and the denominator is just the count of uh count of want to go to and so what this has allowed us to do is it says all right i we have these four occurrences right but instead of saying we have a 50 chance of maui 25 chance of class 25 chance of campus instead we've decreased all these counts a bit we've we've turned this down to you know whatever 1.8 over 4 is chance of maui 0.8 over 4 chance of class 0.8 over 4 chance of campus and now we have this 0.6 over 4 times a lower order model so this up here is the three gram uh actually sorry four gram absolute discounting probabilities for uh austin in now a shorter context so the nice thing about this is that we can do this recursively so then uh pad of austin given to go to uh is you know basically a big expression uh the the sort of count expression before plus i'm going to call it lambda prime times p a d of austin given go to and if you keep if you keep like unrolling this uh eventually you just get uh the probability of austin at the very end and then this is all this is always gonna be greater than zero if austin is a word that's shown up in our data but likely we're only placing a distribution over words that we actually have seen so the the key property of this is that it allows us to basically allocate some probability mass to unseen events and it backs off in this very natural way where we we first kind of say okay we're not just going to like throw out all the information we're going to successively decrease the amount of information that we're looking at and get a sort of accurate estimate as a result so one of the techniques you may see in the literature is what's called kinaser nye smoothing kinaserkni smoothing is very similar to absolute discounting there's one kind of extra trick that they use where these lower order these lower order probability distributions depend on essentially the number of unique words seen in a particular context because the assumption is when you're seeing a new word what matters is what's called the fertility of the context how many unique things can come after it so go to would be a very fertile context because lots of things can come after it so they they kind of adjust the probabilities in one additional way but uh if you ever do any kind of engram language modeling it's probably going to be uh with canaser 9 which uses roughly this framework so this is not something we're going to be exploring too much more in the rest of the course but this gives you an idea of what kind of techniques get used here in order to let you estimate these language models and that's the end of this segment you\", metadata={'source': 'Yfug5eIQh5w'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about evaluating language models so we've talked about language modeling as a next word prediction task and so given that it's a prediction test you might think well should we just check the accuracy of those next words that are getting predicted but accuracy doesn't really make much sense we've talked about how language modeling is sort of impossible in the sense that you never really know what word is coming next you can try to just place a probability distribution over reasonable possibilities so it'd be a little weird to talk about oh this language model got five percent accuracy and then going up to 5.5 accuracy is like a great advance so we're going to need a different metric the the typical metric that people use is very similar to how these models are trained which is likelihood of data or log likelihood now instead of using the training data which is the training objective we're instead going to look at log likelihood of held out data and we also average over the number of tokens in this data to kind of normalize for the length so basically if we train our language model on a big Corpus of a bunch of documents then what we can do is take a few of those documents hold them out and evaluate the probability that the model assigns to the tokens there now the way that this actually gets reported is in a slightly different metric still called perplexity perplexity is the exponential of the average negative log likelihood so this is a metric where lower is better so if we think about log likelihoods they're often like minus two minus three things like that so negative log likelihood is going to be a positive number then when we exponentiate that we're going to get a kind of higher positive number now you might think okay this is just this weird transformation perplexity actually kind of has a nice interpretation which is that suppose that we're predicting all four words and we give them probabilities a quarter a third a quarter a third now the perplexity has an interpretation of the average branching factor of this model and you can think about that as the average of these denominators now rather than the arithmetic mean the perplexity computes the geometric mean since it's averaging is happening in log space but basically the perplexity of a language model assigning these probabilities is 3.464 . so it's a somewhat intuitive metric that said uh these numbers are usually only reported in language modeling research and not used much elsewhere and you really have to kind of interpret them with respect to other systems but I did want to introduce that these numbers will range anywhere from 10 to 200 depending on the application you're looking at so you can kind of be aware of the scale and how this metric is used that's the end of this segment\", metadata={'source': 'ImW4vJ5XZQc'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about neural language models so from the definition we had before we can define the probability of a sequence of words as uh the probability of the first word times the probability of the second given the first etc and so on and so forth for n-gram language modeling we made the assumption that only the n minus the kind of previous n minus 1 words matter we're not going to make that assumption anymore instead what we're going to say is how can we model this distribution p of w n given w 1 through wn minus one in a smarter way in particular rather than using these uh relative frequency counts based on uh you know that we had in the n-gram models can we use a neural net to predict the next word conditioned on maybe more of the context than we were able to use before so the reason that uh the reason by the way that people didn't explore this for a long time in the literature was just because when you have a very large corpus this kind of training gets very slow so this idea has been around for a long time but wasn't necessarily explored because it didn't scale as well as the counting and normalizing scheme and grant models all right so we'll start with a very basic uh neural lm and this should look familiar basically the probability of a word given a previous word so we are appealing to something that looks like the skipgram model where we have a vector for the current word wi uh and a vector for uh let's say a context vector associated with uh the the word at position i minus one and in the denominator we have to sum over all the possible words we could be generating at this point so this kind of gives you an idea of uh the way we might approach this so skip gram obviously only uses one word of context and in general skip gram is about modeling uh were you know words that are possibly larger distances away or both sides so this would be a specialization of skip gram to this particular setting so but this gives you an idea of the kind of way we can approach this and so more generally the way we can parameterize this is like pwi given w1 through w i minus 1 might be a soft max of i'm going to write u w i here dotted with some function of w 1 through w i minus 1. so what this this function f could be a neural net to embed the context so there's a couple of choices for ways that we could do this uh so from what we've seen so far we could have f be something like a deep averaging network the nice thing about this is deep averaging networks for tasks like sentiment analysis work very well but the problem is that this ignores the order of the words in the context it's going to treat wi minus 1 and w1 the same because it just sums everything together and so this is not going to do a good job of capturing the idea that probably the word that's immediately before wi has a highly constraining effect on what wi should be and the word that's you know 15 positions back has a much less strong effect we could also explore using a feed-forward neural network and so what this looks like is let's say our context is i saw the dog we have you know some sort of neural net here that embeds this into a vector and then we put through a final matrix multiply and softmax and that yields the probability of wi given w1 through uh wi minus one all right and so uh this has been explored in prior work uh by menin hinton and uh quite a while ago back in 2003 the problem with this is that it also has the same problem as engram language models in that it doesn't necessarily scale to long contexts so if we for example have uh if if we want to include 20 words of context suddenly we need a very large neural network in order to do that and the number of parameters sort of grows in the amount of context that we want to use so these are both viable approaches for uh for building neural language models but where we're going next is to look at how recurrent neural networks can solve this problem what recurrent neural networks are going to allow us to do is they're going to allow us to break away from this dependence on a number of parameters that like has to scale with the amount of the input that you're looking at or the amount of let's say context for the language modeling problem and so that's going to be very very useful here because we're going to be able to build models that can just look at arbitrarily long contexts it may not use all the context but that's okay it at least theoretically scales to be able to use it all without running into the computational problems that engram lms and these sorts of feed-forward neural network-based lms give that's the end of this segment you\", metadata={'source': '59NrmwAdOWA'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about recurrent neural networks and why they might be a good idea for language modeling and classification and things like that and as a bit of a spoiler we're also then going to talk about sort of some of the Thorns with them and why we're not going to focus on them as much and instead going to use a different architecture called Transformers so feed forward neural networks as we've seen they don't do a good job of handling variable length inputs and there's some reasons that are purely practical but uh sort of conceptual one is that they assign each position in their feature vectors of fixed semantics so suppose we take four words the movie was great and we concatenate those four word embeddings into a single feature vector now if we learn some parameters there's going to be a certain set of parameters associated with position four in this Vector that are going to learn to model the fourth word here then if I get a new sentence like that was great this great is kind of com in a completely different position in the vector than the other great uh sort of technically these are in orthogonal subspaces of the vector space and so the model is not actually going to share any parameters related to these two usages of great and that's not what we want instead what we want to do is we want to kind of process all of these words somewhat uniformly right if we're doing something like sentiment analysis or even language modeling knowing that the word great is in the context somewhere is kind of useful and it would not be nice to be able to do that without having to like observe it in every single position but we still want to make the uh function that we're using context dependent we don't want to go as far as a deep averaging Network in terms of just erasing all Notions of position so rnns are uh theoretical way to do this so an RNN is defined in terms of a cell that takes an input X which you can think about as a word and the cell has a state which we're going to call H there's a previous state that's kind of coming in from the history of the RNN and the RNN is going to do some computation and produce and output Y and a updated version of the state H based on the input in the previous state um now sometimes you'll also see other variables like C for cell state which are used in architectures like the long short-term memory Network um so I'm going to just kind of optionally put that here in the graph okay so what's the sort of benefit of this well we can instantiate this architecture over the sentence here and it kind of consumes each of these words and then produces an output and basically if we want to know that this output is positive which here we're going to denote by Blue then the model will see the word great at some point and it can kind of flip a bit in its representation and produce an output that's aware of the fact that the word great showed up here and when I talk about this model also being able to use context we can feed in a sentence like this and the word not here the model can kind of recognize that oh okay I'm going to change something in my context because I just saw not then when I see great I know not to sort of go Blue that is not positive but I'm gonna I'm kind of in a negative uh sentiment sort of State still so theoretically if the model had seen great in two different positions it would do the right thing but it's also able to account for the fact that not is here all right so the issues with rnns that I mentioned stem from a problem called The Vanishing gradient problem so I've listed here a formula for computing the updated uh hidden state representation based on the input X and the previous hidden State ht-1 and B is just a bias term so we're not going to go through this formula in detail but the really key thing here is that there's a tan H involved and what that looks like is this and what that means is that when you're trying to do back propagation if the input to the tan H is kind of very far out on either end of the spectrum the 10h function is very flat and so if you are doing back propagation and if your values for the tan H are kind of out towards these extreme ends of the spectrum then the gradient is going to be kind of very diminished as you go back through the network and this is going to be a problem for learning how to remember pieces of information over many time steps there's another problem here which is that if you just sort of ignore the tan H and you look at what's going on with the H's we have h t equals 10 H ignoring that of V times HT minus 1. so over time as you scroll through a sentence you're repeatedly multiplying by this V Matrix many times so what that can do is that can either cause H to kind of blow up or Shrink depending on what the eigenvalues of V are that's also going to cause problems and so we have kind of multiple reasons why it's going to be hard to have gradients kind of give us information over long time scales of these Networks now this was a part of the sort of observation that led to the development of architectures like long short-term memory networks that do better at this but they really don't still don't help enough it's still hard to learn over say a thousand word uh kind of time scales of information for things like language modeling and rnns are also slow and they're kind of slow in a bad way which is they don't parallelize well if you're going to encode a sequence you have to fundamentally go through order and kind of steps that don't parallelize because you have to read each word in and update the hidden State accordingly what we're going to see is that Transformers solve both of these problems they kind of index into the context very effectively and they also parallelized very effectively so we're not going to have these uh kind of long sequential uh chains in our computation graph that are going to cause things to be slow so we're going to kind of put rnns to bed here and turn our attention to Transformers for the rest of this course that's the end of this segment\", metadata={'source': 'xvnnA04JVQo'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about attention and that's going to be our jumping off point to kind of getting into the Transformer architecture so the Transformer the paper that introduced Transformers by Ashish vaswani at all was called attention is all you need and so I think that kind of appropriately situates the importance of attention for how these models work and kind of more generally in NLP how much of a change they made so attention is going to be a mechanism for accessing information in the context to be able to make predictions this is kind of the problem that we've been wrestling with as we've looked at feed forward networks Dan's rnns things like that and here we're going to talk about how attention can impact language modeling and how it can be used there and we're going to do that via a running example uh that's going to be kind of cooked up to show you what it can do so we're going to consider these fixed length sequences of A's and B's that look like this and our goal here is to predict the last letter in this sequence now I've constructed these according to the following rule if it's all A's for the first six characters then the last one is going to be an a if there's any B anywhere in the sequence then the last one is going to be a b so the reason that we need something like attention is because it's going to allow us to go arbitrarily far back in the context from where we are trying to make a prediction and that's going to help us answer this question of basically is there a b anywhere so I know I said there would be fixed link sequences but just as a thought experiment suppose we were trying to uh do language modeling over sequences like this now this is the kind of thing that rnns sort of struggle with now it's difficult for these models to like we said back propagate through many layers of computation and learn how to remember things well and so they they'll have a tendency to forget information over long periods of time and we're going to see how attention can fix that I'll caveat that by saying that of course this example is doable you can actually hand construct weights to do it uh if you want to give that a shot okay so let me introduce the basic uh kind of ideas of attention in the context of this example so uh are actual sequence that we're going to use is going to look like this and of course the kind of true next thing here uh should be B right because there's a B in the sequence so I we're gonna have two Concepts that are really important here one is the notion of keys which are just going to be the embeddings of the sequence for now uh and we're also going to have a query which is what we want to find Okay so in terms of embeddings we're going to assume the following word embeddings that a has the embedding one zero and B has the embedding zero one so these are just one hot encodings of these and we're going to call them uh e i so for the attention computation it's going to have a number of steps the first step is to compute a score for each key given the query okay so what does our sequence look like well if we uh use that embedding that I listed here we get vectors that look like this for our four words now the score here is going to be computed by taking the I key and dotting it with the query so this representation allows us to basically say okay what are things that have high dot product with our query let's give those a higher score and in this case we're going to set the query equal to 0 1. and what that's going to do is that's going to allow us to basically find B's foreign scores here are going to do and so if we go through and take the dot product we get the scores 0 0 1 and 0 like this all right so then let me kind of rewrite this over here for step two which is going to be to softmax so we had our sequence a a b a with scores zero zero one zero and when we soft Max that gives us the following Vector of probabilities if we assume E equals three um so again we're exponentiating these things and normalizing them so when you exponentiate zero you get one when you exponentiate one you get three and so uh this this stuff kind of ultimately adds up to six and you get a distribution that looks like this and then for step three we compute output as a weighted sum of the input and so what this means is that the result is going to be the sum of Alpha I times e i from I equals 1 to 4 here and we're going to denote these this Vector of probabilities as Alphas so basically this is 1 6 times the 1 0 Plus 1 6 times 1 0 plus a half times zero one plus a 6 times 1 0 which ultimately gives us half half okay so it feels like we went through actually a whole lot of work to get a uniform kind of split between these two positions which kind of represent A and B right but in reality if we kind of compare uh to an average the sort of average embedding of this would be three quarters one quarter right if we just sort of averaged over the inputs that we were given so what the attention has done is it's kind of Amplified the b-ness of the context here and we could see that by the fact that the weights assign a kind of higher weight to that uh that uh position here right so we were kind of taking a weighted average where the weight is higher here now the reason the weight isn't even higher is because well zero when exponentiated still kind of gives you something right it still gives you one uh so suppose that we actually like really want to make the attention even more peaked than what we might want to do is uh kind of amplify these embeddings more when we do the attention computation and so we can do that by scaling them up by this Matrix WK here uh where this is just a diagonal matrix with tens on the diagonal and so it's just going to multiply all of the values of the keys by 10. so now if you go through the attention computation again you're going to get something a little bit more peaked and I'm going to leave that as an exercise for you to do but you can kind of convince yourself that it's going to give us the same effect uh but something even more extreme so this uses a formulation of attention called dot product attention where the interaction between the keys and the query is done is kind of mediated by the dot product operation in reality what we're using here once we start introducing these matrices is a form called scaled.product attention where you have a matrix that basically forms this bilinear function that computes uh an inner product between K and Q and what we're going to see is actually the case and attention is we multiply the keys by a matrix w k which we saw a kind of teaser for on the previous slide and we're also going to multiply the the query q by a matrix WQ and uh this is a kind of form of attention that is going to get used in basically all the standard Transformers there's other mechanisms for attention that were introduced in settings like machine translation some of these older papers talk about them but we're not really going to use them here and what we're going to see is that in fact there ends up being a third Matrix for values and that's going to be uh sort of a key part of self-attention so we'll come back to that later but for now what we've seen is that we can take these Keys which are each token in the sentence and use a query to form a distribution over them take awaited some of them and then get a kind of output finding the stuff in the sentence that matches the query that's the end of the segment\", metadata={'source': 'q7HY7tpWWi8'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to bridge from the idea of attention which has a single query indexing over a set of keys to the idea of self-attention which is going to be the mechanism in a Transformer that processes an entire sequence at once so self-attention takes each word in a sequence or each vector and it assigns that Vector both roles of key and query at the same time so it's going to use the same idea of attention that we saw before but kind of extended and the way we're gonna start introducing it is to look at these things as matrices so Q is going to be a sequence length by D Matrix where D is going to be our embedding Dimension and for the purposes of these slides that's going to be two and the keys again each word or vector is a both a key and a query so the keys and the queries are going to be the same size so keys are also sequence length by D so now we have basically many uh queries at the same time now in order to kind of represent the fact that what we're looking for and what each token is are not the same thing we're going to dive right back into that idea we saw of multiplying these by matrices to kind of transform them so we're going to set up a matrix here WQ that we're going to multiply Q by and that's going to take this form it's going to have uh basically 0 1 in both rows and what this is going to do is it's going to tell us that no matter what the value of this particular token is we're going to look for B's which correspond to the second kind of position in our in our Vector space and we're going to go back to the idea of using this WK diagonal matrix that kind of boosts the scale of the embeddings that we're dealing with so because we're multiplying q and K by these two matrices and because ultimately they're going to get multiplied by each other there are actually many different equivalent parameterizations that are going to do exactly the same thing that we're seeing here I'm just kind of setting up this one to sort of illustrate things but in reality what a model learns through back propagation is not going to have this nice clear structure okay so let's kind of go through what happens in each of these steps so the first thing that we do is we compute uh well we're going to compute our Matrix Q based on the embeddings being multiplied by WQ and what that gives you is it gives you this Matrix where every row is the same like we said the kind of semantics of this is that every single token is going to be looking for all right are there other B's around remember that's our kind of objective here if we're thinking about making predictions and then the keys get the same treatment but they're multiplied by this WK Matrix and what that does is it just scales up the raw embeddings in e all right so now the scores are going to be computed in the same way that they were before uh we're basically taking an inner product between the queries and the keys but now we're doing it by just multiplying these matrices together and so if we want to kind of break it down in terms of an individual component in one of these matrices uh we end up with the ijth value of this s Matrix that we're going to about that we're about to compute is going to be qi dotted with KJ so essentially in some sense measuring the compatibility or the similarity of the I query and the J key so it turns out if you go through this computation you get a matrix that looks like this it's going to be a square Matrix because uh the sort of dimensions of these things that we're multiplying we're multiplying by a four by two by a two by four and we're going to end up with every row being the exact same thing and the rows represent attention scores so if we think about uh kind of Row one it's saying I care about word three right that's kind of what this row here is telling us um and when we softmax this we're going to see that it puts a high probability distribution there or a high probability in that point of the distribution and this is what we wanted right because our original uh sort of sequence here aaba uh what we care about is finding the B and it turns out we've now successfully cooked up this self-attention computation where now we have a distribution at each position that's kind of placing high probability Mass on the B token and we did it all with these kind of linear operations and then ultimately this kind of soft Max which is going to turn this into a distribution uh per row so yeah so this is a row wise softmax in this Matrix all right so then the final step here is we compute the output which is just going to be this Matrix a uh which is formed to the softmax times e and I'm going to kind of put a little asterisk here which is that this is actually going to be a times e times WV so we're going to use a separate so-called values Matrix to transform the embeddings before we multiply them and take this kind of weighted sum with the uh with the probability Matrix here the attention Matrix a so what that basically gives us is uh the formula that is given in boswani at all for the attention computation we've kind of gone through all the different steps of this so their formula involves q k and V which are computed based on the kind of input embeddings like we've been doing multiplying by these parameter matrices WQ w k and WV and we take this soft Max of Q times K transpose so that we've all seen and then there's one extra little step that they do here which is they divide and rescale everything by square root of DK uh which is the basically Vector Dimension that all of these uh inner products are happening in it basically just has the effect of making the soft Max less peaked it's not too important and then they multiply by V here in order to get the output so they're taking their uh attention Matrix a and multiplying that by V and so what they're getting is the output is a kind of weighted combination of V so this is what we call one head of self-attention so we're going to talk about multi-head self-attention a little bit later but the key thing here is that this is not some just like uniform non-parameterized computation it's parameter parameterized by these matrices wqw K and WV so as you change those matrices through uh learning and things like that what this computation does changes as well all right so in terms of what this produces like I've been saying we get this Square attention Matrix a which is the output of the softmax here uh so this is what's been giving us our our attentions a and then we multiply that by the input or this transformed form of the input and ultimately it gives us something that's the same Dimension as the input so we've gone through all this all this kind of complexity here but the idea of self-attention or the the API for it is in some sense very simple which is that you take a sequence of words and then you get back a new sequence of vectors which are the same length as the original sequence that now represent those words in context and so we have this very powerful general purpose transformation that can kind of contextualize uh representations of structured data all right so we've gone through all the steps kind of mathematically there's this very nice uh blog post by Jay alimar called The Illustrated Transformer which is going to walk through it all uh kind of graphically so I'm just going to show this here and talk through it really quickly you can go read the blog post in more detail if you want to see a another breakdown but basically we have a link to sequence thinking machines are the two words and so we have two embedding vectors and then uh we have these three matrices WQ WK and WV that are going to embed these and ultimately we get Q's K's and V's which are uh now two by three because we've kind of gone from Vector Dimension four to Vector Dimension three uh with these Matrix multiplies and now the softmax operation produces a uh kind of attention Matrix which we're not seeing and then we multiply that by V and as output we get this Matrix Z so I this attention Matrix again is sentence length by sentence length so you see that we're multiplying a two by three by a three by two so we're going to end up with a two by two and then we get a sentence length by hidden Dimension as the output uh and it's a weighted combination of the rows of V so just to show one more uh kind of representation of this attention uh computation this is an example of a kind of character level classification task that and we visualized the attention Matrix that's in kind of one part of a transformer for this task and so again what we see is that we have a square Matrix where each cell is a probability and these probability distributions sum to one over the rows so kind of higher or brighter colors or higher values so you could see that uh when there's like a white cell in the row the the rest of the row is very faint but you can have a like number of red cells if the distribution is more uniform so finally kind of bringing things back around to why are we using this kind of self-attention mechanism why did we beat up on rnns a little bit well the nice properties that self-attention has are the following the first is that it doesn't involve any so-called sequential operations the whole thing is fully parallelizable right if you have a thousand tokens each one has to attend to 999 other things but all of that happens in parallel um so you just put everything through that layer and there's no order of 1000 um sequential dependents in your computation com computation you also can kind of see every other token with a distance of one right like the word at position 847 can see the position at you know 200 word at position 232. so you can access and figure out information from the context a lot more easily than something like an RNN where you have to kind of go through all the different steps to pass the information along um that said the big kind of drawback of uh self-attention as it's presented here is this N squared uh complexity right we have to form these Matrix this attention Matrix that's sequence link by sequence length that's going to be big hard to store it's going to be a lot of memory there's a lot of subsequent work that's kind of tried to make this better and we're going to talk about some of that uh a little bit later but that's sort of the one drawback but other than that there's a lot of kind of benefits of this architecture compared to others that we've seen that's the end of the segment\", metadata={'source': '10l2NXStROU'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about an extension of the self-attention mechanism which is going to allow us to make it more General and more powerful and generally use it as a strong component of modern neural networks so the one of the drawbacks of attention as we've seen so far is that while it can theoretically look at a long context of information and mix a lot of different information together in practice that doesn't always happen uh the distributions that come out of softmaxes tend to be very peaky partially because it's hard to balance the values of different logits very precisely when they're sort of computed based on different inputs so in reality what we kind of want for language modeling in a situation like this is if you're saying I visited new blank we really need to know that this is like a place that's new something and it's a place maybe that I visited or I want to visit right so you know maybe we think that the attention distribution should be something like this where it kind of places equal weight and on visited and new um but then usually what we end up with is something like this where it'll kind of like have much more weight on one than the other and so we're not going to kind of be able to blend information about these two concepts uh together so one solution to this is to just stack layers right we can have many different layers of the network and kind of over time gradually be able to integrate this information together if like layer one looks at new and Layer Two looks at visited but it would be great if we could get this uh capability within a single layer of the model as well and this is what multi-head self-attention is going to allow us to do so we mentioned the idea of an attention head before and basically we're just going to copy paste the whole architecture that we had and do multiple independent copies of attention so we talked about it as a process that's parameterized by wqw K and WV now what I said is that those were matrices that you could just randomly initialize and then we're going to learn through back propagation but what you can do is you could say all right I'm going to randomly initialize two sets of these matrices right uh just have two different wqs two different wks two different wvs four the same sets of inputs now these are going to compute different queries keys and values and so the attention computations you get are naturally going to be different so to kind of show it in a little more detail we take our word embeddings X and we take all of these different matrices in this case we're going to look at eight copies borrowing this figure from Alamar and that is going to then give us eight different outputs of the attention Z zero through z7 here remember that these Z's were kind of linear combinations of the values produced by uh taking the attention matrices from each corresponding q and K so at each point here like these two are going to combine to form uh something we're going to call A1 that's going to have you know some distribution these two are going to form a0 which might have you know a different distribution so at the end we get all these Z's and what we need to do is we need to kind of collapse them all back down so instead of having a three-dimensional embedding for each of two words we're going to concatenate them all together so we actually got a 24 dimensional embedding for each of those two words then multiply it by a matrix and kind of bring it back down to the same size as the input so now we've got our input and output the same size but we've been able to incorporate all this different information from the different distributions that can do things like A1 looks at new a0 looks at visited they can look at this different different these different words in the context for example so ultimately we've kind of achieved our goal here where we can have one head attend a new another attend to visited and this stuff gets all kind of Blended together in this model and turned into a new contextualized representation here and we see now with all these pieces of the Transformer in place it's really enabled us to kind of deliver on this this view here of taking this information and using stuff from all throughout the context in a kind of non-uniform way but still be able to kind of scale efficiently and do so with a small number of parameters that's the end of this segment\", metadata={'source': 'nHXrdLMo8Uk'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about positional encodings these are going to be a key new feature of self-attention multi-head self-attention that's going to let us really deliver on what we want these layers to do and let them let us integrate them into a Transformer architecture that's going to work well so if we're trying to do language modeling and predict what comes next here we need to look back at the context as we've been discussing but one kind of key problem that we're we'll run into if we Implement what we've discussed so far is that we don't actually know where this blank is sitting remember that we instantiated all these Keys all these queries but from the perspective of a query on this blank visited an eight actually looked like the same thing right because we have no notion of position in these in this model so far so we're going to use a family of schemes called positional encoding that are going to provide this kind of information to Transformers so that they're going to know oh okay you need a word here that is something that you eat rather than something that you visit all right so here's the most basic version of this called absolute positional encodings and it's so kind of I think it's it's it's in some sense almost so simple that it's counter-intuitive what you do is you take the embeddings of each of your words and you add to them a separate embedding of an integer that represents that word's position in the sequence so if we think about the word embeddings as a big table associating the word the with some Vector movie with some Vector we're going to have another table of independent impedings that associate the word one with some Vector the word two with some vector and these are kind of fake words right and then we just add these vectors together this is what's the sort of thing that's stipulated in the attention is all you need paper you just mix this information into the vectors at the input layer and then you kind of keep you do you do your computation in the rest of your network and you let things attend to each other and ideally what the model is going to learn is that okay I need to make embedding one and embedding two sort of Drive the model to make these things attend to each other but maybe embedding one and embedding 50 don't need to care as much about each other's values so during the learning process we've kind of given the model enough parameters to say these are the words that should pay attention to each other and these ones shouldn't but uh you know we've infused this model this information in maybe a very simple way so there are a few drawbacks of this I'm going to let you kind of think about those and ask a question after the segment this is a little bit different than what's stipulated in the vaswani at all paper uh but I think it's a lot simpler and it's also much more standard for example like gpd3 uses the kind of embeddings we just saw but in the original paper vaswani at all derived this way of producing positional encodings which are still vectors that are still going to get mixed in with the words but they instead instead of using parameters actually use this kind of fixed representation based on signs and cosines of different frequencies so what we have on the y-axis here are basically the indices of words in the sentence and then on the x-axis we have the value for the corresponding embedding Dimension so basically the first row here is like a vector that's going to be the positional encoding for the first token in in any sequence that you're given and this is computed based on the formulas below which are these uh kind of crazy sine and cosine formulas that basically uh kind of move up as you increase the position and they also uh depend on the index in the embedding Dimension which is going to basically make it so that uh some of these embedding Dimensions move through the sine and cosine with a kind of higher period this is in some sense a sort of basis like and what it does is it makes it so that word one and word two are almost identical they're very very similar and only these very high frequency components of the representation are changing but when you compare word one and word 20 let's say you could see that the vectors actually look pretty different and so the the dot product representations the dot products of these representations are going to reflect the fact that some of these words like are closer together and then probably should get a higher dot product all right so that's another way to do it uh it's been a little bit set aside in the literature more recently I'm just going to mention a couple of uh more recent variants here of these mechanisms one is called relative position encoding uh which is used in the T5 model that we'll talk about later in the semester which uh uses self-attention and in that computation it actually injects the uh position encoding directly into the kind of query times key Matrix computation by modeling the distance between any two tokens so it doesn't capture absolute position but instead how far apart are these two things there's also a form of this called attention with linear biases which is also attractive in that it reduces the number of parameters of this uh kind of dramatically rather than trying to model uh you know each position as a vector of learnable parameters instead we just say all right we're going to add a single uh constant M times the distance between the tokens that we're looking at so it's also a kind of relative position scheme and this is just going to disrefer attending to things that are farther and farther back it's kind of pushing down the self-attention weight by M for each token that you go further back in the sequence and as a kind of key thing here they take each attention head and they give each attention head in the model a different value of M so the different heads of self-attention will learn to either prefer things that are closer by or be a little more uniform over the whole sequence and then finally uh in 2023 there was a result looking at just not using positional encodings at all now this is on a slightly modified version of the Transformer from what we're seeing so far that only attends to things that are in the past and this is really important because uh if you only look at the past you can sort of learn to count up tokens and figure out where you are organically without having been told that information if your self-attention mechanism lets you look in both directions this is totally not going to work um but many of the modern models like Chachi BT do use this causal looking into the past self-attention variant which we're going to come to a little bit later and uh kind of surprisingly this seems to work okay even without positional encoding that said using one of these positional encoding schemes is still quite standard in models today and it's important to know how to inject this kind of information about position into Transformer representations that's the end of this segment\", metadata={'source': 'a8sTGth7PoU'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about the Transformer architecture I've been a little sloppy in some of what I've said so far and sometimes when I talk about multi-head attention or I call things Transformer layers when I really mean multi-head attention but it's important to note that the Transformer architecture itself is based on multi-hat attention but involves a few other very important components which now we're going to look at so in particular we start off by taking our input sequence a bunch of uh kind of embeddings laid out in a in a sequence and feeding them into the multi-hat attention uh layer that we've been building up over the past several segments after that uh the kind of main other piece that functions to make Transformers do what they do is a feed forward layer that operates over each word individually so we've talked a lot about feed forward neural nets for language modeling and this is something a little bit different if we have let's say three words as input to our multi-hat Attention our multi-head attention is gonna you know have all those look at each other and produce three vectors as output then each of those three vectors independently gets this feed forward Network run over it to produce three new vectors that are outputs from that process um and the speed forward network is just a very simple standard like one hidden layer feed forward neural network nothing fancy going on there now there's two other things in this diagram which is from the attention is all you need paper the first is all these arrows so what are these arrows doing the arrows are reflecting residual Connections in the model which basically say we're going to take the input to the model that's given to the multi-head attention mechanism and we're going to add it to the output from this mechanism and this is a trick that's used in training deep neural networks to enable gradients to flow more easily between the output and the input because you don't actually want the model to like have to use every layer so you provide these residual shortcuts where I can kind of skip the layers and the way you do that is by adding the inputs and the outputs um and the final piece here is what's called layer normalization where we do a kind of rescaling so that things are more on the same scale and this helps uh the optimizer do a better job of uh training everything even when there's like a whole lot of different layers going on but we want to train everything with a single uniform learning rate all right let's talk a little bit about the vector dimensions of everything here because this is pretty important if you want to understand how Transformers are implemented so the inputs are going to be a dimension we call D model and this is also going to be the output of the whole layer and this has sort of in some sense the native dimension of this architecture now the queries and keys these have to have the same Dimension but remember we're multiplying them we're multiplying the inputs by w q and w k in order to get these so WQ and WK don't have to be square so we can cast things down to a smaller size Decay that we're then going to do our DOT products in and do our attention computation with and these are going to be smaller because it's going to save a lot of computation in order to do things that way thank you the values DV also have a separate Dimension um remember that when we get to the output of this layer uh we are going to multiply by another output Matrix to basically make everything the right size again and to combine things across the multiple heads so uh you know we can make these whatever size they want and they'll eventually get back to D model before this residual layer however because of the residual layer we're going to add the input to the output so the output has to be the same size as the input here so once we get through all the multi-head attention we need to get back to D model and then finally we have the feed forward Network which can blow things up to a larger Dimension which we're going to call D internal here and then kind of collapse it back down so it has like one big hidden layer in the middle there and then again comes back down to D model because we're going to have another residual connection and so now this whole Transformer block is going to have D model as input and D model is output and then we can stack that as many times as we want and keep everything the same size and what sizes are those if we look at the original vaswani paper uh we've got some numbers in the top left here uh we've got six layers that's what the N is 512 for D models are kind of a moderate size there the feed forward is 2048 so like I was saying that's a big that's a kind of big layer uh we're blowing things up by a lot and then uh kind of shrinking them back down and DK and DV are actually quite small they're 64. and part of the reason for this is because it get it gets very expensive to use very large uh values here because you're doing these expensive Matrix multiplies uh and um these seem to be kind of sufficient for uh you know modeling the interactions between tokens which is what the attention mechanism has to do now if we look at how these things scale the table below is from the gpd3 paper we see that the D model has gotten a lot bigger up to like 12 000 and then the D head here of 128 this is what we call Decay it actually hasn't gotten that much larger right so the complexity of the interaction around attention is not really where most of the action's at most of the action is in the D model and also the D internal of the feed forward Network if we look at where the actual uh floating Point operations are happening in this model we can look at this as a fraction of the overall computation this is a model called opt that's like an open source version of GPT so we know a little bit more about what's going on here the multi-head attention uh floating Point operations if we go to the largest model at the bottom of this table are only 17 percent of the operations eighty percent of the operations are in that feed forward Network so even though I've been saying yeah okay attention Transformers like these are the same thing attention is a very important component but most of the computation is in the feed forward Network and when we think about you know why does chat gbt kind of know all these facts about the world a lot of that is because these huge feed forward layers can just store so much of that stuff all right so now we're kind of almost ready to understand the full model we're going to come back to it a little bit later when we talk about seek to seek models because the original Transformer paper presents an encoder decoder model so it's not for language modeling it's actually for mapping like one sequence of tokens to another sequence of tokens um we don't really need to think about that right now we can just sort of think about uh the encoder being a language model and ignore the decoder entirely entirely there's going to need to be some modifications to do that but we'll come back and talk about that more when we see how this connects to language modeling that's the end of the segment\", metadata={'source': 'sLsUD-RcDqg'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to zoom out from the Transformer architecture itself and look at how Transformers can be used to produce classifiers and how they actually kind of function if we think about them from an API standpoint so as a reminder what a Transformer does is it takes a sequence of vectors as input and produces a new sequence of vectors as outputs same length as the original sequence so we can view that as an encoding of each word and what that's going to allow us to do is it's going to allow us to stack these things but then ultimately take those and use them as kind of proxies for those words to make predictions uh so this is actually the same sort of API that rnns have rnns also produce these kind of cell States uh so Transformers don't really materialize those in the same way but this kind of producing context dependent representations of items in a sequence they're kind of exactly fungible so the one of the things we can do is we can actually just make a classification decision for each item in a sequence so if we want to do something like part of speech tagging we take those output representations for each token uh we put it through some kind of classification layer like a linear and a softmax and now rather than having predictions that are in word space like we have for language modeling we instead have a distribution over let's say a bunch of part of speech tags we can also make a classification decision over an entire sequence at once so just like deep averaging networks allowed us to take a bunch of words average them together and make a prediction we can do the exact same thing just with a Transformer kind of in the mix as well so we can take uh the contextualized representations we get from the Transformer layer average those and then do Matrix multiply and softmax to do something like sentiment prediction now that's one way to do it but there's actually another way which is a little bit more standard when we look at how people use Transformers in practice and how the pre-trained models are set up what we typically do is we append a placeholder token uh which is typically denoted by bracket CLS for classification at the start of the sequence and rather than averaging over the contextualized embeddings of the whole sequence instead we just take the vector representation of the CLS token feed that into a matrix multiply and softmax and then predict sentiment from there now why is this a good idea or why does this work well if we remember kind of what is happening at each step of the Transformer the self-attention means that everything is attending to everything else so what that means is that uh this CLS embedding at the very end of the network here is attending to all the other tokens and we might imagine that the model can learn a distribution of what tokens are important for this particular classification decision so it's actually a little bit smarter than average pooling because this final layer is going to learn how to mix the information together for us a final thing that uh Transformers have kind of proven to be really really effective at is sentence pair classification uh so one of the big kind of highlight results of the initial Bert pre-trained model was its ability to do tasks like this textual entailment tasks that we see here so we have a pair of sentences the woman is driving a car and then the woman is walking what we want to know is does the second sentence contradict the First is it implied by the first or are they kind of unrelated and in this case it contradicts it's or it contradicts the first because the woman can't be driving a car and walking at the same time so Transformers are really good at this partially because the self-attention mechanism gives them so much flexibility to do computation anchor to all these different tokens so for example uh you can kind of get each of these words uh sort of attending to each other and then maybe you'll get driving and walking uh kind of mutually attending and over a few layers of the Transformer computation the model will really figure out oh no like you know driving a car and walking these are sort of not compatible and so ultimately it'll be able to the the model will be able to make a prediction that uh reflects this kind of mismatch in the two sequences so that's another thing that Transformers can do they're really quite flexible because self-attention lets them you know kind of align different parts of the input in different ways and so you don't need custom architectures to do things like sentence pair classification or document classification um you can just kind of feed whatever text you have into it and see what comes out that's the end of the segment\", metadata={'source': '1Efx04lHa7w'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're finally going to see how to use Transformers for language modeling we've seen all the details of the Transformer architecture and how Transformers can kind of be used in the abstract and now we're going to bring it back around to the task that we started with so we have an input sequence of words here I saw the dog which we embed and feed into a Transformer and our goal in language modeling again is to predict the next word so we're going to do that based on the contextualized representation of the word dog here which occurs at the end of this sequence and then like we've talked about you can bolt on a sort of classification head basically multiplying by a matrix and doing a soft Max in order to get probabilities of next words here so we can write that in a couple ways we can write a probability of the next word being W given the context as a exponential of a w dot hi where this bold W is a kind of word embedding for word w and then we normalize by the sum over all uh possible words in our vocabulary V we can also write this as the probability of the next word given the context is just the soft Max of a big Matrix W Times h i so this Matrix W again is just as a standard linear layer for classification it's uh kind of num classes which is vocab size by hidden size here uh and the kind of cool thing about this Matrix is it actually can be interpreted as word embeddings which is one reason I wrote the form I did up above as well so we're basically learning kind of which words is this hi representation compatible with and let's put high probability Mass on those from our language model so the uh that's the sort of computation process of the model how do we train this thing so effectively what I showed on the previous slide was talking about taking four words and then just predicting one next word as the output however in reality when we're training we can kind of take a sequence and slice and dice it however we want and what we're going to do is we're going to take every single prefix of that sequence and simultaneously at every Point predict what the next word is going to be so starting with the start of sequence token at the bottom left here we want to First predict that the word I comes next and then uh you know from start of sequence and I we want to predict that saw comes next from starter sequence I saw we want to predict the ETC so we think of this as a model that takes the as sequence of words as inputs and then the target is basically those words shifted over by one so uh you know based on each prefix you're trying to predict the next token so this is nice because it's going to enable us to efficiently train on this entire sequence at once rather than saying okay I'm going to do all this computation and then predict one next word we're going to predict all these next words and then get all this nice gradient signal to update our parameters kind of simultaneously so it's the same idea as batching but I want to be a little bit careful here because when we talk about batching and language models we mean something a little bit different which I'm going to come back to in a minute okay so in terms of actually the kind of mathematics of it the model places a probability distribution over the next word given the context at each point and we can turn that into a loss by Computing the minus log probability of the actual next word so the that happens at each point independently and then we can get a total loss value by just summing up all of these negative log probabilities which is the sort of log of the product of probability so it's exactly what you would want to optimize for log likelihood if we want to compute this uh efficiently in pi torch we can use this loss function called nll loss and what this allows us to do is it allows us to take a uh Matrix of probabilities which is sequence length by number of output classes so basically all of these blue vectors I'm kind of stacked up into a sequence length by vocab size Matrix and then this x dot output tensor is just a sequence length list of the ground truth word indices for the next word at each time step so if basically you don't need to work worry about oh I need the minus log of the thing at this index and then sum all these up like this will just do all of that for you the one warning I will give you uh if you use this as the batching becomes a little bit tricky because uh you can't just feed in something that's a three tensor of batch size by sequence length by num classes into this you actually need to collapse it down um and merge the batch size and sequence length Dimensions here okay so I said I would come back to batching and say explain what I meant when I said that we weren't really batching yet batching in a uh language model training looks sort of exactly like batching in most other classification tasks we just have a bunch of independent examples and we're going to do computation on them in parallel now in this case we've taken one sentence and kind of chunked it up into three different chunks but given each of these chunks we can treat each one as a language modeling example and just train on them independently now notice that this is a little bit different from training on one sequence of length 15 because the Transformers here actually only see five tokens at once um so this is kind of important because if you're using for example your positional encodings they're only going to know that there's five tokens in this input and the Transformer may not learn to attend the things that are very far away so it's different from just training over all 15 tokens at once uh but it's the kind of default way of parallelizing LM training as you form your sort of sequences and then as many of those as you want to train on simultaneously just kind of stack them up and uh you know you can batch your computation up that way um so this batch Dimension is sort of what I'm showing going into the screen for you and uh we have both multiple sequences that we're training over and multiple time steps uh per sequence okay so as we've described it so far this all sounds great uh and mostly it all just works but there's one problem which is that the Transformer language model that we've described so far is going to get perfect accuracy I'll give you a minute to think about why that is and then I'll uh kind of reveal the answer so the reason for this is because of the nature of the attention mechanism that we've been using so far uh there's no reason or there's nothing prohibiting the word I hear from attending to the word saw and so the model is just going to say oh okay I'm going to learn to look at what the next word is you know and using positioning codings it's very easy for it to figure that out and then propagate that information up here and it's like aha the next word is going to be saw and now the model's not really doing what it's supposed to do right like if you see I saw the blank you're supposed to model the distribution of things that you can see right you're not just supposed to like look at the next word this is just this is not language modeling anymore so we need to make sure that this doesn't happen and this is going to use an idea that I've alluded to a little bit which is the idea of a causal mask or a causal Transformer or a Transformer that only looks back into the past so essentially if we think about our queries and our keys what we want to prohibit is we want to prohibit the word saw from seeing the and dog in the self-attention right saw can see itself I and beginning of sequence but we don't want it to see the and we don't want it to see dog so we end up with this kind of triangular mask where we want these red values to be prohibited and we basically want the model not to um you know be doing any attention there or assigning that any probability Mass uh so again this is called a causal mask and part of the reason is because there's like this sort of time process where only things in the past can influence the present um so it's not really causality in the like Judea Pearl sense of it it's not when people talk about causal machine learning uh this is not what they're talking about uh but basically this is a way of making the kind of information in a Transformer really flow in in kind of One Direction um and so if you want to uh kind of implement this uh there is a way of passing in these sorts of masks to uh Transformer encoders in pi torch and uh you could do that with a mask that has the form here basically this upper triangular Matrix of ones that gets multiplied by negative Infinity um and then the Transformer encoder is called with that particular mask and it'll mask out that in the attention computation so this is one kind of way to get around that so that shows us how we form the model for language modeling how we train it how we do batching and finally this last little detail about uh masking needed to make it actually a valid language model that's the end of the segment\", metadata={'source': 'htyspM3FrMg'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about a kind of grab bag idea of ideas related to Transformers and Transformer language modeling and some kind of important Concepts that help you understand where some of the current research is headed so the first thing I want to mention are neural scaling laws which are a kind of really important driver for why Transformers are probably the default uh model these days and why they're used so much for language modeling so what we have here is a graph from uh Jared Kaplan at all his paper which looks at uh basically a set of kind of test losses so this is basically perplexity but not exponentiated so just negative log likelihood across plotted against three different uh x-axis compute data set size and parameters these are all basically scale like as the Transformer gets bigger as the data gets bigger and compute is a kind of mix of both of these and on the compute graph we see that there's these many blue lines and these are basically different Transformers with different parameters that are all trained to basically with the same amount of time and the kind of fascinating thing here is we see that we get these kind of uh these plots that have this very regular uh kind of log linear structure whereas the parameters increase we get these very regular decreases in test loss and what this indicates is that compared to a lot of other architectures like recurrent neural networks where sometimes if you just like increase the number of parameters in the model it just doesn't get any better you've like hit a saturation point we see that Transformers scale really really well where if we could just make a model that's 10 or 100x bigger than we have right now there's really good evidence that it will actually perform better as a language model which is which is quite rare and didn't exist in the era of linear models it didn't really exist for engram language models it didn't exist for a lot of Prior uh neural architectures so this being the goal in mind one of the challenges that prevents things from scaling is the uh kind of lack of you know compute in order to do this and one of the big bottlenecks there is this quadratic nature of the Transformer so this has been uh one of the kind of headline problems in some sense of uh NLP machine learning more broadly how to address this there are a million proposals here this is a figure taken from uh ete at all survey of this stuff which is uh now even a few years out of date um just to give you an idea of the sort of flavor of one of these things there's a model called performers which was from 2020 where uh basically rather than forming the attention Matrix here uh so on the left side here we have uh our standard computation where uh you know a equals the um the keys the the queries multiplied by the keys instead of doing that we are instead going to multiply things in a different order we're going to multiply the keys by the values first and kind of collapse things down along the sentence length this L Dimension and then we're going to multiply by the query and this is going to make things faster and it's going to make them more memory efficient however it's doing a different computation right this this a matrix requires multiplying queries and keys and then taking a softmax and that's not what's happening on the right side here we have a fundamentally different computation that's approximating that and I'm not going to go into the details of the approximation but it's doing something different and one kind of common theme in all this work is that usually these approximations work a little bit less well so the kind of standard Transformer is just very hard to beat as a baseline by doing alternative variants so another kind of line of things is to kind of directly go after the fact that the uh attention is quadratic and just try to make that not be quadratic so long former is one model that devises a bunch of custom attention masking schemes that's going to make everything kind of more efficient when you want to encode very long documents so the idea is to enable these things to scale to much larger contexts um so these basically rather than using the full N squared attention Matrix they have this kind of combination of patterns like a sliding window pattern this dilated sliding window pattern and This Global pattern where you're able to look at certain other tokens and kind of combining these in clever ways you get most of the benefits of the original Transformer but with a sparse self-attention Matrix so what this does is it actually doesn't uh cut down time by very much so um on in the left here we see that the uh the there's a blue line here which is full self attention um and this is actually like reasonably fast there's a chunked version of long former which is uh faster but um you know the uh the the um kind of other versions of long form are basically the same in terms of time the real benefit comes in terms of memory um and so based on what I kind of showed I want you to think about what the memory is should look like both for uh the original Transformer and also for long former as the sequence length uh it goes up and one final thing I'll mention is that much of the work here has been not on developing new architectures or kind of changing the math but instead like kind of systems level things for how we can handle GPU memory and stuff more efficiently uh in order to scale Transformers to longer context lengths so uh there's a lot of other Innovation that's kind of a bit outside the scope of this class that's made these things uh kind of get better in in recent years as well that's the end of the segment\", metadata={'source': 'DPvDL8L4Dqo'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about beam search and more generally other decoding strategies for producing language out of language models so the decoding strategies we're going to talk about are going to apply the two types of models that we think about in this class the first is language models uh just placing distributions over the next word and the second are sequence to sequence models placing next word distributions over an output y conditioned on an input X so for in both of these cases at the end of the day we have this model and we give the system some input either a prefix of y's or an X or you know we just wanted to straight up generate a story or generate something for us so how do we actually do this generation so one option is just greedy search so we just take the most likely next word at each step and we kind of repeat this and just crank out a sequence that's one approach uh it's going to be the sort of worst of all possible worlds but it actually works pretty well in practice so uh it does get used um the one we're going to talk about right now is using beam search which is going to be a kind of improved version of greedy search that's going to allocate a little bit more computation but ideally find something with even higher probability and the third option which we're going to talk about a little bit later is drawing samples from the model so rather than getting the most likely thing instead draw a random thing so let's dive into beam search so uh we're gonna assume that we have uh vocabulary of size V and a sequence of length n that we're going to generate so at the first time step we can think about basically the probability distribution over the first word and I'm just going to you know make up some words and some probabilities here blah blah blah and uh each of these things you know has a pretty low probability the model doesn't really know what it's supposed to say um Etc so they're V of these so there are a lot of different words that we have to consider here and then if we think about what happens at the second time step each of these has a number of different things that can kind of produce right so we could say the dog the cat the fish Etc right and each of these gets a probability and these are going to be even smaller now because we're sort of multiplying you know small number after small number together and there are generally V continuations uh of each of these and so we end up with v squared hypotheses here or size of V squared hypotheses uh and at the end of the day we are going to end up with size of e to the N uh total sequences so I mean uh I think we're kind of familiar with exponentials being bad so the fact that this is exponential in n and you might want to generate something potentially very long is a problem so what this is this is the kind of exhaustive search mechanism that's going to enable us to find a uh highest probability sequence but we're going to do something better with an approximation so beam search is going to approximate exhaustive search but with less compute and basically the idea is keep the top k ypotheses at each step so if we kind of go back to the uh this lattice of options we were building above we're going to kind of revisit it and let's say let's say k equals three then what we're going to do is we're going to cut the model off here and ignore all of the rest of the options that come afterwards all right and then each of these still generates a whole bunch of possible uh possible next words right so we get the dog um you know all these all these things and we generally end up with uh K times V of these okay but once again we're going to chop off most of that we're only going to keep around the top things which are maybe the dog the cat and let's say uh cat or something like that so we end up with a relatively small number of options at each time step and then uh you know there's an expensive process of checking all the stuff that comes next but then we always prune back down to this small set so the run time here we can kind of think about it in terms of several uh different mechanisms we're not really going to talk about runtime of data structures like you need to maintain some kind of Heap or priority queue or something like that to actually manage all these items but like that overhead is so small compared to the overhead of running big neural Nets that we're not even going to think about it instead the kind of relevant things are that uh we're going to have K times n Transformer calls in the sense that we think about just how many times do we need to run the Transformer and get a distribution over the next words well it happens kind of once at the first time step and then it happens three times or k times for each subsequent time step because you need to consider each of the things in your Beam run the Transformer over it get the distribution over the next words um and we consider K times V times n hypotheses ultimately so it's much smaller than uh V to the N um you know we're just not thinking about that many items but it still allows us to explore the space in a way that kind of scales with K essentially okay so uh beam search you're going to see used most frequently in sequence to sequence conditional generation settings like machine translation uh where you really want to get a high probability thing and you know that okay greedy is going to find me something but then beam search is going to find something of even higher probability than greedy will um greedy can get kind of derailed down some path and pick some word that's then going to lead to some low probability stuff later and veeam search might avoid that that can lead to like better translations um but we'll see later that for certain types of applications beam search is not what you want and it's also the case that once you get bigger and bigger Transformer models the beam search becomes less crucial because these models already do a little bit of kind of not explicit planning ahead but the modeling of the distribution of the next word already accounts for the fact that more stuff is going to be generated later so um you know there's less of a need to use this with the kind of latest and greatest models and it's also very expensive which when you have a model that's already kind of Breaking the Bank in terms of compute you probably don't want to use this however it's an important search technique to be aware of and uh sort of an important way of producing hypotheses out of these models that's the end of this segment\", metadata={'source': 'wltqDbhlcJ0'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about sampling strategies for generating text from language models so previously we talked about how there are some strategies around taking the most likely next token and then subsequently we looked at using beam search to find an even higher probability sequence but the third option that we outlined before was to draw a random sample from the model now you might think why would you want to do that well for an application like machine translation where you really want a like high quality translation you probably want to use something like beam search but if you're using a language model for some sort of open-ended generation task like let's say you're trying to generate a story there may not be one right answer and you may be okay with more variety so this was one of the capabilities that started to arise with the GPT series of models and particularly gpt2 so uh one of the kind of headline examples that they had back when they released that work was the ability to continue stories like this one in a shocking finding scientists discovered a herd of unicorns living in a remote previously unexplored Valley in the Andes mountains even more surprising to the researchers was the fact that the Unicorn spoke perfect English so then we might say okay what does the language model like to sort of say next can it continue this story and beam search eventually gets stuck in this very interesting Loop where it just keeps repeating the name of this University again and again so this is exactly the kind of case where uh you know we might say okay this is somehow not random enough right so then we can consider sampling and if you do sampling from the model's distribution you end up with stuff that ends up looking quite weird like they live in a remote desert under inner uninterrupted by town and there's lots of grammatical errors in this so what's going on here so this phenomenon with beam search is called degeneration and this arises from the structure of the language model that we're using so let's like think about how this string actually gets generated here so first we uh let's say we kind of work going along and then we ran into University dad as a token and it might be the case that the probability of Nacional given Universidad is high that's fine and then autonomous maybe coming after that is also High de Mexico also High probabilities and then the model is kind of at this Choice point where it then start sort of puts a slash and then keeps continuing things these might be low probability things but once you start looping here ultimately the probability of this entire sequence looks pretty high right like it's found this uh sort of name that it can generate with high probability and then it's just going to keep looping this and beam search is going to think this is great so these words are likely given the previous words but the whole sequence is unlikely there's a separate set of models called globally normalized models which include things like energy-based models that can be used to judge this we're not really going to talk about them in this class so much they're sort of computationally hard to use but basically I you know we want to do something more random than beam search we don't want to just find this like highest probability thing that's kind of degenerate okay but then this sort of Goldilocks style sampling is then too far in the other direction it's too random so what happens here if we say the distribution over the next word given they live in a remote desert uninterrupted by blank well there might be a whole bunch of different options here uninterrupted by roads towns people civilization Etc and maybe town is here but it's a little bit less likely so in general there's going to be a lot of options and maybe the top you know most of the probability Mass uh accounts for these but then there's going to be a long tail and let's assume that this long tail has ten percent of the mass well then what happens is that on average every 10 words you sample you're going to get something from this 10 tail and if you think about if you're going to sample 100 words in expectation you'll get something from the one percent tail so like even rarer stuff and if you're going to generate a story you don't really want it like going completely off the rails every 10 words or every 100 words so nucleus sampling is this idea to correct for this and what it is is basically chopping the distribution truncating it after P percent of the probability Mass if you sort the tokens from high probability to low probability then you just renormalize and sample from the distribution that's left so it's a relatively simple technique to implement all you need to do is get have your options and sort them and then truncate them and it turns out it works pretty well so in the paper by Ari Holtzman at all that presented this they compared the uh stories generated through various different methods across several metrics first is perplexity we see that nucleus sampling achieves like decent perplexity not as low as greeder greedy or beam search but greedy and beam search achieve very high self-blue and very high repetition which basically means that they're falling into these loops and repeating themselves and then this last column Hues is this kind of human in the loop scoring function that nuclear sampling also does quite well on so it seems to be able to balance uh the sort of naturalness of the story while still maintaining uh you know quality and reasonably uh good likelihood from the perspective of perplexity you see it's much lower than the perplexity from peer sampling so when we talk about decoding strategies from language models we're often going to think about these three uh Max decoding or greedy decoding beam search and then nucleus sampling and if you use a large language model API like chat gbt uh the outputs are very often come from an approach like nucleus sampling that's the end of the segment\", metadata={'source': 'JETxaSaj6_k'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to introduce the ideas behind bert so that we can understand what's going on inside that model and how it's trained and then further on down the road we'll look at all the things that it can do so the basic history is that elmo came out in the spring of 2018 and then a follow-up called gpt which was more or less a sort of transformer-based version of some similar ideas came out that summer and when burt came out it had several major changes compared to elmo one was using transformers and but the main kind of critical one that informs how we think about what these models are doing is using a different type of training objective called masked language modeling instead of the normal language modeling training objective that elmo used so we'll talk about some of these other differences later but for now let's talk about this idea of masked language modeling and see why we might want to do it so in elmo and also gbt we have a unidirectional model so if we think about encoding this sentence and trying to get good vector representations or contextualized vector representations of the words in it the way that we would understand the word copeland is by having elmo run in each direction and taking the representations from those lstms at the time step corresponding to copeland and so maybe you know the the forward elmo should tell us that copeland is a ballet dancer and the backward elmo should tell us that copeland is a performer right and so these pieces of information are then stapled together into a single vector but they're not deeply merged right like we're just concatenating these two independent views of what's going on in the rest of the sentence burt is going to do something different where it's going to be able to look at the whole sentence and produce a vector that ideally captures both of these aspects so how do we learn this model that's so-called deeply bi-directional so one thing we've said was that bert compared to elmo replaces lstms with transformers and so transformers allow us to look at the context more broadly with the self-attention mechanism and so maybe we can just train a language model and use transformers instead of lstms and then we'd be done so what happens if we do that so it turns out that this does not work and the reason it doesn't work is because in language modeling you're always thinking about predicting the next word based on the current word and so you have to be very careful to set up your model so that it doesn't look at the next word if you're feeding all this stuff in as a batch right i mean elmo is set up so that every prediction going on here even though it's all one neural network only looks at the past in order to predict the the next word and that's correct but here if we just naively through transformers at this um you know this prediction of madagascar for example can be informed by self-attention from visited looking at the next word here so what i'll say is that gpt uses a one-sided transformer which only looks at which only looks back at the past here and so there are ways to fix this but that doesn't solve the problem of how we integrate information from both the past and the future in order to inform the representation of the current word all right so that's where math language modeling comes in what we do in mass language modeling is we say all right we're going to take a chunk of text and rather than try to predict every word based on what came before instead we're going to say let's just forget some of the words let's replace them with mask tokens and then we're going to try to predict the words that fill the that fill in those math tokens and we're going to do this not to every word because if we replace everything with masks there would be nothing to predict from but roughly 15 of the words we're going to try to predict based on the context and this is going to allow the model to then look at everything but not cheat right because it has no way of seeing madagascar um if just this this mass token is in the input and madagascar is not in the input at all all right so this is the one of the fundamental ideas behind burt and a lot of the other models that have followed on it so what they do is they have a input that consists of a couple of different sections here it starts off with this cls token we'll talk about the function of that later and then it starts with one chunk of text a separator token and then another chunk of text and these two chunks of text might be from let's say contiguous paragraphs on wikipedia and in this case we mask out fit you know 15 of our tokens and predict words based on the masks okay so why is this in red here and why is it suddenly start talking about madonna so 50 of the time what they do is they take two paragraphs that are not contiguous on wikipedia or whatever corpus they're training from and they ram these together and then what they want to do is they want to use this cls embedding to predict whether this second chunk is the true continuation of the first chunk or not so the idea behind this was two-fold one was to try to get even more non-local reasoning in this model by saying all right like i want this i want this model to have to really think about the interactions between these two chunks in order to understand whether these are continuations of each other or not and so the then that prediction gets localized at the cls token and the cls token can attend everywhere right so kind of where it shows up is is not necessarily so important but the fact is we have this one token in our model that we're saying is going to be useful for making this classification decision that looks at the entire input and that's something that we're going to want to do we're going to see in a future segment how bert is applied and we're going to understand that that cls token is usually what people use in order to aggregate information about the whole context so the overall bert objective combines this masked language modeling and this next sentence prediction now it turns out in some future work that the next sentence prediction part is not so important but i think this gives you the intuition for how we're going to get the model to look at all this context understand these tokens in this deeply bi-directional fashion where to figure out madagascar you want to look at everything that came before and after it um and then how that's going to be the foundation of our how we build classifiers that live on top of bird that's the end of this segment you\", metadata={'source': 'dya_QNFvtiQ'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about what the bert model actually looks like and how it gets applied to downstream tasks so the there were two versions of the burp model presented in the original paper both are transformer models so following the architecture that we discussed before and these are big models bert bass has 12 layers and burt large has 24 layers um each of which has the dimensions listed here so for bert large a thousand dimensional uh embeddings for each wordpiece token 16 attention heads at each layer and that gives a total number of parameters of 340 million so a very large number of parameters compared to the past neural models that we've seen so the one other aspect of transformers we discussed was the need to represent position in them somehow because by default the self-attention mechanism doesn't it sort of treats the input as a bag of words you don't have any notion that these two words occur next to each other which is important for dealing with sentences and so what they did was they had an additional set of encodings one set that captured position by basically just embedding an integer index that represents where the word is in the sequence and then they also had what they called segment embeddings which we'll come back to in a minute so this model is the one that they first pre-train on a large corpus using the mass language modeling objective that we talked about before then the question is how do we apply it to other tasks and what can we actually do with that so one other important thing about bert is the presence of this cls token at the beginning which was used to do the next sentence prediction task that we discussed earlier this the basically the vector at the end that you get associated with the cls token is the vector that you typically use for doing classification and burn so what you'll do is you'll put uh you'll use bert as a kind of black box to encode your sentence and produce this single vector associated with the cls token and then you take that you feed it into whatever classification layer or maybe a sort of small feed forward network to do your final prediction for whatever task you're doing so the nice thing about bert and one of the big advantages compared to prior work was that it could really do sentence pair tasks a lot better so what this means is a task like paraphrase detection can you tell whether these two sentences are paraphrases of each other and the way this works is you just kind of ram these sentences together and feed them into bert and again use this cls embedding to do classification and then finally burke can do tagging as well uh similar to what we saw for lstms you know any model that can produce a contextual embedding of each token you can take that embedding and map it into something like an ner tag here okay so why is it so good at these kind of sentence pair tasks well one reason is that transformers are pretty good at taking sort of seemingly independent chucks and chunks of the input and using the self-attention mechanism to figure out an alignment between them and so they capture interaction between these two sentences and i'll emphasize that most of this happens during fine tuning even though there was this next sentence prediction or nsp objective that's not really all that important for performance mostly it's just the fact that when you have two sentences like this self-attention allows you to really figure out okay which parts of each of these sentences is kind of supported by the other one and then if we want to predict entailment for example does the first sentence imply the second one that mapping between pieces is something that the model can sort of figure out somewhere in these uh you know 12 layers and however many attention heads there are right so bird seems great one thing it cannot do well is generate text the only way you can do this is basically populate a whole sequence up to this point stick a mask token on the end and then say okay fill in this mask token the problem is that this ends up being sort of order n squared in the length of what you want to generate because then as soon as you fill in that mask token you need to rerun the whole computation with another mask token on the end uh and you know you might be able to generate multiple uh you know fillers at a time but then you don't know whether like this filler and this filler are really going to be coherent once you actually you know take the real words there so these models were not intended for text generation they were intended for these sorts of analysis tasks like uh sentiment classification and things like that so that's okay and and this is a kind of feature of how they were designed all right so the general procedure is that once we have this pre-trained model we have to do this fine-tuning step of continuing to train it on whatever our actual task is so uh the model's pre-trained on the mass language modeling objective and then when we have our sentiment data we say okay we want the class label and whatever matrix of like output basically mapping into the mapping into the actual prediction space for our downstream task we want to train that matrix now and fine-tune the rest of bert so the way you do this is you run gradient descent on your new data set typically for a small number of epochs and also with a very small learning rate and so kind of graphically the way we can think about what happens is that like i said the the next sentence prediction loss isn't actually all that impactful for bert and so the cls embedding one of the main things that the fine tuning process does is it kind of rewires the last step of self-attention here to try to feed the useful information into the cls token so the way i'm kind of drawing this is that there's big changes here at the top we fine tuning we change the the parameters closer to the output of the network a lot and then kind of less as we go back through it that's where the more sort of pre-trained part kicks in so the you know with the with these smaller changes that means that the weights aren't changing as much and as a result we're going to be able to keep the benefits of pre-training and the kind of you know general language understanding capabilities let's say that's put into this model by the mass language modeling objective all right and so the the you know the the the way in which you fine-tune bert is is very complex there's a lot of different schemes for this using various optimizers kind of weird learning rate tricks like warm up where you start with a very small learning rate you increase it and then you decrease it again and you know sometimes you do that multiple times so there's a lot of sort of black magic here most of the libraries that implement this stuff will uh kind of walk you through that i guess right and so this is very different than how we talked about elmo so we talked about this uh we talked about this table which showed that back propagating into the elmo parameters was generally not a good idea uh that's shown in the uh kind of fourth row down here where we see this delta between the fine-tuned which is the fire and the frozen almond beddings which is the ice uh is negative meaning that fine-tuning actually hurts elmo but this is flipped around for bert so bert you almost you you rarely want to use it as a source of pre-trained embeddings you almost always want to fine-tune that as your network for whatever task you're doing so like i said that's a big difference in paradigm and most approaches these days follow the burp paradigm where both fine-tuning is encouraged and also of course works better all right so they evaluated it across this range of tasks um a slightly more formal benchmark than the just the collection of tasks in elmo and this benchmark was called glue and it captures a range of different types of uh mostly sort of semantic inference tasks with either single sentence input or a pair of sentences as input and most of the outputs are categorical and you know it's sort of classification tasks like are these things paraphrases or not etc so the impressive thing about this was just how well it worked so the pre-open ai soda at the top this is basically the kind of best systems that people had put together with neural models for each task independently and so this reflected in some cases like for example sst-2 years of work on this particular data set and many research papers optimizing architectures for that particular you know sentiment classification task and then you can see that uh uh elmo and gpt kind of were doing on par with this stuff and that was impressive given that they were able to do it with the uniform architecture but then bert especially bert large just blew these things out of the water the gains on this are are really quite substantial and uh again a lot of that comes from dealing with better with these sentence pair tasks so in terms of what bert is doing it's very hard to say and i mean we'll get some functional understanding of how bert works going forward but there was some work due to clark at all which looked at visualizing some of what's going on inside the attention heads and so there's there's some interesting different behaviors behaviors you get some of the heads like the second one we're seeing here attend locally like for example they look at the next or the previous word uh to inform a word about its context um there's also a sort of weird interplay between some of the placeholder tokens like sep or you know tokens that normally don't mean a whole lot like periods to end sentences typically not that useful for text classification problems there's a weird sort of interaction where these tokens sometimes attend very broadly and maybe the model is using these to kind of store information about an entire sentence in one place the other analysis they looked at which is pretty informative was tying this to notions of dependency parsing like we established earlier so for example one thing you might expect is that if you have a word it should attend to things that are syntactically relevant for it and in the dependency formalism we were able to directly say okay this word is either a parent or a child of this other word and what they found was that uh there was there were some heads that could do things like for uh verbs pick out their direct objects or for nouns pick out their modifiers and you know the the the results from this aren't like better than supervised parsers by any stretch of the imagination but this is learned in a totally unsupervised way internal to the model so it's very cool to see it discovering these useful notions of syntax for what it's doing so the final thing i'll say about bert is that there are now better variants i mean burt is still relatively commonly used but probably one of the most common replacements is called roberta for robustly optimized burt um largely the improvements are not conceptually very uh you know it's not conceptually different from what we've looked at before they have about 10 times the amount of data rather than masking the same words in each sentence as you do multiple passes through the data they mask different words which is you know something that just kind of makes sense um and uh another another thing which which helps these models is whole word masking so rather if you have a word that's multiple word pieces rather than masking just one word piece and leaving the others instead mask out all the word pieces um so that's an improvement that's even been pushed into the original bert at this point and so there's libraries that allow you to access all this stuff hugging face is a company that maintains a fairly well-known transformers library with all of the pre-trained models and a bunch more contributed by the community and fair seek maintained by facebook also has some of this stuff in particular roberta so there's plenty of libraries for that that help you use these things and and kind of give you access to the latest and greatest in terms of pre-trained weights here so generally what we what we've seen is that we can take these different uh you know these architectures like bert and use them for a wide variety of tasks and as we go forward in the course we'll talk about and and see basically that almost all the best systems are are now using this stuff so it's an important ingredient to be aware of and something that you're definitely going to want to consider if you're building kind of real applications and care a lot about the performance that's the end of this segment you\", metadata={'source': 'g96oi4ihc_E'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about sequence to sequence model it's also called encoder decoder models so we've talked about both language modeling and also classification but there's an important class of tasks which look like mapping from an input sequence of tokens to an output sequence of tokens and a lot of kind of surprising things can be cast into this uh it can be viewed in this way so for example syntactic parsing which is going to be about building tree structures over sentences can actually be viewed as producing this kind of goofy sequence of bracketed tokens as an output given just the natural language sentence as input um semantic parsing which is a kind of variant that resembles kind of translating natural language into source code can also be viewed as basically mapping from a sequence of tokens here which is a query what states border Texas into a Lambda calculus expression that can be executed against a database so there are lots of reasons why rather than just kind of doing autocomplete with language models we might want to view these or sort of use Transformers more as kind of transducers to produce outputs that are of a different modality than their inputs um machine translation is another great example like kind of translating things into other languages um and so what I'll say is a kind of note that conceptually we're thinking about the inputs and outputs of sequence the sequence models as very distinct things compared to language modeling where we think about there is like there's just one sequence and you're gonna kind of crank through and autocomplete that sequence now models like chat GPT actually do all of this basically as language modeling with a shared vocabulary so the distinction's been a little bit blurred but if you look in the literature it's still important to kind of understand the the distinctions here all right so what are seek to seek models these are words that are going to generate next words conditioned on the previous output which is very similar to what language models do as well as their input so if we want to translate a sentence into French we have this input that we're going to call X the movie was great and we're going to start generating our output sequence kind of like language modeling uh we're going to produce a contextualized representation and then we're going to form a distribution via softmax over the vocabulary and get our first predicted token y1 now critically the Transformer here is going to be able to look back or you know for any sequence of sequence model but especially for Transformers they're going to be able to look back at the input and do this generation conditioned on what's in the input so we can write the distribution that we get out this way which again looks exactly like a language model but everything here is conditioned on x uh and well the other kind of key difference here is we actually have two models floating around right so unlike a language model where we have like one Transformer here we're going to have a decoder that has separate parameters and can actually have a separate vocabulary from the encoder okay so how does this work well it's very similar in many ways to language models just with this kind of encoder block hanging around so we're going to kind of generate the first word if we're doing inference and then uh we feed that in to the uh next step of the model just like language modeling kind of consumes the the token for the next time step um and we're going to do that repeatedly until we generate a stop token so unlike language modeling when a lot of times we're just kind of happy to go on forever um here when we're doing something like machine translation we have a notion of okay we're done and now we're going to stop so that's a little more common to see in this uh kind of way of doing things so just like in language modeling we need to run inference up to a particular point in the computation graph take the ARG Max get the next word and then feed that into uh basically feed that into the next step uh of the model to predict the next output word uh and this will kind of Move Along one at a time the kind of key thing here that uh saves us time and makes this not be super slow is the fact that the encoder only needs to be run once so the whole the movie was great peace doesn't interact with the decoder actually the decoder looks back at it but um the movie was great is just encoded separately and so uh we could just do that once in advance um this whole thing what I've drawn up here is a computation graph you can train this end to end to maximize the probability of the gold sequence now the model will learn to kind of look back at the input and it'll use that information when generating these outputs so we've said that language modeling is this impossible task well Machine translation is difficult but not impossible in the same way right like we basically know what the sentence is going to say and so the entropy of these distributions is a lot lower all right so I keep talking about models looking back Transformers Etc so the actual Transformer which we showed uh uh a little while ago has this ability kind of baked in and that's what's going on inside this uh unit here so there is a multi-head attention component of the decoder that's able to look back at the representations computed by the encoder uh so this include there's there's basically two steps of multi-head attention there's one in the output for just kind of looking at prior things in the output and then there's one that allows you to look back at the input as well uh and otherwise like all the pieces here are basically the same so there's nothing really new architecturally but this gives you the kind of full fully General uh mechanism that allows you to uh kind of model all of this stuff um and uh that's basically the uh kind of architecture that really is getting used for everything these days um you know the kind of Auto regressive models like Chachi PT can be viewed as special cases of this but this is kind of the the main idea that's the end of the segment foreign\", metadata={'source': 'TKZkvqb-qpM'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about pre-training methods for sequence to sequence models and in particular a technique called Bart so we've talked about standard language models which have this distribution they place over sequences and our trained kind of unidirectionally where they just generate the next word based on the previous words then we talked about Mass language models which have a kind of bi-directional nature to them and use masking to avoid the kind of cheating problem so if we think about tasks which have the form P of Y given X things like machine translation how can we pre-train a model that's going to be good at these it doesn't feel quite like these other problems somehow so we could think a little bit about why Bert and its variants were effective and one reason is that this the kind of way that we did masking required some kind of understanding right like if we said you know John visited blank and saw parrots then like making a prediction here required uh the model to kind of understand what kinds of places you visit and you might see parrots and we could think about the same kind of skills from the perspective of the sequence prediction so if we're thinking about something like machine translation really what we want to do is we want to have a kind of Gap here and then we're trying to let's say generate the output which is uh maybe uh translated into another language or summarized or something like that but we want to be able to kind of make inferences about the content in the input that was missing so this is exactly going to be the motivation behind the pre-training schemes that get used for sequence to sequence models so in particular the BART paper from Mike Lewis at all introduced several different methods where you effectively treat your model that you're pre-training as a kind of denoising function and so what you're going to do is you're going to feed in various masked versions of your sequence or kind of sequences of words or documents that have had Transformations applied to them like for example you either kind of block out big chunks of the input or you permute stuff around and now you're trying to predict the unpermuted order so these are all various pre-training tasks kind of akin to the masking and Bert that are going to ideally enable our models to learn good operations for transducing sequences right uh so basically the model is going to copy some stuff from the input but then also make inferences about certain tokens so how does this work architecturally well what we do is we take a seek to seek Transformer so the kind of full version of the Transformer architecture and we feed in into the encoder this noise version of the input and then the decoder needs to produce the clean version as output so if we compare this to Bert Bert in some sense only has this encoder going on right it uses a bi-directional Transformer over the encoder and fills in these masks Bart also has that but Bart also has this decoder and the encoder doesn't isn't actually responsible for mask prediction instead that happens in the decoder but theoretically Bart is sort of a superset of Bert in that you could take the encoder and use it for any task where you are going to use Bert but now instead you also have this uh kind of seek to seek Transformer that is going to be able to do other tasks for you and as an example of what it can do one of the headline results in the BART paper was on text summarization so just to kind of go through the data steps of how this works we pre-trained the model on the BART task so these random chunks of text that are noised according to what we described a few slides ago and then you try to produce the clean text and then you fine-tune that model on a data set of human written summaries basically a news article and then a summary that an editor wrote of that article and the kind of thing it will produce looks like this so it can take this article about Elliot kipchoga running a marathon in less than two hours and produce a summary that kind of summarizes this succinctly now this kind of suggests why the BART objectives might be a good idea because if you kind of mask out some information from the input and then Force the model to infer it that's kind of the same as being able to consider a bunch of stuff from the input and be able to infer oh okay the important thing here is that he ran a marathon in less than two hours so this kind of special pre-training technique the fact that it's fit to the summarization task is helpful and kind of suggests why some of these seek to seek approaches are going to be effective for some of these tasks and idea which we'll come back to later that's the end of the segment\", metadata={'source': 'M9L3gk4ITec'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about T5 which is another sequence to sequence pre-training scheme similar to Bart in fact they were developed basically at the same time where T5 was developed at Google and Bart was developed at meta or at the time Facebook and t5's pre-training objective is very very similar to Barts it doesn't have as many of the bells and whistles but it has the kind of important component which is that we take a original text we mask out pieces of it we have that as our input and then we feed that into a Transformer and in a seek to seek fashion attempt to predict the targets here now one key difference from Bart is that unlike in Bart where we predict the entire original text as the output here we only predict these masked sequences so it's a slightly different shape for the output and it leads to T5 being slightly better than Bart at some task and slightly worse at others Bart for example was used a lot more for summarization partially because it's possible that the pre-training on to produce entire grammatical outputs uh was a little bit more compatible with fine tuning on summarization but largely these models do the same thing and T5 was also shown to work well at a lot of tasks where sequence to sequence models were standard in particular English to German English to French and English to Romanian translation numbers here were given and this was one of the first uses of big pre-trained models in machine translation now one of the other things that the T5 paper started looking at which has since become a very important aspect in all of pre-training is the data that gets used so they tried to collect the basically biggest data set they could called the Colossal cleaned common crawl or C4 and they looked at how much actually having all of this data was important for pre-training so across these different rows of the table they look at basically make taking smaller amounts of the data but then copying and pasting it so you end up with the same total size of data set so you'll train for the same amount of time but it's just repeated more and we see that actually having more unique data here does lead to Improvement and it doesn't really seem like we've necessarily hit the limit of that yet which is going to come up later when we talk about the bigger more modern pre-trained models the other thing that T5 introduced was a big focus on actually multitasking between the fine-tuning tasks themselves part of the idea was that we would learn one model that would not just be pre-trained on this data but would also be fine-tuned on a bunch of data sets and be be able to do different things as a result so this wasn't really delivered on uh fully in the T5 paper but there's a nice example of it uh from a year later from khashabi at all a system called unified QA where they took a big set of QA problems and tackled them all with a single T5 model that was fine-tuned on a bunch of existing data sets so for example the squad question answering data set has these inputs that look like these paragraphs and then the outputs are selections from that paragraph but we could just call that a seek to seek problem right we could just take this input and then say hey the model needs to generate this output we can do the same with question answer data sets where the output does not appear in the input we tell the model you need to generate a new string as output but that's fine uh it's kind of within the capabilities of the model whether it'll work well or not is another question uh so the model can be fine-tuned over this type of data set and what we saw previously kind of at the same time and in fact they unified it with a couple other for uh formats as well like multiple choice question answering where you're given a set of options and you need to return one of those options and Boolean yes no question answering so they took a model and trained it over all of these data sets at once and this big T5 model ended up doing quite well at all of them so I'm not going to walk through the results of this specifically but it shows the idea and the flexibility of these kinds of pre-trained sequence the sequence models you can take them and throw them in a lot of tasks and start to build these kind of generalist models which started to pave the way for the capabilities of things like chat GPT that's the end of this segment\", metadata={'source': 'b6KFaT8mK4g'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about techniques for handling rare words and uh kind of breaking up things into smaller pieces for the purpose of sequence to sequence modeling so words although they're kind of linguistically well motivated they're sometimes a fun kind of difficult unit to work with we've talked about how there you can kind of hack in stuff to do copying of words that you haven't seen before from the input into the output but you always have to struggle with this idea of how big is your word vocabulary and then if it's too small you need to do a lot of copying and that's hard to implement and if it's too large then things get really slow because you always have this soft max or you're placing distributions over the whole set of words so one solution that people explored kind of briefly this was big in about 2016 2017 was character level modeling so why not just kind of produce things at the character level rather than the word level and the problem is characters are kind of too small a unit uh we're with uh we've talked about how lstms can't actually remember things for that long a distance and so now instead of producing eight words you're producing something like 60 characters and that's not uh you know that's pretty hard to model so the compromise solution that the field has largely converged on is to use what's called what are called subword tokens or a subword tokenization scheme uh and these tokens might be whole words or they might just be parts of words and so for example in this example that we've seen before the ecotax portico and pond we um we have a couple of words here like ecotax where an underscore indicates that uh it's a unit beginning a word so what we've done is we've broken ecotax into two pieces underscore eco the beginning and then tax which doesn't start with an underscore so we know that that's continuing the previous uh the previous segment and then portico and pawn debut also get chunked up in some way here right and then there's also a similar chunking on the output side and the nice thing about this is that we see that ecotax can now be directly mapped into eco-tax and so for the purposes of machine translation which we'll talk about uh later we can do things like transliteration by sort of translating each of these segments right and this structure is pretty nice and is a lot easier than trying to handle the whole word ecotax all at once so one of the techniques that people use for this is called bite pairing coding what you do is you start with each individual byte or roughly a character as its own symbol and then you merge adjacent pairs so basically you have some kind of vocabulary you get your bigram statistics from that vocabulary what are the pairs of characters that are uh you know that are most frequent together and then you merge some of those most frequent characters and now your vocab consists of your original characters that you had before and also super characters which are now now reflect these merges uh and so when you go through this process you typically use some kind of weighting of your vocabulary to prefer more frequent words and so what happens is you know very common combinations like th will show up first and then maybe you'll get t-h-e the word the in english because that word is very common in a large corpus and so that then becomes a quote-unquote sub-word unit that can actually just be used as a as a word in a lot of cases and so people do a fair number of merges a lot of times the vocabularies here are you know in the thousands or tens of thousands and what this gives you is actually a lot of common words and then rare words get broken up in these nice ways so these are two real examples of what a bpe tokenization does and there were no refueling stations anywhere everything gets preserved as a complete word but refueling gets broken up or in the next example unprincipled gets broken up into three pieces there's an alternative technique called the word pieces method this largely does the same thing but instead of thinking about just merging pairs that are frequent together it actually thinks about this from the standpoint of language model perplexity so basically if we want to explain some corpus of data how can we find a segmentation or sub-word tokenization that is the easiest to language model so there's a library from google that implements this uh and this is how they do their machine translation on google translate the uh you know you can ask how this might compare to the bpe method um one of my students kai bostrom has looked into this question and what we found was that actually bpe does a much worse job at coming up with linguistically plausible segmentations than this unigram lm method so even though the bpe method is popular and initially got some traction the unigram lm method is much better at producing segmentations that look morphologically and linguistically plausible for example in the top right here tricycles is broken into these very natural units tri cycle s rather than just some kind of random you know segmentation that doesn't really correspond to the underlying morphology and this actually matters when we'll talk later about pre-trained language models like bert and gpt these all rely on subway tokenization schemes and so having the right subway tokenization scheme is then very important because that influences your pre-trained model which will then influence everything that you do based on that model there's also been some nice work which looks at ensembling across multiple tokenizations so having multiple different subword schemes and then having kind of different models that then uh work together to do a better job at translation or or whatever so this kind of shows how we can keep this modeling paradigm that we have that we have built up where we've talked about things in terms of words not have to worry quite as much about copying because now every word can be represented as some sequence of sub words so we don't have to think about having to copy to introduce new words and uh this is going to be a very effective approach for kind of navigating this boundary between word level and character level modeling that's the end of the segment you\", metadata={'source': 'WA16JelEkkg'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to introduce the concept of part of speech tagging so a lot of what we've talked about so far has been uh things like bag of words techniques and particularly for various classification problems so this is a you know fairly effective way to do a lot of classification if we want to uh you know if if we want to predict things like sentiment and do so with uh end end neural networks but if we want to start to build up towards more structured linguistic abstractions we need to start thinking about how to represent uh basically how to represent what words in language mean and how they function in a slightly deeper way and so part of speech tagging is going to be our first step towards doing this we're going to look at language as kind of the sequence of uh syntactic abstractions over the words and see what that allows us to do so the reason we might want to do this is the following let's say we have this word and we want to feed this into a TTS system now what should TTS say so there's two ways to pronounce this it's either record or record um and we can thank the English language for being weird in this regard uh but basically in order to determine this we need to know whether this is a noun or a verb right are we recording something or are we putting on a record to listen to so another another thing we might uh kind of be interested in is let's say we're doing some kind of information extraction um and we're going to take the example of arms so is this a noun or a verb are we talking about arming people or are we talking about you know either our arms or weapons or something like that and so the you know obviously there's kind of deeper questions that we want to be able to answer here for example if we do have a noun form of arms what kind of arms are we talking about but this is a first step towards uh at least getting some idea as to uh what's going on you know deeper than the surface structure deeper than just the sequence of uh characters associated with each of these uh associated with each of these words and the other the the last thing that this is going to really be is it's going to be uh a kind of first stage towards uh thinking about syntactic parsing and parsing is going to let us take large and complicated sentences and uh you know turn them and and basically start to unpack their structure and so this is going to be a building block towards allowing us to do that okay so we have to think about uh what the part of speech tags are and and what they mean and we're going to kind of go through a crash course in that so that we understand the kind of linguistic abstractions we're dealing with going forward so there are going to be two rough categories here um we are going to have what are called open class tags and roughly these are ones where you know you can imagine like you know new words uh can join these categories um and so you know an example of an Open Class uh part of speech tag are the nouns uh so we have both proper nouns like you know IBM Italy and also what we call common nouns um than things like cat or snow and so the reason this is open class is because we're inventing new devices all the time right you know computers didn't exist and then they did um you know sort of new proper nouns associated with companies and things like that there's always new words joining this class um verbs this is another uh another one uh adjectives uh and adverbs um you know things like swiftly that are that are kind of modifying verbs here all right so in terms of Clos class tags we have uh a few more categories that are typically related more to kind of function words and uh these you might be less familiar with so the first one we have here are determiners um which are going to be the some and and other words like this so this includes articles and basically uh these are words that uh can modify nouns as part of noun phrases and so what they allow us to do is say you know the cat versus a cat the choice of article or determiner there is going to change how we uh interpret that noun those each of those noun phrases um whether we think that we're talking about some generic cat or you know a cat that's already in the discourse or something else conjunctions are another one and or um pronouns um and then we have a couple of verbal categories here like auxiliaries um for example had when it's followed by a verb so um you know just saying like I had three apples uh is not an auxiliary but I had gone to the store when blah blah blah that that is an example of it as an auxiliary um and then also modals which uh you know are words like could like I could have gone to the store that implies uh a certain modality of the uh you know about the statement you know your ability to do it in this case so uh these are both uh these are both sort of types of verbs that typically get tagged in a different um in in a different fashion and uh you know they're closed class because we're not we're not always coming up with uh new constructions basically surrounding uh different types of modality or or different ways of expressing tens and and things like that uh and then the last two uh list here are prepositions um things like up into um so for example when you say like hike up a mountain you know that's a preposition there's also a notion of what are called particles um which overlap heavily with the prepositions um but these are going to be uh used in a slightly different way so for example when we say we I I made up the story um this is what's called a verb particle construction and you there's sort of like one way to think about it is there's not really a kind of spatial aspect associated with making up a story right um it's really this uh this word that's kind of combining with the verb in order to get its semantics and uh so in in contrast with the preposition it's it's behaving a little bit differently but the the words that actually get used to overlap there and so this is an example of why things are kind of ambiguous from a part of speech perspective Beyond things we've already seen before okay so these are examples this is kind of a crash course in some of the basic uh syntactic uh part of speech categories that we're going to see now let's look at an example and this is example is going to help us think about what kinds of ambiguities show up and why this task of trying to take a sentence and analyzing what part of speech uh each word falls into might be difficult for uh an automatic system so fed raises interest rates 0.5% so a lot of the examples a lot of the early uh work on uh syntax and NLP for English was done on this data set called the pent Tree Bank taken from the Wall Street Journal so there's a lot of uh a lot of kind of financial text in here so what you could do is take a minute and think about what are the different Poss possible part of speech tags that might be associated with each of the words in this sentence so if you if you look at it and kind of think about each word in isolation you should be able to come up with uh a kind of set of possibilities and so I encourage you to do that you know but now I'm going to tell you what uh what they are so I'm going to use part of speech tags that correspond to the categories on the previous slide but are actually a little bit more refined these are the actual um categories in the pen Tree Bank data that you uh are going to see some throughout this course you're not expected to have uh kind of encyclopedic knowledge of these I'll kind of Define them as we go so fed the you know in the canonical way that we're interpreting the sentences the the sentence um you know the uh you know we are we are using this as a proper noun uh so that that gets the tag nnp um two other possibilities are two different types of verbs so what is the difference between these things um so when I say like I fed someone something that's uh vbd that's just like past tense if you said I had fed there we get this participial usage of it and uh it's vbn in that case Okay so actually for that word which you know the first time I showed you the sentence you probably had no trouble understanding there's actually three different tags uh and three different interpretations for it all right raises um this is a PL this is a plural noun um that's what nns means um but the other possibility is it can be a verb right um like he you know raises up the his hand interest uh again this can be a noun um or two different types of verbs for example uh I interest you in NLP hopefully that's a vbp um or I want to interest you and this is the this is the infinitive form of the verb um so that's that's just this this this kind of bare VB here all right rates again plural noun or verb uh and then 0 five in perc uh each only have one interpretation okay but even setting aside all of the kind of crazy tags that can't show up in the sentence we actually get a whole you know pretty large number of different interpretations right um so the number of paths through this sentence if we think about it uh is 3 * 2 * 3 * 2 which is 36 so we're going to need a model that can look at all of these 36 paths and decide what is actually reasonable now another thing I want you to think about is what are the reasonable paths to this sentence so which one corresponds to the kind of correct intended interpretation and are there others which correspond to some interpretation that that might be true even if it's a little bit weird so the correct interpretation is this one the FED raises uh where the FED fed is a noun raises is a verb and then the interest rates uh is this noun noun compound um now notice you you you know we're often going to see cases where like two nouns get like jammed together into a noun phrase that's okay it doesn't interest is not an adjective in this case um it's okay to have two nouns kind of combining in this way all right there is another interpretation though which I want you to see if you can spot another interpretation that you can get for this sentence is the following so roughly what we have here is we have an idea of fed raises as both nouns and then interest becomes the verb and then rates and and uh you know rates is a noun again so uh you know thinking about this sentence for example um what we what we sort of have going on here is we have this idea of fed raises and they are interesting to the rates they are causing rate to be interested rates in this case are some sort of animate thing that can express interest um and they're they're interesting them a little bit they're they're interesting them half a percent okay so this is a little bit kind of nonsensical here um you know it it sort of doesn't we can't really think of why someone would say this but uh you know if you stretch your brain a little bit this you know the sort of an Alison Wonderland way where there's rates that are kind of running around like you can contort yourself to believe in this interpretation and in particular this is going to be something that's very easy for a part of speech tagger or later syntactic parsers to produce because you know we're really using our our kind of World Knowledge about the fact that like rates are inanimate and so blah blah blah but like syntactically this is completely valid so this kind of illustrates two things here one is it gives you some practice thinking about what part of speech tags we have here how they interact Etc um and it also shows you an example of the kinds of ambiguity that we're going to need to be dealing with and how challenging it is to to part of speech tag even a relatively simple sentence like this one that's the end of this segment\", metadata={'source': 'Llw6qfeAWDs'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to approach the problem of part of speech tagging we're going to define it as a sequence labeling problem and we're going to talk about how to use basic classifiers like we've seen so far in order to do this so the idea behind sequence labeling is that we have an input uh which we're going to represent as x here and we're going to think of this as a sequence of what in our case is going to be words x1 through xn so previously when we thought about uh you know when we thought about extracting features and things like that uh you know we didn't necessarily think about the words as the sequentially structured object now we're going to think about them in this way and our output y is going to be the same length as the input now so previously we always thought about why as some kind of uh binary or multi-class prediction right some single discrete label associated with a document or sentence or whatever now we're going to think about it as one prediction per per word and so this is an example of what's called structured classification and the reason it's structured is because these y's are not somehow just like a bag of independent predictions but they actually have this sequence associated with them where like y2 comes after y1 and comes you know a certain number of positions before yn all right so what we could try to do i think is instruct it's instructive to think about how to do this with the current tools that we've built up so far so uh so let's try to predict each yi independently with logistic regression so that's going to be something that looks like this now uh over on the left here we have y i equals y so we're going to use y to reflect the kind of uh i guess part of speech tag or whatever that we're thinking about predicting here um and we are going to need to uh and and we're thinking about assigning that to uh position y i and notice we're conditioning on two things on the right side we're conditioning both on the input sequence and that was how we kind of normally did it in logistic regression and also on this index i um and here's why that's going to be important so previously we had bag of words features and we said okay you know when we extract features like f x uh uh actually i'm gonna use the um the different features notation here um this is because this is really where that stuff kind of comes into play here um you know if we are thinking about assigning a uh label y to a you know to a an example that we're gonna associate with a particular bag of words feature set well the the kind of basic bag of words features might look something like you know this where you have you know blah blah blah where you have a one for fed a one for raises um you know and then other uh you know other kind of ones in there as well all right so you know let's say we want to make this uh prediction here we want to say okay what's the probability that uh you know and we're gonna think about let's say i equals three um so we're gonna think about uh kind of making a prediction associated with the third word and you know maybe the tag we're going to think about here is nn then what we get remember is this kind of block structured thing where we have this f of x vector and we have a bunch of different copies of it and we're going to get the ones associated with the nn tag and then we're going to have 0s elsewhere there and we're going to have zeros zeros kind of everywhere else okay the big problem from our perspective is that this is independent of i so this model is go or or rather this set of extracted features just looks at the sentence and the part of speech tag it doesn't look at the word that we're actually trying to tag here and so that's kind of a problem from the perspective of tagging right in that we're not making use of that information we're just going to predict like the same tag distribution everywhere and this is generally not going to work as a tagging model instead what we need is a kind of positional view of our features we need to look at x we need to look at the fact that the label that we're thinking about is nn and we need to think about the position so uh one way to do this one very simple way is to just have a single feature on the current word here let me let me write the example up here fed raises interest rates 0.5 percent so when i say a single feature on the current word what i mean is that uh we're going to have a vector where uh you know again we're going to have all zeros except in this nn segment so zero's everywhere else and we're just gonna have a one associated with interest and everything else is going to be zeros all right so basically what we've done is we've taken this back we've taken our bag of words and sort of masked it out right um we've said okay we're only going to look at the current word and so this is at least a little bit better right in that we can now say okay we're predicting a part of speech associated with interest and so uh you know we're at least gonna get the uh we're at least gonna have some notion of what the word we're currently looking at is and what and what part of speech it might have we could do better than this though um and here's what that's going to look like all right so what we have is we have our big uh block structured feature vector where we've got you know the vbz tag over here and then we're going to leave ourselves plenty of room for the nn tag and then we have all the other tags right okay and then within the nn tag what we could do is we could say okay we're going to have one set of positions that correspond to the current word and then we have one set of positions that correspond to the previous word so in this case the indicator here would be about raises and then we can have the next word as well and this word is rates so the way we can think about each position in this vector space is as a conjunction of several properties uh and we we call this an indicator and in this case it's an indicator that the current word you know which is defined with respect to this position i equals three right current word equals interest and the tag equals nn here all right and so the the tag information remember came from uh you know came from kind of where we are in this big block structure thing the current word bit came from the smaller block structure within that and then um interest was associated with this particular position in the vector space so it turns out we can we can avoid having to deal with this whole big you know block kind of block feature vector idea just by saying okay we are going to treat this as a word in a bag of words space essentially we think about our feature space now as talking about properties of uh the classification instance and in our conventional bag of words the property was just is this word included or in some cases how many times was this word included now we're thinking about this sort of more complex set of properties in this case this position has the associated property ker word equals interest and tag equals nn remember the feature function you know depends on y in this case um and so that kind of word lights up and uh gives us a one in this whole big feature space so we don't have to think we don't have to actually think about kind of managing this block structure we can again just kind of sort of throw these things in a big index and uh access them later okay so if we take uh so we could take these indicators and feed them into a classifier and so we can we can basically take a labeled data set of part of speech instances treat each position in every sentence as a example and feed that and train on that and produce a classifier all right so what goes wrong the the kind of problem is that we're not making use of the output structure at all so for example we had different possibilities for raises and interest rates right and it turns out that some of the edges that we kind of considered here are not good like for example we're not typically going to have a plural noun followed by a noun we're also not typically going to have a vbz followed by a vp of vbp and so the the predictions of a classifier may be incoherent uh meaning kind of locally each prediction looks reasonable right but the overall structure doesn't kind of add up to be the thing that we want and so this is going to lead us into our idea of sequence modeling and we're going to think about two models hidden markov models and conditional random fields these are going to be two things that we're going to spend some time unpacking throughout the next section of the class and essentially these are going to be two kind of contrasting ideas for how to deal with this hidden markov models are going to be our first example of a generative model and the they're gonna look a little bit different from other things we've done but they have some attractive properties uh conditional random fields are gonna look a little bit more like building off of this classifier view of uh tagging but uh making the output predictions coherent that's the end of this segment you\", metadata={'source': 'yQZ0mDW-U3g'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about hidden markov models we're just going to set up the basic definitions and see how this model is defined what its parameters are and what it can do so hidden markov models are a generative sequence model that is going to allow us to uh represent uh tasks like well particularly tagging tasks like part of speech tagging in a nice uh kind of modeling framework so uh first we have uh so we we have we have basically two types of objects here um we have uh tags y i which we're gonna say are in this uh this tag set and we have words x i which are in a vocabulary all right so the definition of a hidden markov model is the following well this is this is kind of one way well this is there there are a couple of slight things that that might differ between definitions but uh this is basically the the kind of standard form of these okay so what's going on here so let's look at this graphically uh using uh bayes net notation so we start off by sampling a tag y1 and then based on that tag we sample a word x1 so when we decompose the probability in this particular way one one kind of term that's used for this is the generative story of the model and so the generative story of this model is saying that all right we have some distribution over tags that can start a sentence y1 and then condition on that tag we generate a word which is going to be the first word of the sentence and then we generate a second tag y2 and then the second word x2 conditioned only on that tag y2 and then we kind of continue until we have y n which gives x n and then stop okay so this is a this is a model that makes uh certain independence assumptions and essentially the reason it's called a hidden markov model um is because the y's form a markov process meaning y i depends uh or i'll say this formally is conditionally independent of y1 through y i minus 2 given y i minus 1. so it's not right to say that it's completely independent of the uh the kind of earlier states in the process because the earlier states in the process influence what y i minus 1 is but if we know what y i minus 1 is then kind of that fully determines the distribution over y i and so the kind of idea here is that the part of speech tags themselves are giving us this sort of syntactic skeleton of a sentence right like okay you're going to have a maybe if you have a determiner and then you're going to have a noun and then maybe there's some chance of having a verb after that right and then after a verb doesn't matter that there was a noun before that you know we're probably going to have a noun afterwards or maybe a preposition and then after a preposition okay now we're going to get the object of that preposition uh etc and then the other assumption here is that the words are kind of conditionally independent of each other given the tags so x2 doesn't depend on x1 but instead is only going to depend on y2 so this is not necessarily a good assumption because if we have x1 and x2 as uh for example new york the fact that we have you know a kind of proper noun phrase or maybe san francisco is a slightly better example you know given that we see san there's only you know not that many words in english that are going to come after that and uh but but right now we're just saying okay we have a we have a proper noun and then a proper noun and you know san francisco you know that's possibility that's a possibility um but san york or new francisco these are also possibilities so these are assumptions that the model is making um and the reason you make these kinds of assumptions in generative models is for uh is for tractability basically um so that you're not having to deal with um you know conditioning and thinking about a whole bunch of different variables at the same time okay so what we also want to think about are the parameters of this model so we have p of y1 which is called the initial distribution and so i'm just going to draw that like this we're going to call that this s and this is a this is a t length vector so it's going to it is a probability distribution so unlike a lot of the other models we've seen like bag of words models where the parameters can be any real valued uh or any real number here the parameters have to be real numbers between 0 and 1 that sum to 1 here right because they're probabilities and this is a distribution over what the possible starting tags are so then we have uh these y i given y i minus one terms uh these this is these are called transitions and so the way that's going to be represented is by t um so we're going to use the kind of square t here for the transition matrix um and the you know tau or squiggly t for the uh the set of tags and we could think of that as a t by t matrix here where uh essentially the value of a particular cell is telling us uh what is the probability of why i give it of uh let's say tag uh well actually let me change this a little bit um i'm going to write uh y cur and y next um so it's going to tell us what is the probability of the y next tag given the y-curve tag and so these transitions are independent of the positions in the sentence so there's there's no dependence on i or anything there it is just a matrix of these probabilities defined over um every pair of tags all right and then we have these p x i given y i terms and these are called emissions and the parameters here i'm going to draw a long kind of long matrix here uh we'll call it y cur and x curve and we're going to call this thing e and is going to be t by v again a matrix and again uh it's it's just encoding the probability of given tag say nn um you know i'll draw kind of a slice here it says okay given tag nnn what is the probability of seeing each word in the vocabulary and so we do make an assumption that there is a fixed vocabulary here we're placing a distribution over words in that vocabulary and so given tag nn we need each word to have some probability and those probabilities need to sum to one okay so there's two steps that we need to take in order to use this model the first is parameter estimation you know given data how do we get values for these parameters and the second is inference so given this model uh how do we actually use it to part of speech tag a sentence and so these are the two topics that we're going to talk about next and that's the end of the segment you\", metadata={'source': 'FeLtLLbn4qU'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to do parameter estimation in hidden markov models so we're going to assume that we have some labeled data for our tagging task that is going to look like the following okay so this is similar to our notation for labeled training sets for classification um the only difference is now we assume that we have a whole bunch of sequences um that which are the xi's and then for each one every word is annotated with a particular tag and those are the why i sequences all right so we're going to appeal to the same idea of maximizing data likelihood that we saw in logistic regression but now we're going to maximize this and again we've uh kind of taken the log of this already and logged some um so this is the generative probability of the data or the or the rather the joint probability of of y and x so we can call this the generative likelihood so recall that this is a generative model is a probability distribution over y comma x not y conditioned on x like logistic regression is and so when we look at this data we're not just thinking about maximizing p of y given x but we're in fact thinking about maximizing the joint distribution here all right so we can unpack this using uh the uh using the definition of the hmm as follows okay i'm not going to get too precise here about the sums um but essentially the sums over i are sums over the training data so if we consider this first term we're thinking about the log probability of y i uh of y one for each sequence i in the training data and so that's a different y one for each sequence right and then for the second and third terms we are looking at a sum over i which is over the over the uh training data and then sum over j which is over the like sentence um the index or the kind of position within the sentence right and so it's saying you know we have to loop over all the sequences and then loop over all the words and uh accumulate the probability of in the second case seeing the uh you know that x that x given that y um and in the third case seeing that transition okay so this is a big long gnarly complicated thing but fortunately the estimation for this is going to be very simple um so so uh i'm going to call this mle with frequency counts so suppose we have a biased coin with probability p of heads and then we observe the sequence h h t and i ask you what is the maximum likelihood probability p for this coin so given these observations we don't know what p is what's the value of p that maximizes the data likelihood so if i if i were to just ask you you know or you know asks ask an elementary school you know student like what's what seems most likely given this um you know ignore your prior that it's a it's an unbiased coin you know you might just say it's three-fourths right because uh we see three h's and one t and so uh it looks like h shows up three-fourths of the time right uh and this actually turns out to be correct um and so the way you can kind of validate this for yourself is the likelihood of this data is 3 log p plus log of 1 minus p and again that's just basically saying the log probability of the first event h is log p log p log p and then log one minus p because tails shows up with probability one minus p um so that's uh the the kind of well that's the arg max value here of of this expression okay so the basic moral the story is hmm parameter estimation doesn't involve gradient descent or anything like that you could do it by counting and normalizing and what i mean by that is that we're going to look at our data we're going to look at instances of each of these events of initial samples of emissions and of transitions we're going to count up occurrences of those and then we're just going to normalize them to make them probability distributions and it turns out that this is the correct procedure for maximizing this likelihood okay so let's get that gnarly looking likelihood thing out of here and we can look at an example and see how this works so the tags we're going to use a simplified tag set with three tags noun verb and stop so what i showed in the initial definition of the hmm is that at a certain point when we get to the end of the sequence we have to transition to the stop tag and that's what's going to tell us that you know okay the sentence is done and our vocabulary is they can fish so all these they can fish examples are taken from the jacob eisenstein book it's a nice simple example to deal with here and so let's say our data is the following remember these are going to be labeled instances so so we have they can and they fish and each of these is labeled as noun verb stop okay so if we think about the start probabilities remember that this is going to be a vector with two positions um n in v so technically it's defined over the entire tag set but stop is never going to be the first thing in the sentence so we're just gonna kind of drop it and not and not show it here and we can count up our uh we can count up our occurrences here and we get two sequences that start with n and two that's and zero that start with v and so when we normalize this we get probability one and probability zero for n and b okay tags so again we're going to draw this as a rectangular matrix i said it was going to be square but uh we are never going to transition from stop into anything else and so given that we don't need to we don't need to ever have it as the kind of you know probability of the previous thing okay and so if we collect counts we get this and then when we normalize that we get the following and then finally for the emissions all right so we have two occurrences of they as nouns and we have one occurrence of can as a verb and one occurrence of fish as a verb all right and so when we normalize this one we end up with something a little bit more interesting which is which is the following i'm just going to abbreviate these uh these words so the thing to notice is that this one half this is not the probability of like verb given can or anything like that right it's not saying if we see can what's the probability that it's a verb it's the probability of can given verb so it's like when we see verb what is the distribution over possible verbs and this is generally going to be some very large very flat distribution where there's going to be a ton of possible things they're all going to have like one percent probability or less but it's very important to keep straight the direction that we're thinking about this uh you know this probability distribution going in all right so the the last kind of detail i'll add here for the parameter estimation is smoothing and the idea behind smoothing is just that uh it's a form of regularization in the sense that we might not want to directly maximize the data likelihood we might want to instead add a small amount of kind of fake data to avoid getting a sort of assigning zero probability to sequences that you know maybe you don't actually have zero probability if we really think about it so the this is called smoothing because the distribution right now is very peaked like you know for example we say there's a hundred percent chance of noun occurring as the first tag um and when we smooth that we're gonna end up in a situation where we say okay there's actually only you know 99 chance so for the sake of math let's just say that we turn our transition counts into the following um so we just like added one everywhere and then if we normalize this we get the following okay so this tells us how to uh estimate these parameters so now given an example um like they can fish what you should be able to do is compute the probability of the joint probability of this example here given these estimated parameters and if we do that so we have to think about y1 x1 given y1 y2 given y1 x2 given y2 y3 given y2 x3 given y3 and then stop given y3 and the numbers you get for these let's kind of do this in place are 1 1 three-fifths one-half one-fifth one-half and three-fifths and so notice that this uh this v to v transition would have had probability zero if we had not done the smoothing and so even though we only smoothed the transitions it turns out it was enough for our hmm to be able to say okay you know they can fish that's at least something that you know i can imagine seeing and assign it non-zero probability now the uh the model still assigns it a very small probability but in some ways this is kind of correct if you think about it in that this is a distribution over all possible sequences of words and part of speech tags and even in this relatively small vocabulary there's still a whole bunch of different uh kind of sequences that can show up and you know the model has to assign some probability mass that sums to one over all of these possible sequences so here we see how to go from a corpus and counts to basically count up this data and normalize it and then that gives us our parameters which turn out to be the maximum likelihood parameters here so now that we have the parameters for this model we can come back to our original question of how do we actually use this for part of speech tagging and that's what we're going to talk about next that's it for this segment you\", metadata={'source': 'dVF7LZkbl9g'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to do inference in hidden markov models using the viterbi algorithm so hmms are a model of this joint distribution over p of y and x given by the following formula that reflects this kind of markov assumption over the ys and this emission distribution where each x is emitted from a corresponding y but the thing that we really want to compute for inference especially from the perspective of a task like part of speech tagging is the arg max over y of p of y given x that is to say given a sentence what is the most likely part of speech tag sequence that could have produced that sentence so this is a little bit tricky because our model is kind of set up in the other direction in some sense like it it kind of generates a sequence of y's and then generates x's conditioned on those y's how do we flip it around to actually figure out what the most likely y's are given the x's so we can do this by a little bit of rewriting and rearrangement so this is equal to the following and so what we've done here is we've multiplied the top and bottom by p of x and when you multiply the top by p of x it kind of combines via the chain rule to give you this joint distribution and now the bottom is this p of x term um and this term is constant with respect to y so from the perspective of arg maxing over y we don't actually need to worry about it um so we can say that all right the kind of numeric value of the max we get from this won't be the same as p of y given x but uh it'll give the same maximal sequence y um and then we're going to apply our other favorite trick here when dealing with maxes which is that we want to max the log probability uh of this all right so now that we've rearranged this we've got something that feels a little bit more tractable um and now we're going to expand this out and i'm going to write these as y twiddle here and so on and so forth so we have this fairly complicated expression and we can't max over it directly obviously because this would be require an exponential search over all the different possible values of the of the y twiddle eyes and so what we're going to do in order to do this max is appeal to dynamic programming and so i'm going to tell you basically what the dynamic program looks like and then we're going to see how that kind of falls out of the structure of this computation and in particular how it takes advantage of this markov property of hidden markov models okay so so we're going to define our dynamic programming matrix here and this is an n by t matrix where n is the sentence length and t is the number of tags in our tag set and the semantics of this is going to be the score of the best path ending in y twiddle at time i so kind of starting from uh position one at the beginning of the sentence we're going to compute a partial max but that max is going to be anchored to this particular uh tag y twiddle at this time step and so uh you know work it's going to represent some amount of kind of maxing and and having done inference but it's also going to maintain uncertainty in the sense that at a given time step we uh are tracking the best max for each possible tag that we could be in at that time step all right so let's define our dynamic program uh here so v1 of y twiddle is log p of x1 given y twiddle plus log p of y twiddle and this is basically uh you know when you tell me okay i have to be in this particular state at the first time step uh this is the score that is associated with that and uh you could think about this as kind of all of the terms that in that that kind of come up to that point in uh the computation of the uh the joint probability so um kind of graphically again we're kind of thinking about okay what are all the scores up until this point well you know there's two nodes that we need to generate here uh we need to generate this one and we need to generate this one and then it is true that this next node over here depends on uh the value of y1 but that's going to fall out in the recursive case and so here's what that case looks like so we have the emission at the current time step and then we have max over the previous time step so what we're doing here is we're saying okay in order to compute the you know we've computed basically the scores for let's say the first time step here and then in order to compute the scores for the second time step what terms do we need to account for well we need to account for this emission here which corresponds to which corresponds to this term and we also need to account for the transition now the transition depends on the value of the previous uh the previous tag here and remember that at the end of the day our goal is to compute the the kind of best sequence ending in this tag at this particular time step and so what we do is we think about okay what are all the possible previous tags that could have brought me here and so that's what this max over why twiddle preeve is doing we are considering all of these different possibilities and saying okay for each one we already know the best path that gets you there and then we can compute the transition probability and then add the new emission and then that's going to give you the best tag or the score of uh why twiddle for the best path getting you there at this time step and uh you can kind of proceed from there all right so just to motivate a little bit why this works um like recall that what we were doing uh was maximizing y twiddle n through y twiddle one i'm gonna write it in this direction um etc if you just look at this sequence of terms which i'm not going to write out it turns out what you can do is you can move the max over y twiddle one basically all the way over here to the beginning and you just end up with those first couple terms involving this uh you that are involving y1 here and so it kind of very naturally emerges from just looking at what happens when you apply these maxes to the sequence you can kind of commute the maxes past all the terms that don't involve them and then if you kind of deal with the sequence in this like right to left fashion and say okay uh i'm going to first figure out what's going on with this y1 term and then and then kind of push it forward from there and the only thing that makes it tricky are these transitions now notice again that the reason that dynamic programming was necessary to begin with was because we had these transitions involved right if we didn't have transition probabilities if it were just emissions we would be able to just independently max at each time step but the transitions sort of end up coupling everything together but because of this markov property where we only depend on the one previous time step we can get away with this relatively small dynamic programming chart that just maintains basically a score for the kind of tag at the current time step all right so let's just make this algorithm a little bit more explicit okay so this com this computation of vi of why twiddle is just going to use the formulas up above uh and then at the end and you you basically have to push things forward and account for the stop symbol and then uh this equals the max over y log p of x comma y and so that's the result that we want although hey wait a minute what we're doing right now is we're computing a max and what we wanted was the arg max we wanted to know the actual path rather than just like the numeric value of this log probability um and so there's an extra step here that i'm not going to show where you have to track what what are called back pointers uh essentially at each step when you compute uh when you compute the vi of y twiddle during the recurrence there's some why preve that this came from and you want to keep track of what that is because then at the end of the day you want to say all right here's the you know here's the final score i got what was the kind of previous tag that along this path that got me here and then you kind of unwind it and you can go back and actually reconstruct the sequence of tags that got you here in addition to just the score okay i'm going to very briefly show an example of what this looks like so we're going to follow uh the same rough template for this that we've been using for a couple of examples so far and what i'm showing here are log probabilities so these are not exactly going to sum to one but the viterbi algorithm although we're defining it in terms of probabilities you could actually deal with any uh re kind of real valued scores here we're gonna see that when we come to conditional random fields and uh in fact the eisenstein book kind of defines it in that terminology as well so we have our favorite they can fish example all right and so what we do is we define our chart v um i'm going to write vi of y twiddle here it's a little bit overloaded with the verb tag and we're going to do the following sentence they can can fish meaning they can put fish in cans right and then we're gonna have to have this extra uh kind of column i guess for the stop symbol um and you know we're gonna we're gonna think about basically transitioning to a kind of a special a special state there so we are not going to go through all of this it's a good exercise if you want to check your understanding i'm just going to fill in the first two values here so for the first cell we have the initial probability of noun which is minus one and then the emission uh probability of they given noun which is also minus one giving you a minus two uh and then we have the same thing for verb which is minus four uh taking again this value here and then also this minus three over here all right so then what the recurrence looks like is for this cell what we need to do is say all right we have to think about what are the two possible places we could have come from here um and we could have come from this cell and so what we would get there is the transition from noun to noun which is an additional minus 2 and then we would also get the emission of can given noun which is a minus 3. so we have a minus 2 from the value of the previous cell we have a minus 2 from the transition and we have a minus three from the emission and that gives us a minus seven here all right and we have to think about uh we have to think about another possibility here as well which is the following which is what if we came from the -4 cell here so in this case we would have minus 4 from the prev we would have the transition from verb to noun which is a minus 1. and then we have the emission which again is minus three so that's actually the same between both of these and in that in this case we get minus eight okay so uh when we're applying this max then we kind of discard this possibility and minus seven is what gets chosen and so the most likely way to get to the second can being a noun is to have they also be a noun okay uh and i'm not going to go through the whole computation but it turns out that the best uh the best way to get to this cell is have they be a noun and then can be a verb uh and the you know this kind of makes sense given our intuition about the the sentence so we end up with minus two uh the transition from noun to verb which is minus one and then the emission of can given verb which is minus one and that's a minus four all right so you could push the rest of this computation forward notice that we do need to keep both of these possibilities alive even though uh this lower path you know having can be a verb looks a lot better and the reason is because there could be some other word coming up that makes us reevaluate how we analyze the sentence so far and say okay actually you know it really does seem like that can was a noun you don't want to kind of commit yourself greedily to doing the wrong thing when there might be more information that tells you something further down the road okay so that sort of tells us how the viterbi algorithm works the last thing i'm going to mention about it from the perspective of implementation and debugging is that a very good way to check this is to try it explicitly on a small example so it's exponential to enumerate all the possible sequences but computers are pretty fast these days and when you have just four or five words in a sentence you actually can just enumerate them with a bunch of nested for loops and so you should be able to confirm that this dynamic program in this algorithm is giving you the same result as explicitly doing this max over all these possible sequences so this is how we're going to do inference in hidden markov models and it's going to set the stage also for inference and conditional random fields so generally this is a very useful tool for dealing with these kind of sequence models where you have pairwise dependence between adjacent decisions and that's the end of this segment you\", metadata={'source': 'Ks7IrsjhqSo'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to come back to the task of part of speech tagging now armed with hmm and see how they can actually do on real data and kind of compare them to other approaches so to start off with we're going to talk about the version of uh hmm that actually works well for part of speech tagging so what we've talked so far is what I'm going to call a quotee unquote byr hmm where the states here are just single tags now recall that hmm make this marov assumption which is that the next step depends only on uh the current state and not anything in the past and it turns out that this is a little bit too aggressive of an independence assumption instead what we want to do is we want to be able to condition on stuff that's a little bit farther in the past but kind of modifying the actual hmm uh model and and and algorithm is a little bit uh annoying so instead the way we can do this is we can think about changing our state space to actually be a pair of tags consisting of the current tag and the previous tag so corresponding to this fed raises example above now the state for y1 is going to capture this uh this uh kind of bracket s start of sentence symbol and for Y 2 it's going to remember that an nnp was the previous symbol here so these transition probabilities are now these more complicated things that look at uh you know the look at basically transitioning to a pair of tags from a pair of tags but one of these tags is always shared right like if it's nnp in the kind of second position at the current time step then at the next time step it you know it has to have nnp in the history here so the transitions become more complicated and there's certain optimizations that you might want to think about when uh actually implementing this because only some of these transitions are legal now but uh from the perspective of the rest of the algorithm you know even though we have this more complicated State uh everything else can kind of proceed as before in particular the emissions now condition on both of like on on kind of both of these tags you know the tag and and it's kind of History um and that's useful so we could go farther we could try to incorporate more of a history it turns out trigrams are kind of The Sweet Spot for hmm okay so taking our existing tagger um we can apply this to the pent Tree Bank uh data set which uh is one of the earliest data sets that was created for large scale English part of speech tagging now there are part of speech data sets for a lot of languages now we'll we'll come back to those a little bit at the end of this segment um but the pen treebank data set has 44 tags in it it turns out the Baseline of just assigning each word its most frequent tag seen in the training data actually does very well on the test set it gets around 90% accuracy and the reason is I I think I mentioned before that we have tags for things like period in there and then there's plenty of words like the and a determiners that never show up with any other tag so you're always going to get these right they're just totally unambiguous regardless there are still a number of ambiguities right I mean 10% uh kind of 90% accuracy means that one in every 10 words is wrong and so what that means is if we imagine a 20-word sentence which is a fairly standard length sentence in a news news article we're going to have about two errors in that sentence is what we' expect so our trigram hmm can cut that error rate in half it can take the accuracy to 95% and in particular it can get 55% accuracy on words that don't show up in the training set now these are hard words because uh you know recall that for this Baseline uh it actually wouldn't be able to say anything about those words right but the trigram hmm knows about the context and so even though the emissions uh aren't useful for determining the tag of that word it still can do something reasonable so uh a kind of souped up version of this system due to Taren Brands uh gets you know slightly higher accuracy overall but much higher accuracy on unknown words um Christina tanova and Chris Manning uh created a maximum entropy tagger so again maximum entropy is kind of a synonym for logistic regression and this is going to be more or less doable once we see crfs we're going to understand we're going to look at named entity recognition but it's going to be more or less the same kind of system as they build here um and that does even a bit better and this is actually not too far behind the state-of-the-art if you do kind of crazy stuff with uh you know bstm crfs or Bert you actually don't even get that much higher than this um you get around 97.5% and you get higher performance on unknown words so let's take a look at what some of the errors in these systems look like so this is from the tanova and Manning uh work and what they have here is a confusion Matrix uh showing basically uh which pairs of tags tend to get confused between Golds and predictions um so for example what this cell up here means is that the correct tag was JJ adjective so that's what's on the um you know the kind of left side here and then the column means the erroneous tag that was predicted so in this this case nouns uh and the kind of the the reason that adjectives and nouns can get mixed up is cases like official knowledge um where official can either be a uh an an adjective meaning that uh well it can be an adjective or it can be a noun like for example a government official and you might think okay why why would you ever have a noun in this context well there's actually a lot of noun noun compounds that have a ilar structure for example tax cut Art Gallery um tax and art can't be adjectives um tax can be a verb but uh you know in these cases you would want noun noun as the analysis but here it's the the knowledge has the property of being official it's not like associated with like a government official or something here's another uh example so this is a verb particle error like made up the story uh what is happening here is that up again is not a preposition uh it's a particle and made up is kind of this unit that uh kind of goes together and up does not really have the the spatial semantics that it normally does When It's associated with uh being a preposition and so uh in this case it gets analyzed as a particle but of course uh it's fairly challenging for a model to know that uh and then a final case here is recently sold shares uh there kind of without seeing more context you actually don't know which of these two tags it is um if I said I recently sold shares that's vbd that's uh just the straight up past tense of the verb um but if I said you know he has 50 recently sold shares that means these shares were recently sold we don't know by who and that's a participial usage of sold so again those those so you can see that these errors can get kind of tricky here and so Chris Manning in in 2011 um wrote a paper called part of speech tagging from 97% to 100% is it time for some Linguistics and looked at the remaining errors uh in part of speech data so one of the one of the tricky cases that we've talked about is when you have a word that uh words that aren't seen but another tricky case is where you have a word that has been seen but it shows up with a new tag in the test set right and this is very hard to generalize to because generally you're going to assume that okay we've seen this word before and we've seen it as a verb you know most likely it is going to be a verb so this this accounts accounts for around 5% of the errors um unknown words are also around 5% uh in his analysis around 15% of the things were were uh models were errors that models could get right maybe you need a better model but it's at least doable 20% he said had quote unquote difficult Linguistics and so here's an example of that they set up absurd situations detached from reality uh the tags here differ whether on whether set is past tense or present tense and you don't know without seeing more of the context whether this happened in the past or the present and so uh you know we would have to think about models that go beyond a single sentence that's much tougher and so you know we're not going to do that the and then what he found was that 60% of the remaining errors were things that were either underspecified or unclear in the text or the gold standard was inconsistent or wrong um and so here's an example of the of the kind of unclear case uh a $10 million fourth quarter charge against discontinued operations um this is a case of a participial usage versus an adjective do the operations have the property of being discontinued or are we saying that like these operations were discontinued it's it's almost difficult to describe what the semantic difference between these two cases is because in order for something to be discontinued now it had to have like been discontinued sometime in the past right so this is the kind of ambiguity that you writing this or someone reading it may not even resolve for themselves and so uh how can we expect a model to necessarily come to a decision about one of these tags to use here okay so I'm going to switch gears a little bit and talk about uh a different way of producing part of speech tags which is coming back to using neural Nets here and in particular feed forward neural networks so rather than appeal to part of spe these like hmm taggers we can instead go back to this idea of using something that looks more like a classifier we can take uh these words around uh the word that we're tagging here so we look at the previous word the current word the next word we embed them into vectors we can catenate those into a feature Vector here and then we're going to put that into a neural network uh and so again what we're doing here is we're being very careful to capture this positional position sensitive information we're looking at the previous word the current word and the next word we're not using some kind of bag of words thing or that deep averaging Network where you like add everything together here we're concatenating these three vectors uh and so Yan Batha had all looked at this for a number of NLP tasks and they did something a little bit more clever which was they also uh looked at uh Byram and character byrs and trigrams and had a way of incorporating those into the model as well um and then that all gets kind of mixed together in a hidden layer and then uh the model makes some sort of prediction and this actually turns out to work pretty well across a whole range of languages so they compared it to previous work that was doing recurrent neural networks and found that it was it was better than that work and uh you can see on the right here that uh across a whole bunch of different languages the accuracies are you know at least above 90% in all cases um again the kind of you know the error rates differ a bit and uh this isn't comparable to the other data that we were seeing because it's a it's a new data set in a different part of speech tag set but uh the kind of General principle here is that these kind of models that we've built up so far can be used for lots of different languages and uh you know com kind of using this feed forward neural network version of them can be fairly effective so that gives you a sense of what a few models for doing uh part of speech tagging are that work basically as well as the fancier stuff that we're going to come to later in the course uh things like recurrent neural networks in Bert so uh you know now you kind of understand where the state-of-the-art is in this area and what some of the strong models are doing that's the end of this segment\", metadata={'source': 'wijpAX_LLXo'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about syntax so we're moving from thinking about language as a sequentially structured object in terms of things like part of speech tag sequences or sequences of named entity chunks and now we're going to think about it as a tree structured object so the study of syntax is basically the study of word order and how words form sentences and this is an old discipline of linguistics that basically tries to explain why we're able to say certain things and not other things and why we view certain constructions as grammatical or correct and others is not grammatical so from the perspective of nlp why do we care about syntax so there's several reasons the first kind of calls back to some of the examples we saw earlier in the course where we have to resolve ambiguities about whether things are for example nouns or verbs now that can be done at the part of speech tag layer or sometimes it really needs to be resolved syntactically at a higher level because it's about how things are kind of combining together so we saw this example very early on ban on new dancing on governor's desk and the question is is the ban on the governor's desk or is the nude dancing on the governor's desk and that's exactly what syntax is going to allow us to answer syntax is also going to be a gateway towards thinking about things like verb argument structures so if we want to extract information out of text one kind of shallow thing we want to we might be interested in looking at is just like okay what are the events going on and who's doing what to whom and in order to identify that well we could use like a whole bunch of regular expressions or something but that doesn't necessarily scale instead of syntactic parser is going to tell us okay here's a syntactic analysis of the sentence here's the verb here's the subject of that verb here's the object now you have a more direct view into what's going on and finally it just provides us a higher level of abstraction beyond words so for something like machine translation if we're translating from a language that subject verb object into a language that's verb subject object or subject object verb that a language that is fundamentally different word order parsing can help us recognize and kind of canonicalize the representation between these languages rather than just seeing these big surface strings and not knowing what to do with them so the place we're going to start in terms of syntactic formalisms is what's called constituency parsing so this is just one parsing formalism out of many there's many ways to diagram syntax we're going to see another one dependency parsing a little bit later but this is one of the most common ones in nlp and so it's a natural starting point in terms of thinking about what these syntactic structures look like so constituency trees give us a tree structured analysis of sentences in terms of their constituents and so we see on the right here an example of a sentence with several different constituent types so at the very top we have sentence below that we have np noun phrase vp verb phrase farther down we have prepositional phrase pp and you know there's there's kind of other types that show up in other trees as well the bottom layer of these trees always consists of part of speech tags so the tags at the very bottom here are going to be exactly the same as the tags that we were dealing with in part of speech tagging throughout this uh you know throughout the these segments the examples will primarily be in english and what i'll say about constituency with respect to different languages is that there are a lot of languages that it does make sense for there are some that it doesn't in particular there are languages that have much freer word order you can kind of put things in different places and the way you inflect the words like kind of suffixes and things like that tell you uh the grammatical function of those words so those words it makes much less sense to use this kind of rigid tree structure and the dependency grammars that we're going to talk about later are better at handling that okay let me show you a little bit of a more involved example just so you can see a little bit more of what's going on here so what do we have this sentence here she told me that i would never amount to anything we have a what's called sentential complement here s bar she told me that something that gives us a whole there's a whole sentence actually kind of embedded inside this other sentence so this kind of indicates the level of complexity that we might end up dealing with here when we can kind of recursively get whole sentences inside other sentences we have a few things that we haven't seen before we have an adverbial phrase and we have here what's called a unary rule uh we'll come back to these in a little bit but the idea is that this is a rule that only has one child so the np only has one child which is prp here personal pronoun and finally we have this ternary rule here so these trees are not necessarily binary trees the nodes can have any arity binary is fairly common but that's not a hard constraint the other thing that this shows is that english is typically right branching meaning that the tree doesn't look like some kind of balanced binary tree but instead kind of goes off to the right in a right branching fashion okay let's take a look at one more example just to kind of understand a little bit of the level of complexity here so this whole phrase here is going to end up being a noun phrase a refund that the court estimated fundamentally we're talking about a refund here all right so within that we have a smaller noun phrase over a refund i'm just going to draw this triangle to indicate that there's some other structure here that i'm not going to write out like the part of speech tags and stuff like that so we're just going to use that to kind of ignore ignore what's going on there and then under this noun phrase is again one of these s bar things and within that we have an embedded sentence s and what's going on in that s is that we have this the court noun phrase and we have a verb phrase and you know i'm not going to draw the structure here but we have the star star-1 thing that ends up being a noun phrase and the question is what's this star-1 doing well so it turns out the way we think about this that is we label it in the following way we label it as this whnp-1 and what that tells us is that that this this kind of that argument is the thing that the court is estimating so this is something called co-indexation where we're saying that uh okay you know the court has to estimate something the court can't just estimate so in reality there's some kind of hidden word there that is being the object of the verb estimated but what is that well it kind of gets extracted here into this that i mean what's being estimated really is the refund here but this kind of structure here actually kind of breaks the tree structure assumption in that you know essentially what we've got going on is we have this edge here connecting these two parts of the tree so these structures as much as we talk about them as tree structures really can be thought of as graph structures we're not really going to talk about this level of complexity and parsing much more typically this is pretty hard to deal with and it's not necessarily clear what dealing with this correctly necessarily gives you from the perspective of downstream applications but this kind of shows the level of depth that's syntactic analysis can go to um we you know we're not just kind of chunking up things into trees but we're really thinking about okay what's going on with each of these arguments how do they link up etc all right so what's hard about parsing so we've seen that these analyses can be complicated but we don't necessarily know why this is going to be hard from the perspective of an nlp system so this is a classic example of prepositional phrase attachment so we have two analyses of the same sentence here the children ate the cake with a spoon and what i want you to do is i want you to look at these and try to decide which one or rather what meaning each of these corresponds to i'll give you a hint which is that on the right here we have a noun phrase that corresponds to the cake with a spoon and on the left we don't have that we instead have a verb phrase and a prepositional phrase ate the cake with a spoon okay so the kind of kicker here is that uh by changing the structure we change how we are viewing what's going on with this cake and this spoon on the left we have with a spoon the prepositional phrase combining with ate the cake of verb phrase and so this eating event is happening with a spoon on the right we have a noun phrase which is the cake with the spoon and this is the same parse that we would have for the sentence i ate the cake with some icing uh meaning that you know there's icing on the cake right so this indicates that the spoons are somehow like a property of the cake i mean this doesn't actually make a whole lot of sense but you know if it was like you had a little spoon made out of fondant sitting on the cake um and you were eating the cake with a spoon as opposed to the you know chocolate cake um you know this this might be something you would say the key thing is even though we think that the one on the left is much more likely it's much harder for nlp systems to have these same sorts of priors because they need to know about cakes and spoons in order to figure this out so this is why even pretty good automatic systems are going to have a hard time resolving ambiguities like this all right i'll show you a few other ambiguities uh modifier scope um we have two different parses here of plastic cup holder is it a holder for plastic cups on the left or is it a holder that or is it a holder four cups that is made out of plastic uh on the right here uh compliments uh the students complained to the professor that they didn't understand are they complaining to uh you know there's a professor that they do understand that there's a there's a professor that they don't understand and they're complaining to the one professor and not the other or do they have a complaint for the professor and the complaint is that they don't understand and then finally coordination scope so uh the man picked up his hammer and saw uh is he picking up a hammer and picking up a saw or is he picking up a hammer and then seeing something so the kind of comparison here is if he picked up his hammer and swung we would know that he's uh picking up the hammer and doing something with it but uh yeah so the the ambiguity here is is kind of where the and is attaching and it's tied up with this ambiguity about saw okay so the last thing we can kind of say about this constituency formalism is how do we know what constituents are like how did how did linguists kind of decide what these units should be and the answer is that there are a bunch of so-called constituency tests that let us kind of reason through it and i encourage you to think about these just from the standpoint of increasing your own understanding about why kind of this these syntactic structures are the way they are one is called substitution by pro form um and a pro form is either something like a pronoun or just any kind of general way of abstracting a phrase away so for example if we said they ate the cake with the spoon the fact that we can replace the children with they indicates that the children is probably a meaningful unit here or we could say the children did so with a spoon that indicates that this ate the cake is a meaningful phrase here there's kind of other constructions like clefting we could say it was with a spoon that the children ate the cake the fact that we could extract this with a spoon piece tells us something and we can also think about this in terms of question answering what did they eat the cake how with a spoon again it tells us something about the structure of these different units this is not always clear and there are places where constituency starts to break down so i don't want to portray this as too clean or the perfect formalism she went to and bought food at the store if we said she went to the store and bought food at the store that that would be a kind of nicely formed constituency structure but you have this very weird thing where there's this preposition to that kind of really wants this argument but they've been kind of bundled up with uh and bought food at and so the store is is kind of doing double duty here um and so understanding constituency in this case gets a little bit more complicated so the general takeaway here is that this is a formalism that we can do syntactic analysis with it looks like it's kind of identifying these tree structures over language and generally this is going to enable us to do some interesting stuff going forward that's the end of this segment you\", metadata={'source': 'zDPUKQKDaMM'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about probabilistic context-free grammars so we're going to start off by talking about non-probabilistic context free grammars or cfgs so a cfg is defined by a four tuple ntsr which consists of non-terminals terminals a start symbol and rules so if we think about the constituency grammars that we've seen so far and kind of cast them into this terminology the non-terminals consist of these symbols we were seeing like s np vp pp as well as the part of speech tags that we had dtnn vbd in nns and we're going to kind of build up a little grammar here so these pos tags are what we're going to call pre-terminals and this is kind of special terminology for constituency grammars in particular where we think about the layer before the terminals as being something special okay then the terminals are going to be our words so they're going to be the children ate cake you know with spoon etc like you know basically we just have some kind of vocabulary and that's our terminals all right the start symbol is s and the rules are going to consist of two types we're going to have binary rules uh which we think of you know like this so we write the source symbol on the left and then the yield on the right so we can have generally a bunch of different productions for each for each non-terminal symbol and these are so like like i said these are binary rules because each uh symbol rewrites into two symbols we also have what we call unary rules which are going to fall into two types we can have unary rules involving non-terminals like this or we can have what's called our lexicon which is basically the last layer of the tree where part of speech tags rewrite as words all right so we've got a grammar here so what we can do is we can start with the start symbol s and we can just iteratively apply these rules so maybe our only option here is to rewrite this as npvp then for this np we can decide what rule we want to apply maybe dtn and then you know the only rules we can apply are well we can apply we can get the from dt and then we either get cake or spoon here um we pick one etc we can generate a sentence this way okay so what's a probabilistic context-free grammar so a pcfg is the same thing as before but rules have probabilities okay and the probabilities sum to one per parent so what i mean by this is if we think about attaching probabilities to these rules above you know maybe we have a probability of one-half one-half associated with these two rules about np and so these these are constrained to sum to one because we need to have a probability distribution uh basically p of rule given parent of rule and so all the rules that rewrite from np have to sum to one so in this case uh vp goes to vbd and np you know maybe this one would have probability of one-fourth then the our unary rule here would have probability three-fourths these are our two rules that involve vps and this s rule has to have probability one because there's only one rule for s here all right so the probability of a tree is simply the product over the rules in that tree of the probability of the rule given its parent uh so if i show you a parse you can just multiply the probability of each production together in order to get the in order to get the probability of the whole tree so the question is where do these probabilities come from uh so we actually need a few steps before we can think about taking our uh probabilistic context free grammar and doing parsing with it okay so the first thing we need uh is well okay so so first typically what we think about being given is we are given a tree bank of sentences labeled with trees and we want to go from this tree bank we want to extract a grammar from this tree bank and then we want to be able to parse with this grammar so the first step of this process is going to be what we call binarization and this is going to be turning the trees into trees consisting of binary and unary productions rather than kind of higher order n reproductions the second step is going to be estimation of rule probabilities all right this step is going to be very easy it's going to just consist of counting and normalizing occurrences of different rules so for example if we see noun phrase rewriting as this rule three times in this rule four times you get probability three sevenths with the one rule and four sevenths with the other rule and this falls out of the fact that this is the same kind of generative count the same sort of generative multinomial model as we had for hidden markov models and so the way that maximizing the likelihood of the data works out ends up being the same so this process exactly corresponds to maximum likelihood estimation all right and then the third piece here is inference so that's asking about the most likely the most likely tree given a sentence so i give you a new sentence and uh we ask okay what is the tree that most likely generated this sentence and so again this is going to look like uh kind of similar to hidden markov models in the way that the probability computation works out and again we're going to have a dynamic program that allows us to do it all right so that's where we're going here the one piece of this i'm going to talk about before finishing out this segment is binarization and binarization is the process of as i said turning trees into uh binary or unary trees consisting of only binary or unary productions so for example we can have fairly high arity productions like sold the book to her for three dollars this is a verb phrase but what we we don't want to have these four-way branching rules and the reason is that when we try to do well first of all it's going to make estimation of rule probabilities more fragile because we're going to have all these like really big involved rules that don't work so well and then when we do inference uh these are going to cause a kind of computational blow up uh it's gonna it turns out that our inference is gonna scale uh with a kind of exponent in the uh corresponding to the highest arity rule that you have in your grammar so what we need to do is we need to turn this into a binary tree so this looks roughly like the idea of turning a grammar into chomsky normal form if you've seen that before we are actually not turning our trees into chomsky normal form because like i said we're going to still allow unary rules that don't just happen at the terminal layer so there's a couple of ways to do this this is the correct one so what have we done here we have taken our rule and split it up into three different rules and introduce these intermediate symbols so the and what we're doing is we want to assign probability p to this particular rule and then we have probability one associated with the next two rules and the reason those are probability one is because the rules themselves encode what should come out of them basically we are making the entire decision at the first step but then when we get we you know when when we get down to these subsequent steps we've already encoded the yield of the rule in the grammar symbol itself and so then the the parser can just kind of spit out these you can just kind of spit out these symbols okay so this is what we call lossless binarization and we can also have so-called lossy binarization where let's say we were to just say okay we're not going to bother trying to do this crazy rule encoding we're just going to put vps all the way down here all right this is this does not preserve the semantics of the pcfg because now all these vps are kind of competing for the same probability mass and in particular this is not actually a great assumption to make for a context-free grammar because you're going to have some probability of now any vp you rewrite is going to just want to spit out prepositional phrases which is not the right thing for a vp to be doing so it's really important to think about the binarization and what intermediate symbols get introduced now normally people do something in between one we'll talk about that in a later segment here but what we've seen so far is basically we can take uh a tree bank we can binarize it turn it into this set of trees that are binary and then we just read a pcfg off of that so we estimate these probabilities through counting and that's going to equip us with a pcfg that we're then going to be able to parse with and that's the end of this segment you\", metadata={'source': 'q3dLP9YQLPA'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the cky algorithm so the cky algorithm is going to help us compute the most likely tree for a given sentence x so like we saw in the hmm case we can think about this uh most likely posterior tree as also the tree that maximizes the joint probability p of t comma x because just by kind of multiplying the top and bottom by p of x and then sort of get you know that that term is constant with respect to t okay so recall that the viterbi algorithm was this dynamic program that had this sort of tag versus sentence matrix and there was this whole uh kind of idea of for each position having to evaluate which previous tag was the highest scoring one for getting us to this particular tag at this position in the sentence so we're kind of maintaining uncertainty over each tag at each position and this combined with the markov property of hmms allowed us to do efficient inference so cky is going to be exactly the same idea but it's going to be a little bit more involved because instead of predicting a sequence of tags we're now predicting a tree structure so let's go to a kind of diagram of what that structure actually looks like all right so we have this uh a chart now that looks like this so what each of these cells okay so uh this and then the um so there are actually three dimensions of this chart uh we are going to index it by i j and then x where this is the score of the best derivation of x over span i j so the chart right now represents all the possible spans i j we're going to index them using what we call fence posts here and so for example this cell here is the cell 0 comma 1 and the cell at the top is 0 comma 4. all right and we also have this kind of dimension you could think of either within each cell we have a set of constituents x or you could think about there another dimension kind of going into the screen um capturing the the different possible symbols all right so let's see how so so then uh the recurrence is going to be the following so this is the base case all right so this is the uh applies probabilities from the lexicon so what we do at the start is to say okay over each possible word what we need to do is we need to consider which symbols could have yielded that word and what their log probabilities are so uh we're going to have a example grammar here all right and each of these rules is going to have probability one and that's just because each you know we only have one rule for each of these parts of speech uh and so the probability of the word given the part of speech uh has to be one in order for this to normalize notice though we have two ways of analyzing raises we can say it's either a noun like you know the the kind of money raise or uh sort of spatially raising something up all right so applying this first rule remember that we're going to be dealing with log probabilities so uh we're going to express things so for example the score of dt in the first cell here is going to be 0. and all the other scores are going to be minus infinity i'm not going to bother writing out the rest of them in general i'm just going to write the scores that are non-negative infinity here and so this is what we get by applying that and again every other every other symbol in every other one of these bottom cells has score negative infinity all right now the recurrent case for the dynamic program works as follows all right we have to max over two things and we'll kind of continue this on the next line here so all right so what we're doing here is we're saying what is the best way to build symbol x over span i j well we need to think about all the possible ways we could build that and so this k variable is what we call a split point k is going to be in the range uh between uh it's going to be less than uh it's it's going to be between i and j here and uh essentially the idea is that we have to pick a point in which the binary tree is going to split so for example if we think about this cell right here we have two possible ways to build it we can either build it by a kind of let's see i'll do this with the highlighter we could either build it by taking a unit here and combining it with this or we could take a unit here and combine it with this and those each represent two possible split points all right so we have to max over split points and then we also max over rules so this is a max over all the rules in our grammar x1 x goes to x1 x2 and then what is the actual score associated with this well we have to look at the probability of that rule and then also the chart values associated with uh x1 over the first half of the span or rather the span before the split point and x2 over the second half all right so let's finish out our grammar here all right and so this these have probability 1.0 and then we'll say that the np's have probability 0.5 and and we're going to say that the log of 0.5 is going to be minus 1 here just for simplicity all right so now we can start to fill in the rest of our tree so let's take a look at this cell from zero to two so we only have one possible split point right so we basically have to go with k equals one here that's the only possible value that works and then we have to consider every rule in the grammar all right so there's a lot of rules and a lot of them are going to combine things like for example uh nn on the left over here uh with n and s and both of those have probability negative infinity and that's going to give us a negative infinity in this cell as well so we're just going to ignore that rule i mean we have to think about it we have to loop over it but the fact that it's going to give us negative infinity we're not going to draw it in our picture here in fact the only rule looping through the whole grammar that works here is one that combines dt and nn so we apply np goes to dtn and that gives us an np with a score of minus 1. and the reason is because we had to apply the score of that rule uh additionally take the scores from the two uh the tags here the dtn all right now it turns out that we can build an np here with score minus one as well by taking nn and nns but there's no way to use like the nn and vbz or anything like that on the right here though we can combine the vbz and the prp to make a vp and that's going to have score zero all right so you could consider you know you can you can kind of think through the rest of this tree yourself uh we're just kind of got to jump to the solution which is that we're going to end up taking this np and vp and we're going to make an s here and the score of that is going to be minus 1. all right and so the algorithm at the end returns this score of the uh well it assigns scores to every constituent over every span and then what we say is all right what's the score of the root so in this case the score of the root symbol s is -1 and so we know that there's a tree that can possibly be built and what we can do is we can kind of trace the back pointers in here like we talked about in viterbi in order to reconstruct that tree all right so this is the this is the basic kind of flow of the algorithm there's a fair amount of complexity to actually implementing it you have to kind of think about building the right data structures for indexing over your grammar to do it efficient efficiently and one thing we're not going to talk about is how to handle unary rules which make this more complicated all we saw here was this recurrence for the binary case but there's another case we have to deal with as well and then the last thing the last thing i'll say about this is that the run time ends up being cubic and the reason it's cubic is because we have n squared chart cells here and then for each one we have to consider a loop over order and split points and then we also have this grammar constant times g so g represents here the kind of shape of the grammar with respect to how many possible values it allows you to build and how many rules it has and those those factors are very important for the performance in many ways more important than the kind of asymptotic uh behavior in terms of n so that's why we kind of put it in this runtime explicitly but you know without going kind of deeper on what factors matter the most there it's a little bit hard to reason about so that gives you a sketch of the cky algorithm it's a dynamic program in the same style as viterbi that allows us to do the same thing and this is an example of its execution that hopefully gives you a kind of rough sense of how it works that's the end of this segment mike's clear that was cool you\", metadata={'source': 'QeDb6mSDSqs'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the independence assumptions underlying pcfgs and why these maybe aren't so good so we've set up this idea of building a pcfg by estimating a grammar from a tree bank and then parsing with that grammar if you do this it turns out you actually build a pretty bad parser and the reason is because context-free grammars are too context-free so what we're showing here is the distribution of rewrite rules for all nps on the left and so 11 of the time they rewrite as a noun phrase followed by a prepositional phrase um for example uh the cake with icing that would be a kind of big noun phrase that rewrites in this way nine percent is a determiner and a noun like the cake and six percent of the time is a personal pronoun but when an np is occurring under the s symbol meaning that it's probably the subject of a verb in a sentence it's 21 of the time going to be a personal pronoun but under a verb phrase that's only true four percent of the time so the this represents the fact that language is really not context-free these nps in different positions are like should be handled differently so in order to build better grammars we're going to have to think about how to take our basic pcfg and make it a little bit stronger and so the way we can capture this is through an idea called what's called vertical markovization so this is the basic pcf this is the basic tree over a sentence that we're going to read our read our parser off of but what we could do is we can transform this tree so what i've done here is i've added a carrot after each symbol and then added the identity of the parent symbol after that carrot so this is a the fairly simple pre-processing step that we can apply to our tree bank we can just go through every tree that you give me and apply this transformation and now if we read a grammar off we actually have two different well we have many different types of noun phrases but in particular here we have this np carat s noun phrase that is going to be able to capture the distribution of what goes on in noun phrases that occur under the s symbol and similarly vp carets can capture the the distribution of verb phrases under the s symbol so why is this a good idea well it allows us to greatly increase the size of our grammar which is maybe a little bit of a problem from a statistical efficiency standpoint but it enables us to model these distributions in a much more fine-grained way there's another type of transformation we can do that deals with binarization so first i'm going to show you another way of doing so-called lossless binarization so before we had this idea of underneath the verb phrase uh right here we basically encoded this like super symbol uh with like everything listed out here now what we're doing is we're only generating the uh the yield bit by bit and we're not doing it all in one go but this is actually going to be equivalent to the original way of doing lossless binarization because of the chain rule of probability so we can decompose this rewrite of this complicated vp rule into a bunch of atomic steps and we're abusing notation a little bit here but basically you know what this represents is that the probability of a kind of big complicated event can be written as the probability of you know the first step and then the second step condition on the first the third step condition on the first two etc and that's what this this different grammar transformation is giving us all right so this enables us to uh think about a new way of uh transforming trees when we're doing binarization called horizontal markovization so we have a kind of slider here which controls the amount of information that we remember in the grammar here so uh if we talked about this idea of lossy binarization just replacing everything with a vp symbol here and that we call h equals zero and that's going to be the kind of most compact grammar but also the weakest grammar in terms of modeling what actually goes on at these intermediate stages of generation h equals one what we're going to do is rather than remember all of the symbols inside this bracketed thing we're only going to remember the previous one and so if we look down at the second uh the second symbol that means we forget about the fact that we had we came from v b z all right and then h equals 2 we remember two symbols of context and so you you've got a kind of trade-off here that you can apply now there's again there's a fair number of implementation details for this in practice in practice you always remember the the the head tag meaning the tag that's a verb in this case so you would actually always remember the vbz in this case uh but this gives you a way of sort of like vertical markovization controlled the amount of context we paid attention to here we're also paying it con you know controlling the amount of context we pay attention to except this time when dealing with these high arity rules all right so what we do is we typically when pre-processing a tree bank we first apply vertical markovnization like what's on the left here and then apply this horizontal markovnization and so we can form some fairly complicated structures and symbols in this way but we're only going to see a kind of small fraction of the total number of possible symbols so even though this kind of theoretically is like an exponential blow up in the size of our grammar um in practice it blows it up by a good factor but it doesn't get too out of control so this was investigated by klein and manning back in 2003 and what they found was that if you know you can take a basic parser um v equals one h equals zero which can parse to a accuracy of around 70 and you can increase that to almost 80 percent just through these markovnization tricks so this is pretty cool because uh it reflects basically needing to uh or it reflects it reflects a fairly simple transformation that can improve our performance pretty substantially and so now we're going to contrast that with some more complicated approaches that improve performance earlier but that can kind of underscore why these simple transformations are sort of nice so the dominant paradigm at the time that klein and manning were doing this back in 2003 was what's called lexicalization lexicalization captures a different idea and one that's going to be fairly important but is going to be very hard to implement at least in constituency grammar so we have two examples here of different analyses for dogs and houses and cats so on the left side here we are saying that there are dogs in houses and there are also cats and on the right side we are saying that there are dogs that are in both houses and cats which doesn't make sense the problem is that even doing all this parent annotation stuff actually both of these trees still look the same because the rules applied are exactly the same in both cases the difference ends up being the words and so in order to understand the fact that dogs and cats is a more likely set of things to conjoin than houses and cats we really need to be looking at the words and so what lexicalized parsers do is they annotate each grammar symbol now not with something about its context like you know things about his kind of siblings or parents like we were seeing before but instead with a so-called head word now the head word is the most important word of that constituent and so the kind of simple heuristic to think about is like for a verb phrase the head is a verb for a noun phrase the head is a noun for a sentence the head is also a verb because because the idea is that a sentence is fundamentally describing some kind of event and of course there are sentences that don't contain verbs you know we're not going to go into that all right so what we can do is we can write down a set of rules that identify these uh these these heads for example in a noun phrase the last word or the last word before preposition is typically the head so we can extract these head words and kind of annotate our grammar with them right so now okay can't we just take our pcfg approach read a grammar off of our tree bank and parse with it so the answer is no because the grammar is now way too large to deal with and the fact that we've done this lexicalization means that we need a whole bunch more tricks in order to deal with it so in the late 90s my cons and eugene charniak built parsers that were parsing to almost 90 f1 on the standard english pen tree bank with this kind of technique um and so this evaluation metric looks at basically how many nodes in the parse tree you get right uh and so if we think about getting you know almost roughly we'll say 90 of the nodes correct that's pretty good uh and it was it was true before a lot of these kind of fancier machine learning techniques had been developed but these parsers are incredibly hard to build so what we're going to talk about going forward is a way of capturing some of the same idea as lexicalized parsers but much more simply and that's why we're going to turn to dependency as a formalism but even in constituency there's sort of tricks for uh you know incorporating the words into the parsing process using discriminative parsers and so we'll come back to those a little bit later as well so what we've seen here is if you want to make a pcfg work you need to do some kind of refinement on it and so we've seen a couple different ways to do that and then we'll talk about uh kind of what what actual parsers look like in a few segments that's it for this one you\", metadata={'source': 'f1o1_bPWzM0'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about dependency parsing but what we're going to start with is lexicalized parsing so if we have the sentence the dog ran to the house this is the syntactic constituency analysis of that where each constituent has been annotated with its head word so again the head word is the most important word in that segment and so in a noun phrase like the dog the noun dog is the most important piece all right so one thing we can think about is the fact that this sort of induces relationships between particular words independent of the actual constituency structure and here's what i mean by that we are taking the house and combining that into a unit this noun phrase that's noun phrase headed by house and we're saying that house is the most important unit in this noun phrase and so in some sense what we can do is we can think of the as being kind of subsidiary to house in this case or modifying it all right so following this intuition up the tree we can kind of think of this to that two house this two np combining into a prepositional phrase two is the most important unit and so house ends up being subsidiary to that and so we can draw these arrows from between the words kind of ignoring what's actually going on in the syntactic constituents here and this is the idea behind dependency parsing if we apply the same idea going up the tree we can produce a structure of basically words kind of pointing to each other expressing relationships between them at this low level without introducing any of these notions like noun phrases or verb phrases or whatever and this gives us the rough skeleton of dependency parsing or dependency syntax so this is a syntactic formalism that doesn't introduce these non-terminal symbols but instead defines the structure through these relationships or arcs between words and so the terminology we use is we say that a headword or a parent or a governor is connected to a dependent a child or a modifier so house and the for example house is the head and the is the dependent or child each word has exactly one parent except for the root symbol which doesn't have a parent and the dependencies have to form a dag so we cannot have uh we cannot have cycles here so the you know if if we want to kind of anchor this and stuff that we've seen before the part of speech tags are the same as well what we saw with part of speech tagging and then also with constituency parsing and a lot of times what you do in a dependency parser is actually run a tagger first as a pre-processing step historically that's how a lot of the parsers worked okay so it looks like we've kind of thrown out the notion of hierarchy that we had before but i want to emphasize that this actually isn't in some way so different from constituency parsing if we kind of draw out the tree like this i've thrown out the root symbol here what we can see is that if we look at the sub trees we roughly recover the constituents that we had before so for example the sub-tree rooted at dog it contains the dog and dog is a noun so we could say okay there's some big phrase here and it's headed by a noun all right we're not calling it a noun phrase in this case but that's basically what it is and similarly for two to the house that's a whole that's a sub tree rooted at two and so it's rooted in a preposition all right you know it's not a prepositional phrase but essentially it is the one constituent that we're missing here is the verb phrase and so dependency syntax does not uh kind of posit the existence of a verb phrase in the same way instead the uh in this case subject and indirect object to this verb are both kind of treated symmetrically so that's one big difference representationally between what's going on in dependencies and what's going on in constituency all right so the one piece where we haven't shown and we're not going to talk about that much are labels here so we can label these dependencies according to their syntactic function a lot of these labels are not very creative for example a determiner modifying a noun phrase that gets the determiner label now modifying a verb well there are actually a few different labels here this one's a subject so the label here actually does say something about the grammatical role um preposition label um object of the preposition and determiner again so many of these are kind of very easily determined based in a pretty shallow way based on just the tags of the head and the modifier so uh the major ambiguity that we're going to think about resolving is the structure and if what if if you want to assign labels to these a lot of times what you can do is just take the structure and then run a classifier at the end so yeah we're not going to worry about the labels too much when thinking about dependencies here and i'm going to drop them from the slides all right so thinking about how this contrasts with constituency um there are kind of pros and cons of both formalisms so in this prepositional phrase example ate the cake with a spoon in constituency it was sort of complicated how this actually worked right we had these rules kind of rearranging in this fairly major way several of these rule productions had to change and kind of be reordered we do get the same vp goes to vbdnp rule in both cases but it's in a different position now with respect to everything and it's just sort of a big change in dependency instead we are making a much more direct decision about which is the parent of with so with a spoon is is going to be our quote-unquote prepositional phrase and then does width attach to cake in which case it's the you know cake having the spoon on it or whatever or does it attach to eight in which case it modifies the event this makes a lot more sense all right so now we're going to see a case where constituency kind of wins when we think about this dogs and houses and cats example um the way that uh this coordination works is a the presence of a ternary ternary rule np goes to npccnp and this makes sense for representing this kind of coordination in dependency the first item that of the coordinated structure is the head and then the and is a child of that and then the other item is also a child so for example if we have this phrase dogs and houses and cats uh dogs is going to be the head of that whole phrase doesn't necessarily make sense as a kind of representative word right because we actually have two things that are being combined if we have mary and joe just saying that mary is the head of this phrase doesn't really reflect the fact that there's actually two people involved right um so this is a place where dependencies kind of do something a little bit weird so this is to illustrate that these are different formalisms even if a lot of times the way they represent things is sort of isomorphic and you could map back and forth between them it's not always true so let me kind of illustrate why dependencies might be something that we're interested in and the reason is that they can be a little bit more of a direct lens into predicate argument structure and they can be useful for tasks like relation extraction so for example we have the sentence bills on ports and immigration were submitted by senator brownback republican of kansas so we have the so-called standard dependency representation on the left and we have a collapsed representation called stanford dependencies on the right and so we have these kind of cool effects here where we've taken this preposition of on the right republican of kansas and kind of folded it into the label on the dependency arc and we also have this builds on ports and immigration we've kind of post-processed this to make something a little bit more sensible um and so now we can see very directly that we have this submitted event that's going on at the top here and we have a few different uh kind of arguments to it one of them is the agent so this is a linguistic term for basically kind of representing the argument of a particular verb like who's doing it so senator brownback republican of kansas is doing this in kind of doing this and then you have the other arguments of submitted as well so this is exposing a little bit more easily than constituency does this idea of kind of this who did what to whom view of events all right another big win for dependencies is going to be their ability to represent stuff cross-lingually so there's a very cool project called the universal dependencies project that is using a canonical set of tags and labels and annotating dependencies across a whole bunch of different languages so i think they're up to 70 or more languages now and the reason that this is doing uh this project is using dependencies is because these dependencies are more portable cross-lingually we've talked about how constituency really kind of depends very heavily on the word order and when you have free word order constituency doesn't handle that very well and so a language like check where you can rearrange the words a little bit more freely than you can in english is not well handled by uh standard constituency representations so this what this shows is that this kind of uh this kind of technique can generalize to lots of languages other than english and the kinds of parsers that we're going to build can then uh parse in these other languages as well given the fact that we have labeled data for them all right the last thing i'm going to talk about is the idea of projectivity which again is something that shows up primarily in languages other than english so a tree is what we call projective if any sub-tree of it is a contiguous span of the sentence and all the trees we've seen so far have been projective and you know another way to think about this is we kind of draw these dependency structures and we don't have any crossing arcs what would a crossing arc look like here's an example of that in a non-projective tree a hearing is scheduled on the issue today you see that this basically there are two arcs here that are crossing uh well okay there's a couple of points where arcs are crossing each other um and uh the basically what's going on here is that we can't write this in this this kind of linear structure um anymore we have to uh or we can we can't write this in this we can't draw this in this projective way we we need to draw the arcs and they end up crossing each other so the way to make this sentence non-projective would be to write a hearing on the issue is scheduled today if you kind of do that rearrangement you remove this sort of problematic arc that goes from uh hearing to on and uh it turns out this this all kind of end ends up working out um but uh basically because of this phenomenon that we see here which by the way is called extra position where uh on the issue gets kind of extracted from where it sort of should be with respect to the hearing we end up with a non-projective structure uh so this that's kind of a weird example in english uh but there are plenty of examples in other languages where this shows up here's an example in swiss german um where basically if someone is saying that we helped hans paint the house that's what this sub this kind of piece of the sentence looks like um with what's called a gloss here where we see a kind of word by word translation and so this is uh this is some some fairly this could be fairly common in other languages for example roughly a quarter of sentences in check are non-projective and so we are going to primarily talk about algorithms for projective dependency parsing but it's important to think about how these algorithms can generalize and and handle these cases in other languages and by the way these are exactly the kind of cases which constituency has a very hard time representing and so they kind of motivate our choice of dependency to begin with so this gives you a kind of idea of the considerations behind the dependency formalism that we've been developing and maybe some of the linguistic ideas that go into it so we can understand um why the parses that we're going to produce from our dependency departures that we'll build end up being useful that's the end of the segment you\", metadata={'source': 'dbDjKCc4R3E'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about building a transition-based dependency parser so we could try to build our dependency parsers in the same same style as our constituency parser using something like cky so there are chart parsing algorithms dynamic programs for doing inference in dependency spaces the time ends up being the same cubic time in the length of the sentence the algorithm actually ends up being quite a bit trickier than the basic cky that we saw so the but but instead we're going to talk about a different style of parser just to give you a sense of the space of kind of algorithms that people use in this space and this is going to be transition based or shift reduced that's another term for it and this is more similar to the kinds of parsing that get used for compilers so when doing compilation of programming languages we don't have ambiguity in the same way that we have in natural language and so we can kind of just go through it and make decisions greedily as we go and not have to worry about oh i shouldn't have attached this prepositional phrase here because actually this other information later in the sentence leads me to think that i should have interpreted it this way so the way these work is that a tree is built from a sequence of incremental decisions moving left to right through the sentence and we have two data structures we have a stack which contains the partially built tree and a buffer which contains the rest of the sentence so how does this work so we are going to start off with an initial state where the stack just contains a single root symbol and the buffer contains all of the words in our sentence here and our running example is going to be i ate some spaghetti bolognese and so probably the only kind of unique thing about this example is that uh spaghetti is actually the head of the noun phrase here typically the heads of noun phrases come at the end but here spaghetti bolognese is a kind of borrowed term from italian and so uh you know it retains the italian word ordering all right so uh the first thing that we do is have what's called a shift operation and we always have to start with the shift because what the shift does is it moves items from the top of the buffer and puts them on top of the stack so after one shift operation it looks like this and after another shift operation it looks like this so the the kind of operations that we have then uh involve combining items on the stack in order to produce a tree so the operation that we use in this case is going to be called a left arc and the way to think about this is okay well at the end of the day we need eight to be the parent of i and so what we need to do is we need to apply this operation here in order to build the correct tree so uh we use this notation here where sigma denotes uh the stack and then sigma bar w minus one means that the stack has w minus one as its last uh symbol and so what we do is we uh transform the stack with w minus two w minus one of the last two items in it and we put an arc between them and then we kind of put them back on the stack with w minus one uh now being a new element of the stack and so that gives us a state that looks like this so the stack technically only contains root and eight but i'm drawing this arrow here to i to indicate that you know we've also added this arc in here so you know we can kind of think of that arc as being part of the data structure as well so this is a way of building up this tree that we see above now what we're doing right now is we're doing a quote unquote oracle derivation we are talking about what are the correct operations to do at each step our transition system is going to generally allow us to do all kinds of crazy stuff build all sorts of trees that we don't want here but what we're doing is just a walk through of how we get the correct tree in this case and then we'll revisit the idea of how to get the tree that we actually want okay so the system that we're using is called the arc standard transition system uh and it this features three operations two of which we've seen so far shift left arc and then right arc which is the same as left arc you take two things off the stack but now you add an arc going the other way and then you add w minus two back to the stack and the end state is going to be when the stack just contains root and the buffer is empty and you might say okay where did all the words go all the words are now children of root i'm just not showing them here so as a good little thought exercise we can ask how many transitions do we need if we have n words in a sentence so you should think about this and try to come up with an answer and based on the operations that we have here you should be able to convince yourself that we need two n operations because we need n shifts in order to get all the words from the buffer onto the stack and then n reduces in order to get all of the arcs that we need to actually build the tree so there there are other systems other than arc standard there's they they you know typically use operations that look like these but they might directly pull things off of the top of the buffer or they might be a little bit more flexible we're not going to discuss them here the kind of basic ideas are the same in all cases so the idea is that we want to use these operations in order to build up the tree that we've got here so again this is what we call an oracle derivation this is how we construct the gold tree um and what we hope to be able to reproduce when we actually uh build a parser that runs at test time so we saw these operations already we shift shift left arc and we end up in this state and one thing i'll note is that we actually didn't have to do the left arc now we could have waited and done a whole bunch of other stuff and then come back and done it at the very end um but typically the uh sort of decision that you make when designing these systems is you want to do the left arcs as soon as possible and so we decide to do it at this point in the sequence all right if we put in a right arc now that would actually be a correct arc from the standpoint of the parser we would get a 8 being the child of root that's correct but we cannot do that right now because if we did we would not be able to attach more children to eight and eight still needs this whole spaghetti bolognese thing as its child and so we need to hold off on doing this particular reduce operation and in general we can't do we can't combine something with root until the very end because otherwise you know you'd have to stick a second child under root in order to actually deal with the rest of the sentence okay so what happens next well we need to shift two more words um and then we left arc again and so we get spaghetti as the parent of some and this sort of follows what we've seen already all right and then we do a last shift operation to get bolognese onto the buffer all right so this is kind of characteristic of how this parser execution looks we have a bunch of partially built structures and then we're kind of waiting to do right arcs uh you know sort of go all the way with right arcs essentially um and so then we do a whole bunch of right arc operations to kind of combine these things um and we end up with our final state here okay so what we need to do when we actually build one of these parsers so we need to make the correct decision at each step and so we can go through the steps of this process and ask ourselves how do we make the right decision here okay so in this case it's actually very easy there's only one legal move which is shift so we're going to look at a more complicated state where we actually could do several different things we could shift we could left arc we could write arc all three actions are legal but we need a way to figure out what to do and so we frame this as a multi-way classification problem of which action do we take and we can do that with something like a linear classifier where we have our three actions and this is our different features view of classification where we extract features based on the stack the buffer and the action and then we take a dot product with weights so what are the features to actually tell us that this should be a left arc here so this is a pretty tricky uh feature design problem um you know in particular deciding whether to left arc versus shift you know we like we said there's actually kind of ambiguity in how you even define the transition system to do that but assuming we we know we want to do left arcs as soon as possible the question sort of boils down to why is it okay to do a left arc here and really the um the the kind of characteristic thing to look at here is what's going on on the stack and we can look at the part of speech tag sequence which i'm not showing here but it's generally information you could have available if you ran up tagger in advance we look at the stack tag sequences and this sequence is pretty informative because when we have a determiner before a noun a lot of times that you know that's going to be the direct object uh and then that determiner is going to be a child of the noun now this isn't always the case for example if we had the art museum here we would not want to do a left arc yet we would want to merge museum and then we would need both the and art to be children of museum so we also need to be looking at the buffer as well so in particular we actually in this case need to look at the words in the buffer for example the word bolognese to know that we shouldn't merge that yet uh sorry should we we shouldn't shift it yet onto the stack that we should do this left arc first but representing these dependencies is very complicated we need to look at kind of a bunch of pieces of information at the same time and kind of balance them this is much trickier than kind of thinking about what's going on in our grammar for constituency parsing which was a much more straightforward kind of way of thinking about things so we're going to come back to this feature design problem and look at some neural net methods for this task and that was actually one of the this was one of the first places where neural nets really sort of took off in nlp just because designing the features for this task is so challenging all right so typically the way that these models work is we make our decisions and we need to train that somehow so we turn a tree into a decision sequence using what's called an oracle so that's basically the process that we went through earlier where we had the gold tree there and we said what is the sequence of operations that gives rise to this tree all right and then we train a classifier to predict the right decision using this as the training data and then sometimes you might use something like beam search we're not really going to talk about that here instead you know just imagine that we run this model as a greedy greedily we say all right in this state what's the next action to take okay shift great let's do it and then we ask ourselves what the decision is in the next state so you can definitely make mistakes and wind up in places that you don't want to be and then it's you know well it's impossible usually to get the back to the correct tree from there but sometimes it's even tricky to make the right decisions going forward so in particular the training data that we've talked about so far it assumes that you made the correct decisions up to this point right if we just take our oracle and kind of follow along and train to predict the correct thing at each step we've only actually seen cases where we've always made the correct decisions up until now so from a kind of state-space perspective i'm going to draw a picture that looks like this where we have some path through the state space from the start state to the gold end state with the correct tree and what we're doing is we're basically considering each of these choices in a dotted box like do we take the correct decision or do we take a wrong decision here and we train the model to take the correct decision so each tree gives us two n training examples but the kind of thing that should worry us a little bit is that we actually never observe what we call non-gold states here we never think about let's say what happens if we made some bad decisions and then want to correct them and kind of you know at least make the correct decisions about the other arcs in this tree we just never even see that data during training time so this is a problem called exposure bias one thing we can do is we can formulate this as reinforcement learning and think about this as exploring a state space with actions and getting some reward based on how good our tree is we actually don't need to do something that complicated though because normally at any point in the space we can say always what the correct decision is there's none of this kind of delayed reward uncertainty that shows up in reinforcement learning but there's lots of techniques for thinking about okay how do we sample kind of trajectories that are off this gold path and then train with awareness of those etc so building these parsers while it's sort of appealing in the sense that all we're doing is putting together a classifier which is a lot simpler than um you know pcfgs and cky and things like that um while it's it's simple there's a lot of kind of tricky decisions to make in how we actually train it so this gives you a sense of how transition-based parsers work structurally what these transition systems look like and some of the kind of considerations that go into building and training them so we'll talk about what real-world transition-based systems look like but that's the end of this segment for now you\", metadata={'source': 'ypoaw7lJ6Rk'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about the GPT series of models and I start to look at how models got so big and so powerful and how that led to the capabilities that we have today so all of these models are structurally simple and kind of go back to the sorts of Transformer language models that we developed earlier in the course so they're all just left to right language models that are trained on Raw text that are Auto regressively predicting the next word so there's none of the kind of sequence the sequence capabilities of some of the other things we're talking about uh we're going to skip GPT one this came out before Bert and uh isn't really all that important to understand from today's perspective uh gpd2 was uh the one of the first of these to make a really big splash and Advance the state of the art it was trained on around 40 gigabytes of text and it was a lot larger than the other models that existed at the time so for example Bert was around 300 million parameters and gbd2 was around five times that size and this was at a time when folks were still kind of reeling from how Big Bert was and how expensive it was to use uh so when in March in the context of March 2019 when this was released it was really a kind of Head and Shoulders ahead of the rest of the models and one of the things that it did that we hadn't really seen happen before was its ability to generate a sequence of coherent sentences so remember that Bert can't actually generate text so people had been using bird and achieving really strong results on all these classification problems but gpd2 was the first one that sort of kind of hinted at the future capabilities of things like Chachi PT to like spit out an entire response that kind of makes sense so subsequently about a year later gpt3 was released and this was a much much larger model 175 billion parameters uh so the number of heads and layers had gone up dramatically and the dimensionality of the vectors in the Transformer was also up to about 12 000. so you can see this comparison here to a lot of the other things that existed at the time in terms of the amount of compute used and compared to T5 which was a major effort from Google this was about another order of magnitude higher so really really big really really expensive to train probably the main thing that this started to sort of revolutionize was the ability to do something called in-context learning which we're going to be talking about a lot so previous models had been fine-tuned we've talked about how with Bert you take it you have your classification data set you back propagate into it for a few epochs and you get it to do whatever you wanted to do similarly for these other models you can fine tune them and get them to generate text in the format that you want or answer particular questions in this case we don't do any kind we don't make any gradient updates to the model at all we're just going to run it off the shelf and the idea here is that if we give the model a kind of pattern in its context it should be able to continue that pattern and what we're showing here is an example of translation where we say translate English to French sea otter goes to Lucha de merman goes to meant pabre Etc and then cheese goes to blank now this is entirely just a string that is being given to the language model there's no kind of structure here at all I mean there is a kind of implicit structure in it but it's entirely formatted as a string and the model conditions on it so the reason why this should sort of work is well if we're modeling kind of the probability of the next token given all the tokens that came before I mean as humans we can look at this and say all right well if someone is giving us a list of English word translation in French English word translation in French then presumably the most common thing or the the highest probability thing to come next would also be the translation of this last word however the model is not learning through any kind of back propagation into its parameters this concept is entirely communicated through this language model context and so the model needs to kind of associate these things together and say okay well I'm seeing a bunch of these examples and over here I see oh okay this stuff's all in French and by the way it has to have seen French Data before otherwise this is just totally not going to work if it's never seen the word fromage there's kind of information theoretically it's like going to be super hard to come up with the answer but through all the layers of the Transformer it can kind of associate these different examples together and then ideally actually produce the right answer which in this case is fromage so the kind of capability here it it seems almost obvious in some sense that well okay well just just ask the model what the answer is but it was very non-obvious that this would work and they have a graph in the gpd3 paper which really kind of shows how surprising this is so we have on the x-axis here the number of examples that are given to the model in the context so like the number of translations and we see all the way on the left is zero so that's not giving any uh examples that's just asking the model to do the task now this works quite well with chat GPT but you can look at with gbd3 this was actually still at like 10 accuracy with this natural language prompt line now if we go up to one or ten examples the 10 to the 0 or 10 to the one we see that this 175 billion parameter model really takes off it starts to get 50 60 accuracy on the tasks they're looking at here but this 1.3 billion parameter model is still like kind of down in the dumps on the bottom of the slide here it's not working at all and remember that even a year before this was already a model that was like shockingly large and how are you going to train and manage models that are this big so this was really something that started to emerge and only made sense once the models got to a certain size and that was necessary before these ideas could even be explored in any meaningful way so in terms of how well it did the gbd3 paper evaluates on a bunch of standard Benchmark tasks and if you look at the top line here fine-tune state of the art actually the model is not kind of blowing us out of the water on any of these tasks right on the super glue data set which is this meta Benchmark consisting of a bunch of other tasks it's actually around 20 percent worse than what was possible with existing methods however it was the gpd3 model was using only this few shot in context learning so it's only exposed to a small number of examples of each task so if you're a kind of glass half full person it's like wow it's great that it can do well at all do kind of decently at all of these tasks without having been trained on them with gradient updates on the other hand you know it still hadn't kind of completely exceeded the state of the art in all of these different areas so you could kind of look at the different data sets and sometimes it's like pretty impressive and pretty close and sometimes it's farther away and in general you know the results were kind of mixed still strong for this fuse shot setting but this also wasn't really playing to the strengths of the model because these weren't the sort of uh language generation oriented things where large language models have really taken off so this was this kind of start of this fundamental Paradigm Shift there's a lot of other models that are now in the same class basically as gpt3 uh llama Palm opt Bloom pythia there will probably be many many more by the time you're watching this video and uh you know these all have decent capabilities here the most modern set of things like chat GPT gpt4 have an additional ingredient which we're going to talk about a little bit later reinforcement learning from Human feedback or instruction tuning basically additional ways of infusing supervised data to get them to be even better at these sort of capabilities than just training on language modeling gives you that's the end of the segment\", metadata={'source': 'jn41DLgnqek'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about zero shot prompting which is going to be a way to get large language models to do various tasks for us just by giving them natural language instructions so this is a capability that we started to see with gpt3 and then the modern LMS like gpt4 and chat GPT have really kind of taken off with they have this ability to basically do some tasks based on being having been exposed to them during pre-training so this started to be remarked on with gpt2 but just didn't work very well yet what the authors noticed was that if you put the token TL semicolon Dr at the end of you know a paragraph and then started generating the model would generate a summary and this is because this is used frequently online at too long didn't read to preface a summary so this kind of indicates how training a language model can actually get you a model that's able to do other tasks so then the question is given that we have the ability to do these other tasks or we've at least seen them before can we elicit these from the model can we get our language model to do something for us just by giving it a text specification or by giving it examples like we've we've talked about previously so these are both very flexible paradigms that can handle classification tasks text generation sequence the sequence tasks and and other things okay so let me kind of show you the basic idea behind ZR shop prompting and some of the basic terminology so uh what we're going to do is we're going to use our language model to label or predict on a single unlabeled data point x and we're going to go come back to sentiment analysis here and suppose that X is this short movie review now what we need to do is we need to take that raw data and we need to wrap it in a form that's going to be passed to the model which we're going to call a verbalizer and so in this case what that's going to do is it's going to do two things first we append review to the beginning just to maybe tell the model that this is a movie review is this necessarily the best template I don't know there could be other ones it's just kind of one way to indicate this sort of thing to the language model and then we uh give it a prompt before it's going to make its predictions out of positive negative or neutral this review is blank and then we uh generate with gpt3 and in this case ideally it generates neutral and we can use different prompts to get different kind of sentiment judgments so for example we could say on a one to four star scale the reviewer would probably give this movie blank and maybe it would generate three stars as a continuation so immediately you can see that there's a kind of Greater flexibility than in a lot of classification tasks because we're able to specify some sort of label space and then ideally the model will respect that however there can be problems with this for example if we just generate from the model what if we ask the model for a star rating and then it just starts generating something else we don't necessarily have a good way of constraining it only to use the labels that we give it we can take another approach which is to basically constrain it that way where we can say all right we're going to specify these three labels and then we're not going to let it generate free form we're only going to compare the probabilities of the token positive given X the token neutral given X or the token negative given X so we're basically asking the model OKAY within these three things score each of them and then we're going to return the highest scoring one uh so it's this is nice because we can sort of normalize these probabilities to get a distribution however I will say that it's still not very clear that the model totally knows the semantics of all of these labels right if you fine-tuned a model on a data set of sentiment examples and you see examples of all of this the model can learn okay here's how the you know neutral is getting applied in this data set whereas with zero shot prompting you're completely at the mercy of how this sort of happened in the pre-training data basically did the model C neutral being used in a way that's like how you want neutral to be used and that may be true but if you're asking it to do something slightly unexpected it may the fact that it has this prior from its pre-training may actually cause problems for you Okay so we've talked about several of these different approaches how much difference does kind of changing the prompt make we're not going to compare these two approaches specifically but we are going to look at different verbalizers here this is a paper by Gila gonan at all where they looked at making a lot of prompts by manually writing some and then basically paraphrasing them and kind of auto-generating a whole bunch of different ones so on the y-axis here we have task performance on a topic classification task so higher higher is better and we see that a prompt that works really well for example is this one in which section of the newspaper would you expect to find this article seems to work work pretty well now the x-axis shows perplexity which is perplexity of the prompt itself so what they're doing is basically evaluating how natural is this prompt and if you look all the way on the right they have what's this news as one of their prompts which you know is probably not something we would write down and expect to give good performance but we see that it it has much higher perplexity the model doesn't really think it's a very likely string and it achieves lower task accuracy so the kind of interesting thing is this General shape to this this plot right where uh as the perplexity gets higher as you're giving a less well-formed uh verbalizer the model starts to do less well however there's another interpretation of this which is that actually many of the prompts are doing just fine so one thing that this tells us is that a little bit of prompt engineering and basically getting in the realm of reasonable prompts will often get you to a pretty decent Performance Point at least on this task so uh they evaluated this a little bit more systematically across a wide variety of tasks and looked at the average accuracy across all prompts and the average accuracy across the best 50 percent of prompts and that best 50 accuracy was typically quite a bit better however again the gap between the sort of median or sort of medium good prompt uh and the best best prompt may not actually be that high so there have been a number of techniques proposed to do sorts of prompt optimization techniques either via gradients or black box optimization things like reinforcement learning uh and we're not going to talk about this literature the story may change but at least at this point many of these do not dramatically improve the results over just doing some manual engineering they may be really expensive to run and oftentimes if you just try four or five different prompts you'll find something that works pretty well and another kind of asked ingredient here is that the latest models like Chachi BT are specifically trained to do better when given prompts compared to basic language models so they're just much more accommodating of ill posed prompts and things like that because of how they were trained and so it's a little kind of role of prompt engineering and how much that's going to specifically you know drive performance improvements uh is a little unclear going forward that's the end of the segment\", metadata={'source': 'YCq6b31Jb6E'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to return to the idea of few shop prompting and we're going to look at specifically for this kind of in context learning what properties of examples are needed to make few shop prompting work well so the basic idea is that we are going to take existing X Y pairs labeled training examples and verbalize them in order to form an input to a system like gpt3 and what you do then is you feed in a number of examples of your task for example review colon or review sentiment colon the sentiment and then you end with an unlabeled example or x-test so we just show in this case review blank and then sentiment and the models kind of teed up to give us a sentiment judgment and ideally if we generate now the model will generate positive now compared to zero shot prompting the model has seen these examples in the context and so it's typically a little bit more likely to follow the pattern and it you have fewer problems with it sort of deviating from the label space that you've given it it also tends to work better than zero shot uh prompting and we'll discuss some comparisons of that in a little bit so generally this seems great and it's a very convenient way to give a small number of training examples and get an output from a model however there's still some things that we need to think about particularly when providing such a small training set so here is a uh training set that I've kind of cooked up for sentiment analysis that I think is going to be great it's got more examples than we had on the previous slide we've got a bunch of reviews and sentiment judgments and now we're going to predict a new sentiment I'm going to give you a second to look at this and decide if you think this might work well or might not work well so it turns out this won't work well and the model will predict positive here and why does it predict positive because all of the examples I gave were positive sentiment now you might think okay well that was pretty dumb of you like just don't do that but it turns out that actually getting the right set of examples can be subtle in a lot more ways than just this one so I in particular if we take random sets of training examples we can look at the variance in classification performance and I in this graph here we see that when you're at four examples the model can vary between 50 and 70 accuracy if you're just using raw gpt3 and trying to get classification decisions out of it now I will caveat that this is with raw gpd3 and when you start using instruction tune models this very introduces dramatically however it still shows that the examples that you give actually are pretty important uh and do kind of govern the task performance a lot uh the moreover they it doesn't just impact things what examples you have but actually the order of these examples can impact things a lot so uh we have here 10 different training sets this is from work from jow at all uh where they took permutations of the data points within those training sets and found that accuracy even within the same training set or you know in context examples could range between 50 and 90 depending on the ordering that you give now why why is that why does like The Ordering of these examples matter so much well one of the things they found was that the models have this kind of high positional bias based on the order of the examples that you give them and so if you do things like give a negative example and then three positive examples the model is really going to be likely to think that the next example is positive as well whereas if you go like you know positive positive negative positive it's a little bit more balanced and what they're looking at here is the probability of a label given just a null input like how how much is the model sort of biased towards one of these or the other um and what they do ultimately is they use that to calibrate the decisions that the model gives so they look at that uh probability on a null input and then compare when you actually give a real input is that higher or lower than that probability on the null so all right we've seen basically you can give different numbers of examples how well does this work on more modern models and how much does the number of examples matter so uh this is from a paper called Helm uh from the Stanford center for foundation models led by Percy Liang at all and uh the results here are kind of surprising I mean they're using a lot of you know stronger language models and they do find that incon that few shop prompting helps a lot you can see that most of these models each line is a different model goes up dramatically when you go from zero examples to one example so one example seems to really help and then beyond that actually subsequent examples don't give as much benefit as we might have thought and this might be uh you know the nature of this particular task which is a question answering task um you know maybe all you need to do is show that the model show the model hey you're doing question answering and actually showing more examples of question answering is not really helpful because the content of those questions isn't meaningful we can look at this for other tasks as well like sentiment classification and this kind of civil comments toxicity classification data set we actually see the same story here where showing one example seems to have big benefits in terms of uh kind of getting the model into the right space to make predictions on this task but does not lead to we don't see much benefit when we increase the number of examples further so one other result in this vein is due to say one minute all and uh it they studied how necessary is it to even have uh the demonstrations or what's the role of the kind of label in the demonstration and the somewhat surprising thing was that uh if you replace the Gold Label which is what's shown in the gold bar here with a random label like you just mislabeled the example you say you know here's the review sentiment positive and then you flip that to negative randomly they find that actually the performance is almost the same and basically what this kind of indicates is that the model is seeing this format of okay I've got I've got a review I need to predict a sentiment you're showing me examples of that and that's important to surface the right kind of behavior from the model but it's not really teaching the model the semantics of that sentiment label instead the sentiment labels uh semantics are largely imparted by the pre-training process uh so they look at you know the percentage of correct labels in uh the demonstrations and they do find that it is better to have correct labels so there's a small benefit from it but it's much less than you know let's say if you took a traditional machine learning model and you flipped half the labels to be incorrect the model would just completely melt down and not do well at all probably not even converge so this is a sort of interesting result that shows that even though fuchsia learning is powerful it's not always doing what we think it's doing and it's certainly not learning from the examples in a kind of traditional manner that's the end of the segment\", metadata={'source': 'JSBjj09xJeM'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about a proposed method of analysis for understanding the behavior of large language models and their ability to do in-context learning and specifically this is a mechanism called induction heads so there's a kind of theory of round Transformers and their computations that's been developing that looks at basically what sorts of computations happen when you start kind of putting multiple attention heads and feed forward layers Etc together and this analysis is broadly in the vein of what's called Transformer circuits and there's work by Catherine Olson at all that looks at trying to understand how mechanisms like in context learning emerge based on the kind of capabilities of Transformers to do pattern recognition so what they're trying to do is establish a causal relationship between the emergence of attention capabilities or surrounding this stuff and the ability to do in-context learning so let's take a look at their experiment so uh the idea of induction heads is that this is a pair of attention heads in different layers that are going to work together to complete patterns so what we have here is a sequence of random tokens that then gets repeated later so it's very much like the sort of text degeneration like stuff's just getting repeated and the model is just happy happily going along and copying things but why is it able to do that well they claim that there's a pair of attention heads that enables this Behavior and the first one which were not really showing is a head that just copies information from each token into the next token okay so now the instruction the instruction token kind of knows about the word node here now later on when we're repeating the word node that enables node to intend to struction and what that means is that remember in the Transformer the self-attention computation is ultimately going to lead to some kind of vector which is going to lead to some kind of prediction here over the vocabulary and maybe the thing that gets predicted to come next is this structure token so the fact that the model can look back and identify using the attention mechanism structure is going to allow it to predict that as coming up next so this kind of suggests the same sort of idea as is happening in in context learning and in context learning you get these examples uh you know of these pairs X followed by y followed by another X followed by another y showing you your label training data and if the model is able to say oh okay I'm going to associate what I'm seeing now my input X back with a previous label y that might be the label then that's a sort of almost nearest neighbor way of making a prediction about what that example should show so we're we we're still very far from a kind of full explanation full account of how you go all the way from data and the basic Transformer operations to this sort of capability but that's the idea that this is going after so uh what they do here is they look at uh basically modeling these long sequences of tokens and they're going to look at the loss of certain tokens and uh kind of characterize that and they're going to associate models over time based on the patterns of their losses and in particular what they do is they look at the loss of the 500th token minus the loss of the 50th token which they call this ICL score that's the ability of a model to do a better job of modeling things later in a sequence because it's able to recognize that the stuff there is repeats of earlier stuff right so if both of these losses go down that just means the model is doing better in general but if the loss of the 500th token goes down a lot and the 50th token only goes out a bit that shows that there's a lot of this in context learning happening okay so the kinds of things they look for are patterns like this where on the left what we're seeing is basically loss and they find that if they have a two layer Network or or two or more layers there's this kind of big drop in the loss where the model seems to be able to eventually figure out how to crack the code and recognize these patterns and model them with low perplexity then on the right what they have are various measurements of the emergence of induction heads and the claim they're making is that uh this Improvement in the score for in context learning correlates with when the induction heads emerge and that starts to give sort of causal evidence that uh the induction heads might be causing this uh decrease in perplexity now this doesn't actually show there might be some kind of shared cause that's that's causing both of these things right so it doesn't actually prove causality so they try to do some additional experiments where for example they change the architecture to promote induction heads developing faster and then they find that this this phase change in the loss happens even earlier they also uh remove induction heads and see if they can uh kind of reduce the model's ability to model this and indeed what they find is that uh you know if you do that you're able to change the profile of what you learn and kind of reduce the changes in loss that you were seeing previously so I think this is a very interesting direction to explain why in context learning works it doesn't do so from a kind of theoretical mathematical point of view but it's this very sort of empirical like let's pull apart the elements of the Transformer and figure out what's going on um what I'll say is that at least as of now I'm not aware of any developments where this kind of understanding has directly led to okay we're going to change the transformer in this way and that's going to impact how we want to train gpt5 however I'm not saying that this knowledge isn't valuable so it's one of several Avenues sort of studying these things theoretically this is a vein called mechanism mechanistic interpretability which is trying to understand these mechanisms uh and then finally there's the kind of fully empirical way of just saying well okay what works and what doesn't how do we prompt these models and get them to work well how do we kind of understand over multiple trials what sorts of things work and don't work and we're going to talk about that more later that's the end of the segment\", metadata={'source': 'mUthsZ_Aivo'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about instruction tuning which is one way to take raw language models and make them much better at doing the kinds of things we want them to be able to do like in context learning so we basically have this kind of mismatch between how models are trained which is as of what we talked about right now sort of vanilla language modeling predict the next word and how we actually want to use them which is to predict an answer given a prompt either a zero shot instruction or a few shot examples of how a task should be done so this instruction tuning is one way of trying to bring back the idea of fine tuning but do so in a kind of more broad Strokes way that's going to enable these models to do lots of tasks at the same time so it's one of two approaches for doing this instruction tuning is one of them that's going to use supervised data that's derived from existing NLP tasks and then the other one is reinforcement learning from Human feedback or rlhf uh where we're going to do something similar but with RL using human judgments about outputs so here we're going to talk about the first of these so the basic idea uh is encapsulated by a model called t0 which confusingly enough is actually a successor of T5 and what it's trying to do is actually do a bunch of tasks with one model so really deliver on the original goal of the T5 work and what they did is they got basically all the data sets they could get their hands on and then they crowdsourced people to write prompts for these so for example we see this uh kind of summarization prompt in the top left starts with an article and then someone wrote The Prompt how would you rephrase that in a few words and then they fine-tuned the model to take that as input and produce the corresponding output on the right graffiti artist Banksy Etc now the model is fine-tuned over data from all of these tasks basically at the same time right so it'll get the summarization thing and the instruction at the end tells it to do summarization it'll get paraphrasing and it says uh pick one these questions are duplicates or not duplicates and it has to do classification now right so it's we're we're training the model to kind of jump back and forth between many different capabilities only controlled by this prompt but the idea then is that at test time once we've trained the model on all these data sets we should be able to give it a new prompt like the one at the bottom can we infer that the banker contacted the professors and we're asking this question that requires inference and hey the model scene question answering as a task before so maybe the fact that it's seen a bunch of question answering enables it to do this and maybe because it's seen paraphrasing it sort of understands generally the relationships between sentences a bit better and so this is the idea behind the kind of task generalization so what they did in this work was pre-train uh a model like the you know take standard T5 out of the box and then train on all of these tasks in yellow here and then run testing on all these tasks in green so you haven't seen data from these tasks before and they were careful to even hold out specific task families because some of the data sets are so related that even if you generalize to a new data set it would basically be the same task as something you'd seen before so that's one approach there's another very similar approach called flan Palm which came out of Google again basically training a model on just every task they could get their hands on and this had a few extra bells and whistles uh since it was done a bit later and but basically this the idea was fundamentally the same so I'll show results on this model so they looked at primarily two uh kind of Flagship data sets for very challenging language understanding tasks uh the first on the right here is called Big bench which was just about a kind of crowd-sourced collection of tasks where there wasn't much training data given it's mostly a sort of instruction following data set and then this other one is massively multitask language understanding which involves answering a bunch of multiple choice tests from a bunch of different academic disciplines so really kind of testing broad domain knowledge about a bunch of different things and what we see is that when these models are fine-tuned on increasing numbers of tasks going all the way up to 1800 tasks for this last Model so like I said they really tried to get their hands on just like every data set they could we see that the performance of the model still increases you know even over a huge model that's been trained over the whole web so there's still some benefit to fine-tuning on all of this stuff and cot results use this additional technique called Chain of Thought prompting which we'll come back to in a little bit but that kind of leads to even stronger results and so you can see uh kind of good performance on these these benchmarks using the sorts of techniques and the sort of instruction tuning we've discussed here that's the end of this segment\", metadata={'source': 'YT3VSlDjrVU'}),\n",
       " Document(page_content=\"foreign we're going to talk about reinforcement learning from Human feedback which is one of the uh sort of bells and whistles that makes Chachi PT and gpt4 really effective systems so I'm kind of framing this as an alternative to instruction tuning instruction tuning uses labeled data and RL from Human feedback we'll use human feedback in sort of a different way some of the limitations of instruction tuning are that if the data sets that you're tuning on aren't very good your model won't be able to really go past what you see in your labeled data and it also is not clear that it's going to enable a model to generalize to new tasks Beyond what's in these tuning data sets right so we can get all these examples of existing tasks but is it really going to enable us to do stuff that's not exhibited in any of them it's a little unclear so our lhf provides an alternative where what we do is we basically get models to Output you know give outputs on whatever sort of inputs we want and then we ask people which of these is better the nice thing about this is as long as people can have an opinion this can kind of work on anything right so if we have a super state-of-the-art model but there's still we can identify that this response is slightly better than this one we can still tune it and make it better without relying on data that was kind of originally authored by people so this kind of sets the stage for this rlhf process I'll show the overall kind of diagram here so the idea is to learn from Human comparison of two system outputs which is very different than the reward and standard reinforcement learning where you like take some action and then you get some like scalar reward and then that drives your learning so instead what we have here on the left we're basically just seeing uh the standard either pre-training or fine-tuning pipeline for a model so that's just sort of what we already know then in the middle here you need to take that existing model you have let's say base gpt3 and then you are going to generate a bunch of different outputs like how to explain the moon landing to a six-year-old and then have a human rank them then these rankings are used to train a reward model that's going to ideally assign scores to these different outputs that reproduce the human ranking and then that reward model is what you do RL on okay so to go through this a little more formally we have our quote unquote base language model which we can think of as like gpd3 and we're going to train that in advance and then you know set it aside and then we have our reward model which we're going to learn that's going to map these completions y to real valued scores now the nature of the data used to train the reward model like I said is you get two completions for a single input X and then an annotator says y1 is better than Y2 okay but that only tells you which one is better it doesn't actually tell you scores for each of them so then the way that this model is learned is using what's called a Bradley Terry model which is a model from preference learning that basically says we're going to take we're going to assume there's some underlying scoring function r and the probability of a human user saying that y1 is better than Y2 is going to be proportional to the exponential of the score for y1 divided by the sum of the exponentials so basically standard sort of soft Max over the r values what this allows us to do is it allows us to derive log probabilities of classification and then maximize likelihood of this preference data while learning an underlying reward function while learning this r as a way of mapping outputs to real valued scores then once we have this R we could set the human preference data aside and say now we've got something we've got a reward model so we can just do reinforcement learning so what we're going to do is we're going to optimize our expected reward over a big data set of prompts and what we're going to do is we're going to sample a prompt X and then we're going to take the model and say okay model produce the output y for us and then given that output y we're going to assess the reward and then use a method like PPO proximal policy optimization to optimize it in this case we're not just optimizing for the expected reward but also there's a penalty that kind of forces the model to be close to the base language model so ideally the model kind of continues to get better and better it's actually producing better and better responses but the reward model is still able to say okay you know even though you're making better responses that you were to begin with I'm still able to judge that this one is better than this one whereas instruction tuning you're just always fine tuning on the data that's provided by the people so the open AI reward models are trained on data that looks very different from instruction tuning as well so for example this brainstorming use case lists five ideas for how to regain enthusiasm for my career this is about 10 of their data is human judgments about this and that doesn't look like anything in a supervised standard NLP data set and similarly these generation and rewriting examples can be quite a bit different and more diverse than what we see in existing data sets so this is part of the way that open AI can make something like Chachi BT work very well for these sorts of things even when there's no real collected data for it so the earliest models that used uh kind of that were sort of gbd3 plus plus or what what's been called GPT 3.5 we're actually using something that looks basically like instruction tuning but what they would do is they would get completions from the model and then whenever a human would rate it with the highest score they would train on that data then text DaVinci 003 which came out shortly before Chachi BT was actually using RL with PPO uh I think what this indicates is that you know given that the idea of RL with human feedback had been around for a while and these earlier versions of the model were not using it it's a little bit difficult to get working reliably uh and you know part of what we see is that uh it's a little bit hard to know this because the data that openai has is not public and the data that you get from your human annotators is really really important it seems like having good quality human written demonstrations and good quality ratings uh are is key for this process so there are a bunch of recent uh efforts to show that we can actually get models that are as good as chat gbt without doing RL most of these use some kind of supervised fine-tuning over outputs that are actually derived from chat gbt itself uh now RL may be brittle although but it's a little unclear if these other methods really kind of generalize as well as gbd4 and these other systems and a lot of them rely on a stronger system to produce the data that you're then going to fine-tune on so the jury is still sort of out on whether we can actually build stronger systems without reinforcement learning or whether that's necessary that's the end of the segment\", metadata={'source': 'DwAdhx6GFh8'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about factuality of generations from large language models so we think of large language models as modeling distributions over text and a lot of times we're asking them questions and expecting to get facts out but these are slightly different things I think the easiest kind of thought experiment to think about to conceptualize this difference is that these models are trained on the web and there might be kind of widely popularized falsehoods that the models have been exposed to and can reproduce but are not necessarily the kinds of things we want them to be saying right another way of thinking about this is just that there's a lot of information out there right like think about all of the cities and the people on Wikipedia and all the cities have latitudes and longitudes and all the people have birthdays and things like that a language model may not be able to store all of that information uh if you just look at the amount of data that's there and try to kind of compress it using standard compression algorithms there's actually a bit too much even for the kind of largest models that we have today even if we assume that they're kind of perfect encode uh perfect kind of compressors of information so as a result what will happen is that sometimes the model assigns a kind of moderate probability of several options it's sort of unsure of an answer and spreads out probability mass now the reinforcement learning from Human feedback process really improves this so this is why when you ask chat gbt or gpd4 things it'll say oh I I don't know a lot of the time rather than giving a wrong answer but this is still not perfect and we're going to talk here about how we can actually figure out when these models are kind of tell you know saying correct facts or not and this is very much an emerging area so I'm going to give you a kind of overall view of one way that we might do it with the sense that this is rapidly evolving as the research here progresses so the concrete thing we're going to think about is also referred to as grounding language model Generations it's a little bit different from kind of perceptual grounding but basically suppose that we have some text generated by a language model and suppose that we have a source document how can we check that if what the model says is factual with respect to this source so we're going to think about there being a few steps in this process the first is figuring out what this Source document is which may involve something like information retrieval the second is saying all right if we get some long response out of chat gbt how are we going to break that down and figure out all the like individual things that need to be checked and then the third piece here is actually checking each of these so we're going to assume for now we're going to kind of skip step one we'll come back to it a little bit at the end of this segment but we're going to assume that someone gives us this reference text or these documents okay so let's think about the second stage we want to think about if given a long response what are the pieces of that response that need to be checked well one approach is to just say let's break it into sentences and say we need to check sentence one sentence two sentence three Etc now you can actually go a little bit deeper than this because a sentence might be long and complex and express several of what we call propositions so this comes back to ideas that we talked about in syntax and there is a long history in frame semantics of thinking about what is a proposition uh basically what are the units of meaning and sentences that really reflect kind of something happening in the world a lot of the time you could think about this as being anchored to verbs so let me give you an example we're not going to use something like a frame semantic parser here instead we're going to use large language models to extract propositions so we have a sentence here the main altar houses a 17th century Fresco even going just up to that point in the sentence we already have a kind of unit of meaning that can be checked this red underlined thing then uh the 17th century Fresco of figures interacting with the framed 13th century icon of the Madonna so we know now what the Fresco is of and that's a kind of separate proposition from where it's housed right so we can kind of break things up here and then icon of the Madonna painted by Mario balasi so there's this kind of sense that we can break this text up into these pieces there are other pieces that we could use as well things that this sentence presupposes for example when you talk about something like the main altar we are presupposing that an altar exists now we're not going to write that down in our decomposition but I think what that does show is that there are many different ways to do this so what I'm showing is not the authoritative right way to do it it's just one way of kind of roughly breaking this down and a lot of the current systems for doing this are using large language models in order to do it all right so now that we've got all these units of meaning how do we check that they're correct well one idea is to come back to textual entailment which we've talked about a bit throughout the course to actually see if each piece is entailed by the source so one way to do this is to just say all right if you've got a whole bunch of sentences in a document and a sentence level entailment system we're going to look at our sentence and then everything in this document and take a Max over those entailment scores so concretely if we're trying to check this first piece here in the generated text there are strange shape patterns on Arcade Arcadia planitia then what we need to do is we need to look at its entailment score with every sentence in this document here and we're basically saying hey does one of these sentences support this generated thing so we compute those sentence level entailment probabilities and then take a Max over that we can also use document level entailment systems which we haven't really talked about and involve kind of training on separate data sets but we can kind of dispense with that sentence level classification and maxing and just run a document level system this kind of makes sense right because textual entailment is about saying does this sentence kind of follow from the input and We're looping over sentences and trying to establish that now it might be the case that you need multiple sentences to support what got generated right and in that case this kind of approach would fail and you really need the document level thing so there have been some recent systems including this one called fact score that try that Implement these sorts of ideas so this one breaks things down into these propositions sort of like I was talking about and then tries to validate each of those against Wikipedia uh it uses large language models for all of these pieces so it doesn't use textual entailment here uh but it's kind of achieving the same goals as some of these earlier systems we're going after finally I can show you a pipeline uh this RAR system from Google that tries to kind of accomplish everything so it starts with a uh input passage down here Milli in between premiered on 24 February 2014 on CBBC and it generates in this case rather than statements or propositions it generates questions and these questions here are passed into a retrieval system they used Google to retrieve a bunch of documents and then the system checks the facts against those documents in this case using language models and then eventually produces a revised version of the uh of the input so their idea is to go all the way from checking to then finally revising and improving the factuality of the output so this shows the kind of overall pipeline that you can use now the challenges with this we're not going to get into evaluation are that you can sometimes take things that were actually correct that the language model generated but you're not able to find evidence to support them so you say oh I don't know about this or I think this might be false but it was actually correct right so you can have sort of false positives and false negatives when you're trying to detect these errors and I don't think there's a system here yet that's perfect or that really like takes chat GPT and eliminates all the errors in what it generates this is very much an ongoing active area of research but it's something that seems to be viewed as important by the research Community because trying to figure out how to identify check and uh you know confirm all of the pieces of information here seems to be important that's the end of this segment\", metadata={'source': 'bQZvmQUlqcs'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about explaining nlp models and this is going to kick off a little mini segment on explanations so this is a big topic in machine learning more broadly particularly with the rise of deep learning models because these models have very complex behavior and we want to be able to understand them so for example you know when we looked at question answering and we saw this example where the model is trying to say who caught a 16-yard pass in this drive um and it turn it you you know the model picks the wrong thing here based on the highlights the answer should be devon functions and instead it picks the uh you know the last name stuart and so we we want to ask okay well why did this happen what can i do to fix it how do i debug my system and what does this say about how this system does but when it's all a big neural net it's very hard to answer any of these questions right we can look at the kinds of errors our model makes and try to get a sense of what confuses it but that's a very like uh kind of black box way of studying the model right we're treating it as this object and now we have to look at just how it behaves and try to assess what it's doing based on that it would be much better if the model could explain its behavior and that would give us an idea of how to improve it so here's another example from sentiment when we talked about deep averaging networks and we want to know okay well why is the model producing these particular uh predictions particularly for not bad which should be positive but the model is predicting negative and one thing that iret all did in their work was they looked at the predictions on each individual token at least as a way of saying okay well maybe it predicted the movie was not bad because we have two tokens here not and bad which individually the model thinks are negative and so you know it's not able to correctly model the compositional relationship between these so this it would be useful if we could take our models and generally try to produce an explanation of why they make the decisions they make and so why what exactly do we want from explanations and what should they be able to tell us so these are some bullets taken from zach lipton's paper on the mythos of model interpretability which kind of outlines some of the factors here and why we really care about explaining our models and so the first one here is trust basically you know if there's a model that's making some really kind of screwed up errors we're probably going to think okay there's something we don't understand here the you know the model is not behaving in any kind of human-like way and so it's going to be very hard for us to understand where it's making mistakes and maybe we're not going to trust it whereas if it makes understandable mistakes like okay you know it picked out this answer and it said that it was for this reason and as a human i totally understand that justification we might be more likely to think that the model is doing something reasonable uh another is a notion of causality so if a model is explaining its prediction for why it predicted a certain class why it predicted positive sentiment then we have a sense that that explanation should mean something right that basically if you say i predicted that this was positive because i saw the word good then if you didn't see the word good necessarily the model would have predicted negative now if the model says i predicted positive because of the word good and then there's you know you remove the word good and in fact remove the whole sentence and it still predicts positive with exactly the same score because instead it was you know using some crazy overfit feature about you know the number of does in the first sentence then that's not a good explanation even though it somehow sounds like a just a good justification because it's not actually telling us what the model is doing so we want this this kind of causal relationship between the input and the output that the explanation reveals uh informativeness is a third thing that we might want uh basically rather than just returning a prediction the model should also give some other information that that might be useful so for example in medical diagnosis contexts if you see something like an mri you don't just want to know okay the neural network thinks it's glioblastoma you want to know more information about it because as a doctor you need to be able to figure out a follow-up course of treatment what you're going to do and just having a kind of course label doesn't actually help and so in a lot of cases being able to provide more information would actually make the classifier strictly more useful in addition to you know building trust and giving us an idea of what's going on and then finally the the last piece here is fairness so this relates to trust in that we want to recognize that models are making human-like decisions but we also want to make sure that these decisions aren't discriminating against certain protected classes and so by you know having an explanation of where the decision came from we can make sure that it's based on factors that we think should be considered and not those that we think should uh you know basically be ignored or set aside all right so over the next few segments we're going to talk about a few different forms of explanations so the thing we're not really going to talk about is so-called transparent models now ideally we could just train up models that are so simple that we can look at them and understand what they're doing for example a decision tree with a relatively small number of nodes in it this is a model that doesn't really need to be explained because we can just look at it and understand what it's doing but when we have more complex models we're not going to be able to look at them and understand them so we're going to have several different ways of approximating their behavior or kind of thinking about what they're doing the first one is called local explanations so these are going to basically tell us what features in the input led to the decisions of this particular classifier and this ties in with the causality idea from the previous slide and and the idea of counterfactuals if the input were something different the model should have predicted some different output and so the reason this is local is because it's reckoning with small perturbations to one particular data point can we explain the model's behavior basically in a kind of neighborhood around this data point that we're looking at here we can also have text-based explanations so the model should actually be able to give a text description of why it's behaving the way it's behaving and finally we can probe our models so this doesn't provide an explanation per se but it gives us some idea about the intermediate structure of the computation that's going on and so there's a bunch of different things in this vein we're going to primarily talk about auxiliary tasks when we get there but these are all ways of understanding okay the model can do this and it can't do this and that gives us a more refined sense of what it's of what it's doing even though it's not producing an explanation per se so over the next few segments we're going to talk about these different types of explanations see some examples of them and try to understand why they can help us build better nlp systems that's the end of this segment you\", metadata={'source': 'Nr0_xYEso-4'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about a form of local explanations which is going to be various ways of highlighting the input in order to try to show what input features are particularly relevant for understanding the model's prediction so this returns to the idea of counterfactual kind of examples or perturbations of a single input so essentially what we want to say is if the model's input were x prime instead of x what would the prediction be and so here's an example from sentiment where we say the movie was not great in fact it was terrible and the model predicts negative now if we hide the word great the model still predicts the the model still predicts negative but if we hide the word terrible it predicts positive so what this should tell us somehow is that terrible is important for this model and great doesn't seem to be as important so this gives us maybe the idea that one way to think about explanations is that we might perturb the input a whole bunch of times and see how perturbations around each token impact the model's prediction and so this is the idea behind a technique called lime due to marco tullio ribeiro and co-authors from university of washington so lime stands for locally interpretable model agnostic explanations so there's a couple of pieces here it's a local method because we're thinking about explaining the model's behavior on a single example and it's model agnostic because we are treating the model as a black box we are not for example assuming that the model is you know a fully differentiable uh model that depends on parameters you know that we learned through gradient descent or anything like that we can apply this to any technique that we have so here's how it works we take our input here we're going to show some examples in terms of images but this can be applied to text as well like we saw in the previous slide and you break it up into a bunch of chunks so here these are little regions of an image for text these might be short phrases or single words and then you run prediction with different subsets of the chunks being present and this gives you an idea of which chunks kind of impact the model's prediction although of course you can't run with every single subset of chunks and so what you have to do is you have to try to figure out okay over this over the kind of sample of the sample of chunk subsets that we have which chunks are actually you know contributing to the classification decisions so the basic idea here is that we want to take these predictions on subsets of these components and then we actually want to train another model that explains which components contribute to the model's predictions most strongly and then whatever that model thinks is important is what's overall the explanation that we're going to return so let's dive into this in a little bit more detail we take our input and we have this notion of binary masks where we have d prime little chunks of input and the way that we're going to think about modifying this input is by sampling a binary mask over these uh d prime chunks all right and so we can draw a bunch of new input samples and then actually well we could draw x primes which are masks and then we're going to say x double prime represents the new input example that's formed by applying that mass to the input and then we compute f of x double prime this is just going to be the result of our classifier so what this does is then we uh we get a bunch of individual decisions based on f of x double prime and then we want to learn a simple classifier that says okay based on x prime based on which chunks i'm including or not can i predict what what the model's final prediction f of x double prime is going to be so let's say in this model we say all right you know chunk three has a very very high correlation with the positive class here and the model overall predicted positive what that means is that okay if i see chunk three i'm likely to predict positive if i don't see it i'm much less likely to predict positive and what that tells us is that chunk 3 is probably very important for the model's prediction because across all these different subsets that we're seeing we're kind of getting this understanding that chunk 3 contributed a lot to the probability of the kind of label being positive so there's a there's a lot of kind of tricky things about designing and applying this method uh one is that if we have a you know imagine in a limiting case our chunk is just the entire input like the entire image or the entire sentence or something like that then we know that that whole chunk is extremely important for the model's prediction but of course we have no fidelity here right we can't actually say anything useful about as an explanation because we're looking at something that's too big or too coarse so and then conversely you know you could break it up in terms of say individual pixels in an image but then maybe you can't necessarily interpret what's going on you get that okay these pixels you know you know we got eight pixels over here eight over here 9 over here and kind of random patches that doesn't really give you a very satisfying explanation so they evaluated this by looking at a couple of models where we know the explanation so they looked at a sparse logistic regression model and a decision tree model and again these are models that i would say are transparent they you know just by looking at them we understand how they behave but what they were doing was trying to say all right can our black box explanation technique explain the behavior of these models and what they were looking at was can it recover the features that we know these models to be using and you know it turned out it was able to do this pretty well and better than some other existing techniques at the time okay so lyme is great but there are some issues with applying it to a wide range of problems and one of those issues is that when you zero out features you're changing the input in a fairly dramatic way and so for tasks like sentiment analysis where we often think about models as kind of additive right like you know it's like you have two sentences with positive sentiment and one with negative and if you remove the negative sentence you would just get more positive and you know if you removed a positive sentence it would just kind of balance out but that's not actually how it works every time a lot of times the model really expects data to be taken from a particular distribution and we'll call this the data manifold so a manifold is basically just a shape or a kind of slow dimensional surface in a very high dimensional space and the idea is that there are certain natural sentences or natural images and that's what the model is going to expect to see so once we start zeroing out a bunch of features or masking out a bunch of stuff we're no longer seeing a natural sentence and our model might just behave unpredictably here and so what comes out of this lime procedure may not actually be that reliable because maybe the model's you know making certain decisions when it's kind of seeing data points it's comfortable with and then as soon as you're masking stuff it's doing something totally weird and crazy that it wasn't because it wasn't trained to see those examples so another approach is to think about sort of going even more local like why don't we think about instead of zeroing out a whole patch of uh input features what happens if we just think about a sort of very small perturbation and one week we can do this is by taking gradients so uh again this method was kind of pioneered for computer vision back in 2013 by simonian at all and the basic idea is that we're going to think of the current data point as yielding some prediction and we're going to do a first order taylor approximation of the scoring function around that point so what we do is we take the derivative of the model score with respect to the input image or the input text if you're doing natural language processing this is different than when we normally think about gradients normally we think about taking the derivative of a loss with respect to the parameters and then updating the parameters so we're kind of flipping it around where now the parameters are constant but we're taking the derivative with respect to the image so what this tells us is that all right let's say there's some pixel that has a very high gradient magnitude what that says is if i change this pixel then i will change my prediction by a lot and that tells us that this pixel maybe is very its value is very important right the model seems very sensitive to it and so if it had a lower value in this counterfactual way of thinking about things then the model would make a very different prediction and so you know the nice thing about computer vision examples is that they can have some nice visualizations and so uh here we can kind of get a sense of for certain object classes why the model is making its predictions and you know it's it's hard to fully tell what's going on here but uh we at least get a sense that okay the model seems to be clued into pixels that are related to these kind of distinctive central features of the image uh and you know we might think that it's likely to be looking at the right thing now importantly we can't actually tell whether this is a good explanation method just from looking at these pictures because what we don't know is if we remove these pixels the model said is important would the prediction change this kind of counterfactual thing is not being evaluated here but regardless it at least gives us some confidence that either something is reasonable is happening in the model or you know the explanation technique at least aligns with what we kind of expect the model to be doing all right the last and the last uh kind of variant of this i'll mention is a technique called integrated gradients and the idea here is the following so one way that the this gradient based technique can fail is if our local approximation around our final data point is not very good so suppose we have two features a and b and the prediction is an ore of these two then if both of these are set to one you know and imagine this is a soft or operation you know changing either feature in isolation doesn't actually look like it changes the prediction very much right because the or just kind of pulls it up back up to the other one so we can look at this and say okay well if we changed a or if we change b you know neither of those actually is contributing very much to the prediction so the idea behind integrated gradients is we think about a bunch of points along a path from the origin basically some you know neutral sentence that just has like zero word vectors in it up to our actual sentence and we kind of you know incrementally like make the sentence look more like the real sentence along this path and we do the same kind of gradient-based trick everywhere along the way and so what's going to happen here is that uh you know we'll we'll see that okay rather than everything being kind of saturated along the way we'll say okay well actually if we changed b here and raised it a little bit that would improve the prediction or if we changed a here that would that would improve the prediction because we're not you know we're not at the saturation point anymore so by going along the path we get a better sense of maybe which features are uh you know are kind of important without thinking about it so locally as like all right every other feature is here and i'm just missing this one which doesn't do a good job when you have uh kind of multiple independent signals that tell us what the right thing is so this gives you a kind of tour of the ways that people produce these sorts of highlights like like we were seeing for images and you could produce the same thing for text you know these these sort of sentiment style or these these highlights for tasks like sentiment where we see them pulling out the words that are most crucial to the prediction so these these sorts of techniques can be very useful for helping visualize models and and understanding what they're doing and they can be part of your debugging process as you think about building a model and trying to kind of push its capabilities and make sure that it's doing something that we really want it to be doing that's the end of the segment you\", metadata={'source': 'ZVElc4CvHpk'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about model probing so probing is a different form of explanation than we've been looking at in that it's more a way of analyzing the kinds of representations that our models are using which gives us some idea maybe of why predictions are arising but it's a little bit less explicit than some of the other techniques we've been looking at so essentially our goal is to figure out what kinds of information are being preserved in a neural network and the way we can do that is we can take intermediate representations from the network and then try to predict that information out of those so what we're seeing on the right here is a pre-trained encoder model producing these blue vectors and we can aggregate these into representation well we can either grab token representations or aggregate these into span representations and put these through a very simple classifier so we might use a neural net with one hidden layer for example and then we'll try to predict something like say part of speech tags or dependency relations or something like that and the idea behind this is that by using a simple classifier we are forcing most of the heavy lifting to happen in this pre-trained encoder and if the model is able to do well it indicates that these blue vectors being passed in are pretty good now one critical thing here is that we're going to keep this pre-trained encoder and these blue vectors frozen we're not going to like back propagate into them as we would if we were trying to build a good model for this task because what we're saying is all right given this existing model how well can we predict this information from it and so kind of changing the model to make it predict better is a little bit defeating the purpose of this experiment so in some work from uh the one of the johns hopkins university summer basically summer kind of research series they looked at having a set of tasks that they would put through that they would use to probe the representations of a bunch of different pre-trained models and so here are the results for bert base so what they have on the left side is a so-called lexical baseline which just uses glove embeddings or essentially context-independent representations from uh the model from the burp model itself then they look at how much of a gain there is from the actual contextualization and so that's what's shown in the next two columns here and the conclusion here if you look at the average for these different tasks is that contextualizing these vectors with bert leads to a very large performance increase across these different tasks and so we can also understand certain things like for example if we look at the part of speech row on the top the kind of individual predictions actually aren't very good these uh you know we can't tell a lot about a word part of speech in isolation but then the predictions from the contextualized representations are quite good around 97. and so the takeaway here is that this model seems to preserve part of speech information and because we can read it out from the representations that we get using a fairly simple classifier and then you know we can look at other aspects of this like for example constituency and see that the results are a little bit lower this might indicate that that trying to kind of aggregate over spans is a little bit less reliable and the model has a bit less of an idea of what the kind of relation between these different units is even though it's still doing quite well uh you know given that we're essentially taking some off-the-shelf representations and feeding them into a fairly shallow model so we can look at these results and and again try to kind of decode what's going on and and it lets us compare different pre-trained models and assess their strengths and weaknesses so some some other work from google showed that the basically this kind of idea can also be used to understand how bert relates to the conventional nlp pipeline where you know we might think that okay the first thing we run is a part of speech attacker then we run a parser then we extract entities and then we you know figure out the relations between those entities and co-reference um and so what they did was they looked at all the different layers of bert and they probed those layers for which basically for being able to solve each of these tasks and what they see here in purple they have the kind of delta from the mean representation using representations from that particular layer and so in part of speech at the top for example we see this very high purple bar to start with which then drops off and what that shows is that early on in the network the representations are much much better at predicting part of speech than they are later what this says is that you know early on the the model form some representations that have a lot of information about part of speech in them and then those representations gradually become less and less aware of part of speech but other things kind of pick up after a while for example the second to last one here semantic role labeling does well pretty close to the input but then also kind of in the middle of the network and then uh co-reference resolution at the bottom is sort of doing okay throughout and so they the title of their work is bert rediscovers the classical nlp pipeline and so their argument is that there's a little bit of the same progression of tasks going on inside the model in that it's you know first learning to do simple low-level things and then building up more and more complexity so while this is not an explanation in the sense that it doesn't tell us why the model made a particular prediction it does give us some insight into what bert is doing and we can start to think about like okay we think bert has the capability to do co-reference resolution for example so when we look at a task like multi-hop reasoning that requires resolving references we can think okay well bert is able to do co-ref and so this task involves ko-ref and so i think that bert should be a good model that works well here and my understanding when it gets the right answer is that it might be doing that due to co-reference this reasoning is not you know it's it's a little bit hard to apply and there's reasons why you can't just uh take all these conclusions at face value um there are some issues with probing where you know if you start building very sophisticated probes suddenly those have a lot of parameters in them and it kind of complicates the story but in general this is another useful tool in our toolbox for trying to understand what these models are doing and giving us a sense of uh just what's going on inside these super complicated pre-trained models that's the end of this segment you\", metadata={'source': 'a6u6WM5wcLQ'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about an issue in nlp data sets known as annotation artifacts so these have become more prevalent or sort of awareness of them has become more prevalent in recent years due to a higher focus on what's going on in our data sets and how exactly they're constructed and so what we're going to see here is that in order to understand what a model is learning we can actually under try to understand the data set a little bit better rather than just try to explain the model's behavior using one of the other techniques we've looked at and actually trying to understand this data set is in many ways just as important for understanding what's going on and can give us just as much information so one of the tasks we're going to look at here is natural language inference and so recall that this task says all right we have a premise a woman selling bamboo sticks is talking to two men on a loading dock and then we have a sentence that we want to determine whether it falls into one of these three classes so these are examples of sentences that fit each of these classes that are annotated in the data set so for example the sentence there are at least three people on a loading dock would be entailed by the premise because there's the woman and the two men and so this sentence is implied to be true based on the premise um a woman is selling bamboo sticks to help provide for her family this is neutral because it's not like false we can't tell that it's incorrect but we don't know whether she's helping provide her for her family or selling them for some other reason and finally we have contradiction a woman is not taking money for any of her six that's just wrong she's selling them so the you know you know people crowd-sourced big data sets that looked like this and trained models and then the idea is that these models should learn general purpose uh you know types of textual entailment so by having a large data set like hundreds of thousands of examples we might think that we have very good coverage of phenomena that we might expect to see in the real world like if you tried to formulate a uh you know a premise and a kind of sentence that's entailed by it or something like that you might expect okay well if they have so many sentences they're very likely to have seen the kinds of patterns that i would come up with but it turns out there's actually a few common patterns that annotators used in order to construct these sentences because this data was constructed somewhat artificially people were shown premises and just asked to write down statements with each of these three falling into each of these three categories and so for neutral statements annotators ad would add information they would say okay you know i'm going to take the scenario that's here and add extra stuff which we can't verify and so that won't contradict but you know it won't be entailed either and so that's how i get neutral um and for contradictions they would do things like negate something that's actually going on um so a woman is selling bamboo sticks okay a woman is not selling bamboo or not taking money for her sticks right and the problem here is that that's there are plenty of good statements that don't have negation in it and there also aren't that many entailed things that do have negation and so one way we can understand our model's behavior is by in this case training a model that just looks at the hypothesis so so we've got the premise and the hypothesis the hypothesis is what we're trying to classify and so um a couple of different teams of researchers identified that if you actually just look at the hypothesis you can get around 70 accuracy or 60 to 70 depending on the data set which is much higher than random chance of you know around a third so that's a little bit alarming in that this data set is fundamentally about saying all right is this statement true given this other statement if you just don't even need to see the other statement then clearly there are patterns in the hypothesis statement that are revealing to the model what's going on and so these are what we would call annotation artifacts because it's a it's completely a function of how the data was annotated there's nothing universal that like this statement is like more likely to be a contradiction statement somehow it's because they're you know there's a huge space of premises that would imply this this statement as well um so lacking some kind of underlying distribution over the text we instead have the synthetic distribution that annotators gave us which has these issues with it so why do we care about this the the reason we care is because when we train a model on this all it's going to learn is oh okay negation nah contradiction and we wanted to actually learn to be able to do sophisticated reasoning and when we give it new examples that don't have negation reason about those correctly but instead that's not what gets learned uh and so again tying this back to explanations this is not providing an explanation of the behavior in the way that some of these highlight methods are for example but it's still providing us a way to understand what these models are doing and give a sense of how we might go about improving them which in this case probably requires sort of doing some kind of data augmentation or inoculating the training data to try to reduce this uh you know to reduce this bias and so you know we can we can approach this by just sort of throwing everything out and trying to make the task harder we could try to tweak the data or we can or we can use a different training objective all right and so this is not a phenomenon unique to nli so in question answering we've talked a little bit about the issues in the squad data set but one of them is that uh a lot of times there are only a few instances of the right semantic type to be an answer to the question in the context and so from this short paragraph if we ask where did you know luther spend his career at the university of wittenberg well it turns out if you just ask where let's look through here and see what the possible answers are i mean we actually see two instances of university of wittenberg and there's not anything else which like it kind of obviously could be an answer to a where question right and so the model in this case starts to learn uh again these sort of shallow cues sort of like we were seeing in nli there um and we also talked about the issues here for uh for multi-hop question answering where uh the model can jump directly to the answer based on things like lexical overlap and then a final instance i'm going to describe is this data set called swag which is about common sense reasoning so we have the following statement that we're going to complete on stage a woman takes a seat at the piano she you know does one of these four things and she nervously sets her fingers on the keys is the correct answer so this was a multiple choice data set and the the authors of this uh paper the and and you know who are constructing the annotation protocol said okay well you know maybe people will just put down kind of totally random stuff that's going to be super easy to reject and so what they did was they had a filtering step where they had an elmo based model that said okay well if we can classify it correctly with with this model then we know it's not like a really good alternative and you know our models are going to easily pick up on this and so we want to push our annotators to come up with actually hard examples so they they did this filtering with l mode in order to get hard examples and then immediately after that bert came out and was able to completely solve the data set um get performance matching their uh human performance of around 85 percent or so and the takeaway here is that uh you know we might try to come up with answers that to us seem like reasonable alternatives but a lot of times there are sort of subtle patterns that systems can pick up on that you know enable it to just hack a data set and solve it without actually being able to do the real underlying task so if you ask us like okay does bert have the knowledge of like what's going on here when you take a seat at the piano does it understand why like someone might be setting her fingers on the keys or like what's happening in this situation the answer is basically no but it has enough co-occurrence information that it knows that oh like piano keys that's a really good pair or whatever and so it's able to it's able to figure this out despite you know just despite not having a true deep understanding of the task and so this is an issue as we've gotten more and more sophisticated models is that designing these tasks that can help us push their capabilities without just being somehow trivial for them to solve is a really challenging problem and so one solution is to look at you know hard real world problems and try to actually solve those but otherwise we need to kind of think about this issue of annotation artifacts figure out how to address them and if nothing else use them as a way to understand what our models are learning on these data sets and how they're behaving that's the end of this segment you\", metadata={'source': 'RXYaMZcDIWU'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about explanations that take the form of text or sentences trying to explain a model's decision so the basic question is can we generate a natural language explanation of a model's behavior so this looks very different than some of the other explanation formats we're considering there's some clear advantages one is that unlike a lot of other ways of of kind of showing why a model did what it did like um you know providing highlights of the input or things like that you know text should theoretically be something that anyone can read and understand and another useful property there is that on the flip side someone might be able to explain to a model why a certain decision should be true and so we can annotate text and then you know we should be able to let's say compare a model's explanation with a ground truth human explanation or use human explanations to improve model training so there's a couple of advantages to using text but uh there are also a couple of challenges here the first is that as we've seen throughout this course working with natural language is challenging and so if we want to generate even just a grammatical sentence you know is the kind of thing that lstms and neural networks can typically do but making sure that that's grammatical and semantically meaningful and relevant you know it's hard to check all these boxes and get an explanation that's really really good and another question that we're going to dig into a little bit is is text truly a way to explain what's going on in a model so we're going to look at one of the first examples of folks using visual x folks using text-based explanations and this is actually for a object classification task so this is work from lisa and hendricks at all and the the task here is to classify these images in basically what type of bird they depict this is a very fine-grained classification task and so in order to differentiate between different types of birds the classifier needs to look at very fine-grained features of an image so what we see here for each of these two images of a lace and albatross are three pieces of text the first is a description of the image so this is just a basically what would be produced by an image captioning model which you know is trained on a bunch of images and captions to describe what's in the image the second is a class definition so this is the kind of properties of a lace and albatross and the third is what they call a visual explanation so this is going to describe why we think that this image depicts a lace in albatross and so the kind of key thing to note here is that for example in this top image the model the well the visual explanation denotes that this uh bird has a large wingspan um and in the bottom image the explanation says that it has a black back now notice that you would not talk about the large wingspan in the bottom image because this actually isn't something that's obvious from the image itself so the visual explanation should have two properties it should be both relevant to the class basically it should explain why that label was predicted but it should also invoke features of the actual image and so this is this is i think generally a property of these sorts of text-based explanations they're supposed to explain something about the model's reasoning which has to be relevant to that particular instance but also explain why that class label was picked as opposed to a different one now the kind of elephant in the room here is the model can say or this explanation can invoke certain properties are those features actually what the model used so what we're looking at here is just kind of ground truth explanations given by a user let's look at what an actual model for this task looks like uh so this is a this is the model that they used in their work and uh it's got two main components uh the first one on the left is the object classification model so we're not going to talk about the properties of the uh the the kind of image neural network but the important thing here is that everything gets cooked down into this feature vector here uh which is then used to predict the label so these are the features that are basically at the last layer the neural net which are going into the classification and those features also get combined with the label that's going on here and then fed into this lstm decoder in order to produce the explanation so the there is a path here for or or there is a kind of if you blur your eyes a pretty reasonable story here for okay the model made some you know abstract kind of representation of the image in the feature vector here and now it's feeding that into this decoder and it's going to read off properties of that explanation into text the issue here is that you know while this sounds good it may not actually truly reflect what's going on in the model's decision making and the reason is the following suppose we're going to take a look at this feature vector suppose this feature vector is some very long vector and it has two parts the first part is let's call it image features and the second part is something about the label now basically this first stage neural net has done two things it's remembered a whole bunch of stuff about the image and it's also computed a label and these could come from completely different things for example we might know that this is a cardinal because there's a green background in the image because somehow all the cardinal pictures we have in our data set have a green background but the the model's been trained to produce these explanations as well so it's going to also remember these features like you know it's a red bird it's got a you know a small beak etc and those get stored in the image features here and then can get read out in this decoder so the the kind of key property of these explanations is that they're likely to produce plausible things that look reasonable to humans you know the model is essentially trying to reproduce a human explanation here and a lot of times it will do a very good job of it i mean at the very least it will probably describe true properties of the image most of the time however those explanations don't necessarily reflect why the model made its decision again you can imagine a very successful model that classifies the bird based on the background but also manages to pick up all the other properties and feed them into this decoder okay so let's turn our attention to text so a lot of the work in the text domain using textual explanations looks kind of relatively similar so this is returning to the natural language inference task and we have some explanations here of why these particular labels were chosen so for example the top here an adult dressed in black holds a stick and an adult is walking away empty-handed and the explanation here is holds a stick implies using hands so it is not empty-handed so this is great it it highlights you know the key issue here which makes this a contradiction um similarly below uh we have a child in a yellow plastic safety swing is laughing at laughing as a dark-haired woman in pink and coral pants stands behind her a young mother is playing with her daughter in a swing and so the person here picks up on the fact that child does not imply daughter and woman does not imply mother you know again these are explanations annotated by humans this is not what a model is producing for us yet so these these definitely reflect something about the underlying reasoning and it would be great if we could get a model to follow this sort of reasoning and turn in making its decision so what do what are the models for this look like so this is still a relatively new task but uh one of the you know models that's used looks basically like what we saw for the birds you know the bird classification task so we compute this feature vector f and we compute a class label from that and then we also compute an explanation conditioned on the label and f so the issue is that the again the f vector might contain stuff that's useful for the explanation and stuff that's useful for the label and those might be two different things so the explanation might be generated based on the label and on stuff from f i mean it has to look at f to know what the premise and hypothesis are but um you know it might basically be just kind of remembering raw features of the input and then using the label and again not reflect the reasoning going on in the model so again there's all kinds of useful cues that can arise here but they don't but but this isn't necessarily a most a faithful explanation of what the model is doing okay so the last example i'm going to show is a model that does a little bit of a better job of maybe making explanations that are actually used by the model so this is for a common sense question answering task and so we have a question like a good interview after applying for a job may cause you to feel what and this gets fed into a generator which is going to generate basically hypothesized explanations for an answer or kind of additional information that we might want to know and then the actual answers here this is a multiple choice task and so the actual answers are hope hostile rejection income or offer and some of these don't don't make a whole lot of sense but the right answer should be hope so the by kind of generating this explanation first and then conditioning our classifier on it we're not using we're not using the label you know in this kind of post-hoc fashion and moreover the actual information that gets picked up here is likely the kind of thing that is going to influence the classifier's decision so we've kind of flipped things around a little bit here and so the mic model is likely to be a little bit more faithful although you'll note that it could still just completely ignore these hypotheses these hypothesized words and predict something based purely on the question and the answer candidates the the big downside of this work uh due to lacinic and barren is that the hypothesis isn't constrained to be natural language so it ends up looking like a bunch of key words and not really what we would consider an explanation uh and so this this highlights one of the challenges in this work which in in this general line of text place explanations which is if you want the model to generate nice natural language explanations you probably need to train on them but then once you train on them the model is going to learn to produce those possibly independent of whatever classification procedure it's actually using so this gives you a little tour of some of the ideas that folks are exploring in the world of textual explanations it's a very active research area obviously with the rise of pre-trained models having models that can talk about kind of what their you know talk about their reasoning explain their decision-making things like that is very appealing but it's a little bit harder to kind of put together than some of the other techniques we're looking at that's the end of this segment you\", metadata={'source': 'bXHM5t_ejsc'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about Chain of Thought prompting which is a way of revisiting the ideas of zero shot and few shot prompting now armed with our kind of understanding of explanations so we've been thinking a lot about explanations as ways of interpreting what models can do but CH Chain of Thought is going to kind of flip this around and instead it's going to use explanations as a vehicle for actually improving model performance particularly when you're doing task involving some kind of reasoning and what this is going to allow it the models to do when you have large language models is they're going to be a to kind of work through computation over multiple time steps of inference rather than just having to provide a result right away so the ideas behind this predate uh large language models pretty significantly um the first version of this that was using neural models uh dates back to 2017 from Wong lingad all where they looked at these math question answering data sets where humans had written rationals of how you actually solve the problems and what their model did was rather than trying to just predict the answer directly they actually predicted the tokens of the rationale in order to work through and get the answer now with the lstm style models at the time this didn't work very well and didn't lead to great performance on this data set but the kind of idea was there there subsequently there was a lot of interest in question answering models or things like that that would use kind of programmatic like methods to assemble and put together different pieces of reasoning so uh this is a data set called strategy QA where the idea was that you would need strategies to figure out the answer to a question like did Aristotle use a laptop well answering that yes or no is tough because it's very out of distribution from what you've seen instead you kind of need to figure out okay Aristotle lived way before laptops were invented and that's why this is impossible right so Chain of Thought tries to unify a bunch of different ideas here and does a bunch of computation in order to provide an answer entirely in natural Lang language so for mathematical reasoning it's going to you know enable large language models to work through steps individually rather than have to just magically crank out some answer which they're not very good at uh for question answering it captures a lot of Prior work on question decompositions that look at decomposing the structure of questions and figuring out how to answer each of those in isolation and build up an answer uh and it can also capture the sorts of things things that are written in rationals for various other tasks so the way it looks is you normally have a f shot setting where you're doing Chain of Thought and what you do is you provide examples of your question and then rather than providing the answer directly remember at the top here with the input uh we're providing training data to the model rather than providing the answer directly instead you provide this string and purple here and the string in purple is showing how you got the answer and now when you actually come to the example that you're going to do your your test example now the model is not going to just try to generate an answer immediately it's going to follow the pattern that you've shown it and it's going to try to generate this reasoning and ideally by doing so it does a better job of getting the actual right answer so uh I'm going to show an example that uh We've looked at in some of our work which has a kind of synthetic test bed that kind of illustrates some of the concepts so we have all these sentences which just establish kind of relationships between people and then uh what role each person has and you can ask a question who hangs out with a student and the answer is Mary because Mary hangs out with Danielle and Danielle is a student so if we think about standard fuse shot prompting we just ask the model for the answer you can also ask the model for an answer and then generate an explanation which is something that we've seen in in kind of other models like ESN or you can ask the model to generate an explanation and then predict the answer and that's the Chain of Thought idea and so whichever method you pick you feed the model a bunch of examples like this and then give it the test input and then ask it to generate and it'll generate in this case your uh answer plus your explanation so on the synthetic test uh I think we can see sort of the progression of chain thought over a number of different models so uh opt 175 billion here in Da Vinci are just raw language models and what we see here is that comparing the black bar which is not using Chain of Thought at all to these blue bars Chain of Thought actually doesn't help it really only kind of kicks in and starts getting better when you get to these very strong models like Tex Da Vinci 002 which again is kind of a close predecessor to chat gbt so the rhf and instruct tuning seem to be improving model's ability to use various explanations uh and this technique really kind of works but only when you are using it on the biggest and best models there's also been a lot of uh analysis of this for various kinds of arithmetic or mathematical reasoning data sets um and this is from a paper by coim all that introduced a zero shot prompt for doing Chain of Thought which is to just tell the model let's think step by by step right before you generate the answer and so you can see here that this model is actually able to uh do pretty well at certain kinds of arithmetic reasoning if you look at uh this line here this is basically using their special prompt they can get around 80% on a data set that previously if you just did standard zero shot or F shot prompting you would get less than 50% on um but it still kind of works less well than using uh sort of real few shot instances so uh Chain of Thought still seems to be the best when you uh do F when you kind of combine it with few shot demonstrations of the kind of reasoning that you want to be able to do that's the end of this segment\", metadata={'source': 'tNGu3EqJbKc'}),\n",
       " Document(page_content=\"foreign we're going to talk about some newer ideas for understanding how to extend Chain of Thought prompting and how to analyze and understand maybe why it's working well so we kind of went through this with a few shot in context learning thinking about all those properties of like example ordering and stuff like that so we can ask some of the same questions about Chain of Thought and then also think about okay given what we know about what it's doing how do we make it even better so one of the things that we can ask is basically are the language models actually following the explanations that are given how important are the explanations and the prompts remember we talked about that work that for in context learning that showed that you could just mislabel things and stuff would still work fine so is that the case here so we can look at two types of perturbations if you perturb the stuff in green here you're just changing the language expression of what's going on in the example so for example in this case we're just trying to concatenate the last letters of these two words so a very sort of trivial operation but it's still not that easy for a language model to do and we can change up how we describe this we can also change up the actual computation Trace here where we can introduce errors in uh you know let's say the actual extraction of that letter that happens that's what's shown in red so if we look at these perturbations somewhat encouragingly we see that either of these things causes performance to drop by a lot so compared to the purple bar here the accuracy when you perturb either the computation or the natural language drops by quite a bit across three different tasks this the first one is this letter concatenation the second one is this task about flipping a coin and tracking what state it's in and the last one is this kind of math reasoning and you see that one is kind of very negatively impacted by um having kind of bad computation traces or bad natural language expressions of the reasoning now one of the things that is kind of you might think is well okay it's really necessary to show the model the right kind of computation that it's doing but particularly this last example of this letter concatenation thing seemed really trivial right like we should just do it that with python and not with large language models so there's been a lot of work to try to unify the capabilities of things like programmatic interpreters that can actually execute code with large language models and in particular all this focus on mathematical reasoning has led people to look at the fact that sometimes chat CPT just makes arithmetic errors and wouldn't it be great if it could not do that so there have been a number of approaches uh program aided language models program of thought faithful Chain of Thought tool former a number of uh methods for generating code fragments and then executing those code fragments and basically having the Chain of Thought reasoning not entirely happen in natural language but happen in a mix of natural language and code and it turns out that indeed this does lead to better performance for all this math reasoning stuff if you basically predict these code Snippets and formulas and then have some way of executing them there's another method uh for generalizing that same sort of idea to other tasks which is called self-ask uh where you take uh a complex question that you're going to answer and you break it down into smaller questions and ask them and then equip the model with the ability to do things like retrieve information so this is a lot like what Bing chat and Google bard are doing which is they're able to take your query execute some searches based on it take that information and somehow integrate it into the actual response that they give you so this is a kind of other really powerful tool that we can use because it enables the models to go beyond just what they sort of know in their parameters so there's a lot of interest and excitement about the capabilities of this uh you know we've seen two tools that can be integrated with language models which are programmatic execution and search uh there's a broader set of chat GPT plugins that enable you to use other kinds of apis and uh when you have future models they're likely going to be even more tightly integrated with all these capabilities and probably fine-tuned to use this stuff to a higher degree we haven't quite seen that yet though um another kind of element here is that when you have these chains of thought and you're doing this kind of reasoning you want to check all of the steps of reasoning and make sure that they're all correct and so there's a lot of interest as well in verifying the factuality of all of these things and uh one way of doing that is to just ask the llm itself to sort of check its own work so uh kind of combining all of these ideas from Chain of Thought with these existing tools and with llms themselves is going to enable these to kind of go even further and do even more exciting things that's the end of this segment\", metadata={'source': '9sFyzMywKmo'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to introduce the idea of what's called reading comprehension as an nlp task so reading comprehension is broadly a type of question answering but we're going to distinguish it a little bit from the question answering that we saw before so what i'm going to call what we were doing before was knowledge based question answering and this for this task we had this pipeline of taking a question mapping that into a formal representation and putting that into a knowledge base to give us an answer so this captures everything we saw of the form you know what states border texas etc that kind of geoquery data set or things like you know when was brie larson born we saw that we could map this into a lambda calculus expression that you know roughly looks something like this um it's you know it basically is a kind of query which is looking for a birthday associated with brie larson in our knowledge base so this is one way of doing question answering which is great for these sorts of factoid questions like if we want to know facts about famous people or whatever uh that you know this is probably the right way to do it have it written down um come up you know figure out how to map into a query and then return the answer so uh reading comprehension is gonna be a bit of a different a bit of a different target for this that's going to be optimized for answering a different type of question so if you have a question like what temperature should i cook chicken too this you know this is the kind of question where okay maybe we have a knowledge base that has a bunch of foods and you know food safe temperatures or whatever but it's actually not obvious that there's one right answer for this i mean you typically want to cook white meat and dark meat to different temperatures and you know do you want the answer in fahrenheit or celsius or things like there's there's kind of a bunch of uh sort of extra stuff that maybe we would need to give and it's not obvious that this is going to be written down in a knowledge base that you have unless you have some very specialized cooking question answering system or if you ask an open-ended question like why did world war ii start i mean again you know you might have conceivably a database with a whole bunch of like events and things like wars um but do you have a like why like and you could again imagine having a cause field or something like that but generally in order to answer questions like this you know now for each new question we need to roll out a huge domain specific knowledge base and if you're google you have a limited ability to do this but you still want to be able to answer any question that someone typed into the google search bar right so um i'm going to reframe this question as what event led to the start of actually we're going to talk about world war one rather than world war ii um and the way that we would probably do this as a human would be to go to google right and you know you would google like causes of world war one right and then you get some you know there's probably blog posts that talk about this exact thing or maybe you get the page from wikipedia with a snippet or whatever and maybe you would find uh some text on the web that says something like you know uh like the assassination of franz ferdinand took place on blah blah blah and then later they you know you know it says like this event like was a direct cause of world war one okay so cool we found the answer by essentially googling and finding a document that talks about it and then uh we see that this is basically the answer that we want right okay so this is essentially what the reading comprehension idea is it's a question answering pipeline where we imagine having a set of documents which we can think of as the whole web a question and as a first step you have some kind of information retrieval and this is going to give us a set of filtered documents which we are going to pass into a qa model which is going to give us an answer and so we are going to think about this part of the pipeline today and we're going to follow a framework called span extraction which is basically find a span or a contiguous chunk of words of the doc of some let's say some doc which answers the question all right so this is pretty cool in that we can think about essentially treating the web as our knowledge base now not having to have written everything down into some very rigidly structured database format and we can also think about extracting answers that are much more flexible now we can return people a very short snippet like a you know a single let's say noun phrase the assassination of franz ferdinand um we can return a larger piece of context um you know and we can and we can think about this as just kind of highlighting part of the part of what we get as the as the answer so we'll come back to this first part of the pipeline uh for how to take documents and a question and and uh do this information retrieval step um i guess let me show the question going into the qa model as well uh and but mostly we're going to focus on the second half of the pipeline like if i give you a paragraph or a few paragraphs and a question how do you go about extracting a span from it so that's basically the setting that we're going to be operating in and we're going to talk about some different paradigms for doing this we're going to start off with some bass lines and then we're going to build up one neural model which represents most of the ideas that have been presented in this space although there's a huge space of models that people have used for this task and of course spoiler alert we're going to see that bert is able to do this very effectively and you know is kind of most of the dominant systems now use burke but it's still instructive to understand this task more to see the pieces that we're going to go through in order to get there that's the end of the segment you\", metadata={'source': 'gnUSE0fCbso'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the general we're going to go a little deeper into the setup for reading comprehension and talk a little bit about baselines that we might imagine which is going to help us think about how to design the neural net models that we're going to be using for this task so we've got here a passage with two sentences in it uh this is something we call the passage or the context it's sort of what we're answering questions from the assassination of franz ferdinand led to world war one gavrila princip killed him on june 28 1914 and the question is what event led to world war one all right so we have a few different baselines that we might think of in terms of how to answer this question and these can draw on various ideas that we've seen so far throughout the course so one idea is to say all right um we've got basically a kind of x led to world war one sort of pattern that we're looking for and what we want to do is then we take various windows of the context like you know maybe we look here and here and here in one sentence and then you know here here here here and we intersect the set of n grams from the question with the set of engrams from the passage window right so essentially what we're saying is we need to we need to find something related to led to world war one and maybe uh if we're lucky enough to grab this up here as our window then what we'll see is oh okay great we have this nice overlap between x led to world war one and the assassination of franz ferdinand led to world war one and so assassination of franz ferdinand that seems likely to be the answer okay so we'll call this the sliding window method and we can also think about doing something similar using syntactic parsing so if we parse the question i'm going to write this in a sort of reduced dependency representation here we can get something that looks like this where we're looking for something that led to world war one right and ideally what we should be able to see is that in the passage we also get led to world war one and then you know we get the rest of the kind of dependency subtree over here and by comparing these two things we recognize okay we can sort of bind x to this sub tree over here and therefore get the answer all right so this looks like a slightly smarter way of doing things than than the sliding window method and we'll call this the parsing-based method all right so one issue is uh what if we you know instead have instead of saying lead to we say caused world war one then suddenly the engram sliding window doesn't look as effective and the parsing technique again we didn't actually talk about how to do this comparison or matching but uh again we're relying on word match here and we generally need to do something smarter than that so uh we this is this is where neural nets come in and we are also going to try to make our framework a little bit more general than tying it to sliding windows or syntactic constituents so this gets us into the span extraction paradigm that we mentioned earlier so yeah i'll reproduce our sentence and we said that this was our answer we wanted to pick this out as uh the thing to show to the user and so if we think back to ner we can use a bio tag set like we did there recall that what's going on here is that we're saying okay we're going to tag each word with a tag that uh where o shows says you're not in the answer at all right b answer says that you begin the answer and i answer says i am continuing a span that is the answer that started with with b answer so this is the span start and this is the span end now one difference from ner is that we only want one span here right in identity recognition it was generally fine to have multiple named entities in a single uh in a single document that's okay but here we want to pick the best one so our model is generally going to be a function of a document and a question and is going to return a start and index pair and the models that we're going to look at are going to place distributions that look like this we are going to get a pair of distributions that i are basically have the range let's say 0 to n where these are the indices in the document that we're considering and essentially it kind of represents where the answer is likely to start and so maybe there's a couple of different possible answers and so the distribution looks like this for start and for end maybe the end model knows that the end has to be here and everything else is very low and so what we do is we find start and pair with highest probability that aren't too far apart so uh what we wouldn't want to do is maybe let maybe let's say way down further in the document you also had a fairly likely end position um you would not want to take this start and this end point and then just like return a you know massive paragraph or like a whole page as the answer um that's not the right thing to do instead what we're going to say is okay these two are reasonably high probability and maybe show up you know within five tokens of each other or something like that so that's a reasonable answer to return to the user um so even though we're modeling these distributions semi-independently and this is something that's not really correct right we should be placing distributions over like all spans in the document um but because there's a quadratic number of spans that gets to be intractable so we approximate that essentially as a you know basically two distributions each over indices and then we have to sort of ram those together at the end to actually get the final answer so it's an approximation but it's a standard one that people make when doing this type of question answering and so what we're going to talk about next is how to take basically how to build a neural net that goes from a document and a question and maps to these distributions over start and end positions that's the end of the segment you\", metadata={'source': 'JRI3RwRBnMY'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how bert can be applied to question answering so this won't take very long because the solution is relatively straightforward so what we've got here is a question and a corresponding passage that are formatted for input to bert and so the changes that we've made are we've appended this cls token which we're actually not going to use but it is still expected by bert as part of the input and so we put it then we have the question then we have this separator token so for all the sentence pair tasks that we looked at for bert and for the next sentence prediction module they always separate it with a separator and so uh you know that'll be part of the input here and then we have uh the passage which again can be quite long so uh then what we do is we just feed this into bert and what are we trying to predict so again as a span extraction task we are trying to predict a start and end distribution and so what we're going to do is we're going to take the output bert vectors at each position in the passage and we're going to put these through two small feed-forward networks or just a matrix multiply essentially to compute a score and so we're going to have s1 and e1 for start and end scores s2 e2 etc and then we are going to soft max over the si to give the probability of start given p and q and same thing for e i to get the probability of end all right so this is relatively straightforward here we just kind of staple these things in we use it as a way of producing a passage in coding and we don't have to worry about any of the sort of cross-attention ideas or these bi-linear products to capture the interaction between q and p because now what we assume is that within bert it's sort of looking at these correspondences internally and so by time we get all the way up here with a word it should have already looked back at everything in the question and bert has hopefully decided whether or not this is the answer or not so then we can just decode that information right out of this vector on top so this is great it simplifies the qa architectures really dramatically there is a big caveat which is the 512 word piece length limit so remember that bert had these additional positional embeddings which basically looked like vectors for each integer in the input and so uh what happens is bert is pre-trained on uh passages of up to size 512. and remember that it uses a wordpiece segmentation and not actual words so you actually don't even get a whole 512 words in there instead you get something less and in general like you you might think oh okay i want to answer this question from this wikipedia article well that wikipedia article may very well be more than 500 words and so once you put the cls questions separator and the article you know you've you've blown your 512 limit so the the kind of takeaway here is that we need to really operate within a kind of pipeline here we need to identify small context as input to bird um and there's there are plenty of methods that will do things like you know they'll have several different 512 chunks they'll run bert over each of those and then kind of pool the results so there are ways there are ways around this if you want to run on more data um but generally what's needed is some sort of course defined pipeline where you use some kind of other model to determine okay what are the what what what is the small passage that i'm going to run my bert model on focus in on that and then use the burp model to get the answer so bert is very convenient and allows us to do qa problems very nicely um so that's good but there it's not without its shortcomings um and it presents some logistical challenges which people in qa have spent a long time well okay a long time over the past two years a lot of hours have been spent figuring out how to try to fix this problem so yeah that basically gives us a kind of picture of how we might use bert here and what it can do for us that's the end of this segment you\", metadata={'source': 'F8hWZ4xaVkA'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about some problems in question answering so we've presented so far a an approach to this reading comprehension span extraction question answering paradigm using either this attentive reader model or bert and these approaches seem very nice so what goes wrong here so one of the common data sets for this is the stanford question answering data set which basically looks have has sort of simple factoid questions like you know what was marie curie the recipient of uh she was the recipient of the nobel prize and so you know calling back to what we talked about when we talked about bass lines we said that sliding word and gram windows wouldn't necessarily sliding sliding a sliding window and then looking at engram match between the question and passage this one we said that this wouldn't necessarily work and it doesn't work but this is sort of just one step up from that where you have to deal with a little bit of paraphrasing um where you know you instead of saying the recipient of you say she was the winner of or whatever but the but bert and other uh neural models can do this pretty easily so we this data set is just not all that hard and bert approaches get over 90 accuracy on it and the performance on this is actually now quote unquote superhuman where uh at least if humans aren't aren't really all that careful about answering these questions uh they will do less well than a bert model does this does this mean that computers are better at question answering than humans are no absolutely not it means that on this narrow data set we've we've kind of maxed out what models are capable of so here's some here's a way to trick these models this is work by john leung from stanford where they made a new set of sentences to add to the contexts that they called distractors so if the question was what city did tesla move to in 1807 they formed a very similar sentence tadakatsu moved to the city of chicago in 1881 but they basically swapped in named entities for just like random essentially garbage and so this is unrelated to the passage but they stick it on to the end and then they ask does the model pick this out or does it pick out the original answer and so here's another here's another example from a paragraph about super bowl 50. so the question at the bottom here is what is the name of the quarterback who was 38 in super bowl 33 and the answer which i've drawn this red box around is uh the past record was held by john elway who led the broncos to victory in super bowl 33 at age 38 and is currently blah blah all right so that that's that's the right answer but it doesn't really look exactly like the question and so when you add this sentence in blue at the end quarterback jeff dean had jersey number 37 and champ bowl 34. even though these entities are like totally unrelated the sentence structure matches very well and what this means is that models that are doing very well at the original squad tasks so that's what's shown over here on the left is performance on the raw task and you know there's a whole bunch of models you can see just how many people tried different approaches here and then on the right this add one sent approach is this adversarial data set where you add a sentence at the end that might trick models and you can see that they go from eighty percent accuracy to fifty percent accuracy these models typically cannot figure out whether the original answer is good or whether the new answer is good and so there's this huge drop in accuracy um and what i should say is that this is not adversarial models in the way that's often used in computer vision where it's like you change two pixels in a very tricky way um to trick the model um this is like adding a sentence that just is is kind of universal right um so it's adding something that a human could very easily discard and which is not even picked like kind of customized for any individual model but this tricks the existing models all right now you can train on these additional sentences and learn to reject them right but that's somehow not the point the point is that we have not learned to solve qa in general we've learned to solve this particular data set and then if you add this one sentence and then you train on that you're again learning to solve this particular data set all right another sort of problem with this was presented in this paper called universal adversarial triggers by wallace at all where what they did is they found out that if you inserted the sentence why how because to kill american people at the end of a bunch of passages you could basically get the model to return to kill american people as the answer between 10 and 50 of the time when the model was presented with a why question um and so this this the the reason this works is because they they sort of engineered this string why how because in this gradient-based way that they they they did a search over a whole bunch of strings and found that this is the string that the model really likes to kind of pick up on right um but again it's like by adding this irrelevant stuff you get the model to pick out something that it really shouldn't be returning and their argument here was that this is actually sort of dangerous in deployed systems because let's say someone edits wikipedia to add this in there and now google is talking about killing american people when you uh type a question into google search obviously this has huge potential for abuse and big problems so this establishes that these models are not actually as robust as we would hope so what can we actually do to make things better so one approach is to actually say all right let's solve real qa problems and so this kind of calls back to that idea of answering questions based on all of wikipedia so like we have a whole bunch of documents and we need to do this initial information retrieval step before we do our reading comprehension span extraction piece the thing about this setting is that it gives us contexts which are much trickier and we'll see an example of that in a future segment the other thing that people have looked at is harder qa problems so this squad problem again you know is is extracting relatively straightforward answers from questions which match very closely to a passage and so people have thought about all right what about questions that are harder or so-called multi-hop questions that require more complex reasoning and so in in the future segments we're going to talk about both of these sort of ways of generalizing qa making it work better making it able to handle more types of data and work in more realistic settings that's the end of this segment you\", metadata={'source': 'tCvAHmrxPvY'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the problem of open domain qa or so-called retrieval-based approaches to qa so the type of qa that we've been talking about we've established is very artificial and is vulnerable to all sorts of attacks when you introduce more complicated data and it's not what we want right like if you're google and you're trying to answer questions that people are typing into the google search bar you're not answering just based on a paragraph you're trying to answer based on the whole web and so if someone asks you what was marie curie the recipient of you might have to deal with more complex contexts that don't match this question as easily so for example marie curie was awarded the nobel prize in chemistry in the nobel prize in physics and then you'll also have a lot of other distractors which are tricky if you're looking for kind of other things that people are awarded on the web you'll get mother teresa receive the nobel peace prize curie received his doctorate this is about pierre curie right so this is also something that you would probably retrieve from wikipedia especially since pierre curie and marie curie are going to be mentioned in each other's wikipedia articles sklodowska received accolades for her early work that's her maiden name before she took the name curie and so uh again you know we've got much much trickier context to deal with if we're just grabbing these paragraphs from all over the web um so how do these so how do these systems work what we do is we have an information retrieval system that's gonna retrieve some documents and then we're going to apply our reading comprehension qa model to actually try to answer the question so in the drqa paper which we've talked about basically for its reading comprehension extraction part they often they also address this other part of the question answering over wikipedia uh and so they use a document retriever that basically uses a kind of uh essentially tfidf standard information retrieval techniques and what they find is that they're able to get the answer in the context for various types of questions you know roughly 70 to 80 of the time and now notice we have squad here this is a variant of squad called open squad where we're answering over all of wikipedia okay so you're able to at least get the answer in there 75 percent of the time but the uh data actually the the task is actually much harder and so the uh the numbers are a little bit cropped here but you can see in the individual paper all of these values are are kind of between 20 and 40 percent accuracy so uh the information is uh the information kind of isn't there or is easily accessible or there are much more confusing distractors in the context and so uh suddenly we're not at 90 human performance we're much much lower all right so there's been work since that since that earlier stuff in particular had to do this with bert and so there's some very nice work out of google um by lee at all where they looked at using bert to actually encode every single paragraph on wikipedia and then also encode a question with bert and use that to retrieve the relevant paragraphs so they're not doing read the reading comprehension step that's not what they're doing here instead they're using bert for retrieval so this is very powerful besides big contributions in nlp it's also revolutionized the field of ir but it is also very slow you have to re pre-compute bert over your whole uh over basically all the web or whatever you want to index uh that's very slow and then also if you want to train this thing to learn parameters that are going to let you do this matching better you face a very difficult training problem so there's been a lot of work uh from both google and facebook kind of handling this stuff but it's at a much larger scale than is possible for most academic researchers to work at all right and the last uh sort of type of question which i'll call out here uh is a departure from the idea of these data sets like squad which are very much focused on an individual passage um so many of the squad questions actually can't even be answered with all of wikipedia because it'll be like where did the super bowl take place and it's like okay well in the passage it was just talking about super bowl 50. now what super bowl there's like there's you know over wikipedia we don't know so google released this nice data set called natural questions which are real questions typed in by users that are answerable with wikipedia and they have both short answers which are just basically span extracts like what we saw before and longer answers which are uh are bigger snippets of a passage maybe one or two or three sentences that contain the answer so this question answering is much messier in this case there's not a short span answer you have to give the longer answer because the answer is long and complicated but this is actually getting at the kind of original goals that we had which is being able to answer these questions like what temperature do i cook chicken to which might have a longer and more complex answer so the f1 scores on these data sets are much lower than on squad like i said on squad it's over 90 um and here it's you know 60 to 75. human performance is also lower um but this is a this is a kind of nice setting for question answering models that we know corresponds to a real problem because these are real questions that users typed in uh so this is a data set that's attracting a lot of attention over the last year or so and represents one of the frontiers of qa in terms of this retrieval based or open domain qa approach that's it for this segment you\", metadata={'source': 'P-j_zeS0Pa8'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about a more challenging variant of question answering called multi-hop question answering so the data sets that we've been looking at so far in particular the squad data set don't really require combining multiple pieces of information if we ask you know when did marie curie win the nobel prize we're basically looking for a statement that you know she won the nobel prize in this year and we don't really need to take pieces of information from different parts of a paragraph or different documents i mean you might but it's only in a small percentage of examples so folks went and built a few different data sets to try to explicitly see if models could do this kind of multi-hop reasoning where we want to be able to take information and combine it during the question answering process and so uh one thing that might look like is this uh so this is from a data set called wikihop and the question here at the bottom is not really formatted in natural language it's more asking for a relation so it's saying the hanging gardens of mumbai are in what country and the annotators were explicitly told to basically follow hyperlinks on wikipedia in order to create the answer you know the kind of question and answer pairs so uh you know the idea here is that you have to follow the link between the hanging gardens and mumbai and then recognize that mumbai is in india another example of this is a data set called hotpot qa and the questions here are in natural language and are quite a bit more sophisticated let's say than what we've seen before so we have the question what government position was held by the woman who played corliss archer in the film kiss and tell so this question unlike when did marie kerry win the nobel prize all kind of already has some structure that indicates that we need to do multiple steps of reasoning so if we have a bunch of documents uh you know we first need to figure out who portrayed corliss archer in the film kiss and tell and so we would ideally kind of jump from the question into this part of the document and recognize that the answer to this let's say sub-question is shirley temple and then we say okay well now we need information about what government position shirley temple held so we're going to go to the article on shirley temple and see that she served as chief of protocol of the united states okay so that's one example uh here's another one the obroy family is part of a hotel company that has a head office in what city so again we need to take obroy family uh kind of figure out what the hotel company is which is the oberoi group and then the obroy group is a hotel company with a head office in delhi so this is sort of interesting right like this is a much more complex style of question and it would be great if our models could actually do this task uh and the question is all right so let's say we train a model on a data set consisting of these types of questions is the model actually doing the kind of reasoning that we're depicting here and the answer is only somewhat so when we look at this example about the obroy family is part of a hotel company that has a head office in what city it turns out you actually don't really need this intermediate link here because there's such high lexical overlap between the question and this sentence containing the answer and so models actually are very good at just jumping directly to the answer particularly when they're trained with bert so you might think okay well the other example we saw that one seemed more complicated right um and so remember that we needed to kind of find out who played corliss archer and then uh you know figure out what government position shirley temple held so the you know there isn't quite the same sort of lexical overlap here but there is still a cue that models can use and that's the fact that we're asking about what government position was held by this woman and when we have even let's say we're doing answering this question over 10 retrieved documents or something like that there are actually going to be very few government positions that occur in the context and so the model can do a very good job at answering these questions just by learning a kind of entity type heuristic like what type is you know what type should the answer be and then let me return something of that type and what that means is that the model essentially takes a shortcut during its learning it doesn't actually follow all of these steps but instead is able to learn a much simpler heuristic which fits the data and that means when we come to new data that looks like this the model is not necessarily going to do very well so in terms of quantifying how bad this is some work from my lab and some others from uw [Music] have looked at basically how well we can do under a variety of uh you know kind of reduced settings that highlight the issues in this data set and so we looked at the wikihop data set and it turns out you can get around 60 accuracy on this data set which is a multiple choice data set without using the context at all so the questions and these relations are informative enough that we can already distinguish the right answer in quite a number of cases and when the state of the art is only 67 or something and and uh not using context gives 60 it's hard to imagine that these state-of-the-art models are really doing a lot of sophisticated reasoning when they're only getting an extra seven percent another test we could do is say all right let's have the model only consider each sentence in isolation and on this wikihop data set it turns out that the model is able to get around 50 of answers correct or 50 f token f1 um overlap with the correct answer so that again you know if a model is only considering each sentence in isolation it cannot be doing this kind of multi-hop reasoning that these data sets were designed to test so while this is a interesting capability for qa systems to have the current data sets fall a little bit short of actually you know requiring it very strongly and so this is a concern and it means that this is not necessarily the easiest path forward in terms of trying to come up with harder qa settings or harden our data sets regardless there have been some recent models which are very successful at this so i'm not going to talk much about the architecture of these but i'm just going to show one from akari asyadal from uw where they looked at basically taking the structure of wikipedia and building that into the model and being able to do retrieval using hyperlinks and so they are essentially treating the documents that you're answering these questions from as a graph and they build a pretty strong sequential retrieval model that's able to find the right context and then answer the question and so all these can piece all these pieces use neural models like we've seen throughout the course uh we have a kind of recurrent model here for the extraction and then there it's all kind of built on a bert framework so uh really the ideas here draw on what we've already seen from other reading comprehension and question answering models and you know but but there's obviously a lot of sophistication in terms of putting these together and getting it to work well on this task so this gives you a tour of multi-hop question answering and uh what i'll say is that there are also several other settings like it where people want to focus on answering questions about new numerical reasoning and things like that and the general problem with these is that it becomes very hard to annotate natural data that doesn't have these sorts of flaws we'll come back and talk about that in a few segments when we talk about annotation artifacts but for now that gives you a sense of what's going on in multi-hop question answering and that's the end of this segment you\", metadata={'source': 'jpRwa2iE_z8'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to start talking about dialog systems and we're going to start by focusing on chat bots and particularly historical ideas behind chat bots and kind of where the field comes from in this regard so the chat the idea of chat bots really goes back quite a long way to the idea of the turing test so the turing test was based on this notion of what's called an imitation game and this was a party game which uh i've never been to a party where anyone played this game but i'll believe that it has been played at some point uh where basically you have uh two people that are locked in a room and are trying to answer the questions of a third person um these two people by the way are locked in separate rooms and they're both trying to imitate one of them so basically you have someone outside the room who is slipping them pieces of paper under the door with questions and then they type out a response presumably on a typewriter or something and hand it back and they're both trying to imitate b and then c is trying to decide which of them is really b so the original idea behind turing's formulation of the turing test was to have a system that could play this imitation game so basically we would remove a from the equation and replace a with a computer and the question is can the computer pretend to be the specific person b when a trained judge who is one of b's friends and probably knows what b would say in a lot of cases is asking them both questions both b and this system so this is the so-called original interpretation of the turing test what you're probably more familiar with and what's been talked about a little bit more is the idea that a computer tricks some observer into thinking that they're human but the important thing is that this observer is supposed to be a trained judge this observer is not just supposed to be a random person on the internet and so a lot of the systems that have claimed to pass the turing test um you know often do so by kind of preying on the sort of ignorance of the people who are interacting with these systems or you know they they don't necessarily present themselves in in in such a way that you would easily be able to refute the the you know automated nature of the system for example one system that was publicized a bit the researchers claimed it was it had the persona of an 11 year old ukrainian boy who was speaking english and so this system would make grammatical errors but that's okay because so would an 11 year old ukrainian boy who had you know is only just learning english so that's that's kind of not the original idea of the turing test and so many of the things that claim to solve the turing test you know don't even really solve the standard interpretation and most certainly don't solve the original interpretation um you know about impersonating a specific person regardless this has spawned a lot of interesting chat bot systems for uh you know interacting with people and having various kinds of dialogues so one of the earlier ones was a system called eliza from mit weisenbaum in in 1966 and the idea here was there were several different types of scripted interactions you could have and one of the scripts was called the doctor script and so this doctor script would ask these questions where if you said you were like my father in some ways it would ask what resemblance do you see and then you would say you're not very aggressive but i think you don't want me to notice that and it says what makes you think i am not very aggressive you don't argue with me why do you think i don't argue with you right so it's this kind of uh you know it's just taking things that the person is saying and reformulating them as questions in this kind of i guess sort of psychotherapist style of of inquiry this doesn't necessarily lead to super engaging conversations but it's an interesting trick that leads to you know a somewhat interesting if you if you view this this doctor is a little bit of an adversarial participant in the conversation i mean again you can suspend your disbelief a little bit and it it starts to to look somewhat you know possibly approaching human-like um and the way this the system works um you know this was before the the obviously the rise of deep learning and all this other stuff so this was largely based on keywords and contexts and so uh you would figure out okay here's a particular pattern that matches the inputs talking about you know something you something me um for example why do you think i blank you um and then the or sorry the the you know the the uh this would this would match against the user's input which would be you don't argue with me and then the output would be why do you think i don't argue with you you just kind of extract uh what the syst you know what the user has said about the relationship between you and me and then kind of turn this around i mean here's the end here's another rule um you know if you talk about your you know blank your father etc uh what else comes to mind when you think of your you know blank so you know obviously this is sort of a trick to to get around having to generate new ideas and bring new stuff into the conversation just by having it be based on these questions um but of course this is not very interesting because you can really only have one type of conversation with this system so a slightly better attempt is is a system called cleverbot uh and you could have dialogues like this where the user says hi there cleverbot cleverbot says how are you doing okay what plans do you have for today i do revision what are you revising for maths history how old are you that's none of your business what is maths history i don't know farming maybe okay so how is this working this system is basically a kind of nearest neighbors where when the user says something cleverbot tries to find a response from its database to a previous time that it said something similar or it you know it had some other log basically of a similar interaction occurring and then it tries to repeat what's going on there so you know again it's an interesting trick right because you can uh you know you can often find okay when someone asks you how are you uh or you know when someone asks you hi there you should ask how are you right um or when someone asks you what plans do you have for today here's some kind of different things that you can say right and so it you know it manages to return reasonable responses to some of these prompts but there's not really a high level structure of this discourse all right and the last the last kind of approach i'm going to talk about which is going to lead us a little bit into the neural systems which we'll talk about in the next segment are ways of viewing chat spots as translation problems so this isn't work from alan ritter at all in 2011 where they viewed it as a translation problem using phrase based machine translation where you basically you have some current utterance in the dialogue whatever the user said to you and you're going to translate that into your response that you're going to send back to the user so in some cases this can kind of make sense right like if you have the question what time do you get out i get off at five you're taking part of the question asked and using it in your answer right and so for this kind of narrow case translation actually makes a lot of sense assuming you don't care about returning some real answer that's that's grounded in some kind of knowledge if you just need to say some time this is a way to maybe produce that so what they did is they took they took a whole bunch of data and had some clever pre-processing for getting phrase rules that would lead to a reasonable dialogue system um and then they uh they did they did this based on twitter data and conversations they saw on twitter um and there's you know sometimes some of semi-reasonable stuff happens you know i really hate movies watch watching movies in the living room the system uh says me too what movies are you doing in room okay so and you know it it sort of agrees with the user it picks up on the statement and then it asks the follow-up question the follow-up question just doesn't happen to make any sense but it's a little bit better than this baseline me too i love those question mark you know closed parenthesis in my mouth that doesn't make any sense but of course these are very far from human responses which kind of bring in other world knowledge they're not just kind of rearranging and you know slightly extending content from the from the input so this gives a sense of maybe the idea of treating this task as one of translation is a way to build some better chat bots and we're gonna see in the next segment how neural networks can let us really double down on this idea and build some stuff which while it still has problems is pretty cool and works a lot better than many of these early systems that's the end of this segment you\", metadata={'source': 'vAZ7VlLXReE'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about so-called task-oriented dialogue this is departing pretty substantially from the chat bot ideas we were seeing before because now we're going to be talking to systems that are actually going to be trying to help us do something and not just carry on a conversation so this comes back to ideas that we invoked at the very beginning of the course you know the idea of having a conversation with siri we often want to talk to these personal assistants or basically api front ends and ask questions like you know find me a good sushi restaurant in chelsea um and hopefully siri would say something like sushi seki chelsea is a sushi restaurant in chelsea with 4.4 stars on google and you ask and then you can ask follow-up questions like how expensive is it entrees are around 30 each and then you might say find me something cheaper so th this is obviously very different than the chat bot setting in that if you throw this into one of the chat bots you know maybe there's some chance it would produce the name of a real restaurant but uh you know it certainly wouldn't be able to accurately tell you what the prices are um or be able to handle this idea of like actually doing a search for a cheaper restaurant and another another kind of domain that often gets talked about is customer service so you know if you want to talk to amazon and ask why your order's not here ideally you would be able to do this through alexa um and it would be able to say okay your order was scheduled to arrive at 4 p.m and then you say it never came and then you know it tries to figure figure things out right so the the kind of big difference here is that we actually need a model of what the user is trying to do we need to back this up with real data real databases information about what the user's order was rather than just kind of generating random crap to say from a language model and plumbing all this together is going to require a lot of different ideas beyond what we saw in chat bots so let's kind of look back at a little bit of a historical uh view of this people have been looking at this problem for a long time um there was a kind of system or interface developed at darpa in the early 90s called adis the air traffic information system or air travel information system where you have these questions that you would ask on a phone like how much is the cheapest flight from boston to new york tomorrow morning and so this is very domain specific and so the system has two things it has a goal or or an intent um and here in this case the goal is airfare i'm trying to tell you a price right and then there are several fields that are effectively constraining a search so the relative cost you're looking for the cheapest you know and then the kind of parameters of the times and the places so we talked about these kind of qa tasks before in the context of semantic parsing we looked at this geoquery data set and we could think of this in the same kind of lambda calculus expression uh and and in fact when when thinking about adis many people are you know many many uh many systems to it do treat it as a semantic parsing problem but you can also view it as this this kind of intent plus slot model and so this is how a lot of task-oriented dialogue systems work is um for atis there's 29 different intents for something like amazon alexa there would be many many more um you know and different questions will have be targeting different things like which flights go from blah blah it's supposed to give you flights um you know this is a question about ground service there's a question about uh you know i'm trying to get a day of the a day name for what days of the week do flights from san jose to nashville fly on or information about meals right so when you have a system like amazon alexa and you talk to it you know what it will do is it will try to figure out what uh what sort of sub module it should use to answer your question and so if you ask about setting an alarm it has a set of intents and things like that surrounding that domain okay so what is this really kind of going on in the back end so when you say find me a good sushi restaurant in chelsea there's really a couple different things the first is this uh this kind of slot filling idea where we need to figure out okay we we want a sushi restaurant and we want it to be in chelsea and then we need to actually invoke an api right we need to execute some kind of search here and this is something that obviously didn't exist in the uh you know in in the chat bot world and then we need to generate a response and then when the user asks how expensive it is where we're you know doing some other kind of query on this and then again returning a response so this isn't this this combines the idea of like intent classification and slot filling along with you know having to actually dispatch stuff to an api and then generate a response that's going to be rendered to the user so a lot of times this generation is templated in these current systems that's why you know when you talk to one of these things they'll often you know use the same kind of phrasing for different queries right okay so there are a lot of ways of framing this of thinking about how to build each of these pieces and plumb them all together i'm going to present to you one relatively new idea that is a little bit of a more sophisticated take on this process um and it's an idea from a startup called microsoft semantic machines uh well a startup called semantic machines that was acquired by microsoft and their idea is to formulate dialogue as what they call data flow graphs so when you ask a question like where is my meeting at 2 this afternoon the graph produced here says okay we have 2pm this time and basically we're using that to trying to to to try to find an event and then we are going to get the place associated with that event so this kind of takes this this utterance and and flexibly transforms it into this uh this data flow structure that is able to transform information the user gives but it gives an input into the response and the cool thing about these is that they can work in the context of a conversation as well so then if you ask can you create a meeting with megan right before that starts what you're doing effectively is referencing this all this event that we already found and so we ask okay when does that start and then we have this string megan that was passed in and we want to create an event with both that start time and also this megan person but here's where the system can say okay i don't actually know who megan is right like this is one of the key features of dialogue which is that you need to be able to ask things like clarifications so then the system will say uh okay which which person named megan did you mean and uh you know the the user will say megan bowen and then now this can this computation can kind of go through and it will create the event so the key bit here is that we can't just treat each of these things as a kind of one and done utterance or you know semantic parse it and just execute it very easily we need to be able to reference these earlier parts of the computation users also might want to do things like go back and revise things like you know actually i meant megan you know someone else and then you want to go back and be able to change the name so you want to be able to edit parts of this graph as well and so it's important to keep this whole thing kind of instantiated okay how do we train these things this is a topic that's kind of beyond the scope of this course there's a lot a huge literature about how to you know build and train models that look like this and and look like many other things a lot of it looks roughly like learning from demonstrations this is the so-called wizard of oz paradigm where you basically have someone who is manually pulling the levers on the back end of the dialog system and you're using that to collect training data so when a user says find me a good sushi restaurant in chelsea that response will get dispatched either kind of live on the fly or you know this data collection would happen static offline in advance some operator of the system would kind of type in all this information indicate okay they're looking for a sushi restaurant in chelsea and now i need to do a search and then for the generation piece of it they're either kind of typing out a free form response or using like again sort of pulling some levers behind the scenes to get the system to uh you know say the response with some particular template all right and this gives you supervised learning data that you can use to uh to basically predict what to do in the context of any given utterance okay so there's you could i mean you really could do an entire course on dialogue systems and task oriented dialogue there's so much in this space and it really encapsulates in many ways all of the problems we've seen so far um you know parsing generation all this kind of stuff you know entity recognition so there's a lot here there's a lot of industry interest and a lot of the kind of drivers of this are industry players uh so you know there's just a list of a few of those here it's really a kind of very active area where a lot of the best stuff that's out there probably isn't public yet or we don't really know how it works um so you know i guess keep your eyes peeled and uh watch for stuff happening in the space but other otherwise you know it's one of the uh kind of one of the big open problems that that people are thinking about is how to build really compelling systems that work really well for problems like this that's the end of this segment you\", metadata={'source': 'JXfAkX7kvnM'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about neural net chat Bots and we're going to see that chatbot systems have dramatically Advanced since some of those early systems that we talked about so here I'm going to give you a kind of Crash Course and the history and some of the ideas that led to where we are now with these chat Bots from some of the earliest neural networks up through the latest models so one of the first attempts to make a neural net chatbot was really inspired by Machine translation and the idea was that well can we just think about this as a sequence the sequence problem essentially translating from something that someone says into a response to that thing right viewing these almost as like two different languages and these early efforts even tried to evaluate with metrics from machine translation like Blues score uh however there's a there's some big challenges with this uh so remember that blue scores go all the way up to a hundred and human blue so you take one human written response and compare it to another human written gold standard response those have a blue score of six out of 100. so there's very high disagreement on how humans actually continue a conversation obviously because there's just many things that can be said right so even if a system based on machine translation sort of does okay it's very hard to evaluate whether it's taking things in a reasonable direction or just kind of getting half the words the same as what a human would say or or what and it turned out that actually these systems were not doing all that well uh one of the problems that was observed was that when you train these models on data in this translation like fashion to maximize the likelihood of the responses you end up with this kind of degenerate Behavior where the models really like to say I don't know and I think the second example here is interesting if I say what is your name you're most likely going to tell me your name however there are many many names out there right so the probability of saying my name is X for any particular X might be somewhat low but I don't know is this kind of generic response that works in almost every scenario so it has this like high prior probability under the model and then the model likes to produce it all the time and it's really hard to kind of fight against this so one of the ways of doing this was to try to imbue these chat Bots with personas so there was a data set collection effort from Folks at meta called Persona chat where they had two people sit down and have a conversation and each person was given one of these personas and the idea was that by in if if these people sort of say things according to these personas we can train models that condition on the personas and then are more likely to be able to say the same thing so when you say my name is whatever you actually report a name given in the Persona or if it's like what do you like to do you'll say for Persona 2 here I love watching Game of Thrones now if you look at these conversations they're a little bit stilted I think uh but regardless training on this data did lead to somewhat better systems however these were still not that exciting to interact with and so one of the big components if you've made it this far in the course you'll be predicting is bigger data and pre-trained models and so uh the idea essentially is to say all right let's just take this approach and just Elevate the data the size of the model used and everything like that and there are some other tricks that we've seen so far like nucleus sampling that can help as well particularly with things like these generic responses like I don't know so there were a couple of efforts uh starting in around 2020 one of the most notable ones also for meta uh was a system called blenderbot which was a 2.7 billion parameter model so actually at the time it was one of the biggest models out there this was shortly after gpt2 released uh and they have a bunch of tricks that we're not really going to talk about uh but they have the ability to retrieve from a corpus so they can kind of have a dialogue grounded in something like a Wikipedia article and they used a bunch of different data sets including this prior Persona chat data set a data set of empathetic dialogues and also a data set kind of talking about factual information from Wikipedia so how does this work we're going to look at a few examples and try to get a sense here so in this example the gray here is the user and the blue here is the chatbot blenderbod so the user says will you sing me a song and the bot says sure what do You Want To Be Sung to I can sing you a song about about baking and then the person says yes sing me a song about baking do you want me to sing it to the tune of sing along with Mitch yes I would love that okay I'll get it to you by the end of the day what else would you what else do you like to do so the system doesn't want to sing You the song yet so this is already a little bit odd of course it's odd to be trying to sing a song with a chat bot uh and then the person says can you tell me what the lyrics are sure it goes like this baking is my favorite thing to do in the world how does the chorus go it goes baking is my favorite thing to do in the world so I think this is actually kind of a funny bit but I think it's like funny unintentionally I don't think the chatbot is trying to make this joke here I think uh you know it kind of shows this conversation maybe it's not really able to follow the intricacies of a multi-turn dialogue yet even with a pretty big model however we see that the response is at least follow what the person asks they kind of are reasonably coherent they do make sense Etc so there's a lot of kind of good boxes being ticked now there are some issues so uh the bot will say I'm from the Midwest but I live in the south now how about you and then the person says born and raised in the South and then the bot says what part of the South I was born in Georgia but raised in Tennessee so it kind of immediately switching up and being inconsistent about information so that was one of the things that Persona chat was designed to avoid and we still that that still see that that's happening somewhat so it's not as good as kind of conditioning on these past conversation turns and then finally there's this interesting joke explaining example why did the mushroom go to the party I'm not sure but I'm glad it did I love museums what do you do for a living so again not really being a kind of helpful conversation partner here no it's supposed to be a joke oh sorry I didn't get it what kind of joke was it I like jokes the answer is because he's a fun guy do you get it no I don't can you explain it to me yes mushrooms are fungi and fungi is pronounced like fun guy and then the system says I get it and then the person asks for an explanation and then the system can kind of explain it back so this is kind of an interesting I think this example sort of highlights something interesting here uh these issues with the model there seems to be a little bit of inconsistency and not a strong ability to use the context it also feels like this you may be kind of talking to someone here but there's something kind of missing and one of these things is that of course the model didn't actually learn this joke in that no parameters were getting updated uh it does learn it in kind of an in context sense where it's able to look at the kind of con the conversational context and re-explain the joke but there's also a sense in which the model doesn't really understand phenology right so like how much can we say that it really understands this joke versus this allowed it to surface something from the pre-training data because this is a pretty common joke so there's a kind of interesting philosophical direction that this leads in order in thinking about sort of what is intelligence and uh kind of getting back to the ideas of the Turing tests and uh are we interacting with uh it's kind of something real here and this came up with a subsequent model from Google called Lambda where an engineer at Google claimed that this was a person and self-aware and was subsequently fired and there was a whole kind of uh kerfuffle about this so I think questions about what is consciousness and things like that are a little bit outside the scope of this course but what I will say is that uh when we look at the kind of next big system that was released after Lambda which was chat GPT uh this system does not really look like these other chat Bots right it's very on Rails and I would describe it much more like a QA system than a chatbot uh so the system will really kind of not want to go off into like some weird Direction with you it'll instead want to provide information and has a lot of this fencing where if you try to get it to go in weird places it'll say like oh as an AI language model I can't do that so I think the reaction to things like Lambda have kind of led to that uh you that those design choices for how to uh Implement and release these systems I will say that there are other players in the space like character.ai which is a startup that is trying to deliver more on the kind of chatbot Vision using the latest and greatest models so we're likely to see more advances in this vein uh over the coming years and months that's the end of this segment\", metadata={'source': 'Hc7P3QukmJk'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to introduce machine translation machine translation is one of the oldest problems in nlp that people have really been trying to do in a large scale computational way there's work going back to the kind of 80s using large computational techniques and people are trying to build automated translation systems even before that so the basic idea is that we're going to take as input a sentence in one language and the output is its translation in another language all right so this the the input we're going to call the source uh and the output we're going to call the target language and so the examples that we're going to use uh in these next few segments are largely going to be english french or rather translating from french into english primarily so that you know we all can understand the output here but these techniques are quite general and uh we're going to see once we get to neural machine translation that they can work for quite a range of language pairs so i want to start off by talking about the data the data is what we call a bitext or a parallel corpus and this is a set of of sentences in one language and their translations and so the nice thing about this data is that it's very widely available because this is what professional translators produce when you have a website you you know ship it off to someone to translate it and then you know you've got a bunch of data that's probably more or less translated at the sentence level so what we're going to start off doing is looking at just the correspondence between some different kind of sentence pairs in english and french to understand what kinds of phenomena we need to be dealing with and how this translation problem is going to look so we have four sentences here uh jeffery on bureau i make a desk jeffery in soup i'm making a soup keska tufay what are you doing and keller what a mistake so what i want you to do is take a look at these sentences and think about based on them you know if you don't know french if you do know french it's sort of cheating but if you don't know french can you translate the following sentence to into english so the process that you might be going through is looking for correspondences that you can kind of deduce based on word co-occurrence that you see so for example uh you know we we can make some maybe rough assumptions about the syntactic structure and word order that they're not going to be too different between these languages and so um you know the fact that we see uh ju over here and then i on the english side makes us think that maybe je translates to i which is in fact correct uh and similarly we see that uh we have fae here and we have make and making which maybe indicates that uh this you know this might be a verb that that indicates something about making stuff and then we know that beer bureau and soup differ between these two sentences and so maybe these correspond to desk and soup and of course soup is also what we call a cognate where it kind of closely resembles uh the translation in the other language and so that gives us an additional clue as well okay so roughly we can take this data and discover this kind of set of word translations however the one of the big ideas in mt is that we're going to need to think beyond the word level so for example we have this translation keska tufay translates to what are you doing and so tufay so fae can mean doing as well as making so there's a kind of ambiguity there which makes things tricky but more importantly from the perspective of this sentence keska translating to what r is really not quite a direct translation um you know the the most direct translation would be something like what is it that you what is it that you are doing and so in order to understand in order in order to understand this correspondence we can't really break things down in terms of these individual word units we really have to think about things at the phrase level okay so then um you know hopefully you can figure out by this point that tufa unerrer means something like you make a mistake or you are making a mistake all right so kind of big effects that we have here are ambiguity or what i'll call many to many translations so for example uh you make a mistake or you are making a mistake these are both fine translations um and so we generally will we'll need to kind of pick one right if we're actually going to type something into google translate and get a single answer out and another big thing is that we need to capture this phrase level correspondence rather than word level so this kind of sets up the ideas of how we're going to have to think about doing this task we're going to need to take this by text and we're going to think about two techniques the first is going to be classic machine translation kind of pre-neural net where we're going to have to discover this correspondence structure and learn phrase pairs of course once you have sequence to sequence models you could kind of treat these as just sequences of strings but it's still instructive to understand what kinds of correspondences exist in order to understand how we have to build those sequence to sequence models and how we have to go about capturing these correspondences so that introduces the problem of machine translation and so uh we're now going to go from here into thinking about a little more concretely what techniques we're going to use how we're going to evaluate it and what these different translation paradigms look like that's the end of this segment you\", metadata={'source': '9KAZ4-gKj9g'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about the sort of overall conceptual framework that we're going to think about when we talk about mt so the the kind of way i want to diagram this is a very useful diagram due to bernard vocus from the 1960s so like i said people have been thinking about translation for a long time and uh the diagram is called the vocus triangle uh so the idea is roughly the following we have a source sentence over here and a target sentence over here more or less represented as strings of words and so we could think about a kind of word level method for translating between these two things that's like the closest to the surface structure of these sentences uh the kind of lowest level form of translation we can think of we can also think about some kind of phrase level translation so we're drawing this further up the triangle because this is at a like higher level of let's say linguistic abstraction all right the kind of next level up here is what we'll call syntactic so rather than thinking about the language in terms of phrases we think about it in terms of bigger syntactic structures and so what we're trying to do is transform a syntactic analysis like for example a dependency or constituency tree in one language into the other language and then the kind of layer above that is the semantic layer so here we think about maybe we want to have some sort of un understanding of the semantics of a uh you know of an utterance and use that to achieve this mapping so for example we could think about the kind of lambda calculus expressions we saw when we talked about question answering and semantic parsing and so if you have a lambda calculus expression that actually grounds the meaning of a sentence in a database query or something like that we could think about what sentences in another language would have a similar kind of grounding and then at the top we have uh what's called interlingua and this is largely a kind of abstract concept and the basic idea is that uh you know if we have a french sentence we could map it into some kind of blob that captures the meaning of that sentence in some sort of language independent way right and then we should be able to convert that blob or sort of generate from that blob into english or japanese or kind of whatever language you want to translate into okay so this is a great kind of concept and framework for thinking about things um the problem is that sort of high up on this triangle is actually going to be pretty hard to kind of pretty hard to deal with in a lot of ways so when we think about for example you know what what this meaning actually has to capture there's a lot of decisions that we uh that we have to make and we would rather like to avoid making them um so for example uh french and english use gender in different ways um so in english if you talk about her cousin you know we know the gender of the person who is related to the cousin but we don't know the gender of the cousin but in french if we talk about her cousin we don't know the gender of the person who has the cousin but we do know the cousin's gender right and so in order to capture this actual abstract meaning we need to kind of nail both of these things down and sometimes that requires making really tough decisions or inferring things from context that we otherwise don't need to infer so these these kind of upper layers here [Music] are pretty hard to deal with and so the kind of history of machine translation is that uh you know roughly i'll say that word level stuff was kind of popular initially especially a lot of the early work at ibm on statistical translation really did operate at the word level and then phrases were really the kind of dominant paradigm from uh you know roughly 2000 to roughly 2015 when neural machine translation started really taking over and syntax kind of syntactic level is also tough uh this was an approach that people thought was very promising for a long time and put a lot of work into it and we're going to see what some of those techniques look like and understand a little bit about what makes it so hard but phrases are really kind of the sweet spot it turns out uh in terms of thinking about all right like we we want to stay close to the words because that's easy from a just computational machine learning modeling perspective uh but we want to do a little bit better than trying to do word by word translation because those units are too small so the the basic idea is that big chunks make translation easier and the kind of way one way to illustrate this is just imagine that you have a whole sentence given to you and you've seen that whole sentence in your training data you know you should just return the translation that someone gave you before right i mean unless there's something in the context that tells you you should do something different like that's a very good baseline and then imagine if you have a sentence someone gives you that's just you know two clauses that you've each seen before now you would want to translate clause 1 translate clause 2 that's probably going to be pretty good again there could be some errors in there but it's a very strong baseline so the questions we need to answer are how do we get these chunks and then how do we translate so the question of getting chunks is going to require us to answer the problem of how we learn these correspondences between uh two different languages and then we'll also talk about how we can then plug that into an actual translation system kind of downstream the thing i want to talk about first which ties into the triangle that we saw here is the idea of evaluation it turns out that eval that it sort of is pretty convenient to evaluate mt the word slash phrase level so people have been working on mt for a long time and so there are a lot of metrics and people have whole competitions about coming up with better metrics that correlate better with human judgments of translation so there's a whole really rich literature on this and i'm going to tell you about one thing which is kind of the lowest common denominator here and that's a metric called blue um and so blue is a metric that is defined as follows is the geometric mean of one gram two gram three gram and four gram precision of of the output plus well i guess it also includes uh let me say times a brevity penalty all right so kind of in the abstract we have a uh we have a reference translation that someone gives us and then we produce a translation from our system and we say okay for four gram precision what we're asking is how many four grams are in the reference and so when i say this is a mix of word and phrase level you can kind of see that we're assessing this translation in terms of these individual phrasal chunks and then we have a geometric mean across these different precisions and then we also have what's called a brevity penalty which i'm not going to go into too much but basically it penalizes translations which are too short otherwise what you could do is you could just output let's say four words you were very confident about and get you know and if that if that 4 gram was correct you would get 100 precision but you wouldn't have translated most of the reference so what this does is it you know it just sort of slides a window through this translation and checks it against the reference now what this doesn't handle is ambiguity you could do that a little bit by having multiple references kind of multiple valid translations that someone's written down and people do have that on some of the test sets that they use but generally what happens is that blue scores are low and when i say low i mean that you know 30 to 40 might be an extremely good blue score it might even be human level translation quality just because there are true differences in how you trend in in how you're able to translate things right so what we get what we what we sort of know and have come to understand is that blue correlates pretty well with human judgments of translation quality but it's not really an absolute scale like we can't look at a blue score of 30 and say this is somehow 30 correct uh instead it's something where we know that blue of 33 is probably quite a bit better than blue of 30 and you know when developing a system on a single language we can use this as our kind of benchmark for understanding how well we're doing now what it also can't do is allow us to compare performance across different languages or across different test sets easily because if you have different numbers of references or if the sentences are more or less complicated it's very hard to calibrate these scores regardless this is still a useful tool that's really powered the last uh you know tools like this have powered the last 15 to 20 years of translation research and it's a very common metric that you'll see and so it's good to understand it so we kind of understand now that if we are able to translate a bunch of phrasal chunks fairly accurately we might do a good job on this metric and that will ideally let us produce things that look good to humans as well so the question that we're going to answer going forward is how do we get the kind of phrase level correspondences that we're looking for and then how do we use those in an actual translation system that's the end of this segment you\", metadata={'source': 'Oup0DEYJXEQ'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to introduce the fundamentals of word alignment so word alignment is a procedure that takes as input a by text and what it's going to do is it's going to produce a set of alignments between what we call the target which in this case is the english it's going to produce a set of alignments between the target and the source which in this case is french and so what we see here is that it's going to draw in these word level correspondences that are going to have some nice properties they're going to enable us to discover these kind of dictionary you know word by word translations that we were using before and they're also going to enable us to start looking at the idea of phrases so for example here we have two words am and making both align to fae that indicates that we probably want some sort of phrasal translation here and the last property i'll point out is that these things can be out of order right so we're going to need alignment models that can handle uh translations that are not so called monotonic where there's some kind of word level reordering between the two languages english and french are fairly close syntactically and so there's not going to be too much reordering but we're still going to see some important cases that we need to model and in general things get much more challenging when you look at more distantly related languages like english and japanese okay so what we're going we're going to define some notation here we're going to let a vector t denote our english words a vector s denote our french words and a vector a denote our alignments where a i equals j means that english word i aligns to french word j so just to make this notation concrete do so the alignments here uh are going to be you know a1 equals 1 because we have the first english word aligning to the first french word and then a2 and a3 are both two because the second and third english words are both aligning to the second french word here um and then uh you know a4 equals three and a5 equals 4 to kind of round out the rest of our alignments here so an alignment model is going to be a probabilistic model that's going to place a probability distribution over both t and a given s and uh the alignment models we we're going to look at are all going to fall under this probabilistic framework and i'm going to say that this is uh i'm going to call this a generative model um and the reason is because uh we are both you know generating or modeling a distribution over a which is the thing that we're trying to predict but we're also modeling a distribution over t which is one of our inputs so the way i want you to think about this and something that we'll kind of come back to later is that if we kind of forget about s here we have a set of words that we're generating and also a set of alignments and those alignments are going to play something like the role that our tags played in hidden markov models and the target words are going to play a role just like the words in part of speech tagging hidden markov models and so we're going to be able to leverage some of the same techniques here and largely the framework is is going to kind of look similar but it's going to be a little more complicated now because this whole enterprise is now conditioned on s so the the other kicker here is that we need to think about this doing this process in an unsupervised way meaning that we are not given examples with labeled alignments all i give you is the by text and this model or you know whatever model you use needs to figure out what the correspondence is between the words are without having seen examples of okay i align to j et cetera i aligned et cetera so that is going to be one of the challenges that's going to lead to some slightly different kind of modeling learning and inference choices but we're going to largely be able to draw on tools that we've already seen for this type of modeling and so this sets up the the problem of word alignment and further on down the road we'll get into what the actual models are that we use and how we deal with those that's the end of the segment you\", metadata={'source': 'dzOuPhBmFtE'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about ibm model 1 which is going to be one of the most basic models for word alignment and the kind of foundation of a lot of the common ones that get used so it's called the ibm model because it was devised at ibm and published in this paper called the mathematics of statistical machine translation in the early 90s so the rough framework is the one that we've seen so far i'm just defining a little bit more notation here so we have our alignment variables a1 through a n our target words t1 through tn and now for our source words we have s1 through sm notice that it's a different uh kind of length index because we possibly have a different length source and target sentence and then we also introduce an additional token called null and the reason is that some of the words in the target are not going to align well to anything in the source and so this is going to give us a placeholder for dumping unaligned words another thing i want to emphasize is that each word in the target is going to is going to align to exactly one thing in s it might be the null token but we have no constraint that everything in s aligns to t uh or that multiple things in t can't align to the same s that's fine so in general we have a lot of constraints on how things in t align but s is just this like bag of targets that we can align things to so what is ibm model one so we start off with our kind of generative formulation of alignment here p of t a given s and we're going to factorize that in the following way we're going to factorize it as a probability distribution over the a's and then we're going to generate the target conditioned on the a's all right and the form it takes is going to look like this we've kind of rolled up the pa and the pt generation together but this is equivalent to what's on the left side of the or it can be expressed as what's on the left side of that last equals up here so what's going on here so the model parameters in model one let's start there consist of a translation probability matrix which is going to be vs by vt so source language vocab by target language vocab and in this case the source vocabulary has to include null as well and basically this is a set of distributions p of target given source so for each source word we have a distribution over all of the words in the target vocabulary and just like in hmms how we had emissions that conditioned on tags and that gave us a distribution over the entire vocabulary for that tag we have a similar thing here now conditioned on source words so when we write p of t i given s a i what that's saying is look up the probability of ti given source word sai so a i is you could think of it like a pointer here that tells us which word in the source we're going to condition on in some ways this is kind of a weird structure for our model in some sense that like we have this variable that we're placing a distribution on that only serves to like tell us what to condition the generation of a different variable on this is quite different than anything we've seen in the course so far but fundamentally it's not actually that you know i think actually dealing with it is not going to be that crazy as as we're going to see and then the last piece here we have is this p of a i term which is just going to be a uniform over 1 through m and null so remember that a i is an index here index into the source and so it just assigns a probability mass of one over m plus one to each of these alternatives and these are all independent so this is a pretty big simplifying assumption and we're going to unpack that a little bit later but this is what makes ibm model one at least kind of simple and and relatively attractive is the fact that uh we can we we make a strong assumption about p of a i and that's going to make this model fairly easy to deal with all right so i want to kind of go through an example but first let me tell you how we do inference in this model so we have the same problem that we've seen throughout the rest of the course which is that what we really want when we have a generative model is actually the posterior distribution over one of the variables conditioned on in this case the other two so we can rewrite that as usual in the form that our model actually takes where we also are generating p of t and then divide in this case by p of t given s all right so again we're you know what we're trying to do is we're really trying to arg max over a here we want to find the best alignments in some cases but not always we sometimes want to actually know the distribution over these and so i'm going to kind of remove that but conceptually this is it sort of relates to what we were looking at in hmm inference in that way and so if we write this out substituting in our expression for uh from from ibm model one of how the model actual actually works uh i'm going to leave this denominator term here and we're just going to ignore this and the reason we're gonna do it it's gonna it'll turn out that basically we could just normalize the model in another way and that's gonna effectively do the same thing as dividing by this term but we don't have to worry about what this term actually is which you have to compute by like summing over all alignments okay and this term is just a constant one over m plus one in this model and so what we're going to say is that this distribution is just proportional to the product of p of ti given sai and we can go one step further and say that the probability of a i is just proportional to the probability of ti given sai sort of letting ai range over all of its possible values all right so to look at an example here let me write out a parameter matrix all right all right so remember that our parameters are structured in terms of distribution over target language words which in this case is the uh the english side conditioned on source language words and so each of the rows of this matrix is going to be a probability distribution that normalizes which says if i align to je or j apostrophe or maj what is the distribution over words that i am likely to be and so uh these distributions that i've written down are sort of plausible with a little bit of a little bit of slop in them and so if we have the sentence pair gem and i like remember that we also have a null token here and so what we could compute is the probability distribution over a1 conditioned on this s and t right and what we have to think about is what are the possible alignments here and so the raw scores we get here are by looking up in the table we see that i aligning to that has this value of 0.8 here i aligning to m that has a score of 0 and i aligning to null that has a score of 0.4 now notice this is not a probability distribution and the reason is because this is these are the probabilities from basically forward inference in the generative model like the probability of the target conditioned on the source and what we did in our rearrangement up at the top of the slide was to talk about the posterior distribution over alignments and we said it ends up being proportional to this but it's not exactly these probabilities uh so i'm going to write i'm going to write this as proportional here uh 0.8 on ju 0 on m and 0.4 on null and so then this yields the actual probabilities of two thirds on j zero on m and one third on null and so the actual correct alignment in this case uh you know we we probably should align i to z and that gets most of the probability mass here okay so that kind of shows you how this process works in model one and so if i gave you a set of parameters you can produce some alignments here and you know it should be clear that if those parameters reflect reasonable word level translations the model is going to do a pretty good job of associating each word with its corresponding let's say most likely translation word in the source all right i'm going to briefly talk about the hmm model which was introduced later after the after the ibm models and it's due to stefan vogel in 1996 uh and the idea here is the following we're just going to change p of a to be a different distribution where a i now depends on a i minus one and the form we're going to give this distribution is a categorical or what we sometimes call multinomial even though there's really just one trial distribution over the difference between these two and so basically what that means is that we assign probabilities to each sort of each integer delta that is possible between an adjacent pair of alignments and so you know i'm kind of drawing a fake probability distribution over here but what this says is that basically moving the alignment pointer by plus one is most likely now remember before it all the ais are independent right and so we don't know one if we align the first word in the uh target side to the first word in the source side maybe the second word in the target is going to align to the 20th word in the source size in the source side what this says is that that's actually not very likely what this says is that it's most likely for the second word on the source side to align to then the second or sorry the second word on the target side to then align to the second word on the source side so two things one is that this is a pretty reasonable constraint and encourages our alignments to have a more regular structure and so it's going to help us a lot especially when dealing with longer sentences and the second is that this turns our model into an hmm and we know how to deal with those so it does involve implementing things like the viterbi algorithm to do the kind of inference that we saw in the example of figuring out the posterior distribution over alignments but it's all stuff that can be worked out based on what you've seen already all right the last piece i'm going to mention here is learning so remember that we have no labeled alignment data and instead what we want is we want our alignments to explain the data that's already there this these source and target pairs so instead what we're optimizing is the following is is the following expression we really want to optimize the log probability of the target given the source uh the target given the source but we want to sum over all of the possible alignments so i guess let me let me write it this way where uh we want to optimize the prob the log probability of the target given the source but this is not actually a distribution that we have we in our model our model specifies p of t a given s and so what we need to do is optimize this distribution all right so this is something that we haven't seen before which we're going to call the marginal log likelihood and the reason is because we're marginalizing out a with this sum here so the what we talked about in hmms was that we can estimate parameters by just counting and normalizing now we have a more complex expression that requires summing out these these basically summing out these alignment variables and so we need an algorithm called the expectation maximization or em algorithm uh in order to learn this and when i say need i mean that that's the can most conventional way to learn this type of alignment model there are ways of doing this with basically gradient gradient descent where you kind of parameterize these distributions in a slightly different way so that you could actually take gradient steps on them and just optimize this directly all this stuff is a little bit outside the scope of what we're going to talk about here what i'll say about em is that basically what this looks like is computing posterior distributions over the data given the current set of parameters treating those as sort of soft labels so going back to the example we had here we would basically think about this as a kind of labeled example that says okay i assigned two-thirds of the probability mass to the alignment j apostrophe to i and one-third to null and then based on those we can re-estimate our parameters and update this translation dictionary and kind of iterate between these two these two procedures so this gives you a sketch of what these models are i we talked about ibm model 1 and the hmm model and how doing inference in them works how they how they actually give us word alignments and then how we might do learning in them which we're not talking about more primarily because these techniques don't really show up elsewhere in the course so with this we're going to be able to produce a set of word alignments over this by text and that's going to power our phrase based machine translation models that we're going to talk about next that's the end of this segment you\", metadata={'source': 'mbtk3VCG_2A'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about phrase-based machine translation now that we've set up our sort of primitive or let's call it pre-processing of word alignment we can understand how to actually take that piece and build it into a real translation system so the first step we need to do is extract what we call a phrase table and we're not going to go through this procedure in too much detail what i'll say about it is that we have a set of sentences from our by text and we can draw a kind of grid and what word alignment allows us to do is it allows us to draw in uh associations between words in this grid and so like i said in this example it's it can be a little bit hard to actually uh align some of these words um so you know we won't get necessarily a perfect phrasal correspondence between these two uh between these two sentences but what it does tell us is that okay you know two and u are likely translations of each other and it also tells us something more it tells us that okay we have kind of two words here that align in this block so tufay and you doing seem like they should be translations of each other this is an okay assumption in this case it's correct in this example but in general tufay means you do or you are doing and you doing just as a single unit doesn't necessarily work well but regardless what we're going to do is we're basically going to go through our data and draw these kind of boxes in our alignment grids and this will give us a set of phrase translations which we can associate with particular scores based on how often they show up there's a lot of heuristics involved in this procedure and so we're not going to again discuss it in a ton of detail but this is the basics of how we go from words to what we call a phrase table all right and now the given a phrase table we can build our phrase-based machine translation system by combining that with a language model this is just going to be an engram language model like we were seeing before and given these two pieces we are going to exploit an idea called the noisy channel which is the the probability of an english sentence conditioned on a french sentence assuming we're translating into english from french is proportional to the probability of that english sentence times the probability of the french sentence conditioned on the english sentence so basically it's like generate the english sentence and then translate into french as a way of sort of inverting the process i mean this is again similar to hmms for part of speech tagging we like generated the tags and then the words based on the tags even though what we cared about was inferring the tags from the words so it's the same kind of idea here and what this is going to allow us to do is it's going to allow us to incorporate two constraints here the first is from p of e which is that this has to have high probability under a language model and the second is from p of f given e which is that these things have to basically associate with each other we have to be able to translate reproduce the original french sentence given the english sentence all right so that's the idea behind this noisy channel approach to mt and we have these two pieces called the translation model and the language model and again if you actually write out bayes rule here you have a p of f term we're just going to ignore that because that's really what we condition on from the standpoint of this inference all right so we need a language model we're going to use an n-gram model for this and we need a phrase table which we talked about how to get and again we're going to get these phrase pairs and associate them with some probabilities that are based on basically counting what occurred what what phrases we see co-occur and then what we want is we want to actually produce a high scoring english translation of this french sentence that is produced by a series of phrase by phrase operations so the space of ease is much much larger than anything else we've dealt with so far really in tasks like hidden uh hmms for part of speech tagging uh this was exponential in the length of the number of tags but it was like 40 to that length and now we're going to have something like let's say 20 000 to that length if we have a pretty large vocabulary so the way we produce these translations is we form what's called a phrase lattice what we do is we take our input sentence here maria no dio une buffetta a la bruja verde and we look at each possible phrase match with our phrase table from this sentence which uh is now going to be spanish not french so on the spanish side une bo fatada for example can translate as a slap and so that's something that we would have in our phrase table here and so we can produce this translation option in this lattice so we find all these possible translations and now the what we're going to start with and what we'll focus most on is the problem of monotonic translation how do we walk through paths in this lattice to translate every word from spanish exactly once not skipping any words and stitch together a translation into english that looks good so this doesn't look so dissimilar from the viterbi algorithm but there's a couple of things that make it more complicated and so what we're going to end up doing is is beam search here so we have to think about what state we need to keep essentially in the beam in order to score things and so remember that in part of speech tagging the state was just what is the previous part of speech tag because that was the only thing we needed to score the transition function here we need uh the score of the hypothesis so far that was also something that we needed in in the hmm case just as the key for the uh the the item in the beam and in this case that score is a product of two things it is the product over all the phrasal translations we've seen so far that's the first term and then the language modeling probability the probability of each word conditioned on in this case the prior two english words if we're using a three gram model we also need to know where we are in the sentence and what words we've produced so far so this is the kind of trickiest thing is that we need to remember the past word or two of english context in order to score uh in order to basically score future words that we put down using a three gram language model all right so for example to start off with what we do is we populate this beam after maria which only contains one possible translation which is mary and so let's say the the phrase translation probability there we're going to use log probabilities let's say the log probability is minus 1.1 all right that's a that's a valid beam state we have the word the word so far where we are in the sentence i'm just writing that as the index here and the score then what we do is we say all right for the beam at index 2 after mariano how do we get there so there's a couple of different options now for paths that we can take that end at this particular time step so we can get mary not we can get mary did not and mary no and so notice that in the case of mary did not we dropped information about mary we can fold that into the scoring from the language model but we don't actually need to remember mary because going forward the language model only needs to look at the prior two words of context and so now we have three hypotheses that end here with associated scores and we're going to keep extending these forward in the sentence and again so the score here has to account for both the language model probabilities which are based on the number of english words you've generated so far and each one has to condition on the prior couple of english words and we also need to incorporate the translation model which is just the score of each of these phrase translation options that we picked up basically the score of each of these underlined segments in the lattice so the score here is a product or let's call it a sum of log probabilities and the in in reality you have to weight these things differently so there's some extra parameters here and real translation models actually use several different features that we then have additional weights on all right so one kind of crucial thing is that once we go a little bit into the sentence there's actually multiple ways of segmenting it and translating it so in the third beam here we're going to have two types of hypotheses ones that come from using three individual translations mary no or maria no dio or ones that translate maria and then or uh maria and then no deal uh so the slide is is wrong here the plus should be before no the beam in this case contains alternatives from multiple segmentations of the input and so we're thinking about multiple paths and they're all kind of competing with each other to get to this time step but they've all translated everything up until this this point all right so that's basically how monotonic translation works you go through the uh you go through the sentence until you get to the end you do beam search and then your top thing in the beam is going to be ideally the most likely translation under the model unless you had uh you know an insufficiently large beam and somehow you uh screwed up and only found a kind of approximation of the best translation we cannot there are also models that visit words out of order these are significantly more complicated and so we're not going to talk about them very much the state in this case also needs to track additional information about what words you have and haven't translated so that you can kind of go back and pick up the words that you haven't translated so far uh and so the you get this extra kind of bit vector thing in the state and generally this is very involved to deal with and in most cases it's not actually necessary because if you have a big enough model that has seen a lot of phrases those big phrases actually already capture big reorderings so even though reordering might be very important to get things like la bruja verde translating into the green witch in this case we actually have a phrase green witch which tells us that uh which which tells us that reordering and so we can get that reordering even operating within the monotonic translation framework all right so the last main thing i'll say is that we talked about how the uh the the language model and the translation model have these weights associated with them um setting these weights is a very complicated process uh there's a procedure called mert for minimum error rate training from franzak where you basically form a small set of translation alternatives and then you're not doing anything like gradient descent you're instead using a kind of line search on your parameters to figure out settings of the parameters which make the correct translations rank highly on in those k best lists or thousand best lists in this case and so it's a very weird paradigm for training is again not something that looks much like other things that we look at in this course and so we're not going to talk about it much more but generally it was very hard we didn't have good gradient based optimization techniques for how to train these models and so there were a lot of sort of crazy procedures that looked like this all right so what i'll the last thing i'll say about phrase-based translation is that there's a toolkit called moses that rolls up all this stuff including this uh so-called phrase-based decoder called pharaoh that we've seen in these slides here and it has a whole bunch of different models for doing uh translation including this phrase based technique uh syntactic models etc and for a long period of time you could build basically things that were on par with google translate if you had enough data and so this is a very nice tool if you want to explore translation systems that look like this and is i think still maintained pretty actively so we've seen here how to actually put these pieces together to build a phrase-based machine translation system we'll talk now about alternatives to this so particularly syntactic translation and then also neural machine translation that's the end of the segment you\", metadata={'source': '0k8b5jGk-h4'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about neural net methods and pre-trained methods for machine translation so we talked about kind of classic syntactic and phrase based Mt systems and neural systems were actually some of the first places that things like seek to seek models or neural Mt was one of the first domains where a sequence of sequence models were really developed and explored and this was true even before pre-training came along and actually pre-training has had a little bit less impact on Mt than on other areas partially because for some languages there's already so much data and also because when you're dealing with texts in other languages just having an english-only model isn't as useful so let's kind of walk through some of the results that have happened over the years so in the original Transformer paper like we've seen it was actually formulated as a sequence to sequence model and the results I got on translation were pretty good uh if we compare it to the the results from their biggest model to this Google nmt system which was a kind of state-of-the-art eight stacked lstms with attention uh system we see that the Transformer is actually getting a couple of Blue Points more than this state-of-the-art Google system which was a very very strong Baseline so these models you know if you have a lot of data like those language pairs we saw there were like English French and English German when you have a lot of data you just like train a big Transformer and like you're good uh there's a lot of challenges when you have much smaller amounts of data and it was kind of previously thought that when you were dealing with let's say a hundred thousand examples phrase based machine translation was actually doing pretty well and neural systems weren't necessarily doing as well I mean you could see that here there's a baseline that's actually getting zero blue score however this work due to sunrich and Zhang showed a bunch of little hacks that you can apply and if you do all these things actually your neural systems are not really behind your phrase based systems anymore even when you have small amounts of data here so there's kind of this period where it was known that neural worked well for the big stuff but then it kind of rapidly advanced in small data situations as well and this has become a big area of research where people are thinking about deploying Mt systems for settings where you don't have a lot of parallel data now remember because you're thinking about having data in two languages any new pair of languages if you don't have a lot of data for it it might be hard to kind of stand up an Mt system there so there's been lots of work on saying okay well can we train systems on related languages and then generalize them or just like fine-tune some of their parameters essentially to get them to generalize to new languages and particularly things like bite pairing coding or subword tokenization kind of allow us to do this generalization because like the same little pieces you see here might it kind of corresponds to pieces that you see over here as well um so basically what this table on the right shows is that there is the ability to do this kind of transfer if we take an existing model and fine-tune it in these settings we can do much better than starting from scratch and in fact the kind of thing that really seems important to transfer is the inner workings of the Transformer the embedding layer just transferring that on its own uh which is the third row here does not seem to work all that well um so essentially what you want is you want the Transformer to have these kind of basic skills to do translation in these kinds of mappings but you don't need all the knowledge of the individual words you can kind of relearn that finally there's been a push to build models that can deal with many languages at once right rather than thinking about okay we're going to train a model on this language and then we're going to fine tune it or transfer it to this other language pair instead why don't we just do everything with one model so uh there's a model called mbart which follows from Bart that we talked about earlier using a similar denoising objective but now trained on multilingual data so essentially we take data in a bunch of languages we mask stuff out and then we learn to reconstruct strings in those languages but we're doing it all within a single model so the model is going to be good at dealing with representations of language of of English of Japanese Etc and then it's not too much of a stretch to fine-tune it on data where you're explicitly mapping from Japanese to English or vice versa right so this is a kind of way of taking the same sorts of pre-training ideas we've seen before and getting them to build models that can deal with multiple languages at once and deal with the mapping between these languages so if we look at how this works we can compare the performance on a bunch of language pairs from a randomly initialized Transformer model so no pre-training to a model that's randomly initialized or sorry that's initialized from this Embark pre-training pre-trained model and we can see that this is quite a bit better however we can also see that there's still languages like Gujarati and Kazakh in the top left where it's getting I mean almost zero blue score so still doing very very poorly so it's not the case that just pre-training magically solves all languages it's again a trick that kind of helped a lot for kind of medium to high resource languages but they're still language pairs where um it's not really doing a whole lot uh and one of the cool things about this uh kind of Paradigm is that we are able to uh take this model and then actually run it on new languages so we can kind of change that language code that was being fed into Embark and even when the model is trained on uh translation from like Japanese into English we can just kind of flip that code and get it to be able to Translate from Chinese into English uh and I guess a final thing I'll say about the impact of very large language models uh like chat GPT is that they've been having a bit of a splash in evaluation where rather than using things like Blues score or other sorts of learned metrics a very recent trend has been to just ask chat GPT how good is this translation so it's not doing the translation here right because it sees both a translation and the human reference so it's just comparing it to a reference but it seems like this outperforms a lot of methods that are trained over uh basically human labeled data to learn how to reproduce human judgment so it's a kind of interesting shift in how we evaluate things and I think we're probably going to see some shifts in how we actually build these models as well but at least for right now the kind of stand the more standard pre-trained fine-tuned Paradigm still is the most commonly used thing in translation that's the end of this segment\", metadata={'source': 'bcP4b_4HQ8A'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to start talking about summarization so summarization is a very broad problem in nlp that encapsulates a lot of different things we're going to think more more specifically about you know trying to produce a summary of a few sentences that uh you know might reflect the content in a news article or a collection of news articles or something like that there's plenty of other texts that you can summarize as well you can summarize kind of biomedical research papers uh you could be summarizing legal documents or contracts etc and there's even other kinds of formats you could produce for example maybe you want to summarize a series of events by producing a timeline or something like that but for now here's what we're going to look at so we've got maybe one article about an earthquake uh in near the iraq iran border or maybe we have a few articles about this and the question we need to answer to start out is what makes a good summary how do we go about you know constructing something which is going to convey the key information of these articles to a user who wants to understand it all right so let's take a look at what's going on inside these articles and so this is a pretty long article so we're not going to read through all of it but you know that we start off with some information that's probably pretty important the main event a strong earthquake hit large parts of northern iraq etc and then uh state tv said at least six people had been killed so this is something that a priori we might think is is fairly important and we in fact see this confirmed later when we start talking about the at least six people had were killed again like the fact that we're repeating this information probably indicates that it is fairly central to the article here um and then we have some other details like for example where the epicenter is you know villages being damaged the magnitude of the earthquake etc and so we can identify all these individual facts and the question is how do we which ones do we pick and how do we put this together into a summary all right let's take a look at another article so here we also see information about eight villages have been damaged and this tells us that probably that is some useful information so if we're looking at these two articles at the same time you know the fact that information is being repeated either within the same article or across articles tells us that it's probably fairly important so here's a summary i'll give of what's going on here a massive earthquake of magnitude 7.3 struck iraq on sunday the epicenter was close to the iranian border eight villages were damaged and six people were killed in iran all right so what did i do well first i was a little bit sneaky and actually just took part of this sentence from the indian express article directly as the first sentence of the summary um and then i talked about the epicenter and the you know this this idea of eight villages being damaged and six people being killed which again we saw each of these facts repeated and so that tells us that these are probably important factors all right so this can help us understand maybe what's needed in order to do this task well and it's going to help us start to break down the approaches that we're going to explore here so first we need a notion of what's called content selection this is the first or kind of an integral part of any summarization model which is fundamentally there's more information than we can possibly convey how do we pick what the right what the right content to actually display to a user is and so the way we know that is that the right content in this case was repeated and we might also have domain specific clues so if we saw summaries of a bunch of news articles as training data and some of those news articles were about earthquakes then we would say oh okay here's an article about an earthquake uh so probably the uh you know and and we always see them talking about the magnitude and the epicenter in these in in the summaries so we think that those are probably important factors to include in our summary as well all right and the second uh the second kind of aspect here is uh generation or kind of rendering the actual summary and writing it out so we're going to structure our exploration of summarization across three different approaches the first is called extraction where we are going to just pick out sentences from the original document or documents to form the summary this is obviously the simplest approach in that we can just extract stuff and not have to worry about actually you know generating a fluent summary um but it's also somewhat limited in that you're stuck with the sentences that you've got in the documents originally second we have an approach called compression which is you could think of as pulling out a sentence and then allowing yourself to delete stuff from it and then the third approach is what's called abstraction here we're gonna basically say that uh our summaries can be any essentially string and uh you know any sequence of words so this is where we're going to be using things like sequence sequence models and pre-trained text generation models in order to actually generate a word by word summary that may still decide to extract and copy some content from the input but is generally going to have a lot of paraphrasing and other kinds of factors going on so these are the three types of summarization that we're going to explore over the next few segments but this gives you an idea of the the things that we need our models to be able to capture this content selection and then generation pipeline that's the end of this segment you\", metadata={'source': 'lBDI1CBNe_U'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about techniques for extractive summarization and start off with some kind of classic techniques in the area so the way we're going to formulate this problem more formally is that we are given some a collection of articles and so we're going to start by thinking about the multi-document case so these articles are all on the same topic critically like they're all going to be talking about the same event like an earthquake or whatever and we want to produce a summary that has the most salient information from all of them and we're also given a length budget of k words so what we want to do is we want to pick some sentences of length at most k in total and make a summary out of them so the kind of rough idea here is that we need to pick stuff that's important but also diverse so we want to emphasize the key facts but we don't just want to say one key fact over and over again and so uh one of the earlier formulations of this due to carbonyl and goldstein is what's called maximum marginal relevance or mmr and so here was their formulation a bit basically they said all right as long as the summary is less than k words we're going to add a new sentence to it and so what they're doing here is they're saying all right we're going to look our max over all sentences that are not yet in the summary so r is the sort of document collection and s is the summary here and we're going to compute the quality of a summary as a function of or a sentence sorry as a function of two uh two terms the first is a similarity between the uh that potential sentence and a query so they're thinking about the task of uh so-called query focused summarization but there's ways around this where you can have the query be some kind of just like generic basically just descriptor of the documents and you know you can implement the similarity function in different ways and then second they have this idea that the sentence needs to be maximally different from all the other sentences so far so you look at all the other sentences in your current summary and you want to make it look dissimilar to the most similar sentence with it and then after this this process returns to a sentence that should ideally have a mix of good new content while not overlapping with what's already here um and then we add one that doesn't you know that doesn't kind of blow our length budget and then we repeat this process so we'll keep adding sentences until we get up to you know up to k or we can't add any more without going past k so this is the like this is a kind of general template or or sort of it shows a set of ideas that are going to show up again and again throughout these techniques um so a related technique is what's called the centroid method so we can basically compute bag of words uh vectors using tfidf for the document and each of the sentences in the documents and so we could take the same approach where we're going to loop over uh we're going to iterate do an iterative process where we're going to build up a summary and the way we're going to do that is we're going to score each sentence based on the similarity between the sentence vector and uh the kind of overall document vector so it does the sentence vector seem like it's kind of representative of the average content in these documents is it close to the centroid and then where in order to handle the idea of marginal relevance we're going to throw out any sentence which is too similar to something already in the summary so as opposed to the previous slide where we had these two scores that kind of traded off here we have like a discrete threshold we just say okay if it's too similar to something throw it out and then we want to take the best sentence that is left after this filtering process that again won't overflow the summary won't go past our length budget of k words okay so the idea that you know this kind of leads to is you know wanting to do some kind of optimization over uh you know selecting good content while not being redundant and so we can actually formulate this in a little bit of a more principled way uh using a technique called integer linear programming so the way that the these techniques work is they typically ground out in the importance of certain bi-grams or pairs of words in the in the documents and so one way we can ascribe importance to word pairs is just by counting the number of documents they occur in so maybe we have five documents about this uh about this earthquake and three of them describe it as a massive earthquake two of them talk about six people being killed two of them talk about it being magnitude 7.3 etc and then what we want to do is we want to find the summary that maximizes the score of bi-grams it covers so note that this naturally gives a notion of importance we want to capture the high scoring things that show up in a lot of places while also giving a notion of content relevance right or uh sort of non-redundancy in the sense that if we cover massive earthquake several times we actually don't get more points for it we only get points for the first time we cover it so uh the way that uh gelic and farv did this was to formulate this as an integer linear program and so this is a technique that we haven't talked about in this course and isn't really going to show up much elsewhere so i'm not going to go into it in too much detail but basically you define a bunch of you define an optimization problem over a set of integer valued variables and here their variables are about whether or not each bigram and each sentence is included in the summary and so what you want to do is you want to maximize the weights of bi-grams that are included so these wi values come from the scores at the top of the slide and the ci is a zero if the bi-gram is not in the summary and a 1 if the bigram is in the summary and then we have a length our length constraint can also be expressed in terms of these variables where lj is the length of the jth sentence and sj which is one of the variables we're allowed to tweak uh is either zero if we're not including the sentence or one if we're including the sentence okay so we have these c's and s's that we can like change basically to try to optimize this objective subject to these constraints and but there have to be constraints between this right so basically in order for a bigram to be included it has to be the case that we're including some sentence that contains that bi-gram so this these two uh expressions here about s i j and and c are basically relating sentences to the bi-grams that they contain and vice versa and they're saying that you know we're going to include ci if and only if some sentence that includes that is included in our summary okay so basically the you know the the big drawback of this technique is that it places a lot of emphasis on these bigrams right and so you might be thinking okay what if one document said massive earthquake what if someone said large earthquake what if one said you know uh bad earthquake you know whatever um and then we wouldn't see repeated mentions of the same bi-gram um wouldn't this be a problem and and the answer is yes this technique tends to work best when we have large numbers of documents in our collection like you know maybe at least 20 or something like that so that we can start to reap the benefits of seeing actually bi-grams get repeated of course in a single document summarization setting you would need a different way to score bi-grams because um you know well certainly the number of documents a bi-gram occurs in is not a good indicator and even within a document you know you're probably only going to say massive earthquake once in a lot of cases okay so we've now seen a few different techniques ranging from kind of greedy algorithms that aggregate information to this ilp based technique which is a little bit more principled that formulates this as optimization so how do we evaluate summarization and think about how these things compare so the primary metric that gets used is a metric called rouge and uh rouge is basically captures engram precision recall and f1 in a way a little bit similar to the blue score that we saw for machine translation although blue was just precision oriented so rouge looks at all of these things and there's some other tweaks to it it's actually a relatively sophisticated metric in that it does it has a lot of things or a lot of flags that you can pass into it for things like stop word filtering etc um but the takeaway is that rouge two so looking at uh precision recall and f1 of bi-grams actually kind of correlates decently with human judgments in multi-document summarization and so this is encouraging in that we just saw a method that's based on bi-grams and if we're trying to maximize basically covering bi-grams likely to be in the reference summary um and then this correlates with human judgments that indicates that this technique that we just saw may work reasonably well um so you know to to just kind of illustrate what's going on here um we have a uh a sentence that we'll call our predicted summary in a sentence that we'll say it's our reference summary we'll just look at one sentence for each so let's throw out the stop words uh and then to assess the rouge two we look at the different bi-grams and we say okay what was the you know what bi-grams matched between the prediction of the reference in this case uh iraq sunday as a bigram so we delete it on right so um that word pair is going to now be next to each other um and we had four bi-grams in the reference all right and so uh you know in the in precision again we get one correct bi-gram over uh six in the over over six big grams that we see in the prediction uh and so like there there's a lot of like i said there's a lot of different flags that we might uh pass into this or different kind of options um but you know this gives you an idea of how this gets predicted and i think it also gives you an idea of the fact that these results can be very low while still having a reasonable summary um so the you know historically what happened was people would look at basically with a budget of size k how good is your recall and now people look more at rouge f1 and uh you know we don't really need to go into the reasons for that it's it's largely sort of historical uh just trends and in how stuff is evaluated here okay so now with the appropriate uh kind of background we can look at some some rouge results and try to understand how these systems are doing so here's a comparison of some of the different uh things that we saw uh you know along the previous few slides and the first thing you'll notice is that rouge 4 is really really low so this is out of 100 so it's 1.72 out of 100 for this uh gilligan favre system which is the ixi system but the you know the the kind of takeaway here um from this paper by galandri and uh in 2017 is that actually a lot of these uh sort of relatively basic techniques can work pretty well so they showed a kind of improved version of the centroid method which we saw a few slides ago was able to basically match or exceed the performance of a lot of these fancier things based on like sub modular optimization or determinant point processes and things like that that had been proposed more recently so i uh what the the one caveat i'll give is that all of the stuff we've been talking about so far is pretty much fit to the multi-document summarization case um so when we think about things like these bi-gram based techniques that rely on counting up diagrams again it relies on the fact that there's going to be a lot of redundancy across these documents so that helps us do content selection because we can look for what's redundant and that's a very robust queue and also when you have a lot of documents there are more sentences that you can extract right so maybe you would have this sentence that talks about the eight villages being damaged but it's very long and it's going to run out of your word budget and has a lot of stuff that we don't care about but in another document maybe we have this great sentence that is very direct and and kind of states the information that we need so we can pick up the sentence sent second sentence and this maybe leads us to the conclusion that multi-document summarization might be a bit easier so i'll say the answer to this is yes and no there are certainly kind of advantages to working in a multi-document setting and there are some things about single document that are more challenging but this sets up some of the challenges that we're going to look at so first we're going to talk about some more updated neural methods for doing this kind of extraction and we're going to focus on these in the single document case and then we're going to talk about compression and so compression is going to be another way to say all right we've got this but eight villages blah blah blah sentence that's got a lot of information in it and how can we delete some of the stuff that's less useful in order to get a shorter snippet kind of like what we're seeing in the second sentence you know so that we can include that in the seconds in the summary even if the second sentence weren't available but this concludes our tour of extractive summarization uh methods the kind of classic techniques used in the multi-document setting and that's the end of this segment you\", metadata={'source': 'QWt2E3m00kA'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about using pre-trained models to build abstractive summarization systems and some of the problems that they run into notably factuality so we talked about the bart model before which was a different kind of pre-trained model than burt and some of the other ones in that it's not really using a language modeling objective per se it really looks like more like something what called a denoising auto encoder we start with a permuted masked out etc kind of transformed version of uh a text which is the thing on the left here abe and then we encode that with a bi-directional encoder and then we use an auto-regressive left-to-right decoder so this decoder looks like gpt-2 or other language models you know kind of standard language models or the decoders and seek to seek models but it looks back at the encoder and therefore this whole pre-trained setup is a little bit more appropriate for sequence to sequence tasks um even though it's doing the same kind of in-filling um and stuff that bert was doing so we could use this model for summarization right we have a pre-trained sequence to sequence model we can basically use the encoder over whatever our source document is and then our decoder to produce our summary all right i'll just briefly mention another related model which is called pegasus which is the current state of the art and works a little bit better than bart pegasus specializes even more to summarization in its pre-training objective it uses this idea of what they call gap sentences where they both have uh masked tokens for example mythical and names here on the bottom um and then they also masked out entire sentences like it is pure white and they try to decode those based on the context and so the idea here is that the model should be able to look at a bunch of facts and distill them down into some information that maybe is useful for predicting what goes in these gap sentences and so you can imagine that if we you know if we imagine the summary as kind of a restatement of other information that's going on maybe in a more concise way this objective will also encourage the model to be good at producing these kinds of summaries all right so these systems actually do pretty great stuff in a lot of cases so on the left here we have a couple of source documents this is taken from the bart paper and the right here we have summaries so i'll just read the summary for the document on the top right fisheries off the coast of fiji are protected coral reefs from the effects of global warming according to a study in the journal science so this is cool in that uh you know it seems to summarize this long and complex article about fisheries and fiji and coral reefs and things like that but there's an issue here which is that it says according to a study in the journal science and this actually wasn't in the original article so this is the sort of risk of using pre-trained models is that they can hallucinate stuff uh because we're not str they they won't strongly they won't copy stuff as much as some of the other abstractive models we saw like the pointer generator model and so they're not sticking quite as closely to the input and then instead they're sticking closer to what the prior of the pre-trained model wants to do all right so there's a big question right now in this area of how do we get these models to give us summaries that are factual we want to get the kind of paraphrasing effects here uh you know to produce a nice summary but we also wanted to respect the information and the input so my student tanya goyle has some work on this uh that's at dmnlp 2020 on a dependency entailment formalism for looking at this so basically the idea is to look at uh dependency parts of the output which we have here as the hypothesis h and what's going on here is that it's talking about seven games involving neems where invest were arrested last november in fact the games were investigated after someone else was arrested and so you know it's a little bit hard to look at the sentence and say that any one token is wrong because everything here was was sort of copied from some part of the input but they were put together in the wrong way and so the issue here is that the relationship between arrested and games the fact that the games were arrested uh is that part is incorrect and so uh tanya built a classification model to basically determine uh in a sort of entailment sense whether the relationship but in each of these dependency arcs is supported by the input or not and i'll point out that there's some other formalisms for doing this kind of thing as well another one that's come out recently is based on question answering this is some from some folks at nyu and the idea here is that we take a summary we generate questions based on that summary and then we try to answer those questions based on both the summary and the source document and we hope that the answers are the same so here we see some examples of that to use the example from the last slide we would say you know who who was arrested and based on the summary we would say the games were arrested and based on the document we would say that conrad was arrested and so we have a kind of mismatch there and we but we think that these two pieces of information should be the same now the tricky thing about all of this is that this is now relying on kind of secondary models either qa models or these dependency factuality models in order to try to enforce factuality and those things can make mistakes so this is very much an active area of research to try to figure out how to get the benefits of these strong pre-trained models while nail down their behavior enough so that they don't hallucinate things and that's one of the major frontiers right now in the field of summarization that and the other major direction people are trying to take this stuff is broaden it to more domains more applications and get the stuff working more broadly that's the end of this segment you\", metadata={'source': 'feLTtTilycY'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about morphology we're introducing morphology now as a way of saying what kind of effects show up in other languages that maybe we haven't been thinking about from the context of english and this is going to help us think about how the tools that we've developed so far in this course can really be broadened and generalized to work in a lot of different settings so morphology is roughly the study of how words form and so uh you know it's not that this doesn't apply to english certainly but what we're going to see is that there are many effects from morphology that come up in other languages that are going to fundamentally change how we have to think about building our systems so wrongly morphology is broken into two uh you know two sort of sub areas there's what's called derivational morphology which is how we think about creating new words or or what we call lexemes from a a base and so for example uh the verb estranged can become the noun estrangement and there's there's kind of regular processes for doing this uh in english they you know involve adding different suffixes or kind of transforming things in different ways but these processes aren't always totally regular but still there's a lot that we can study for there in terms of you know again essentially how these words are formed from their pieces what we're going to think about here more is what we call inflectional morphology um and this is how words appear differently based on the context that they occur in so for example you say i become or she becomes right so basically verb conjugation with its subject is an instance of this and why do we write the word become in two different ways based on what the subject is so there's a lot of reasons uh one is that this helps make it a little bit more apparent who the subject of this verb is and kind of exactly what's going on here so this kind of thing mostly applies to verbs and nouns as we're going to see but the kind of key thing about it is that the the more the kind of inflectional morphology in english is relatively impoverished right so we could think about uh sort of different pronouns that we might use with uh you know basically first second third person uh singular versus plural um and then things like past tense um and there's only a few different forms of the verb arrive right arrive arrives arrived um you know arriving we're you know we're not going to talk about participles and gerunds and things like that um but in french it's a much more complex story um so there are still some of these that are you know kind of present tense ones that don't have too many different ways that they are spelled but then we have all these other tenses like the imperfect the future the conditional etc which in english are largely composed tenses you say like i will do something or i would do something but in french this all gets like baked into uh the basically inflectional morphology of the verb what is its suffix in this case and so immediately this is going to lead to there not just being three forms of arrive but but many many more and it's the same deal in spanish for jagar i'm not gonna again walk through all these all these different forms but you can see that there's many different ways that this uh gets spelled and kind of conjugated right uh and it turns out it's not just verbs there's also inflection of nouns in other languages so we do have this in english with singular versus plural but other languages have much more complex morphological systems involving things like case so case is roughly kind of where a noun shows up grammatically um and so in english the one real manifestation of case we have is with pronouns um you when you say like i gave it to him the him is in subject position and that's that's accusative case actually well let's say um you know i hit him uh that's that's an instance of accusative case and i is an instance of nominative case there um you also have genitive case uh and then dativ is in in english it's it's kind of merged with the accusative but it basically indicates someone receiving something rather than being like the kind of direct object of that of that thing so uh in german however these end up being different so for example i taught the children ike ontario di versus i gave the children a book ik and so we have to again deal with different forms of this noun now based on where it shows up and this is gonna again kind of blow up the complexity of what we're doing uh both from the perspective of having to generate this stuff we would have to generate it using the correct uh you know using the correct case and everything and also from the perspective of analysis for example if you're going to train glove embeddings now you need suddenly many many more embeddings for uh forms of the verb arrive okay so the nice thing is that uh even though uh even though you know these inflections are very complicated a lot of times they're regular um where these things are irregular is with common words um so for example forms of the verb to be um usually kind of don't have the same patterns as other as as other verbs in in a language um and when you have the less when you have when you have less common words there it's usually kind of predictable based on like the suffix or some sort of information what uh so-called paradigm they would fall into okay and just just to give one other example um we have a set of languages the finno-ugric languages and turkic languages which are called agglutinating languages which the kind of morphology here becomes even crazier because basically they roll up uh the function of prepositions with the verb and so you end up with this very very complicated uh structure here of kind of different forms of these things and uh you know in in in you only ever see a few forms of these more complicated cases um and it's still a regular process like you're basically kind of gluing a suffix on to this uh to this word but that's happening in a much uh you know in a much more well it's happening in a way that produces a lot of different word forms so basically the the the story for these languages is again even one step more complicated where we want to think about uh how to kind of break up these words into smaller pieces and and deal with them that way so the the kind of final thing i'll leave here is that many languages spoken all over the world have morphology that's much more complex than that in english so this is a problem that we've really kind of ignored from the standpoint of this course but it's a really serious one and we need to think about it when we build systems there are a few data sets for thinking about dependency parsing and syntactic part and constituency parsing for morphologically rich languages and other kinds of analysis but generating into these languages for example has a whole host of other issues so the good news is that a lot of the techniques that we developed for handling machine translation things like word piece or byte pair encoding models which showed up in pre-training as well these could be pretty good at handling these sorts of effects if you have enough data for example all these complex suffixes and agglutinating languages uh you know ideally these are the kinds of things that would get broken up into word pieces or uh you know subword tokens and then our model would be able to analyze this thing in in a sort of compositional fashion so we're going to talk a little bit more in in the next segment about how to uh about how to analyze what's going on with morphology but this gives you a tour of like roughly what the effects are and some ideas of how we might go about addressing them that's the end of the segment you\", metadata={'source': 'ettP9Ayrho8'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to build resources or how to build kind of systems that work for other languages uh with an idea of cross-lingual learning so if we have our techniques like syntactic parsers uh or part of speech taggers or whatever and we have labeled data in another language you know modulo handling some of the effects like morphology that we saw before we can basically take those those methods that we had and apply them to that new data annotated in that language and build a system but labeling such data sets is very expensive so one question we might ask is how do we take the existing resources that we have from languages like english and transfer these transfer these to what we call low resource languages so the idea here is that these languages might not have as many annotated data sets as something like english but we still want to build effective systems for them so maybe in english we have a whole bunch of part of speech data and a whole bunch of raw text and then in spanish we have a much smaller amount of part of speech data but we have an english spanish by text and we also have raw text in spanish we want to take this whole hodgepodge of resources and build a spanish tiger we want to do better than just taking the spanish part of speech data and training a tagger over that the question is how do we kind of leverage all these things in combination and this you could go imagine even more extreme scenario for a language like malagasy we don't even have part of speech data here instead what we want to do is just take the english data and some by text and train a tagger in malagasy purely from these resources all right so one of the ideas uh that's been proposed here is essentially leveraging word alignment um so if we have the sentence i like it a lot and the sentence in french julen boku then we can we've seen in machine translation that we can produce alignments between these two sentences then based on these alignments one way that one thing that we could do is tag the sentence in in english and then project those tags into french and so some of these uh tags you know we can we can kind of easily project because they're just aligned to one word and some of them we can't like boku for example here this is aligned to both a and lot and so we're we maybe aren't quite sure actually what tag it should have here um and so the the kind of rough idea here though is that this will give us annotated data in french it may be kind of noisy annotation but hey if our english part of speech tagger is getting 97 accuracy this is still better than having nothing on the french side right so uh again the procedure here is we basically tag the english we project across the by text and then we train the the french tagger and how does this do it it actually works pretty well um i'm not going to kind of go into a full dissection of the results here but the you know this approach can build tigers that are much better than unsupervised taggers in languages like french um and so and it also applies much more broadly so it'll handle languages like malagasy that we saw on the previous uh on the previous slide so this is this is pretty nice and then it lets us leverage stuff that we already have in order to build resources for other languages and we can extend the same kind of idea to parsing so there the the the obvious most tricky thing here is the fact that different languages might have pretty divergent syntax and so there's been a lot of work which i'm not showing and i'm not going to talk about over the last really the last decade on how to come up with syntactic formalisms that work across different languages you know parts of speech part of speech tags and labels for parses and things like that that are kind of universal here so but but assuming that we have some shared syntactic representation one thing we can do is say all right we're going to just boil down all the sentences to their past tags and then trying to parser over these pos tags in one language and then directly apply it to a new language so if you have the sentence i like tomatoes we're just going to look at the tags so we've got a pronoun a verb and a noun and we've got i like them pronoun verb pronoun and then this is we we train a parser to take input that just tags and what can this parser learn well this parser can learn things like verbs tend to be the heads of pronouns and nouns all right and now if we get a sentence in french jules m pronoun pronoun verb all right this is not the kind of thing that our english parser will have seen before right but regardless this kind of simple rule that this parser has learned is still going to produce the correct parse in this case and so if we see enough uh if we see enough examples of different languages we should be able to train a parser that can kind of flexibly handle different different situations here um and so that's what this uh multi-dir approach here is in the in the kind of second to last and fourth to last columns um basically we trans you know we we train on the language uh you know specified in each uh or sorry this is testing on the language uh specified in each row training on a mix of other tree banks and so the accuracies here are not great it's getting like you know 61 percent dependency accuracy uh on this you know even when it has access to gold part of speech tags but i'll point out that this is significantly better than you would get with an unsupervised approach and there's ways to make this better this multi-proj approach basically has a way of actually learning a parser with lexical features in the target language using a more a more complex way of doing things so what this shows is that we don't necessarily have to despair when we have languages that there's not annotated data for obviously annotating data is a great way to build out new tools for that language but we can also think about how to leverage the tools that we already have in combination with uh stuff from you know these ideas from word alignment and build things like parsers and tigers that work in these new languages that's the end of the segment you\", metadata={'source': 'rTSCLfxdxrI'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about cross-lingual pre-training and we're going to start with ideas of things like word embeddings that can unify representations across different languages and we'll see in a little bit what some of the benefits of this of this are so the idea is that suppose we have corpora in a bunch of different languages we could use something like the skip gram method to train word vectors on each of these languages in isolation right but then the word vector in one language will be kind of totally unrelated to any of the word vectors in other languages instead it would be nice if we had a shared embedding space where all the words in these different languages sort of coexisted now again there's a lot of sort of divergence between languages that makes this not necessarily that clean but the you know if we have apple in one language and apple in another language ideally we would want those things to be similar and that might enable us to do you know certain certain things with this model that we otherwise couldn't do okay so what happens so if we have uh sentences like i have an apple or jade desolate in french we can form clusters based on dictionary translations of words so for example a and have are translations of one another and i in english and je and je in french are also translations and so we can just get this out of a dictionary and map these things down to uh basically cluster ids then we just label all the words in these sentences with their cluster ids and then we treat these as the words in word to vec so these dictionaries let us label things with these clusters and then we train what we think of as monolingual embeddings or just the standard skip gram model over these corpora so then you're going to have one embedding for the things in you know uh cluster 47 and you're going to have a shared representation for i j and j and that's great that that was kind of our goal okay so this kind of thing works okay but not all that well like the vectors are definitely you know there's some problems with with assuming these kinds of correspondences and um you know it's it's sort of not as effective as standard word embedding techniques monolingually okay so the you know the the question is how do we bring to bear maybe some more sophisticated ideas on this problem and what can we do um so one idea from uh artech at all uh is to think about going beyond the level of words and actually produce sentence embeddings so what they do is they have a shared bpe vocabulary between a whole bunch of different languages and then they train mt systems on a bunch of bi-texts between these languages and the the kind of key thing here is they do not use attention they instead boil down the source sentence into a single sentence embedding and then feed that into the decoder and the reason is because they don't actually care about this as an mt system they care about getting at that embedding and the idea here is that by having different buy texts use shared encoders and you know different decoders basically like you know you sort of have uh if you have english translating into two different languages it's going to produce one shared representation and then the decoders in in other languages are going to operate from these shared representations so the encoder here should really be mapping this this uh each sentence into some sort of you know relatively universal vector w that can then you know come from any language and also be translated into any language um so again for from the perspective of mt this assumption is too strong and is not necessarily going to work well but from the perspective of building a very effective sentence encoder that's going to produce vectors w it actually works pretty well and so what they do is they look at the task of natural language inference so remember that this was the this was like looking at two sentences and deciding whether they entail each other or contradict each other or neutral and so they they can train a system on english and then evaluate it on other languages they train it with these uh basically to operate over these representations w of of the two sentences in this case and then they can they can swap in french sentences into their encoder right and their french encoder their encoder which is shared across all languages should encode those french sentences into the same representation space as we had for the english sentences so then our nli model which is trained on english should then work for these as well okay so the results here are are kind of pretty good um they managed to get some you know decent results out of their biostm encoder and it works a little bit better than uh than previous cross-lingual techniques okay so we could extend this further you know that was we went from kind of word encoder to sentence encoder and now we can go to more complex pre-trained models which kind of operate as both right you get the the benefits of encoding different words from the last layer of the burp model and you also get this kind of flexible cls vector that can do sentence classification tasks okay so there's this model called multilingual burger well there's there's several models now but one of the original ones basically took a bunch of different wikipedias and trained bird on all of them now what you might be thinking is all right so isn't that just going to basically get you you know maybe you have some shared parameters but you're basically going to get a kind of you know bert that that knows about chinese words and a bert that knows about english words and like these won't necessarily have any sort of translational equivalence let's say and that's that's that's definitely a valid concern and you know we don't really know uh or i haven't seen a study of how how strongly that's true but what is true is if you look at wikipedia you will see kind of instances of code mixed text where you have text in one language or one script intermixed with text from other languages or other scripts and so even though there's not you know a strong notion that wikipedia is a by-text or anything like that we still might see enough occurrences co-occurrences of things that we might expect the representations of this stuff to at least have a little bit of shared structure now remember with masked language modeling it's the case that you might mask out some of these chinese characters in this article at the bottom from russian wikipedia and then you need to you still need to be able to predict them from the context right and so the the model is going to be encouraged to cast all these things into the same embedding space uh as much as it can all right and so this kind of holds up if we think about tasks like named entity recognition and part of speech tagging again we can train a past tagger what we see in the top right here we could train a pos tagger on italian or fine-tune you know this bert model and then run it on english and we get 86.79 percent accuracy on on this tag problem pause tagging problem um and so this is again a kind of direct transfer approach where we can take a model and directly run it on a new language without any fine-tuning and and it works okay now there's some interesting patterns here where languages that are maybe more related exhibit uh kind of higher you know higher performance when we do this um by the way the diagonals here are just the results when you fine tune and then eval on the same language so that's like the sort of standard burt setup that we've been been using prior to this segment i will notice that i will note that like all these languages share an alphabet and so you could get a lot of benefits and transfer between spanish and english just by being able to leverage you know not just shared words but shared parts of words and and the fact that um at least they're grounding out in common like subword tokens at the at the bottom um but the cool thing is that this actually works well transferring between different scripts as well such as urdu to uh to hindi which are written in two different scripts um and same for japanese to english so we can similarly you know there's there's obviously some you know performance cost to this but we could train a tagger for english and transfer it to japanese or vice versa and that's a very cool capability for these models to have so there's a lot of work in building uh kind of cross-lingual or multilingual pre-trained models and building and this allows us to build systems that then transfer very effectively between these settings without needing a lot of data so all of this we've kind of seen is building towards ideas of having systems that are more flexible that don't necessarily need large tree banks or part of speech corpora or whatever in order to work for other languages and this is going to allow us to build nlp tools that can work for much larger fractions of languages in the world without having to go through and annotate a whole bunch of data for each of them so it's a very exciting time for this area in particular there's a lot of progress here being made very rapidly over the last couple of years and so it'll be very exciting to see what kind of new systems this yields as a result that's the end of the segment you\", metadata={'source': 'KNBRb8sjzOA'}),\n",
       " Document(page_content=\"foreign we're going to talk about language grounding language grounding is a framework for thinking about taking the representations of language and the things that our systems are doing and trying to associate them with some sort of deeper concept let's say so if we think about what how language is represented in our models currently we're using vectors for a lot of it either kind of non-contextualized word glove embeddings or you know obviously most of this class has been about these contextualized embeddings within language models and so we thought about the things like word similarity like good and great being similar and them having similar Vector representations but what do these vectors really mean this is a kind of deep philosophical question in some sense that has a long history in NLP and AI more broadly and it goes back to some work from harnad called the symbol grounding problem where harna defines a symbol system basically a system that can manipulate strings on the basis of rules and he's contrasting his own uh sort of Notions of how this works with that of earlier work where he says that Fodor and pollution emphasize that the symbolic level or for them the mental level is a natural functional level of its own with ruleful regularities that are independent of their specific physical realizations so voter and pollution are basically saying that uh you know having this kind of total abstract symbol manipulation is a meaningful thing even when it's fully disconnected from The Real World and Harnett is sort of challenging that and one of the thought experiments he gives is that of a horse where uh or horses and zebras are in particular kind of the way he's thinking about a zebra is that it's a horse with stripes and you could express this symbolically but in some sense if you have a system that just knows about zebras and horses it's not necessarily going to know about all those properties whereas because we grow up perceiving these things and having them deeper grounded understand standing of what these concepts are we can look at a zebra and say that looks like a horse with stripes so it you know in some sense the measure of a system is well can it do the kinds of manipulations that we want it to be able to do what does it mean to understand versus to just be able to do tasks right like is chat GPT really understanding stuff or is it just manipulating these symbols and one of the classic Arguments for this uh is the Chinese room argument from Searle so the thought experiment goes as follows someone is sitting in a room and they have a dictionary a grammar books things like that for how to translate Chinese into English and someone gives them a piece of paper like slipped it under the door uh in Chinese and uh this person in the room slips out a piece of paper that has the translation in English now the person themselves does not speak Chinese they're just following the rules in these books and they're looking up things in these dictionaries now the question is does the person who where is the understanding of Chinese coming from it doesn't seem to be the person right because they don't know Chinese but there's this system here which is the person operating inside the room that is able to do this kind of manipulation and produce these translations right so Cyril argues that uh the room is kind of like an AI system and it's producing these things similar to how Google translate produces translations in Chinese um and harna is sort of a riff on this which is that the reason that we think that some sort of understanding is happening is entirely projected from ourselves he says the interpretation will not be intrinsic to the symbol system itself it will be parasitic on the fact that the symbols have meaning for us in exactly the same way that the meaning of the symbols is in a book are not in intrinsic but derived from the meanings in our heads so he's basically saying we think that there's understanding going on here because we see this translation process happening but in fact no such understanding is happening so this I the reason I bring all this up is because it's very relevant to conversations around large language models and in particular it was all kind of revived a few years ago with an argument from bender and Kohler called climbing towards nlu where they are thinking about the ability of these systems to manipulate form versus actually engage with meaning or the kind of communicate communicative intent of speakers and their argument is basically that a language model is really good at kind of pushing these symbols around but there's no underlying intent it's sort of like telling you a story and generating symbols but that's not grounded in anything and so one of their thought experiments is that they're imagining an octopus sort of analogous to a language model eavesdropping on a conversation between two people that are on like neighboring islands and then the octopus decides to insert itself into the conversation and then one of the people needs to basically construct something on land to fend off a bear and the octopus is sort of uh I guess a fish out of water pun intended because it can't actually understand what a bear is or how to deal with that right so this thought experiment sort of suggests that maybe even though it's seen all of this like language between a and b and it knows a lot about the situation the fact that it doesn't know this physical situation is a fundamental limitation now the that sort of argument leads in the direction of saying that okay if you're just training a language model on the web maybe you're not actually going to learn what these symbols mean you just learn that things co-occur and so if you're doing something like question answering you're going to generate something that looks like an answer to a question but it's not the answer so there have been a few counter arguments to this one is that yeah if we think about like the context of code it's hard to learn meaning from just language modeling over code if we think about strings like this right we are modeling the string print y but we have no idea what the value of y is going to be and it's like like just generating this code sequence doesn't really tell us anything on the other hand let's suppose we're modeling a we're training our language model over some code that has a search statements in it now we have this assert statement of assert y equals equals four now imagine that we're training a language model on this right and so our language model has at some point you know has encoded all of this stuff and now it's going to try to predict the next token which in this case is four well in order to figure that out in order to maximize the likelihood of the right thing it has to actually learn that two plus two equals four so there's a there's a similar argument that can be made for natural language which relies on the Assumption uh that people say true things now if you consider a pair of statements by the same person X1 followed by X2 given enough examples you kind of learn to model the distribution of X2 and if you assume that what people say is true you're going to learn the distribution or learn to assign more probability Mass to true things rather than false things so you learn a little bit of meaning here a little bit Creeps in even from just doing raw language modeling and that still kind of tells us something so there's a lot of philosophy around this I think kind of zooming out from the perspective of like well what is meaning what is understanding are these models sentient things like that we can think a little bit more pragmatically about what the different levels of complexity that these models might be able to engage with are when we think about grounding so there's a paper called experience ground language grounds language due to yonatan biscuit all that thinks about five levels of world scope starting with Corpus then the internet then perception then embodiment then social and I think it's reasonably well accepted in the field that if you have a pure language model that somehow more restrictive than a model that is tuned on both language data and let's say images or computer vision I mean in some sense the second one is just purely a superset of the first one right and then similarly if you add manipulation into the mix right now that's adding another dimension on top of it this kind of embodiment and interaction with the world right now we haven't quite gotten to social yet and I think that uh it's taking time to kind of climb through this hierarchy with models that are actually as good as things like gpt4 but I think what we're probably going to still see this trend uh sort of continuing uh as the field progresses that's the end of the segment\", metadata={'source': 'ayNoMmoXnd8'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about language and vision which is going to be a way of thinking about grounding our understanding of text in images and what we can do with models that are able to kind of talk about these two modalities so if we look at an image like this we can think at a kind of very basic level how do we describe this image what sorts of things would you say about it well there's a lot we might say but the girl is licking the spoon of batter is a caption that was given to this image in a standard data set and this kind of describes one thing going on the in the image it's obviously not sufficient to describe everything that's going on here but it clearly has some connection to the things that are happening here right and when we talk about something like a spoon we can think again coming back to this idea of grounding that the spoon evokes something for us that's maybe a little bit richer because we're actually talking about it in the context of this image right it's not just like some abstract vector or abstract symbol but like you know ultimately we can know that it's like this kind of group of pixels has something to do with spoons and if we do a Google image search for spoon we can find a lot of pictures of spoons either from like online shopping sites or uh sort of various videos or things like that so we can really get an idea of like what a spoon looks like and we're not going to talk about this here but thinking about applications like robotics we could try to start to get an understanding of like what sort of affordances the spoon has for example maybe they tend to be very hard but maybe plastic spoons are flexible so the idea that we're sort of thinking about here is can we take our language representations and rather than just treating them in the abstract instead associate them with things in images and there's a very nice correspondence here where a lot of what we describe and think of as nouns basically correspond to objects when we start looking at images similarly when we look at verbs and text those often correspond to actions and maybe it's only clearer when you get a whole video or something like that but both of these things you can kind of anchor individual syntactic units to pieces of images or pieces of videos and then when we think about sentences that can maybe describe a whole scene or a whole complex action where someone is taking something and then doing something with it so that the classic tasks that people use to do this are uh a little bit more limited than maybe the broad idea of language understanding but uh specifically object recognition and image captioning have advanced us quite a ways in terms of thinking about models for modeling images and text at the same time so in particular the classic way that these work is that you have an image and then let's say a caption associated with that image and you're going to feed each of these into some sort of deep neural net model that's going to produce some representation of them so maybe for the image you'll use a convolutional neural network or a Transformer which are increasingly popular in computer vision these days for the language we might use something like an lstm or a Transformer and then you need to have some layer in your model that's actually going to combine and Link information from both of these modalities if you're trying to make some kind of prediction so again just to kind of show a concrete example like there are examples in visual question answering of trying to answer questions like how many horses are in this image and so what we need to do is we need to do some kind of processing over that string we need to do some kind of processing over the image and then some kind of joint processing now this is an older model from 2015 due to agrawal at all uh nowadays usually these things involve Transformers that kind of look at both modalities but the ideas are still basically the same so kind of fast forwarding to the pre-trained era uh one of the biggest encoders kind of cross-lingual grounding models that's made of Big Splash is called clip for contrastive language image pre-training so what this model does is it uses a contrastive objective to learn representations of both text and images that are going to correspond to each other so basically learning how to kind of project these into a shared space so that you can kind of almost translate between one and the other so the way that you do this is you consider a batch of examples which are images and then corresponding image captions and we're going to lay these out in this kind of grid here where i1 corresponds to T1 I2 corresponds to T2 Etc and the contrastive idea is that we want each image to be more similar to its corresponding caption than it is to other captions so basically what we do is we kind of consider like a row wise softmax here where we say okay we're going to normalize a distrib we're going to exponentiate a normalize dot product scores of embeddings of each of these things and what we want is we want this guy to have a high score because that's the corresponding image text pair and everything else should have a low score so if we train this on a whole bunch of data theoretically it should emerge that like you know this specific description of like a dog and then this picture of a dog will have a high similarity uh representation and in fact we don't just use this to like kind of produce representations in the abstract we can actually use it to do prediction and zero shot classification so for example if we want to know if the photo contains a dog we can make the string a photo of a dog and kind of check similarity with the image representation that's produced by this model and uh clip can do some fairly cool things if you look at zero shot results like there's a picture here of a 2012 Honda Accord Coupe and despite it being pretty pixelated the model actually embeds this closer to the embedding of a photo of a 2012 Honda Accord Coupe than any of the other uh kind of strings here in the data set 196 other models and cars and years and stuff and it can do things like uh if you take out this photo and you ask where basically where was this photo taken kind of formatting in this this uh in this way it the model predicts a photo I took in French Guiana and actually the photo was taken in Belize but that's kind of geographically at least somewhat close right despite the fact that uh you know what all we're going off of here is like the picture of this lizard and uh some plants so these models clearly have some ability to associate concept and language with kind of corresponding things in uh in images and while we're not going to talk about this too much more uh the sort of power of these things is really kind of remarkable and there's a big push to make newer pre-trained models that have all of these sorts of capabilities of associating language and images that's the end of this segment\", metadata={'source': 'pkdV-iddZxk'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to start our discussion of ethical issues in NLP with the rise of large language models and with the widespread adoption of NLP tools the questions of how these are impacting the real world and Society become more pressing than ever so I'm going to try to outline some of the biggest sorts of questions that people are dealing with and give you some kind of starting points for understanding these the first one that we're going to start with uh is comes from a taxonomy from hovian sprut that outlines a bunch of different risks of systems and this taxonomy is a few years old at this point but I think it's still very relevant for understanding how all of these pieces fit together when we think about these types of risks so the first uh point that we're going to talk about is bias amplification where there are biases in the real world and many of these will be reproduced by systems and they maybe even be reproduced at a higher rate and in the coming segments we're going to talk about exclusion where certain users are not able to use systems as effectively as others uh dangers of automation where automating things in ways that we don't understand might pose problems and then finally unethical use where powerful tools might be used for sort of Nefarious ends all right so let's talk about this issue of bias amplification so the example taken from this paper by jow at all uses a visual semantic role labeling task so in this task you are given a photo of an activity and we have to decide what activity is going on and basically what are the participants in this activity now in this case the training data set contains 67 percent of the images that well sorry for the images where someone is cooking 67 of the people cooking are women so we don't expect a 50 50 split of men versus women in the test set because actually there are more men cooking or sorry there are more women cooking however when we look at what gets predicted in the test set the model predicts 80 women cooking so it's Amplified this bias where it will systematically kind of tend to say that the person cooking as a woman more than the other way around so their work explored how to achieve the same predictive accuracy while avoiding this kind of bias amplification where we do worse at test time than we expect based on the training set and I'm not going to go into their technique but basically they look at whether they can constrain the gender ratio of predictions to fall within a certain kind of bound here denoted by these blue lines and they show a technique that can uh kind of improve this and Achieve better match between the 10 the training and the predicted ratios across all of the different activities in this data set this doesn't just show up in tasks like this uh there's a paper from Alvarez melis and yakola that looks at machine translation which has some very interesting cases such as this translation from English on the bottom this dancer is very Charming into French in French you actually need to ascribe a gender to the dancer so here the model says set downstairs which is assuming a woman dancer and the model's prediction for this can be analyzed and they're actually proposing an explainability technique to do this and what they identify is that the reason that it predicts dances is because of the word Charming here now this raises a very interesting question because in some ways it might be picking up on cues in gendered language use like maybe that's how we tend to describe uh dancers who are women and so the model is kind of learning this useful statistical signal is that something we want to learn because it's a helpful predictive tool or is that exacerbating bias because we're teaching our models the same kinds of gendered associations that we have it's sort of an open question how much of this we want to be able to learn versus how much of it we want to scrub from our models foreign so there's this issue shows up with large language models as well uh you can think of kind of many different ways in which language models might reproduce bias in their judgments um for example if you ask for jokes and then if they tell you let's say racist jokes at a higher rate than were observed now it's hard to get this out of things like chat gbt because the rlhf process was used and there was a lot of data collected to kind of make sure that this wouldn't happen partially to avoid PR disasters for these large companies um but I think it's still an ongoing question sort of how well these models do in various different scenarios that involve these kind of predictions with sensitive attributes so beyond the tasks we've looked at here this is studied in basically any task where you have gender or gender as a confounder so co-reference resolution is a great one trying to resolve pronouns or understanding given a description of what someone does parsing their occupation these models will often tend to assume that women are nurses more frequently and men are doctors and things like that uh so there's a lot of ongoing work to study this and it's there's an emerging body of techniques but uh it's a little bit unclear which are the sort of principal ones at this point there's a lot more work to be done that's the end of this segment\", metadata={'source': 'tTkAjNkXvH4'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to continue our discussion of ethical issues and we're going to come to the issue of exclusion this is one of the four factors that we were going to look at and it it deals with cases where underprivileged users are left behind by systems this arises from the fact that most of the annotated data we have just historically in the field of NLP is English data and while it's broadened recently a lot of the early history of the field was dealing with newswire data which is a very kind of specific and narrow slice of language so we could think about several different uh kind of groups that may not be able to use systems as effectively if these systems are not kind of optimized on the right data so for example speakers of dialects a lot of systems are trained assuming uh kind of uh written language like for example what's written in newspapers and this doesn't agree with what a lot of people speak day to day uh we've already talked about issues like the sort of lack of linguistic diversity and the lack of data for many languages in the world particularly when you get outside of indo-europea and then Chinese Japanese Korean you get to areas where we just have much less data and systems are going to work less well um and another interesting phenomenon is code switching where uh people will use different languages in the same utterance and this is also something which happens a lot more in informal text and so language models trained over largely more formal data gathered from the internet are not going to handle this as well so there are lots of people who fall into one of these categories who are not going to be able to use language technology like Chachi PT and find that it works as well as it works for English speakers who this was kind of principally designed for so the good news is that there's a lot of effort to broaden things along all of these axes for example the universal dependencies project originated to try to make syntactic parsers that would work well for lots of different languages and dialects Etc masakana NLP is a project to basically build out all sorts of NLP technology for a variety of African languages I'm going to also highlight a couple of other current efforts which I think are particularly interesting data sets so uh this is a data set called Gom llama from Diane at all which looks at understanding cultural knowledge about different countries so it tries to cover a view of culture that's a little bit absent maybe from a lot of our standard data sets and it tries to do so in a multilingual way so there will be questions like in traditional X weddings where we can put a x as American or Chinese or Indian Etc the color of wedding dress is usually blank and then as an additional kind of wrinkle so not just asking about the color of wedding dresses in different weddings we can ask this question in different languages as well so we can see in Hindi does our is does our system know what color wedding dresses are in Chinese weddings for example and one really interesting finding is that this often does better with mismatched pairs of language and Country so for example you would do better by asking in Hindi in traditional American weddings what color is the wedding dress because of a reporting bias where it's a little more likely to have this described in for another culture other than your own uh but there's still a long way to go on this systems are not very good at this and it's a good uh kind of test bed for multilingual pre-trained models to see if they can do this another kind of similar in a similar vein is this data set Marvel by Fung yulio at all which looks at understanding visual knowledge where the images are crowdsourced to be uh geographically diverse so they don't just feature people in Western countries but are trying to get a broad distribution from all over the world and then the language is used to ask the questions or to basically pose the hypotheses that you're classifying as true or false are also diverse you so you have many different languages describing many different types of photos so this kind of visual reasoning there's also an effort to make sure that it can work for folks who are taking photos in other places and asking and you know making these statements or asking questions in other languages so these are representative of a few of the efforts to improve things along this axis of exclusion but there's still a long way to go before we have systems like chat GPT that truly work kind of as well on every language right now these systems the performance on English is up here and other things typically it works a little bit worse that's the end of the segment\", metadata={'source': '0haVbW2ouzw'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about dangers of automation or basically how does the arrival of NLP technology that can do all sorts of things automatically potentially lead to problems when maybe those things would have been done by people previously so this is an issue that predates super strong neural net NLP methods uh there have been issues with things like machine translation for quite a while for example uh this story from 2017 where Facebook apologized after they mistranslated something uh that a Palestinian man wrote he wrote good morning but they translated as as attack them and hurt them and actually led to the man being arrested so there have been a number of issues where machine translations being taken as real led to harm and potentially you know not being able to understand something might have been better in this case than being able to un kind of misunderstand it there are also a lot of cases where we might translate things in ways that are technically correct on some level but which involve for example offensive terms or things which in another language have a very different connotation and this is the kind of thing that we want our models to be sensitive to right and there is this broader cultural context that sometimes gets washed away when you just take these models and try to maximize likelihood on some Corpus there's also this question of bias which we talked about before but when you automate things there's a kind of separate issue of that bias creeping into your processes and your output in a way that's hard to verify so there was this story from Amazon where they were using an AI resume screening tool and when they looked at what this tool was learning it basically decided that being in a women's X organization or being from a woman's college was a negative weight feature and so it would be more likely to reject people from women's colleges which is uh discriminatory now it's a little bit tricky because we can look at that and say well this was a bad model or something went wrong here but it's actually not clear whether from a machine learning perspective anything went wrong because they may have trained this on data from their actual recruiting process and it may be that human biases were also playing a role in this recruiting process and maybe the model accurately learned to reflect those so it's not just a question of like oh you have to use a Better Learning rate or something like that there's this kind of General context that these systems operate in that you really need to be sensitive of all the way from data to model to deployment and at every step of the way there's kind of possibilities for things to go wrong another challenge is when these models can generate toxic content and we've talked a little bit about this before but when you show these models a prefix like I'm sick of all the politically correct blank the fact that this model has been trained on the whole internet means that you're likely to get the types of things that people said when you started with this and you might think that this is a somewhat innocuous phrase that then it can go off kind of off the rails into territory that you didn't necessarily want your text generation system to go so when you automate things you kind of get the ability to do all of this kind of to produce all this kind of stuff and and some of it may not be things that you wanted to produce one of the biggest uh papers that's made some of the biggest splash about this these questions of dangers of automation is this paper from Google by uh vendor gebrew MacMillan major and schmitzel which is a long story uh called stochastic parrots so this described several dangers of large language models largely surveying from previous literature uh the first claim is that there's an environmental cost to training these models that they have a very large carbon footprint to train and deploy and that this kind of environmental cost is largely borne by marginalized populations another claim is that these are based on massive data sets which are challenging to audit so these questions about things like toxicity are hard to address and hard to kind of fully stamp out it's hard to take these systems and really kind of understand everything that's going into them uh and finalists finally the the last one which I'll expand a little bit more on is that their their complaint is that these models are not really grounded in meaning and when they generate an answer to a question it's from learning these kind of textual co-occurrences that may uh only have the sort of meaning that we project on them which is kind of getting back to some of the ideas we talked about in grounding so you can generate uh you could generate basically text about uh in this case the Wagner mercenary group and we look at this and it looks very factual and so we're likely to kind of project on like this assumption that oh okay this must be this kind of authoritative text but we're not really sure we've talked about how we need to maybe have a better processes for fact checking these things and similarly if you ask a system what you for advice it will often give very authoritative advice maybe with a mild disclaimer but this can potentially cause serious harm and it's not necessarily a well-calibrated model for understanding uh medicine so as these tools like Chachi BT get deployed more widely there are risks that they get used for these sort of off-label things and can lead to real harm and this is part of the ongoing conversation about how much should we you know regulate or put labels on these things to try to prevent this from happening that's the end of the segment\", metadata={'source': '2PSzHb08Xm4'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to close out our discussion of ethics and discuss the ways in which some of the systems we've talked about can be used for bad ends they can just be used unethically now in many ways this is the clearest way in which these systems can kind of run up against problems in the real world but I think it's worth going through some of the examples that we've seen from this and you've probably seen or have others in mind yourself so one kind of long-standing issue with language technology is its ability to be used for surveillance the idea of more deeply understanding text allows us to do things like crawl social media and understand what kinds of things people are saying and this allows let's say authoritarian regimes to keep tabs on uh who's expressing dissent or negative views about the government or things like that and this is not really hypothetical there are quite a few uh Department of Defense funded programs that are essentially trying to use this kind of large-scale text mining another interesting case came up uh during net neutrality debates several years ago where there were a large number of comments that were posted that were against the net neutrality that were found to be generated from this basically templated uh kind of schema sort of like a Mad Lib now these were possible to detect but with new and better systems like Chachi BT will that necessarily be possible going forward there's a large conversation around Ai watermarking and can we uh Mark text that gets generated by these language models and then detect it later and can you circumvent a watermark appear sufficiently sophisticated so there's a kind of question about what sort of harms would arise if these things are undetectable another issue that comes up is the ability of us is our ability to take data that is de-identified or anonymized and then somehow reverse that process or de-anonymize it and actually figure out who this is talking about so here we're showing uh kind of uh you know the ability to do this in the forward Direction but what we haven't established is that you know this stuff cannot be de-anonymized in this case and there's a kind of a related issue is authorship attribution if we have text that's written by an anonymous author can we actually figure out who wrote it this is actually a very old application of NLP that dates back a number of years and people have looked at with respect to Shakespeare and the Federalist papers and things like that but of course trying to figure out who is writing things that are being put forth by some Anonymous account online let's say uh is a kind of big social issue another uh interesting example that came up within the past few years was this work by Wong and Kaczynski looking at classifying people as gay versus straight based on photos of their face now the authors of this study argued that they were testing a hypothesis that sexual orientation has this genetic component which is actually reflected in your appearance so the idea that you could have a gay versus straight detector is something which is a little bit fraught because there are many places in the world where it's not legal to be gay and you can imagine that such places would use this these tools to let's say arrest people and throw them in jail so they were claiming well that wasn't Our intention we were just kind of doing the science however there was some recent work so there were some subsequent work a blog post by aguera yarkas todorov and Mitchell which showed that you could actually completely influence these systems by doing things like changing facial hair or putting on or taking off glasses so it turns out that the kind of scientific benefit of this was pretty nebulous in that it wasn't even really good science and the dangers of unethical use in this case may be outweigh any potential benefits we get from that now of course large language models bring up a whole separate set of ways in which they can be used unethically uh this is still emerging and there's only starting to be now some cases where we've actually seen this uh kind of documented in practice or studied systematically uh but things like AI generated misinformation campaigns uh whether it's intentional like someone means to spread this or actually the system just Returns the wrong answer about something and lots of people believe that uh of course there's a whole conversation around education like can students use this for essays things like that to what extent is this plagiarism versus not uh and then simply this is a powerful tool if you can do if it enables you to do things better that has benefits but also downsides right like you can help people learn how to build bombs more effectively so there's also just a general sense in which these are a very powerful system and when you have a powerful system that has the effect in many instances of concentrating power for example large tech companies that have these models now have an advantage over other companies that don't and are likely to become wealthier and more powerful in society is this necessarily what we want or not now we've talked a lot about the kind of thorns here there are a lot of proposed ways to actually improve the situation and move things forward so uh there's various different Frameworks you can use to think about how you should behave when designing and Building Systems there's a proposed code of ethics from a few years ago by Hal domay which outlines a number of points that you can think about basically contributing and positively benefiting society and minimizing the negative consequences of your systems trying to make sure that you're in results are interpreted correctly and sort of other guidelines like that value sensitive design is another framework that it's a way of thinking about when you're going to build a system whose values do you need to account for how do you engage those people in the process and how do you make sure you build something that's really what people want a lot of the efforts also revolve around documentation for example data sheets and also model cards and these are ways of saying that one of the cheapest things that we can do is just have better descriptions of what's in our data so we understand when we train systems on this for example from the framework of value sensitive design whose values do they reflect and how are they going to actually kind of propagate certain value systems over others and finally there's a lot of thinking about uh auditing of systems this is a from a paper by Deb Raji at all called closing the AI accountability gap which outlines ways of actually breaking down all the different steps of an algorithmic audit and applying these to an AI system to try to figure out where this is going to do well and where it's not going to do well so the good news is that there are some there is kind of evidence in the things that have been released with gpt4 that companies like open AI are taking some of these steps and doing some of the kind of testing before just releasing tools widely onto the world uh but I think it's widely viewed within the community that there's more that we can do to improve these processes and make things better so I encourage you as you take the tools that you've learned about how to use in this course that you think about looking at these systems evaluating the work that you do critically from these lenses and make sure that what you're building has a positive impact on society that's the end of this segment\", metadata={'source': 'Hp8qfbsp57M'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_course_transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcrição de um Curso de Teoria da Computação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_theory_url = \"https://www.youtube.com/playlist?list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_theory_playlist = Playlist(cs_theory_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cs_theory_playlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_theory_transcripts = []\n",
    "\n",
    "for video_url in cs_theory_playlist.video_urls:\n",
    "    # print(video_url)\n",
    "    video_loader = YoutubeLoader.from_youtube_url(video_url)\n",
    "    video_transcript = video_loader.load()\n",
    "\n",
    "    # print(video_transcript)\n",
    "\n",
    "    if not video_transcript:\n",
    "        continue\n",
    "\n",
    "    cs_theory_transcripts.append(video_transcript[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"welcome to 15 751 I suppose we prefer this title TCS toolkit my name is Ryan O'Donnell you can call me Ryan and his lecture one is the only lecturer which has slides which is nice because you can see we could not control the lights correctly and it's very bright on the screen but that'll be good when we're using the blackboard which we'll be doing in all subsequent lectures well be you know proving Sterling's formula and stuff on the blackboard but today in this lecture its signature luxury let it lecture so I'm just gonna like tell you some stuff about the course hopefully you can read the screen ok so today's lecture will be divided into three parts part one and I'll tell you just some stuff about the course and then oops part two I'll tell you how to TCS and in part three we'll talk about street-fighting mathematics so it'll be a little bit math at the end of this lecture okay so let's dive right in I'll start by telling you about the course and you know what we're gonna do in the the next 15 weeks so the title of the course is TCS toolkit so we can just figure out what it's about by going through the two words in the title first one is TCS that stands for theoretical computer science naturally there's lots of topics that are at the intersection of computer science and theory that we will not be talking about in this course so for the purpose of this course CCS means algorithms and computational complexity so as I said there's lots of other things that's theoretical and mathy and computer science like logic and PL and verification and so forth but we won't be talking about those things there's actually a nomenclature division between theory a and theory be so algorithms in combination complexity is theory a and the other stuff is Theory B and there's no like ranking or anything it just came from the fact that these two really thick books called handbook of theoretical computer science volume a which is about this stuff and handbook of theoretical computer science vol B which is about the other stuff and we'll be doing the a stuff another way to put it if you're kind of be in the know if you're already a little bit sophisticated is this the stock Fox topics if you're not in the now the two main conferences in which all sort of the top research in this field gets published are called stock symposium on theory of computation and Fox foundations of computer science and so somehow these two conferences that haven't once a year are used as a shorthand for the set of topics that we'll be talking about great so that's TCS and toolkit well that's a little bit self-explanatory so it's toolkit refers the fact that I want to give you an intro to some of the tools and topics especially mathematical tools and topics that arise in TCS research so in fact this course is gonna be maybe like 50% math topics and 50% like computer science topics and you know if you had 28 additional lectures as I do to talk about math and computer science topics relevant for TCS research then you know you could choose some subset of you know a hundred possible topics to talk about and so the ones that I'm choosing are inevitably biased by my own interest so we're probably gonna get you know more I don't know probability and complexity theory then you would get from some other lecturer but that's how it's gonna be and another thing I want to emphasize about this course is there's a bit of a warning it's more reps than depth and actually not all students are you know like this fact some people would prefer a course that you know goes into more depth about fewer topics and this course some extent goes into less depth about more topics that's the way I've designed it and as I said it's a bit of warning if that's not your taste well be prepared for it if you're officially a CMU undergrad it's not completely dissimilar form like the course 251 but for graduate students which is I guess a little nod to that in the the number of the course 751 okay so when I spring this course well back you know what I was really thinking about is the following you know if you're getting into research in TCS or if you're reading a CCS paper or if you're attending a TCS talk there's some level of background knowledge that usually the speaker or writer sort of assumes the audience knows you know they kind of assume you know usually what does the Chernoff bound or like what is the linear program and you know you may not necessarily know that you might not necessary have learned that as an undergraduate and so I was trying to you know scoop up a bunch of such topics that are considered sort of background knowledge all those sort of things that mathematical things especially that you may need to know to try to follow a TCS talk or read a TCS paper and Alfred who this class is for well on one hand it could be for anybody that's interested in doing TCS research again when it's designing the course like the Platonic ideal of like a student I had in mind was maybe like a first-year graduate student that's either going to intending to do CCS research or at least has an interest in TCS and TCS research so it's court who initially the course was designed for and I should finally add that it's a very matthew course so the mathematical maturity is important so I mean a lot of its gonna be on the board you know definition theorem proof and you're gonna be required to write you know proofs in your homework so that's a little bit of either a warning or an enticement if like me you really love math okay so I wanted to flush up here a little bit of the syllabus roughly speaking this is a bit tentative and maybe not exactly precise but roughly speaking the topics of the course kind of fit into like seven units each of which will have between three and five lectures so this is kind of the approximate syllabus for the remaining 28 lectures so we'll talk about asymptotics and probability talk about Fourier transforms in some applications we'll talk about some elementary algebra it's applications spectral graph theory the straight satisfaction programs and linear programming and SDP hierarchies information and learning theory and some hardness computational hardness so these are the topics that you can look forward to and as I said there's many other topics I could have chosen but unfortunately we only have so many so many weeks in so many lectures this is what we'll hear about and the first thing I will tell you a little bit about is of course the logistics for the course I actually I'm not going to spend too much time talking about this out loud because I'd rather spend the time talking about non nuts-and-bolts things the main thing I have to tell you is this website which you must go to so there's a website called dinner oh this is developed by two professors here at CMU and it's kind of going to be the combination of the webpage for the course and the the Piazza or the Bolton Baltimore for the course canvas whatever you want to call it it's all rolled into a dinner oh so maybe you've used it before maybe you haven't but it's essential that all of you go to it and find everything there so feel free to do that on your phone or your computer while I'm rambling up here this where you'll find like all the announcements like you know there's a correction to the problem set or has changed the due dates so you definitely want to watch out for that that's where you can ask questions about homework or about the lecture so you can find the course policy in details and where you get the homework and so forth so make sure you're enrolled in that speaking which as I said I will not you know read the entire syllabus and course policy details here but it's definitely obligatory that you do that yourself I don't know they'll take you to ten minutes or twenty minutes or seven minutes or something but please definitely go online to dinner I'll find it and read all details so you know everything there is to know about the course policy and actually ask for homework it's a little bit of a warning we're gonna have six homeworks and the first one's gonna be a little bit short and just for your information it'll be due and you know ten days or something it'll come out on Thursday so it's just to get you ready for action in this course right so as for grades this is how it's going to break down the main component of your grade is going to be from homework there's going to be basically six homeworks but the first one is a little bit short so there are each worth 12% and that adds up to 68 percent there's also going to be a written project which I'll tell you about later in the course the due date for that is like April 3rd or something and for the beginning of the course just focus on doing the homeworks and I'll tell you about how to start doing that project asynchronously later basically the short story is that you'll be writing some a page document about like another topic such as I might have lectured about but didn't actually after those are turned in you'll also be writing a short review of two of your classmates projects your project will not be graded by your classroom classmates but you know will grade the reviews and these will also be taken into account when we raise your projects this is partly done to just have you give you the right mindset about who your audience is you're writing this project for your classmates and another component of that class is seminar attendance you can find more details about that again in the course policies but in short you'll be required to go to three theory talks from our regular weekly seminar series the ACL seminar theory launch seminar and theory seminar that counts for some of your grade and class participation is the other 3% this is a 12 unit course which means you're supposed to send 12 hours a week on it so this is in my mind how I thought to imagine you would break it down your mileage may vary you can see your main effort I think will be on doing the homework there's a lot of long homework in this course so maybe a couple hours every other day and the other ones don't have like or not for the whole time so maybe these are amortized and of course will be in lecture for 3 hours per week I think this adds up to 12 so I think there are more details online by any questions right now yeah yes all homeworks must be written up in my tech I'll mention that working actually for the first a a bunch of questions I'll ask you your name I may ask you your name even if I know your name because I want to disguise the fact when I don't you know your name even though I should know your name so what's your name Jennifer right I didn't know that Jeff was in my class last semester okay\", metadata={'source': 'prI35GmCon4'}),\n",
       " Document(page_content=\"great so that's the end of part 1 of the lecture so on part 2 I'm going to tell you how to t see us actually that's a bit pretentious but I didn't know what to you know exactly I'll fill this lecture since we're not actually doing map of the board so I thought I don't tell you some philosophizing about TCS so yes it's more or less math I mean as a pastime what are you doing you're proving theorems and that means you're doing math it but it's way better than math which is why I do it and why I hope you'll do it too and let me tell you why it's better than math like better than if you're like a math graduate student so the reason that's better is you can study whatever you want so if your math graduate student you come in and you start studying like some complicated topics and you start get into algebraic geometry and some sophisticated some area of that and some further sophisticated some area of that and then you write your PhD on some perverse sheaves or something which is the name of a real topic and by the end of it you know you're the world expert on some tiny corner of math and then you're kind of obligated to keep doing that forever because I mean you made it that far but GCSE is better because I mean it's still math but you can study whatever you want so you know computation is at the heart of you know every area of science and life and therefore you can study our the complexity us are pre-trained to any interesting topic that occurs to you when you wake up so you can study the algorithm complexity of protein folding or at Auctions or quantum tomography that's something I study myself cake-cutting if you really love perverse sheaves you can study the algorithm complexity of prefers Chief's in fact you know every you know sophisticated mathematical topic has some computational aspect to it in fact I should mention like a very exciting paper came out like last night at like 9:00 p.m. on archive where some authors including former graduate student proof some like deep mathematical theorem or disprove some deep mathematical conjecture cons of betting conjecture from the theory of von Neumann algebras using computational complexity so an excellent illustration of how TCS can apply even to math if that's what you really love or you can just start you out in the collective you know good old classic computer science stuff you know graph algorithms or whatever so it's really I like you know TCS it's like math you know the beauty of math is there but you can really work on any exciting topic you want now on the other hand I identified two TCS is that like math TCS is hard someone once said math is hard and BUTT STUFF because when you're doing research in TCS what are you you're doing you're I mean you're sitting around with your pencil and your paper and you're trying to solve that open problem and that's hard because you cannot predict if you'll succeed and you cannot know how long it will take you and that can be very psychologically tough now in this course you know you'll be sitting around with pencil and paper for a long time trying to solve homework problems and there it's like a little bit better because ideally you should be able to succeed and they're not open problems and supposedly you can solve them in the 10 days or 12 days or whatever you have allotted for you so that's good but as you become a TCS researcher this is something you know that's hard to to struggle with I should on this subject I like to always tell this anecdote just because I think it's funny this is a picture of julia robinson a very famous mathematician who would be 100 years old if she were alive today back when she was working there was no TCS at the time this pre you know computers and stuff but she worked on a topic that's like proto TCS so she we definitely work on TCS if alive today and you know she when she was working she was a famous mathematician working in Berkeley but it's very sexist times back then so she wasn't like accorded the full rights that you'd normally expect and in fact she had some like boss that was like oh you have to tell me I bought a new policy we're like every week you have to write a report on what you did that week which is kind of annoying right so she instituted in the first week she submitted this report Monday tried to prove theorem Tuesday tried to prove theorem Wednesday tried to prove theorem Thursday tried to prove theorem Friday theorem false which okay that's one the other sad ending although at least she knew that theorem was false you know I don't need to be discouraging I just I tell the story because a little bit funny but it you know it's uh it gives you the feeling or it reflects the fact that you know you never know if you're gonna succeed and you never know how long things are gonna take which is it can be a challenge but you shouldn't give up and I'll tell a different anecdote give another quotation to illustrate the courage you should have in the face of this so this as you may know it's the sky at Tarrytown it's like the most famous mathematician alive and here's a quotation of his about how he does math and I'll just read it to you even though it's long and it's up there he says I don't have any magical ability when I was a kid I had a romanticized notion of mathematics that hard problems were solved and Eureka moments of inspiration but with me it's always like oh let's try this that gets me part of the way or that doesn't work now let's try this oh there's a little shortcut here you work on it long enough and you have to make progress towards a hard problem by a back door at some point at the end it's usually like oh I solved the problem which sounds funny but I really I think I mean this resonates with me I really I think it's true and that's how you you I think should go out it's less I mean that's the famous Terry Taos thoughts on it this is a little nice my thoughts on it I have a short version the first of this philosophy which is honestly my true philosophy about how to do research which is just this you know make 1% progress every day for a hundred days and then you will solve your problem so it's a little like you know what you know novelists advice always is know like write every day so this is how you make problem in progress in math as well again more relevant to this course like the the 751 version of this philosophy is well you know you got the homework try to make 10% progress every day for 10 days and hopefully you will solve the homework oh and in the meantime I mean if you're struggling on problems or you're struggling on your research as a TCS researcher you know don't just sit there dying and beating your head against the wall in the meantime you can read and write and watch and learn and do more math and T's yes and so let me tell you a little bit about what sorts of things you can do on this angle okay so if you're a beginning TCS researcher or any kind of TCS reacher researcher it's very important to stay up-to-date on what's going on I mean don't just hide out in your whole and think about the only that one problem you've been working on for so long you know find out what's going on in the rest the world so I'm very important website I'll give you as this one I'm sure I'm gonna mention a bunch of URLs in this talk they're all gonna be on the last slide so you don't still like to scribble them down or anything although you can open them up on your phone or whatever else I'm talking in the olden days people used to write blogs and I guess TCS people are still a little bit old-fashioned and still have blogs and this website collates them all and also has like announcements in TCS and like paper links and so forth so I mean everybody I know follows this as a way to like keep up to date with stuff that's going on in TCS of course these days you know modern people use Twitter and you should follow your favorite researchers on Twitter you know find out what they're working on I mean at least the ones who are tweeting about computer science and so forth another way to find out what's going on is to go to talks and in fact as I mentioned this is worth 6 percent of your grade so you should definitely do it and we have plenty of great talks you can go to here at CMU so here are the three seminar series that you can go to what does the theory lunch seminar which is every Wednesday at noon usually in nsh 30305 there's food and there's student speakers so it's usually understandable and enjoyable yes a CO seminar is usually run by the math department but it covers math and combinatorics optimization in or I must admit I made a colossal mistake by agreeing to have this course at the same time it's the ACO seminar which is Thursday at 3:30 and ween 8220 but you know you can skip this course class and go to the seminar if it's a good speaker because sometimes in fact almost always it's a really great speaker and the last of our series is the theory seminar where actually you know you know faculty from other universities come to CMU to give talks it's not completely regular but it's generally on Fridays at 3:30 and you can see that calendar for all these talks here and you get the announcements for all these talks here the fear now it's mailing list so please sign up for that right of course everybody knows that the real I mean current social network is YouTube and this is the base place to find talks and watching talks is another enjoyable thing you don't have to go to them necessarily imperson you can watch them at 1.5 speed on YouTube maybe you're watching this 1.5 speed on YouTube right now I want Panopto and this is great I mean this is a great way to learn find out what's going on you know you're going to the gym you're on the treadmill throw on a youtube talk I mean you're riding the bus home watch watch a theory talk on your phone put it on in the background you're cooking whatever I mean this is a great way to just hear about what's going on theory and learn new things now I guess looking videos is more of a fun way to learn superficially about things but eventually you got to read papers this is harder I mean to read one paper it takes one month at least for sure but you know this is important to really properly learn things and to make progress in research so where do you find papers maybe the best place is to look at the Proceedings of these conferences in the area so these conferences Fox and stalk is to the top general conferences so does the top algorithms conference si si si is the top complexity conference each of them runs every year you can find all the papers in them online you know how to use Google they often have the video talks as well so this is a base place to find out you know what's happening you can also find all-new talks even more promptly actually on archive so I guess a hundred percent of mathematicians and 90% of TCS researchers excluding like the naughty 10% who don't do it publish their new papers on archive just a free archiving paper service so that's all kind of high volume so you have to kind of pick and choose if you go to this website how to find papers that you might like this repository is also just for complexity theory but it's more curated it has like a good volume level so if you're in a complexity theory it's a good one to follow to to find the latest papers ah of course if you're actually doing research and you're reading a paper one thing you should do is read or look at all the papers at sites and that's very easy right you go to the references you go to the Google Scholar you type some in and you can read these papers but it's also tremendously important to do the reverse read all the papers that cite the paper you're reading and in the olden days I guess this is very hard but in our modern days we have computers and this is very easy and you should definitely always do this and the way to do that is also D is Google Scholar at this website and if you search for a paper you're gonna listing and you can see right here signed it by five and should always click on that and find out who's setting this paper because the paper you're reading may not be the last word the most current word on the subject so I don't we're getting a little bit down into the weeds here but you know this is the only time I have in this course to tell you about I don't know these ancillary research type things so please do when you're you're reading papers see papers that cite what you're reading okay on that subject I'm gonna start to tell you a few you know lifetime things you should start to do as a TCS researcher and one of them is to manage all these papers that you're reading so whenever you read a paper or whatever you like open up a paper for like greater than equal to thirty seconds you must have had a reason for doing it so you should save a copy of that to your hard drive or a cloud storage or both and you know don't just save it as like 106 3.2 496 PDF because then you'll never find it again see you some consistent naming convention and is really great to have like all the papers that you ever cared about in one place you know you're on an airplane there's no internet access you still can read that paper in those uninterrupted hours it's relatedly you know whenever you are writing a paper and you sign a paper you were gonna have to use bib tech and so you should have like one giant file with all the papers you've ever cited in a dot bib file and again you should have some consistent way of storing them and actually this is a bit old-fashioned I mean the real way to do this is you should use software again we have computers these days reference management software that does both of these things you know links together all the papers the PDFs the bibliography references and so forth so please if you don't use these things yet in your life today is the day I mean get started download it and start maintaining you know all the the papers that you look at ah okay so speaking of low tech you'll be writing math in your life and you'll be doing it for homework and also for your research work and you have to become fluent in low tech I mean this is there's no way around it this is the standard from writing math that everybody uses and hopefully you know a little bit of latech already if you're nevertheless the beginner I guess the way to start is with overleaf comm this is a website where you can go and start you know doing tech and it's kind of helps you out and you have to download any programs to get started so that's where to go if you're still feeling yourself a beginner and if you are a beginner then of course if you don't know how to do anything use the Internet in a particular this website is great the stock exchange for a tech tech da Stock Exchange comm every possible question that anybody's ever asked in the history about tech has been asked there so you'll easily find your answer there and if you want to like a reference book to leave through you can check out this one but you'll be doing a lot of low tech writing so hopefully you will you know gain skill in it over the years and in fact as you're doing this you'll find that of course writing low tech is no different from coding really I guess actually latech is like a turing-complete programming language if you really hardcore and you're also all computer scientists or computer science adjacent and therefore you know about coding and you should really think of them in the same way so you know don't be writing you're my tech in like notepad or whatever particularly you should get a really great uh text editor I mean we're not code and just whatever notepad or whatever you know you should get a proper text editor that has like you know low-tech syntax highlighting and hotkeys to like see the compile and see the PDF and the feature where you click there in the PDF and it goes back to the latech and you click here in the tech and it goes to the right place in the PDF and all that sorts of stuff okay this is essential I mean you're going to be writing a lot of l'attaque again don't be lazy like today's the day get it started so again the bare minimum here it's like overly if it kind of has this stuff is slow but you know if you were coding something you probably also would not like go to a website that led you to start typing Python and use that right you would get your own enjoyable editor here are some I'm that weird guy that likes Windows so I use Winnie DT here's some other ones these ones are on all platforms I mean you know about editors I guess but pretty gonna find your your editor make sure you get all the plugins and whatnot that give you all this stuff similarly just like writing code you also need version control even if you're only writing by yourself a solo authored paper it's still good top version control and in particular if you're writing papers with other people which hopefully you will often be doing this is the best way to do math is collaboratively you'll need version control and again it's CS adjacent people like us you know about that the bare minimum again is overleaf I guess it has some but you know you should move on to like github or whatever if you're running papers with other people so please do that and finally yeah it's like coding use indentation you know make your source code look nice you know it's complicated because you know you're also thinking about the actual math and the presentation and how to write well but you know these are all like good hygiene things I mean please also make your document nice look out especially if you're using version control the real bold move is to do like every sentence is a new line maybe that's a little bit too tough but yeah try to try to do things properly as if you were coding great um okay again I know I like getting a little bit into weeds here like you thought you were coming for some like cool math and stuff and now I'm telling you about like you know your style file and suffer let's check but again I don't know this is like that the one place in the the twenty nine lectures I felt this naturally fit so I'm gonna give it to you now so you know when I was first starting out to write tech you know every time my to be like okay new homework I'm like how do you start it again it's like slash begin document or whatever don't do that I mean have like one lifetime like stubb tech file and one lifetime style file that's the one that has like all your macros and your packages and so forth so that you know you can always just get started with the same default setup so like maybe your sub tech file looks kind of like this it's you know it's already like 20 lines or something to get started you know and this refers to your default style file up here this refers to your lifetime bib file which hopefully you're now managing with your you know bibliography reference manager and yeah I mean if you haven't done these things before again today's the day to get in the habit of setting up your future in the tech workflow so this may be like a lifetime style file it's not my real one maybe I'll give you my a copy of my real one in case you want to use it but you got your packages your macros and so forth and so on so if you don't have one of these get a starter or one from your friend but then actually go through all their stuff and see if this is just like leftover cruft that you can delete or it's actually useful but you can find out by googling ok now we're really getting to my pet peeves if you like make a bibliography file please make it I mean you know the references are also part of your paper don't make them like discussing how the center like format for citing proceedings cut the capitalization right there's some like weird bibtex like bug where like people capital letters in the title it will not come through unless you put braces around them etc etc this is all part of the advice of you know once you make your PDF document you should like look at it and see if it looks nice and if it does not look nice you should make it look nice and that's actually a I know it's like when you're looking for the bit tech entries for like citations don't type them yourself but also don't just like take the first bib tech at review you find on the street because most people will give you like a disgusting ugly one so here's a lifepro tip the best place to get bib file big tech entry is is this website a mess auto R / mr lookup so this actually mainly for like math papers but it also now indexes computer science proceedings so how does most people ever need and AMS Starworks website is like not free but this service is free as long as you click on bib tech here when you search for paper you'll get this like beautiful bib tech entry that you can cut and paste its my tip to you this is how you should get your your bib tech entries if you can't find it there Google Scholar has a DB LT also has them they're not as enjoyable but you can find them when you search for a paper and click here okay and uh yeah the last thing I tell you on this subject is somewhere I'll attack pet peeves I still come to know if you get to know me like I'm very like the key of all the tech and stuff and again maybe you didn't expect me to get at this level of detail in this course but I also clubs a chance to have a little audience interaction so I'll now tell you like my top ten or so like latech pet peeves and you tell me that the don'ts and the dews if you don't know what tech that's fine if you're seeing this before I mean you'll learn but again what you should really do when you like look at your latech document in PDF if something looks bad like don't just be like well I'll submit this like the TAS can deal like no you should make it look good okay please so let's say you want to describe the inner product between u and V you should not write this what should you write yes very good what is your name sunny great Thank You sunny yes / Langille and / Rangel otherwise this if the tech thinks this is less than and this is greater than and the typography is horrible also this is very long right so you should make a macro in your style file for like / LA and /ras or you off the tile angle and write no all the time very good okay this is a brief writing text is not a math thing but you're writing texting you put want to put a word in like scare quotes do you not do this it will look terrible you'll get backwards quotation marks what shall you do yeah back ticks yeah what's your name Amy sure yes yeah back ticks for this one forward ticks for this one again if you don't know these things I mean don't feel bad just look them up on tak-tek Dog Stock Exchange when you notice problems this is a math you would never write this what tech thinks this is the variable L times the variable Oh times a variable G the typesetting is terrible yes Jennifer yeah definitely a back slash log to make it look like log this is going to not look too great you do it you will see a towel fraction with tiny little brackets pathetically enclosing it yes yeah what was your in June yeah yeah you should use left and right to get properly scaling size parentheses very good actually if you're very sophisticated you can use a macro that's like does her look parentheses wrapper but it's very challenging to get rights I mean I'd study Stock Exchange together right so that's the advanced mode for now just do this how do you feel about this looks kind of go ahead accept that yes what is your name what is it can you yeah yeah you guys are good you're very good it's excellent yes this will type set a like it's a letter but it's actually a math thing it needs to be in italics so you'll put dollar signs there or if you're like super nerdy at least the slash parentheses but nobody does that I guess I'm glad you could see all these things by the way how about this this is kind of similar to the log the tech will think this is just again like a times L times G so it'll be typeset poorly you're probably don't want this and italics right you want it cut oh you should define it si what's that math upper 80s but it's true what is there's a lots of Biscay yeah yeah math are and we'll put it in Roman for sure or I guess actually this is an error I had for a long time I put it like slash text if you want to like in math mode put a bunch of like just in regular text they still slash text it you can normally do that but if you're doing that inside a theorem statement it'll be in like italics and maybe you want it just in regular rum and so you can do slash text normal ok these are it's not a little picky but uh yeah I like picky I only have I'd only have one more slide of this I'll just tell you this yes do not use equation array and the olden days when you want to you know you've got like a bunch of equations look like the equals and whatnot there used to be this equation array but it's bad for various reasons that you can find the tech nerds telling you about on the internet and so instead you can use yes what what is your name Amen yeah I used to begin a line how about this what happens when you do this for example this lemma perhaps when it's typeset lemma will be the last word on the line and - because it's lemma two will be like the next character on the next line and that looks really bad right bla bla Bal lemma - come on yes somebody said it here your name what is it Tonya yes you need a non-breaking space between a lemma and this so I Tilda is a non-breaking space that says latech never break this across lines are you gonna tell me to use a package like clever ref or something - part of me yeah maybe this is a choice but it's true also that like you're saying you should not sorry what's your name by the way my right uh you would use a package you're saying to like have this appear automatically yeah you can do that in particularly I think you should even probably use a package for this so if you decide later that this lemma you actually want to call it a proposition it'll like automatically change he's the clever ref or maybe P Roth package for that but this is the basic version although I hate this one but many people fail me on this one do you know what's gonna happen here I heard its face did you have something to say oh yes exactly right yes what's your name hey Bailey yes ah yes solet Eric and Donald can use infinite wisdom decided and I think it's a good decision that you know you should have a little bit more space between sentences and you have between words well then it needs an algorithm to determine when the sentence ends and its algorithm is just like hey if there's a period and the next character is a space and the previous character is a non-capital letter it's probably the end of the sentence so if you're reading the PDF version of this there's like a little bit too much space here which most people are okay with but I go and say and when I see it you're going to put a non-breaking space but actually maybe you want to allow breaking here so you can use a breaking space which is backslash based or you can put it in a tilde for a non-breaking space I'm sure it's no big deal isn't this fun yeah I think I've liked one more or two more yeah last one oh yeah I like this one yeah let's say you have a proof and the last bit of your proof is an equation a bad thing happens if you don't do anything else yeah that's right what's your name Ethan what's your name and do you know how to fix it Brian yeah very good yes you guys are so great let's talk maybe I should just delete this from all this these these slides and take it off the homework yes you should put QED here otherwise the little nice box at the end will be like on a blank line after the equation instead of like in a cool spot at the end of the equation okay I will stop talking about this but yeah please this is all I in the effort to help out your poor reader which I will talk about just now oh so these things take time to learn yeah as I said like don't worry if like you're like what all these like nerds talking about but as I said you know please look at your PDFs after you compile them to make sure they look good and if you don't know why something looks weird please take like five minutes to fix it especially for the sake of your beloved TIA is I think the beloved tears are in the back here by the way they're sitting here we have Pedro in blue and a Nash next to him and Kevin nice to him they'll be creating your homework so please be nice to them as I say here uh I mean this is about the nuts and bolts of how to like make a math document but of course even more important is how to make like a document that a human can read until that conveys the math you did well and it's again like all to make your readers including the TAS happy and of course this is like I mean I cannot give you like one slide of advice on this it's like a lifetime task to learn how to write math well but nevertheless I'll give you two sentences of advice so the first is hey this is math so what you write has to be 100% correct like it cannot be 99% correct it really you know a theorem is not proved unless it 100% corrects so even if it's like oh so I really have to say like the floor here because it's not an integer Wow probably should I mean so I mean if you're like I think this more or less proves it you cannot write it it's math it's got to be correct that said I mean you got to take pity on your poor reader as you know reading math is very hard just understanding math paper is very hard and if like some writers like the following you know as ten pages of symbols and equations are technically correct so read it and weep reader then it's terrible and nobody will read your papers and they'll be mad at you so put yourself in the shoes of your reader you know after you solved the problem and you write a correct argument don't just stop there think about how to actually explain it in a way that a human can understand it and try to make things better for your reader you need on that subjects it's getting a little bit back into like nuts and bolts sometimes you're like man this would be a lot easier to understand if there's a figure but I'm tired I'm not gonna make a face don't be like that make a figure okay just it's not that bad you got it I mean if it's gonna help the reader please make a diagram it's very easy to include until attack do not be scared about this this is all writing your style file and you just write slashing clue graphics and then the file name that's it it's just that easy now you have to actually draw the figure that's tougher and that is a little challenging ah that was not hard but drawing it can be hard and you can use other file formats so the theoretical answer I like how you should draw figures is to use proper figure drawing software like Inkscape which is free and can draw wonderful vector graphics Arctic sea which is very sophisticated programming language for I've never tried to use it it's very challenging to me for drawing diagrams but these have a tremendous learning curve I think it's hard to learn to get to use them and although I have been telling you I've been big picky about doing things properly I think this is maybe this is not necessarily the best use of your time I don't know you can get into these things if you want but here's my solution which i think is a good solution for drawing figures which is this see that's like I've seen the next slide if you're never going to like write a paper you're also going to give a talk and you're going to give a presentation and you'll have to make slides about your talk and so you're already using some presentation software like PowerPoint or keynote so you have to learn it anyway in your life and like the whole deal with like PowerPoint or keynote or whatever is like you're drawing pictures and Scott like sir tools and so forth and so on it's not so bad and you can you know like select it and be like save as picture okay so in fact I draw all my figures in powerpoints and I think it's perfectly fine so this is my recommendation for what you should do yeah anybody else have suggestions on how they draw figures yeah Pedro yeah I've got the suggestion to IP is it I mean you like it okay ah that is good that is good you can with effort it's like slightly not easy I believe to export pictures to PDF from let's say PowerPoint you can do it but yeah that's good anything else yes Oh tell them we love hang yes it's your name Tom on the subject of PowerPoint Tom will detain you can also make us like your PowerPoint figures look like they're actually tech figures by setting the fonts to use them ah yes that's another Bugaboo for mine yeah getting the correct font matching tier yes you should download the latex Laurence I've done it yeah so they have the matching font yeah oh my gosh I've heard about this it does it work well Penrose okay I got to try this it's like a research project right of like PhD students Oh fantastic okay yeah let's try it post it to dinner oh you can do it now you got a computer in front of you great okay cool yeah and on that subject as I said if you ever write a paper you also have to make a talk and to make a talk you need to use presentation software and here I include Beamer which i think is a perfectly valid way this is like the low tech way to make a talk you can make nice talks using it it doesn't solve the problem of drawing figures but anyway you know people enjoy using Beamer or some people who enjoy using viewer to make presentations off presentations as well so it's not um cool to be like oh I'm an expert in powerful well actually this cool to be an expert in powerful yeah do you have any questions about it there's somebody you can ask but it's good you're gonna be using a lot your drawing your pictures you're making your slides again like let tech will learn the tricks make them look nice if you can do it and of course as we suggested you gotta get the equations into it and that's challenging for PowerPoint the Microsoft equation editor is now good I feel I mean unless you want to do some strange things you can just use it and it'll make nice-looking math with matching fonts well if you use certain fonts or you can use a plugin this is one is I don't use Mac so I don't know but I'm told latekka it's the thing to do with but maybe this is out-of-date how do you insert the tech into keynotes oh great so it's probably like the equation editor yeah what is your name hey Farmiga oh thanks yeah okay that's great so we're moving into modern times good and Beamer are like all the latoque is perfect in it so there's nothing to do there that's great okay and similarly there's about there not some bolts about how to make a presentation then there's the question of like well how do you actually make a good like content presentation and convey your math well which is another thing you want to do because you you prove that theorem and now you gotta explain to the world why it's cool and here I don't have specific tips of all I should I found there's this professor somewhere in the West Coast Stanford kayvon who has very nice tips so check out this this website yes I incur with all of his tips on how to give a good talk okay so I think that's all I have to say on the subject of how to TCS any questions or comments before I start talking a little bit more about math okay\", metadata={'source': 'YFUIPg8P2sY'}),\n",
       " Document(page_content=\"so part three street fighting mathematics so uh this title I actually stole from sanjo Maha John he's got a book called street fighting mathematics and I don't necessarily like it cuz you know you should like Make Love Not War it's a little violent but it conve like an essential attitude for doing math research that I hope to convey um which is a bit of difference between like how you do like how you solve your homework problems when you're undergrad versus like how you do like new research to solve open problems and it conveys the idea that like you know if it's an open problem and you're going to be the first person to prove this theorem then you can do whatever you can do to solve that problem there are no rules you know just solve your math problem so let me try to explain what I mean by this so you know often you have a math problem and like there's a parameter n you're like how do I solve this and you're like well of course the good advice as you I'm sure know is like Try n equals 1 and Nal 2 Nal 3 and then maybe you get some answers like 1 2 5 20 125 how do you find the next number in this sequence you do not use your brain yes thank you what is your name Isaac yes there's a website for this okay the online encyclopedia integer sequences oeis you just type the numbers into it and it tells you the answer and that's great okay you saved yourself so much time you've got yeah I mean this is just part of the way to solving your math research problem right so do not waste time like trying to figure out that it's the partial sums of the double factorials or whatever um um so just do that okay uh or let's say you're reading a paper and somebody mentions the Sterling numbers of the second kind do you know what those are do you know the formula for them you do what what are they oh wow that's great I do not know what they are but of course oh I exped it of course you guys you you definitely all know this one right you do not don't just sit there puzzled go to wikkipedia Wikipedia is so amazing for math it's like math is like one of the things that Wikipedia does best actually TCS itself is field math is not so great on Wikipedia But for like pure math stuff oh man it's unbeatable so this is a great uh Delight of our era that we live in you can just find out what the Sterling numbers of the second kind whatever the heck those are in an instant so do it you know have some curiosity and just look it up uh okay if anybody recognize this number I'd be very impressed yes I think it's by oh I think it's 5 by8 what is your name KRA is it eight times that uh maybe uh but it's something like that actually this is based on a true story uh when I was a grad student I had this office mate and friend uh called John Feldman and he's doing some research and coding Theory and he was writing some code other kind of code to like compute something that was the key to his problem and like the computation returned this number and there's a website you can look it up it's Bic basically what teer was doing being the inverse symbolic calculator so you can go to this website and plug in this number and it'll be like I think it's best K 1A 1 or bestel k 1A 1 and of course then you're like what is the bestel K function but we already did this one you look it up on Wikipedia okay and so this is what you know you do and then you learn some new math and you um pretend like you already knew it when you write the paper uh so I can in fact illustrate the story uh there's this uh great TCS researcher Ryan Williams some uh place on the East Coast MIT he was a PhD student here and uh I was lucky enough to read a draft of his thesis as he was writing it before he finished it and it proved a lot of cool things and one of the cool things it proved maybe it's number one theorem was this theorem it's not really important to know what this theorem is he's a complexity researcher um but it's going to illustrate a point he proved that any algorithm for solving the SAT problem using sublinear space needs to use at least time n to the power of C where C is some number that's like 1.8 or something now actually uh we believe that any algorithm solving sat with sublinear space needs time like 2 to the N but you know complexity Theory it's very hard we can't prove anything and this is already an amazing result but on the subject of the C I mean the way he did it it's very sophisticated and the C it's actually not 1.8 but it's the largest root of this polinomial C Cub - c^2 - 2 C + 1 equal 0 and yeah cool theorem so he sent me a copy of this thesis and I was reading it I was like huh and so I had my computer calculate some more digits and then I was like I wonder if what happens if you plug this into inverse symbolic calculator and you do it tells you hey it's the root of 1 + 2x - x^2 - x cubed and I was like yeah I knew that but if you look carefully down here you also see that it's cos Pi 7 plus cos pi/ 7 also known as 2 cos Pi 7 uh somewhat sophisticated and I was like hey Ryan check it out it's 2 Co 5 over 7 and okay once you know that you can use some elementary math to prove it so you know his restated theorem looks like this uh any algorithm sublinear space algorithm for sat requires time and to the 2 Co pi over 7 and isn't that much cooler looking it's pretty nice so that's the usage of that uh in 2010 I was working on some problem in complexity Theory and somehow it turned into a problem in geometry and I need to know the answer to this again it's not important exactly what this math is but I'll just tell it to you I need to know if you have two sets that are closed and bounded on convex and they have a smooth boundary like I don't know like a sphere and ellipsoid and you take the union does that have a piecewise smooth boundary and it's like I don't know I mean I'm not that kind of mathematician I I mean but there's the kind of question right where like you're like I don't know but I know somebody knows like a well trained and I don't know analytic geometer or something knows the answer to this so um yeah you may know this too there's a website for that um it's like a stack exchange but for research math math overflow.com I knows but I'm not even sure who to ask then you can ask here and indeed I asked here and this Andreas blast like hopefully I don't know in 6 hours or something I don't know oh less uh told me the answer which is no which was annoying for me but that's life um so yeah when you're doing research at least do not use this for your homework please but when you're doing research there's another source you can have okay and as you may also know there's like stack exchanges for other topics so there's like there's like the lowbudget version of math overflow where like you can ask any question you want about math even if it's like not research level it's like homework level this math stock exchange.com don't post your homework there either though um and the analog for computer science and there in computer science it's mostly actually about TCS which is nice is CS stack exchange CS Theory Stock Exchange is like the research analog uh for CS Theory it's not as active though as math overflow.com answers there as well great let me ask another question uh let's say you're doing some astics as we will be doing in the next lecture and you're like boy I forget Calculus class what's the fourth order tailor series for arine X of course you will solve this problem not by looking at a a calculus text but by using like Mathematica or Maple which is like the Canadian oldfashioned alternative to Mathematica or Sage which is like the free python based version of these programs but does not have as many features unfortunately um so the first two are really awesome uh Mathematica and maple it's so great uh they're not free but if you're a CMU Student then you can get access to it freely so please do that and uh you should use them all the time uh they can tell you everything if you've never used them before I guess I may recommend Mathematica instead of maple because there's a stock exchange for it and you can get your answers questions answered much more easily it's very hard to get questions answered about Maple uh so maybe don't get started on unless you're like an old Canadian like me that somehow got hooked on it and now that's the one they know but they're both equally feature Rich so they're great so you know if you need to know this you just crackle the software this is maple you type series arine X4 and it just tells you the answer like there you go uh or like in wolf from alpha I know you all know about wolf from alpha you can just type it in English and it'll tell you the answer and you know unlike maybe you know you're doing your undergrad homework and there people like don't do that like when you're solving an open problem you know do whatever you can I mean this is all resources to help solve your problems I mean uh I know serious researchers that do not mathematics resers that do not use Mathematica or Maple and I think they're totally crazy like I don't understand why they're intentionally like handy copying themselves in this way so um basically they can do everything I mean they're not going to solve your problem but like you know you can answer like you know little conjectures you have write code to like find out answers to small questions compute things I mean I definitely don't integrate things anymore by hand I don't even like differentiate things by hand anymore uh I don't even do arithmetic anymore like simplifying expressions just type it into whatever Mathematica or Maple there's there's no reason not to and if you haven't like I started with this you know don't wait and be the next time you have like a problem like oh what am I going to download mathematics and try to learn it just to solve this small problem just start it today and play around with it because you'll enjoy using it um so in particular like um basically if you think anybody in the history of computing has ever tried to use a computer to solve some kind of math problem then Mathematica or Maple can solve that kind of math problem so try to do it uh there's also mad lab by the way which is basically in the same category but is more like for numerical things and Matrix Matrix things uh you should also learn that too and use it for such things but at least start here with Mathematica at Maple okay so uh perfect last segment of the talk will be have a little bit of map and it's I don't know some uh vignette to try to illustrate all of these tips about street fighting mathematics that I've just told you uh so we'll try to do an example so let's say you're doing some research and you have like a grand you know computational TCS problem but like it boils down to or a key component turns out to be this math problem actually this is I'll read it in a second but this is like not hypothetical this particular math problem I like know several different papers and areas where it literally comes up like an analysis of woan functions Quantum query complexity this is the sort of thing that like may come up in your TCS research let me read it uh imagine you have polinomial and you know it has degree a one variable polinomial and has degree at most n and has some constraints on it you know that for every input to the polinomial between minus one and one real numbers between minus one and one the polinomial outputs some number between minus one and one okay and the question is how large could the derivative at zero be uh so if you want to think of a picture right like imagine like an old in high school like the the XY plane and like a box going from minus1 one on the x axis and -1 to one on the y axis and your polinomial is inside that box in the sense that minus1 to one inputs it's between minus one and one and what is the derivative at zero it's the slope at zero so you can imagine like you know a little old cubic polinomial it looks like this or something or a quadratic polom looks like this or a quartic polinomial looks like this if I has to stay inside that box it cannot probably have like a slope at the origin that's like a billion right because I don't know it'll probably shoot out of the box in that cas case um so probably there's some bound and it probably depends on n and this is the sort of thing that can come up and you may need to know the answer to this and how can you find out the answer to this CU there's not really like a class like an obvious class maybe that teaches you like a math class even that teaches you how to solve this problem if you're very good at Google you can possibly solve this one by Googling because it's like Elementary enough it's sort of wellknown enough that you maybe do it can do it if you're really good at Google it's tough because you know Googling math is not so easy like how do you what would you type into Google for this but if you're real good at it you can take it as a challenge you might be able to do it um but instead of doing that uh well you could also ask it on if it came down to this and you're like okay I just really need to know this you can ask it on like math stack exchange if you put a little effort into it yourself um but let's try to solve it using street fighting mathematics well I'm sure You' got this advice before if you ever have any problem that involves like a number n you should just try like Nal 1 and Nal 2 and Nal 3 and try to get a sense of the problem so let's do that let's try n equals 3 n is the bound on the polinomial degree so let's imagine we only cared about Nal 3 so now we're asking let's say you have a a cubic polinomial like a plus BX plus CX2 plus DX cub and it has this constraint that you know if you plug in numbers between Min -1 and one it outputs numbers that are between minus1 and one so you know for every real number between min-1 and one like xal 2 you have actually well two linear inequality constraints right this is the polinomial with 02 plugged in and you know it's supposed to be at most one and at least minus one so you have uh well infinitely many constraints like this on a b c and d right one for every real number between minus one and one and you're curious how big the derivative at zero can be what is the derivative at zero of this polinomial it's B yeah if you differentiate and plug in x equals z you get B uh so in some sense we want to maximize it's like a BC and D are like real parameters we want to maximize B subject to all these constraints don't know how big could B could be and no it's you know one place where you can get started with uh you know street fighting mathematics I mean like a very hardcore pure mathema would be like oh I have Continuum many constraints and it's an infinitary problem but come on like let's be friends here and try to keep things finite uh for example you might say yourself probably this problem does not change much if instead of having this constraint for every real number between minus one and one I just had it for like I don't know 5,000 values like a grid of values or like 5,000 random values between minus one and one that's Pro that's probably not going to make much of a difference so let's let's simplify our lives a little bit that way and so then if let's say you pick 5,000 numbers real numbers X between minus one and one and you plug them into the polinomial you get two inequalities for each one so you would have 10,000 linear inequalities involving these four variables a b c and d constraining them and you're wondering how Big B can be and actually if you know uh some TCS you may know that what we really have here is a linear program so uh uh but otherwise let's actually just imagine for visualization sake that you just had three variables a b and c so every linear equation like a plus 2 B plus 04 C is less than or equal to 1 the set of a b and c that satisfy that is some half space and if you have a bunch of half spaces the set of points ABC that satisfy all of them is like the intersection of them so the intersection makes some kind of you know like flat sided shape here called a polytope I've drawn a picture in three dimensions with like a an a axis and a b axis and a c axis actually our problem has four variables but you cannot draw four dimensions so let's just imagine this picture so it'll have like 10,000 sides this shape and you're wondering like what pair or sorry what a point A B C D has the biggest value in the B AIS so if this is the B Direction here then you're like you're wondering what is this point and what is its B value does that make sense um so as I said this is like an optimization problem it's a particular client called the linear program which maybe you've seen before hopefully or if not we're going to study it in this course a little bit uh but you know it's maximizing some variable or linear function uh subject to some linear inequalities and you can get a computer to solve that so let's do it uh as I said I use maple so uh here's the maple code I wrote to do this you don't have to know exactly how it works but just super briefly here I'll like load some packages I'm telling it okay we're going to do degree three I'm going to pick 5,000 points this is defining the polinomial with four coefficients instead of calling them A B C D I called them c0 C1 C2 C3 because we're going to have a General Degree here I generate 5,000 random points between minus one and one I get the 10,000 constraints and then here I tell it to solve this optimization problem so maximize the second coefficient C1 I was calling it B on the other slide but C1 subject to these constraints and then find out what is like the best the largest derivative at0 the largest B value and what is the polinomial that achieves it and then also plot it for me okay so if you run this code for D equal 3 degree 3 Nal 3 I don't know if you can see it but I'll read it out loud it does this it finds some answer for the best degree and it plots the polinomial you can see I think that it's indeed looks like it's between minus1 and one on inputs between min-1 and one and if you look at what it finds the biggest possible value for B that derivative at zero that slope right here it's 3.05 so probably that's three right I mean if you think about it right you didn't constrain the polinomial to be between minus one and one literally everywhere you just did it on 5,000 points so maybe it's actually going a teeny bit above one and minus one here so if you really constrained it everywhere then probably you know we would shave that 3.05 down of three probably right probably so it looks like the maximizer uh when the degre is three is this polinomial in fact if you squint at the coefficients this is actually basically zero this is basically zero this is basically three this is basically minus 4 it looks like it's 3x - 4x cub and then you're like okay I can try to prove by hand that this indeed at least is between minus1 and one on inputs between minus one and one hopefully you can do that and yeah looks looks like the answer is three that's good we made some progress and since we wrote some nice code and we did it properly without like three being hardcoded in and we Ed the variable we can just change this one line here uh it's no longer showing but anyway this one line here where the degree is three and we can set it to anything so we can try 1 2 3 4 5 six so let's do that this is one um makes sense pretty much looks like the best solution is p of xal X which has deriv of 1 at zero okay here's the solution for deal 2 this looks a little weird um looks like it's half + 1X - 5x^2 anyway the maximum slope here is one okay three we did the maximum slope is three now if you do four actually something interesting happens uh the best thing seems to be the exact same polinomial 3x - 4x Cub which is funny you're like oh you were allowed to be degree 4 and it was like that's fine I'm just going to be degree 3 anyway it's possible okay and so if you ask what about degre equals 5 then actually it gets a bigger slope it gets five and this is the polinomial seemingly degree six same polinomial again interesting and now you've halfway solved the problem because uh basically here's the summary of what you found um well there's some weird anomaly of degree too but a couple of things first of all when the degree was even it looked like the optimizer was just the same Optimizer for like the odd number that was one number less in fact actually you could try to prove that um for fun but anyway you don't have to uh so it looks like you might Focus just on the odd degrees now and it looked like you know when the degree was like 135 the answer for the the biggest derivative of zero was like 135 and in fact um in some sense if this is part of a larger research program like you're in great shape right now because probably you wanted to know the answer to this question and it was probably because you probably I don't know wanted it to be small uh and maybe you really wanted it to be at most like square root n or something because then you'd be able to solve the problem this way and now you're like well it's probably not square root n it looks like I don't know looks like pretty much the largest coefficient for degree n is n so I don't have to worry about this problem anymore I mean there's no point in like probably going further right this is very convincing um on the other hand if like you know it's okay for your Global problem you know for it to be n or order n or something then you're super excited you're like oh I'm almost done I just have to somehow prove this thing and then my problem is solved right so I mean you can make progress on your research without like being like oh now I have to spend two weeks trying to to mathematically solve this problem exactly as if it were a homework problem so let's say actually it was good for you if like you know if if this is true that the maximum is when n is odd it's n or if it's like the one less number if n is even then okay you could try to prove it now what would you do well any suggestion all of these coefficients are integers yes oh that's true they have to do with the uh KRA says they have to do with the expression for trigonometric functions in terms of multiple angles true but let's say you were not so sophisticated that I mean if you know that then you're like oh problem solved and that is true but you could also yeah what was your name Danny Danny yeah you could also try that this might not work but you could try it just type these coefficients like 1 34 5 20 16 into ois and actually it does work it works um yeah if you type them in then you're like Bingo and you're like oh it's the triangle of coefficients of the chubby Chev pooms T sub 2N +1 x and then you're like what are the chubby chub polins well of course you go to Wikipedia and you like start reading and you're like oh they're extremal Pol pols for many other properties and there's like a little citation two and you're like oh two it's this book the Chevy sh pooms by Theodor rivlin published in 1974 and you can see like okay you can get the solution out of this book you can see how right like you would never get to this book in like some other way right like how would you ever find this without you know beating the system a little bit right if you like wandered into like the library well there's no books in the library anymore but hypothetically if you wandered into the library and like maybe there's a math book that can help me here it would be a million years before you found this book so then you have to like look at this book as you said there's no books in the librar so you have to find some sort of alternate computer-based way of looking at this book I guess one legal way is to like uh use maybe a little search inside and like books.google.com anyway long story short uh this bit on page like 108 this theorem 2.20 here if you decipher it it literally proves the thing that you would conjecture okay as I said like then you write the paper and a proposition and you're just like we you need the following well-known fact which appears in this book by rivlin uh yeah and it's no joke like that's how it'll go um and I bring this all up because you know often times you're like reading a paper like this and you're like so discouraged you're like this is not a well-known fact to me how did I ever know this book like who ever knew this but yeah not everybody knows all these facts almost nobody knows any of these facts right but you know research grinds slowly and this is how you learn these things um yeah so that's it that's the end of my little anecdote about street fighting math and actually the end of all I guess all my stories about TCS so I will just uh end with this slide reminding you of some things and you can find the slide on Theo and stuff too um but I scolded you a little and told you to do some things today to get started you know go to dito set up lch workflow set up all your lch files get your presentation software blah blah blah here are all the websites I mentioned that you can go to and that's it so on Thursday I'll see you and we'll write some math on the board\", metadata={'source': 'qP4XEZ54eSc'}),\n",
       " Document(page_content=\"okay hi welcome back this is lecture two JCS toolkit and today I'm gonna talk about basic asymptotics so for most lectures I'm gonna try to put up some references for the topic so for today I mean if you want to read more stuff about basic asymptotics big-oh and all that kind of stuff estimating functions here are three suggestions for you okay so we've all I think encountered such basic quantities as this one and some is I goes from 1 to N of I sum of the first n natural numbers and I guess we all know the formula that this is n times n plus 1 over 2 or to expand it out a little bit 1/2 and squared plus 1/2 n ok and I trust that you're all familiar with Big O notation and if you're going to use big notation here then you would write that this is order N squared ok so this is the kind of stuff we're gonna be talking about in this lecture but a little bit of a more advanced level than this ok so let me remind you of the basic definition okay so recall they say that a function f of X is order G of X and we should really add as X goes to infinity to mean that hey there exist some big constant C and some other big constant X zero such that f of X is that most C times G of X for all X bigger than this X 0 ok that's T stands for me such that I'll write that a lot okay and so it says that you know usually G of X is a simpler function than f of X and this says that eventually for large enough X f of X is basically no bigger than G of X up to a constant see okay and most people or let's see most mathematicians like to put this absolute value in here allowing the possibility that f of X is like a negative function or sometimes negative function it doesn't usually come up that much in theoretical computer science because we're you know the offeror functions are things like is accounting things or their running times and these things are inherently non-negative and I kind of personally prefer I don't want to like insist on this in this class but I kind of prefer a definition that simply said that I got rid of this and had that f of X is for large enough X not negative and at most C times G of X but we might be a little bit of a ambiguous about this I'll say a bit more later why I prefer this way of writing things okay so this is the meaning of Big O notation while your parameter X which is often when it's going to infinity it's often called n or that's arising often in GCS but you can also use similar notation for a parameter that's going to zero so we have a similar notation for when the parameter X is going to 0 usually as a positive number and the only difference is that this part changes to for all X less than so there's some positive x naught okay and usually in this scenario or the most common situation in this scenario is you use a letter that's denominating denoting a small number like epsilon or Delta and so generally a few alpha times of people using this Big O notation they don't actually write like X going to infinity or X going to 0 what they trust you get the picture based on whether it's called N or epsilon or whatnot but to be totally formal you should write it [Applause] okay so this is a definition that's usually given if you just want like one function and you want to say you know it's on the order of this simpler looking function but often you'll see extensions of this definition which are look more complicated they're not just at this simple form and people use them even though they don't exactly define them and actually the whole situation of definitions around Big O notation is a little bit slippery but I like to use and I propose this idea has not caught on too much but people have proposed using the following extension and they have implicitly use it and I like it which is this if you see this notation like Big O of G of X not necessarily just sitting there playing by itself on the right hand side of an expression then it really denotes an anonymous function which has the property that it's Big O of G of X okay that sounds a little bit circular but what I mean is you know it's an anonymous function f of X that satisfies this property okay such that it's at most C times G of X for some C and X greater or equal to x0 so you might not quite grasp yet what I'm trying to get out here but what I'm trying to get out here is a different way to write this and in fact a more accurate way instead of merely saying that this function grows at most like N squared would be to write that this is 1/2 n squared plus Big O of n again this is very common notation and you see it doesn't quite fit the basic framework that looks like this you've got like an O of n sitting here but the meaning of this is that you know this exact quantity is equal to this plus some other function which it's simple enough we know it's exactly half an but it's it's some other function and you're asserting that whatever this function is maybe you don't know what it is but it grows no faster than the function and so make some sense I should mention by the way that if you have any question just you're gonna shout it out you don't even have to put up your hand or anything just feel free to ask another thing you another way she could write the same thing which is maybe even a little bit nicer is to factor out the main term so the main term here is 1/2 N squared and if we factor it out we got half n squared times 1 plus well I won't even actually calculate exactly what it is I guess it's 1 over N but nevermind it's a order 1 over N okay and actually this is a really appealing conclusion what it's so simple you could just maybe write this but this sort of thing is really appealing the conclusion when you have a complicated expression usually the thing you dream to get or a really good thing to get is sort of the exact expression in like a simple form times something that's going to 1 okay so this is an expression that's going to 1 as n gets large it's sort of not just giving you the the value of the quantity up to some mystery constant factor but it's getting you up to like you know the fact there 1 so this is like a called a approximation with multiplicative air that's tending to 1 okay so this is not like a little bit better even than saying that this is proportional to N squared ok so I mean this is bread and butter in CCS I mean all the time you're have some complicated expression you're trying to roughly understand how quickly it grows or shrinks and you know that may be the thing you're most useful used to is bounding it from above say upper bounding it up to a constant factor and that's the domain of the Big O notation but probably most likely you may have seen a you know the analogs and for the friends of Big O notation and we'll definitely need to get used to them so there's sort of an analogy here we have this Big O notation it is sort of like less than or equal to use for bounding things from above there's the opposite notation which is big Omega and that's kind of like greater than or equal to and the final friend here has big theta and that's kind of like well approximately equal to up to a constant okay so big Omega is just like the opposite of Big O so you see that f of X is big Omega of G of X to mean that there exists a positive constant probably small so I'll write it as little C and also a large constant X zero we continue here well such that f of X is greater than or equal to C times G of X for all X greater than or equal to X zero and this notion of theta is we said f of X is Theta of G of X just it both holds so f of X is sort of at most up to a constant G of X and also at least okay so the first dream whenever you have like a you know complicated expression is to determine like a simple function G of X say your complicated expression a simple function G of X such that f of X is Theta of G of X because then you've exactly nailed down the rate at which it grows up to a constant factor okay so this one we know is in fact theta of N squared because normally is it most a constant times N squared it's at least a constant times N squared as well there's more though so we're gonna introduce some even more such symbols so in analogy with Big O and little oh sorry with Big O and big Omega there's little o it's impossible to tell on the board what's little and big but that's a little oh and little Omega okay and the analogues here are strictly less than and strictly greater than okay so I'll just to find little o in case you don't know it in little omegas again in elegance so for example we say that f of X is little o Q of X well what we can say this use the language of calculus and I guess I will do that somewhat here if the ratio f of X over G of X goes to 0 okay and this is always assuming that here X is tending to something usually infinity okay so this is saying that not only does f of X grow more slowly than G of X it does so in like a significant enough way but sort of G of X is even greater than f of X okay so for example I mean the simplest example is you know I don't know ten and this function is little o of N squared ok little Omega is the opposite for things when f of X grows asymptotically faster than G of X ok any questions so you may think and we're done but actually there's even more notation like this that people like to use and you'll come to like to use it too because it gives even more flexibility but now we're going a little bit beyond what they tell you at the very first so there's some more notation this one I kind of like f of X is folly G of X this means that it's G of X to the Big O of 1 ok already this is again like using this more sophisticated version of the notation where Oh something denotes an anonymous function so here this is know it's a function some function but it is eventually bounded by some fixed universal constant so you can think of Big O of 1 is just meaning like a constant ok and this is saying that f of X is at most this G of X function to some constant power ok actually this is notation it's a bit more slippery than other ones because you don't even see the variable name in here right so if you see something like order 1 you're not even sure exactly what is the name of the variables so which is why it's you know always hygienic ly best if you explicitly state like you know X is going to 0 or X is going to infinity okay and another popular one just kind of fun I also like to use f of X is Big O tilde of G of X this is the one for when you're really lazy this is one if you can't exactly figure out what like f of X is we need more or less figured it out you can use this notation and this one means that f of X is G of x times poly log G of X okay so let's see like big order a Big O of G of X but you allow yourself some factors which are logarithmic in this expression okay so let me give you some examples so a little example here is if you had something like N squared times log cubed and this is Big O tilde of N squared okay this is like when you have a running time and you're like oh I bound it by N squared log cubed n and you want to like say the reader you know don't worry it's more or less like a quadratic running time even though that's not strictly true then you put a little tilt up here and it looks pretty good another example just to illustrate that this tilde does not always literally mean log in itself if we had some expression like + - v times 3 to the n this is au tilde of 3 - then right because you allow yourself some poly logs in this expression the log of this expression is n so you're allowing yourself some extra powers of n here let me ask you a question it's like a little warm-up question it's this thing Oh tilde of two to that I'm getting some a head shaking why not yeah yeah I guess sorry yeah what's your name you sure yeah I guess I didn't only on the fly did I think of like if you had to like literally prove it as opposed to just like feel like it's not true it's it's certainly not true in this sense that two to the N grows much more slowly than three to the N I supposed to really check that it's not true you should look at the ratio between them which is 1.5 to the N and double check that that's not as small as Polly Ann which it's not okay so see you know it just goes to show that like you know you get a little bit sloppy you cannot get too sloppy there is the difference between like three 2002 then okay in fact this notation has another meaning depending on whether your parameter or really depending on whether G of X is getting large or small so this law a notation is only really used when like GG is itself like a function which is getting large so you think of like log of G is something that's also getting large but not as fast you know for the situation when it's a situation on you know when you have epsilon going to 0 and like maybe G of epsilon is also going to 0 then this lacy notation like f of x equals o twiddle G of X means f of X is G of x times you know write less than or equal to here poly log not G of X but 1 over G of X let me just illustrate what I mean here similarily if you have something like f of epsilon I guess I should have written X here if you have F of epsilon is epsilon squared times log 1 over Epsilon okay this is a function that is basically like epsilon squared but a little bit bigger by a logarithmic factor but notice log of 1 over Epsilon this would be a twiddle of Epsilon okay this is sometimes Sal it's kind of expression of sometimes called quasi-linear and I guess sometimes I should mention and some people prefer to write like Oh star instead of oat widdled but I think oh twiddle is a bit more standard and one last bit of notation and then we'll get into some examples I think there's also you know I just erased it but there's also a big Omega twiddle and big theta twiddle and each of them is designed to say like you have to put the logs in the right place to say that you're allowing for some extra factors that you couldn't pin down so for one more example if you're right f of X is Omega twiddle G of X this means that f of X it's not least G of X over poly log G of X okay so for example you know n cubed over log squared n is Omega twiddle n cubed okay so as I said your your uh : life usually when you're founding a complicated expression is to find like a simple expression yeah like if you're trying to bounce some f of X you're trying to find some simple G of X which f of X is either ideally big theta of or even better yep oh yeah thank you I guess since I didn't know what example I wrote yep oh yeah that's another notation you see the question is is there a canonical definition of this expression that's sometimes see like oh epsilon of you probably most likely see okay there's two things that are used for different things one is sometimes I'll see a sub epsilon of one or au sub n of one and sometimes people write this because they're like oh I didn't say what the letter would like the name of the variable was so I'll just like stick it in here like when you have au of one I'll stick it in here too like tell the reader what the the name of the parameter is another situation is like when you have you're trying to bounce some quantity that has more than one variable in it and like the exact meaning of like Big O notation when there's more than one variable is again like kind of slippery but you may sometimes see something like I don't know o sub K of N squared and usually this means they're two parameters can and squared and usually this means the expression in hand is order of N squared if you treat K as a complete constant so like if you prove a bound it's like 2 to the 2 to the K squared times N squared and you know you want to be a little emphasizing the cool part the N squared you like this is Big O sub K of N squared so I can introduce um suddenly non-standard terminology what I thought recently I kind of like it just so that we have a phrase for the kind of simple function that you're shooting for when you're trying to you know bound things using Big O and so forth so this is not a completely standard notation or terminology but here it is so let's say a function G of n is in standard form if it's a product of some different simple kinds of functions okay so let's say type 1 constants a constant so like I don't know 6 or square root 2 pi or what-have-you another kind of expression is constant powers of login you're right I don't know lon ok it's only about this in a second all right lon n on this subject of logs you know this is lon lon n it's a good way to write log to the base e of n because then whether you have a computer scientist or mathematics not that you should hand they always know what you mean by lon in computer scientist like log base 2 usually and sometimes I've seen and I kind of like L sub G and for log base 2 of n so I'm very standard but if you need to make a distinction that's a reasonable one and finally if you ever writing a paper and you write log n if it makes a difference to you whether you mean log base 2 or log base E then you should say in the paper okay it doesn't come up because if you're not trying to bound things very carefully then all the logs are the same in the sense that you know lon n is theta of log N or sorry let's say log base 2 of N uh let me over here just write lawn in for completeness so for example lawn squared and or 1 over root log in our possibilities that's lawn into the minus 1/2 another category of simple functions is constant powers of n ok so for example and cube or I don't know 1 over N squared and another category is exponential functions which I mean constant to n so eg 2 to the N or 3 to the N or 2 to the N over 2 which of course is root 2/2 then let me throw in one more you know and to the C and where C is a constant okay so of course this is not exhaust all kinds of functions but like in many cases when you're trying to bounce something like the nice bound will you know that your function is they go over big theta of will be expressed as some product of these kinds of functions ok so examples of things that are in standard form we really can have some on the board like n to the fifth times 3 to the N 1 or 6 n squared root log n it's another one or root 2 pi root N and an e to the minus n that one has all five kinds in it anybody recognize this expression you can put up your hand or say it yeah yeah what's your name Thomas yeah we'll see you next time but this is a Sterling's approximation says that n factorial is very close to this quantity which is a good thing to know yeah so as I said this is not exhaust all possibilities for what you see for example it's not uncommon to see like log log n and I didn't put that up there but these are some examples and I put these up here because for these particular five examples like each type is asymptotically smaller then the next type even with arbitrary positive powers okay so maybe I didn't express that perfectly but what I mean by that let's say comparing type two to type three is that like you know even lon and the power of 100 is asymptotically smaller then even little o of and to the 1/10 okay so this is the kind of thing I hope you're familiar with but if you're not I mean you can see examples of them here or maybe comparing type two that's like three we can see that you know even you know 102 the 50 is little o of 1.1 to the end and even within one type if you have like a higher positive power you're asymptotically bigger okay so that's an example of how an example of that is that 2 to the N is little o of 3 to then okay good this is you know well 2 to the log base 3 of 2 or something to the power of n okay since I said it's not a you know complete set of functions but generally when you have some complicated expression and you're going to try to find some standard form function that looks a product of the product of these and first step is to try to maybe show your complicated function is big theta of one such function\", metadata={'source': '_gKb855_3bk'}),\n",
       " Document(page_content=\"okay so let's do a bit of a case study it's a case study that I wouldn't have be surprised if you're familiar with but it's a nice one for illustrating some of these points okay and it's the analysis of the nth harmonic number okay so this scent harmonic number is you know a kind of complicated expression that comes up all the time in TCS for example in the coupon collector problem and elsewhere so the definition of this and it is H n I guess it's usual to write it with a subscript rather than as a function and it's the sum of the reciprocals in the first and numbers okay so you may encounter this and what you really want to know is okay well what is this really you know growing like it's a function of N and you may well know already that this thing is very close to asymptotically the natural log of n log base e of n well we'll get there in fact we're not even gonna start by trying to get like such a precise bound we're just going to try to show understand what this is big theta of in other words that it's you know big theta of log in and [Applause] especially when you see like an expression like this that comes from a song you know you should always try to do the most unsophisticated things you can first okay so try to go for the most simple way top of brown things or the simple way to lower bound things usually by trying to like you know make some of the terms look similar so that you can put them together okay so I'll show you a way you can get a reasonable bound on H of n so one thing you could try to do is get an upper bound and to get an upper bound you can say like Wow upper bound it term by term this one I'm going to leave here half I'm going to leave here and I'll say one-third is at most 1/2 that's undeniable and then the 1/4 I'll leave here the next thing is one-fifth that'll say that's that most 1/4 and one sixth is at most 1/4 and 1/7 is at most 1/4 1/8 I'll leave it okay I'm gonna keep going but you see your computer scientists really love powers of two so this is why we're we're doing this okay so this is one eight the next one is one nine that's at most one eight and so forth and in general you can group these things into you can take each number and bring up to the next highest power of 1/2 okay and why is that convenient well this adds up to one this adds up to one you have four copies of a quarter this adds up to one this next thing is going to add up to one okay etcetera and how many groups are we gonna have log base two of n actually there's like some like you have to fiddle for a bit because there's an annoying edge case like what if it goes all the way exactly to a power of two or what if it goes not exactly to a power of two I'm not going to go for that fiddling but you can try to confirm or deny that the following is correct that it's at most log two and four plus one but don't worry too much about the these things it's basically at most log base two of n okay and okay I guess this sort of if you know this it proves that E is bigger than two but it already gave us a upper bound that's of the right form up to a constant log log n ok so that's an upper bound of course we always okay if we only want an upper bound we're done it's Big O of log n so be nice to also have a lower bound and somebody suggests the similar way to try to get a lower bound yep right exactly so kind of bring everything down to the next look lower power of 1/2 so this is like a little bit slightly more complicated but basically the same thing so it's a suggested well actually just leave one and a half there and we'll say 1/3 is at least a quarter okay and 1/4 is also at least a quarter and then 1/5 is at least an 8 1 6 is at least an eighth 1/7 is at least an 8 1/8 is at least an eighth 1/9 is at least 1/16 and so forth ok so hopefully that's also believable so this is 1 this is 1/2 1/4 and 1/4 is 1/2 okay you have 4 copies of 1/8 so that's 1/2 you're gonna have 8 copies of 1/16 so that's 1/2 etc okay so again you basically have log base 2 of n groups and each group is contributing at least 1/2 so this is going to give us a lower bound that's like 1/2 times log base 2 of n ok and if you're really handle the fiddly bits I believe it's at least I have to check my notes 1/2 times the ceiling of log base 2 and plus 1 all right this plus 1 is 4 this plus 1 but again like ignoring the pluses and the floors and ceilings for these purposes this is great we've got an upper bound that's like an order Big O of log n ok so we got H of n is at most order N and H of n is at least oh my god man okay and therefore H of n is Theta of low whoa these are low logins they'd have login okay and that's in protocol at standard form that's very satisfactory and 98% of the time in TCS this is going to be good enough for you I mean you don't usually need to know it to this closest precision we already got up to a factor of two which is quite nice okay any question all right but while we're here I mean sometimes you really do want to tough it out and get the exact constant so let me introduce one last piece of annotation which I guess I saved analogous to this Big O and big theta and so forth and so on it's like the analog of big theta but when you have the constant exactly right but you might have lower order terms wrong so one writes that f of X is asymptotic to G of X this is like a big tilde its backslash som and in tech let's say as X goes to infinity this means f of X over G of X tends to 1 okay which indeed expresses that they're the same even up to a constant fact they're just lower terms are wrong so quickly f of X equals G of X times 1 plus I like to write this this is my own kind of quirk plus or minus little o of 1 because I don't want to insist necessarily that little o of 1 can be positive or negative anyway if you put this in it doesn't look wrong can only help ok and that's an example you know I see example I just erase but we know that the sum of eyes I goes from 1 to n which was 1/2 and squared well we wrote it like this plus order 1 over N okay if this is little o of 1 means it's something that's tending to 0 as it ends getting bigger so we can say that this is a sum product to 1/2 N squared ok and this is like sort of like the one step better of saying you know this expression is theta of N squared it's telling you even with the leading constant factor is okay so if you're really tough your goal is to maybe find a G of n in standard form such that your function f of n is asymptotic to G event so in fact let's actually do this for the harmonic number H sub N and we'll see why it's asymptotic to the natural log of n and to do this we have to use a kind of trick or technique like this proposition H of n is asymptotic to natural log of n this T is a trick or technique which is like the integral trick or integral technique sometimes I like hesitate to bring it up because it's like a very matthew calculus e trick and I honestly almost never have used it in practice yet both now and in the like next lecture when we talk about sterling we're gonna use it so okay I'll tell it to you and it's it's kind of cool there's a warning though I mean if you don't love calculus as much as I do it doesn't come up that much really comes up mainly if you're trying to really nail down these factors so this calculus trick is to try to approximate a discrete sum by an integral like in calculus class so the function in our sum is a function 1 over T I try to draw it ok so this is the T axis this is the function 1 over T this is 1 this is 1 let's put two in two three four N and this is gonna be a half and this is a third and so forth ok and an idea is that this sum the harmonic number sum is exactly equal to the area of a bunch of rectangles that I'm about to shade I'll try to shade it well this rectangle okay so this rectangle has all the rectangles are gonna shade here have a width of 1 and their heights are 1 and 1/2 and 1/3 and 1/4 all the way out to 1 over n because I stopped the rectangle right when it's on the curve okay and therefore the total shaded region here the area of all these rectangles is exactly H of n ok so proof chip N equals the area of the shaded rectangles and now let's see let's start by trying to get an upper bound and the idea for the upper bound is well let's take this first rectangle which has area 1 and just put it aside so we got 1 and then you see all the other rectangles are lying below the curve so if we took the area of the curve from one area below the curve from 1 to n that would be an upper bound on the area of all the other rectangles okay so this is area under the curve from 1 to n ok and now we use calculus so this is 1 plus the integral from 1 to N of 1 over T DT ok and then this is 1 plus the antiderivative of 1 over T is lon T we're evaluating it from n to 1 actually in some sense in some calculus textbooks this is literally the definition of natural logarithm so arguably this is circular but anyway we all know what the natural logarithm is and we can leave it like this ok and this is 1 plus you know lon n - lon 1 1 1 is 0 so this is lon n plus 1 ok so if we secretly know that this is our target upper bound then we're in great shape this is asymptotic to lawn and in the sense that the ratio of this and this tends to 1 as n gets goes to infinity ok all right so now let's try to do a lower bound and we're going to try to do the same trick but we need to like get some rectangles that count up the sum which are like above the curve okay so we try to do this on the same picture okay let's just take every rectangle here and shift it over by to the right by one okay so that will still be the same harmonic some will just position it differently so when I shift this rectangle over it becomes this one and when I should this rectangle over it becomes this one my shifts that's getting horrible by we ship this one over it becomes this one okay when I shift this one over it becomes this one that goes out to n plus one okay and so you see the area of the rectangles is still the harmonic number and now it's the amount of area enclosed is above the curve at least if you go out to n plus one so hopefully you can decipher that picture but what it's implying hm n is at most well the area under the curve from T equals one out to n plus one okay and this is again the integral 1 and plus 1 1 over T DT lon t evaluated from 1 to n plus 1 which is lon of n plus 1 ok so finally we've concluded that lon and plus 1 is less than or equal to H and less than or equal to lon and plus 1 it's a different thing I just read it the same way for fun yep the signs wrong oh yeah thank you great thank you [Music] great um so indeed actually this one lawn of n plus 1 is also asymptotic to lawn T which was kind of our goal sorry lawn n which is our kind of our goal so in fact this establishes the proposition but let's actually dig in a little bit just to like double check for ourselves why exactly this and this are both asymptotic to lawn end and you know one reason to do this just out of curiosity is to try to figure out which one is closer to being lawn in where do we compare the error rates of between these two things in lawn in actually before I do that let me just say what happens if you want to do this argument even more carefully you can start to worry about you know the error of those sort of sort of triangular II error regions we're not going to worry about those today but actually we're going to do such a thing when we do Sterling's formula next time but just for your own edification you can see that so here we've got it's basically lawn and up to like some additive constant it's actually known that H of n is indeed lawn and plus some constant called gamma minus order 1 over N and gamma is just the constant that makes this true it's called the Euler Master Oney constant it's like point five seven seven or something and you will probably never need it but that's good to know about it's actually a more mysterious constant than ye or PI like I think it's not known if it's rational or irrational I'll be at it's presumably irrational okay so let's actually in fact you try to compare these two quantities to exactly lawn n so and this is an example of trying to get things looking like you know some function is asymptotic to another function in standard form okay so for the first one lawn and plus one it's a little bit easier because we can just factor out desired expression this is lon n times one plus one over lon in okay and that exactly looks like well it no longer looks like what we had written here which was f of X equals G of x times one plus little o of one but indeed it is because this is an expression which this whole thing goes to one as n goes to infinity okay this is the one it's like a little bit trickier and well this is the kind of stuff that you will need to get used to as you're playing around with asymptotic so what can we do with lon of n plus 1 to make it look something like this well you have to think a little bit or like play around but you can use the nice property of lawn to do the following this is lawn of n times you tried to isolate the main term you're trying to make this lawn n term pop out so we'll write this as n times 1 plus 1 over N that's true okay and then log of a times B is log A plus log B so this is lawn and plus lawn of 1 plus 1 over n ok so you know what plus 1 over N is basically 1 and log of 1 is 0 so this should be basically 0 which is great but again we might want to know how basically 0 is it and to understand this we do need a bit more calculus so this is an opportunity to introduce you know one more or remind you of one more calculus tool that's you need to do some more advanced asymptotics which is Taylor's theorem so we'll see Taylor series for log you can look this up in a book or type it into maple or Mathematica or derive it yourself using your calculus your vos calculus knowledge a lot of one of X when you think of X as a small number so when you're in the neighborhood of one there's a series for it's X minus x squared over 2 plus X cubed over 3 minus X fourth over four etc and this holds for all X between minus one and zero and exactly but in general this is asymptotic to X as X goes to zero okay so this in general if X is small this is a really small number and you know near zero it could be positive or negative and it's basically proportional to X you see now x squared is the term you can neglect in this big line notation okay and therefore this thing that we are worried about our questioning about it's a form one over n plus an error term which is like even right I'll just write Big O one over N squared okay and now we still actually want to get it to look like this I suppose so let me just write it here we can finally factor out the log in and get that this is lon and finds one plus one over and lon in ulster - order basically what red squared what there's even another log in here okay so this shows that again this quantity is like log n times something that's going to 1 and this is actually telling you how fast it's going to want it's going something like 1 over n even a little bit faster so maybe the punchline is that like this thing is like even much closer to lon n then this one was where we had this expression\", metadata={'source': 'Ao6aXSyJ6Gk'}),\n",
       " Document(page_content=\"okay so I mean it's good to know your tailor series and you know sitting carefully and so on but most of the time you don't get that watch into the weeds of Taylor series much like most of the time you don't need things as fancy as this integral trick and uh let me just tell you perhaps the highest level take away from this lawn Taylor series bit which is this the thing to remember is that you know lawn of 1 plus X is basically X when X is small ok so when you write the paper you know you actually have to worry about the exact error bounds inside Taylor series and so forth but when you're just trying to figure out for yourself like what the answer is you should just be like oh lon 1 plus X that's like basically X and then you can just have a mental picture of how to continue with your calculation by the way you know we have many symbols now and like this this one swung - that's an official meaning like this this - swung - is approximately equal this has no meaning it's like a vague symbol that just intuitively means it's kind of roughly equal to so that's what I mean by that this is a good flag to know but there's an even better fact to know which is what you get if you do e to the both sides of this if you exponentiate both sides of this if you actually expand shape both sides of this then on the left hand side you got e to the X and on the right hand side you get rid of the lawn and you get e to the X is approximately 1 plus X for small X and this one I want to put in a box because this is like the greatest facts of all to remember look if you remember one fact about I don't know Taylor series an approximation and so forth it's this etho X is really close to one plus X or conversely one plus X is really close to e to the X you might want to use it that way - yeah this I mean use this as much as possible in your life and you'll be happy so I mean okay a few bit few more words about this why it's true and what it means given that I just told you this is a meaningless symbol so what is e to the X well a different way in a calculus textbook that they define e to the X as opposed to the inverse function of log and so forth is by this formula e to the X is 1 plus X plus x squared over 2 plus X cubed over 3 factorial plus X to the fourth over 4 factorial plus dot so even this is you know 2 factorial in this is over 1 factorial and over to 0 factorial but this is the in some sense the definition and this holds for all I mean X but we're gonna think about the case where X is small and again when X is a small number X is small and like x squared is even smaller x squared over 2 is even smaller is even smaller even smaller and so forth ok so that sort of justifies this in particular it justifies or with a little bit of thought you can justify that this is 1 plus X plus order x squared we put them plus or minus just in case but you have Plus this is as X is going to 0 in fact you may like to use the fact that this error term here is in the interval strictly in the interval between 0 and x squared for all X between 0 & 1 or minus 1 1 okay but these are some more accurate facts than this key intuitive fact so I will show you actually towards the end of this lecture like an illustration of using this approximation it's a great effect but we'll do a few other things first actually since we're on the subject of Taylor series let me give you some more important Taylor series facts that will come up for you so quite often you might see this 1 1 minus sorry 1 over 1 minus epsilon where epsilon is again a small number since of course it's a little bit bigger than 1 because 1 minus epsilon is a little bit smaller than 1 okay and this is 1 plus epsilon well really you know plus epsilon squared plus epsilon cubed plus epsilon to the fourth etc at least when epsilon is smaller than 1 but you know the thing to remember is that this is 1 plus epsilon plus epsilon squared ok so 1 1 over 1 minus epsilon basically the same thing as 1 plus Epsilon in fact you can kind of derive it from this you see how I mean you can say and this is not 100% precise but you know it's an example of like well let's just get to the right answer and then when we know what the right answer is we'll figure out how to write it very carefully I mean 1 over 1 minus Epsilon I just told you that one - ok 1 plus X is basically the same thing as e to the X so take X to be minus X so 1 minus X is basically the same thing as e to the minus X all right so this is basically 1 over e to the minus epsilon okay which now we can this is a true fact of mathematics this is e to the epsilon I mean its reciprocal and then this is 1 plus Epsilon it's not a joke it's totally I mean you can justify this but this is not how you would actually I I never actually think this is just something like I noticed when I was preparing a lecture I was like oh that's cute it illustrates that this fact is so useful because I don't have this one in my head so going from this to this is straightforward but you can you can do it another one is that root 1 plus Epsilon I don't ever remember the exact Taylor series for this but I know it starts 1 plus 1/2 epsilon and then the next thing is epsilon squared so another good thing to remember square root of 1 plus Epsilon very similar to 1 plus Epsilon it's 1 plus theta of Epsilon and the exact constant is 1/2 of course you can also get this one using the same idea right right because this is approximately e to the epsilon square root Z to the epsilon is like 1 plus Epsilon now by the true laws of exponents this is e to the epsilon over 2 and by this approximate laws is approximately 1 plus epsilon over 2 which as promised it's no joke this is really uh it's really useful oh let's see how we're doing on time okay let's do um practice example I mean this is like instead of lecturer times like tutorial time will just give you a quick illustration of how to do a problem so how about this what are the asymptotics of square root of n plus 1 minus square root of n this is sort of thing that comes up and yeah it's true square root of n plus 1 it's very close to square root of n so this is basically zero but okay now we're asking how close to zero is it and how would I try to solve this problem well I guess you try to put things into standard form as much as possible so square root of n is already doing great so I mean it doesn't get you can't really simplify this anymore but this is not so great I mean it's not in the standard form that we talked about before but we have this belief that it's very close to square root n so we should strive to make it look like square root n as much as we can so the way I would do that is like the following square root n plus 1 it's a little bit like the thing we did with lon of n plus 1 this is square root of n times 1 plus 1 over N I guess this is the main trick and I did this because now I mean by property of square roots root a B is rude a times root B it's a square root n times square root 1 plus 1 over N ok now things are looking pretty good because we can use this fact here with epsilon being 1 over N okay so this is equal to let's say root n times 1 plus 1 over 2 n plus or minus order 1 over N squared ok so it's basically 1 this is basically you know 1 plus a little bit is 1 plus that thing over 2 so this is basically 1 plus 1 over 2 n just for you I even put in like a you know the Big O of the next term which is 1 over N squared okay now we can multiply this out because we're preparing to subtract root n so this is root n plus 1 over 2 root n plus or minus order 1 over N to the 1.5 okay and then we're subtracting and let's write F of n for this so therefore F of n the root ends cancel and we get the next term which is what we're going for 1 over 2 root n plus or minus order 1 over N to the 1.5 okay and this is asymptotically smaller than this if you want you can factor it so we got 1 over 2 root n 1 plus or minus order 1 over N ok now we finally got down into like this is sort of standard form 1 over 2 root n and this thing is tending to 1 and pretty fast too at the rate of the error being like 1 over N ok so we finally conclude that this is asymptotic to 1 over 2 root end any questions okay I'm gonna skip the other I did one more example for like practice but I'm gonna skip it for time but I'll have notes up and you can see how I would analyze some other expression like log of 1 over 1 minus 2 epsilon or something okay I now want to tell you another couple of asymptotic tricks that it's good to get good at and the first one is concerned with inverting functions this is me trying to come up with the inverse function of a given function it's very rare that you can do this exactly but asymptotically it's usually quite doable so I'll give like one of the simplest examples let's say we have this relationship that Y is X lon X and let's assume that X is think of X is large certainly at least one maybe going to infinity so here like Y is a function of X okay exelon X kind of looks like what does it look like that's not comebacks something like this why okay so y is a function of X but you might also ask you know for a given large value of y like this one I really care about this one what is the x value that achieves that Y okay so we want to come up with X as a function of Y and we want to know you know what does this have synthetic to so there's like this is some well-defined function because X alone X is increasing it does not have a standard mathematical name but that's okay we don't quit we can still find out what its asymptotics are mmm so how do we do this well uh kind of do it like a little bit heuristic Lee at first and then you can make things a bit more proper as you go along so one thing you can observe is why is basically X it's theta tilde of X all right it's like saying if you ignore log factors which are small factors it's basically like Y is proportional to X okay and why do I bring that up well first of all the guides as to what's the final answer will probably be I mean X should be X as a function of Y I should also be kind of linear I mean X should be roughly you can see from the picture Ektron actually be roughly kind of like Y but the reason it's particularly useful is that if you take log of both sides law and Y should be and I'll use this you know fake symbol should be approximately lawn X and this is like even more true than this was in the sense that you know law and really compresses things so if these are like vaguely close and these are gonna be like quite close and let me make that a bit more proper if we just take this defining relationship and take the log of both sides there okay we got that law and y equals lon X plus lawn lawn X okay and this is asymptotic to lawn X because this is growing awesome tonically less quickly than lawn x this is as X goes to infinity and that simultaneously means that Y is also going to infinity and now we're in great shape because it means that X law and X is asymptotic to X lon Y and box Tolls it tells us that this is y so we have that oh I should have written it this way exelon Y is asymptotic to x1 X which equals y by the Box relationship and that's good right because like now we can solve for X in this relationship okay we can just divide both sides by lon Y and we can finally conclude that X is asymptotic to Y over law and why okay it kind of makes sense that's kind of reasonable right I mean this is like saying why it's basically like X but big bigger by like a logarithmic factor and so therefore you would feel that like X is kind of like Y except smaller by a logarithmic factor and the only thing to note is that you know that's correct these are both the same I mean it's the log big factor and Y here and an X here but that's the same thing because Y necks are about the same so when I see an expression like this I say I mean these are the words I say in my head before I write it down correctly I'm like wow this basically means that Y is pretty much the same as X and so this log and X is pretty much the same as log Y and that's good because now I can solve for x ok so let me do one more example of this I hesitate to erase this great fact but I guess I got to so let's say I think to myself alright for some reason I have T squared log T equals and cubed solve for T Brian it's like a little bit annoying if this log T weren't here you'd be like no problem I square root both sides and I'm done but I have this log T here which is a drag well how do I solve this well let's start out by taking the square root that's gonna help us it's gonna get us to t times root log T equals and to the three-halves so now again in my head I always say that like okay like this is Beit this is very close to T so T is like basically on the order of n to the three-halves and particularly log t is really going to be like log n up to a constant factor and that's good because then I can replace this log T with log N and bring it to the other side okay so I mean to say it's somewhat more formally therefore log T if I take the log of both sides again log T plus 1/2 log log T equals 3 halves log in and let me not even go down to the level of constants here let me just be satisfied with big theta so this implies that log T is big theta of log n do the same up to a constant factor that constant factor being roughly 1.5 okay now I can plug this in back here and conclude that T times theta of square root log N equals n to the three halves and therefore T is theta n to the three-halves over root loggins [Applause] okay oh great so now let me actually give you an example of like well look at ripe example but an example of where this kind of thing comes up in jeez yes this is concerned like a comb thing that comes up like minimizing or maximizing the expression with a parameter some of you are trying to design like a fast algorithm for some problem on input size n and you know your algorithm might have a choice the parameter choice let's call that parameter of choice T and maybe you could choose T to be whatever you want and it affects the running time in a somewhat complicated way and now you're trying to figure out what is the best T to choose to make my running time as fast as possible okay so let's say hypothetically your algorithms running time on an input of size n is let me just lay this off order n cubed over T plus order T log T this is sort of thing that like happens I don't know you like one piece of your algorithm takes T log T time if you chose T to be something and another part goes faster if you choose T bigger I don't know maybe your bucketing some things into teague buckets and so this is how many buckets you have and there's some extra processing per bucket you can see like it's not exactly clear how to choose the best T right I mean make T bigger that makes this part go down but it makes this part go up okay and conversely so you know for any T you wish so what T should you wish well uh the here is sick and this is like a little you know life tip I want to impart here the heuristic when you have this expression like you have like a sum of two expressions you should choose and give a parameter to play with and you're trying to minimize them you should choose a parameter to make these expressions the same okay it's not a proof of anything but like this is the heuristic that works and why does it kind of work well let me just draw a little picture on the side one thing is that this is another little life heuristic a plus B is basically the same as maximum of a and B and so sometimes it can be good to pass them for back and forth between these two things so this is another like good Factory number why do I say this and by the way I'm assuming things are non-negative here well a plus B is certainly at least the max but also it's at most two times the max because well you just increase whichever one of these is not the max up to the max and you get 2 times the max so if you don't care about constant factors and clearly in this example like we have bigos we don't care about constant factors these two sums are the sum is basically the same as the max and so your problem is equivalent to like how do I find to choose T's that these are both simultaneously small and if you kind of think of this one is like a of T and this one is like B of T think of n is fixed a of T is getting smaller as T gets bigger and B of T is getting larger as T gets bigger and like the max here is like this so that's kind of my justification for this heuristic that you should try to choose the T that makes where they cross where the two quantities are about the same and another good thing here is that like you don't in this situation you don't feel like rigorously prove much of anything because at the end of the day you're just gonna say like I choose T to be this and therefore my running time is that and of course you want to choose the best to you can well you don't have to justify that you chose the best to you because you don't have to you can just leave the t i chose and of course the reader probably believes that you did it effectively anyway so for this heuristic if we set the snob and cubed over T to be equal to T log T I ignored the constants already well this is n cubed equals T squared log T and I did it yeah so this is the same as my example and therefore you know T is Theta of and to the three-halves over root login you can see that like i was not careful with the constants so like in your in your paper you could just say having done this analysis I choose T to be let's say exactly this forget the theta or you know this rounded off to the nearest integer and that'll be the most effective T and you can see that this final thing what will it turn out to be both terms should turn out to be the same thing unless we've made an error so this is n cubed over N to the three-halves over root log n so this is order and to the three-halves root log N and this is order ok so the log of this is just log into the power of 1 so that brings the root log in up to the top and to the three-halves root log n ok and indeed these are the same thing so the final answer your final running time is this this is an example by the way of like how a strange kind of running time for example like n to the three-halves root log n could arise I mean this is not too unusual this is not too unusual and then it arises due to this kind of minimization process\", metadata={'source': 'flge97Jp6uo'}),\n",
       " Document(page_content=\"all right so um yeah i just want to uh do some stuff today um and since we don't have any homework i thought i'd just do like a little street fighting mathematics stuff and maybe some likely checking stuff and like i don't really prepare these i just try to do some stuff off the dome so we'll see what we'll see what happens um as always if you want to ask any questions like just type them into the chat or pipe up if you want um yeah let me switch my computer to be sharing uh this screen okay let's see okay ideally now you can see a screen where i can write so awesome um yeah so i thought i'd just like show you like a little bit of like uh mathematics some street fighting mathematics some low tech some stuff like this so um i got really interested lately in the prime number theorem which is like the theorem about like how many prime numbers there are and one thing you study when you study the prime number theorem is what's called the central binomial coefficient so this is like um n choose n over two okay you can assume that like n is even and um sometimes people call this c sub n and actually like uh it's like critical to kind of like study this if you want to like know about like how many prime numbers there are uh it's also like uh critical to study this in probability theory like especially if you like um flipping coins so you can just remember that like okay n choose n over two is like the number of ways you can have like a string of like heads and tails uh with like n symbols in it and like exactly let's say n over two h's okay and um in particular therefore that like you know c n over two to the n that's like all possible strings of heads and tails this is like the probability of getting like exactly half heads when you flip it n coins and that's like an interesting quantity to uh care about what's the chance if you flip like you know a thousand coins you get like exactly 500 heads and um yeah what i want to just like talk about right now is like what is the asymptotics of that i mean how likely is it is there like 30 chance you'll get like exactly half heads is it like uh exponentially small chance is it like one and then what is the deal with it and perhaps you already know this perhaps you don't already don't know this this is like a great like fact to learn and like it's like a well-known fact that comes up a lot so let's see if we can uh figure it out and yeah i wanna just think about like how you might trying to go about and understanding the asymptotics of this quantity so we're gonna actually review asymptotics more in the next lecture but you know hopefully you know a little bit about like big o and stuff like this so i was thinking of like just like diving into like how do you do it um but like i want to even like imagine now that like i don't even know how to um you know compute this and i want to like figure out what the deal is with this quantity or just in case like you know it's not obvious this is cn is like n factorial divided by n over 2 factorial n over two factorial so uh let's let's try it like let's imagine and i wanna like know how does this scale is you know n gets really large so let's try to figure this out so i will now switch back to my other computer um see if i can do this and okay so this is uh i got some whatever bologna up here okay so this is maple as i mentioned like the more common thing that people use these days honestly is like mathematica but i got hooked on maple as a youth and so i kind of know it sort of well so i'm gonna use maple um but yeah like let's imagine like we didn't know uh anything about this central binomial coefficient and we wanted to figure it out so we can do like c maps n two binomial and n over two cool so now if we do like c6 that's 20. that should be like six g is three it's like six times five times four over three times two times one i think that's correct okay so we could like plot it so we can plot like c and we can plot it from like and going from one to i don't know 20. okay so it looks like that so obviously it's growing pretty fast it's growing um seems to be growing like exponentially um you know this is at most uh two to the n because like it's the number of all possible strings with like exactly half h's in it so it's at most all the possible binary strings which is two to the m um so we might wonder like suppose we think to ourselves hmm like i wonder if like maybe it's like exponential in n like maybe it's like a constant to the n now actually if you're in like maple for example you can just it'll know the answer you can just like like type asymptotics of like c of n n and it will tell you this actually looks a little gross but i guess you can see down here it's like it's asymptotic to like 2 to the n and then this thing is like something like square root 1 over n it's like a constant root 2 over pi times 1 over n that's the main square root 1 over n and that's the main term and indeed it is true that like the asymptotics of this is we'll eventually uh like know it's kind of like 2 to the n over square root n um and therefore like the probability of getting exactly half heads is like that divided by two to the n it's something on the order of one over square root n and um you can obtain this like quite exactly using uh sterling's formula which we're going to talk about later in the course but just today like i'm not going to worry about like these exact constants like two and pi and things i kind of just want to get at like the knowledge that like this quantity is roughly proportional to two to the n divided by uh square root n um but okay let's imagine like we didn't know about this asymptotic command are we just like you know maybe maple if you have like a more complicated thing like the computer can't figure it out itself so like how would you figure this out um so like one thing you can do is like um okay if you imagine that it might be like uh um some constant the power of n like maybe you're like i don't know maybe it's like rose kind of like 1.9 to that um you know how much you try to discover this by plotting feel free by the way to like pipe up with any questions you have or like type them into the chat um well i suppose that like uh if you think like maybe okay let me just type here if you think that like maybe like c of n is like maybe asymptotic to like um uh r to the n [Music] okay maybe we might think that then i guess like you know you would have that like log of c is asymptotic to like n times log r so if you plot like log of c log of this thing it might look linear so i guess we could try that let's see how this goes let's plot like log of c n um from i don't know like 1 to 20. and yeah it looks pretty linear so like okay cool now you might be like all right it does kind of maybe it's a first order approximation grow like you know some constant to the power of n so i guess like the slope of this is like log of the base of the exponent so we could like try to figure that out by let's look at this like log base 2. so if we divide by like log base 2 of n hmm i expected this to be like uh oh no should we we should be dividing by just n not log base log of n let's divide by n okay so now it seems to be like asymptotically approaching something let's make this like 200 all right let's make it like 2 000 okay it does seem to be i mean i you would be like uh reasonable to guess that this thing is like approaching one in the limit in other words you might think to yourself all right i kind of think that like log base 2 of this like c over n is like maybe asymptotic to 1 which if you solve this is telling you okay like maple wants to tell me that e to the lawn two is okay there we go so this is like suggesting to you that like okay it's roughly like two to the n which is true i mean up to like sort of the lower order factors it is closer to the n so um yeah now what if we want to like refine this more like we have this belief that like c to the n is like roughly 2 to the n so we could divide it by 2 to the n and plot that so let's plot this to i don't know 20. i guess this question i'm probably getting is like the probability of getting uh c of n over two to the n's like the probability of getting exactly half heads if you flip point n times okay so like this is going like down so yeah you might again want to try to figure out what this is looking like it's looking like like one over n or something like this actually we know in the back of our heads it's kind of like one of our square root n because maple already told us the answer earlier um so let's see maybe we should do the i kind of don't like things going to zero i kind of like them growing so we if we know this is like one over something like maybe it's one over n maybe it's one over square root n let's do the reciprocal of it so like let's do two to the n over c and okay this is like growing so now if we like pause it that this is like actually from looking at this you're like okay this is not growing linearly right if this is blowing linearly it'd be like a straight line but it's more like curving right so like maybe this is like log n maybe this is square root n with this square root n i guess we can kind of figure it out by a similar trick right like let's say we think this might be like if this is you know growing like n to a power like n to the let's say a okay you're not really supposed to type into maple like this but i guess if we like take the log of it we'll get like a log n and then if we divide by log n it should go to a and then we'll learn what that power a is so uh let's try that let's do like log of this thing over log of n okay so it's a little hard to tell because it's getting i mean a little confused around zero so let's start it at like 20. okay it's not so obvious that this is like approaching a limit so at this point you might think like oh maybe it's like it's not like end to a power maybe it's logarithmic or something but let's make it big okay now it looks a little bit better what i'm looking at here is that this kind of looks like it's asymptoting here at some like line let's go from 2000 to 200 000. okay okay it doesn't like two billion or whatever i typed there um let me stop this how about this much okay it's a little i mean this is like not the greatest experiment like you know it it does go i mean we know i i told you in the back of our heads or maple told us that it does go like square root and it's like not super obvious that this is like limiting out at like uh 0.5 but um i guess you know from this you might kind of get the guess that this quantity that we're looking at 2 to the n over cn it might be reasonable to think that this is like roughly square root and ish um and uh yeah that's indeed true actually we can even you know maple like knows these things if you go like limit c of n if you think that like c of n is like maybe two to the n over square root n um times some constant i think like maple will tell you even what that constant is by like taking a limit yeah so this is like another way you can confirm like using you know maple superpowers or mathematical superpowers or whatever but c of n is kind of like this funny constant root 2 over pi uh times 2 to the n divided by square root n and um yeah we'll learn this as i said later in the course with like sterling formula but um you know that's how you like might discover this like came up for you and you're like geez or it's like something similar came up and you're like man i wonder where is the asymptotics of this expression and uh yeah so some way you can discover it so uh yeah let's now imagine like how we might um prove it are there any questions by the way there are feel free to pipe up or you can ask them in the chat um [Music] let me go over here okay so now you can see this again hopefully uh so now i'll show you like a proof of this i mean it'll be like a cute proof you know this you know might be something that you have to work a little bit at to discover if you wanted to prove this but you can prove this in like a pretty elementary way so i'll show you how to prove this what we want to show is like you know our belief that we got out of like a maple or whatever is that um c of n well how we're going to write it eventually is like big theta of 2 to the n over root n so we'll talk about this like big theta in the next class if you don't remember what it is or don't know what it is it's like a combination of like big o and big omega so big o means that it's like at most this quantity up to a constant factor and big omega means it's like at least this quantity up to a constant factor you know up to a different constant factor and we're not going to worry about like nailing down that constant factor with the pi's and anything right now let's just try to prove this fact okay that's not my best drawn rectangle but there you go uh okay so a good way to try to prove it let's just write uh let's just like write it out um so cn let's do it let's do it with like n is like 10. so c sub 10. you know always try like small values of n is like another good life tip okay so it's 10 factorial which is 10 factorial over 5 factorial 5 factorial okay so that's like 10 times 9 times 8 times 7 times 6 times 5. it's boring for you but like another like tip is i i suggest is like literally write it out you know you like to be tempted to be like oh just like dot dot and um you know it's fine but i think it's better if you like really write it all the way out you'll really get to see and you won't get confused by your own like ellipses uh okay where was i so five factorial it's like five and four actually one thing you might notice is like the same number of factors in the numerator and denominator so let's even try to like make the spacing all nicely nice 4 times 3 times 2 times 1. okay so one obvious thing you could do here is like cancel right we see like here we have five four three two one five four three two one so okay we got to this far and now what are we gonna do about this it's gonna be like a little bit uh tough um [Music] because like it's a multiplication of like a lot of things together now one route that we won't take in this like i don't know recitation but it's like a good life lesson that we'll take later is it's harder to like approximate like the product of a bunch of stuff uh than it is to approximate the um sum of a bunch of stuff so like you see like a giant like product like a good good tip is to like take the logarithm and then it'll convert you know that converts it to a giant sum it's like somehow easier to like analyze like giant sums if you want to get really fancy you can like analyze like a giant sum by approximating it by an integral and that's what we'll do when we eventually get into like sterling's formula and stuff uh but actually luckily you can handle this one i mean i happen to know you can handle this one without getting into like logarithms and like sums and stuff like that so uh let's try that so uh right so one thing that we're trying to i mean one thing we're trying to show is we kind of believe that like okay i mentioned 10 is n we kind of believe that this is something at first like roughly like 2 to the n and like even more accurately you know it's supposed to be like 2 to the n over root n but let's just worry about like this like okay roughly 2 to the n first so uh yeah if you want to get at that if you believe it's not a like two to the n like maybe it's good to like divide by two right so we might do that and therefore say like okay so therefore maybe like c10 over 2 to the 10. uh looks like ten times nine times eight times seven times six over two times two times two times two times two all right there's like five more twos here uh let's see wait we have like five times four times three times two times one times two times two times two times 2 times 2 times oh there's 10 of them you can tell like i did not prepare this because i vaguely remember that this like would turn out in like a nice and slick way but it doesn't seem to be um i know like i kind of like wanna by the way if you have like a you know trick that you see let me know um i kind of like when i'm like match up these factors yeah uh so actually since we have like 10 factors here maybe i kind of want to go back to the old version where we didn't like cross out these like 5 times 4 times 3 times 2 times 1. um let's try that so if actually we cross this out or maybe we'll just write it here this is like 10 times 9 times 8 times 7 times 6 times 5 times 4 times 3 times 2 times 1. okay i guess like going back here right we had like 10 factors in the bottom 5 times 4 times 3 times 2 times 1 5 times 4 times 3 times 2 times 1 and now we're also going to have like 10 that's 10 factors in total we're also going to have like 10 twos in the denominator so let's put them all together like 1 2 with each factor in the denominator so we get like 10 that's 2 times 5 times 8 is 2 times 4 times 6 times four times two times ten times eight times six times four times two hmm okay so far so good i suppose it looks at least it looks kind of nice right okay so this is like c sub 10 over 2 to the 10. and we have like n terms here i remember where this goes so now we kind of think that this expression based on this like box up here we kind of think of this expression should be like vaguely on the order of one over root n okay so okay imagine this 10 where n we're like okay is this like approximately one over square root n and you know square roots are like not so nice to deal with right so you might square this expression and then hope that that's close to one over n so we can try that too another thing is like you know there's like n things in the numerator of the denominator so you might try to get like n involved rather than square root n so let's try this so um yeah what i'm saying is like this is c sub 10 over 2 to the 10 squared is okay so we could write it out and it'll have like 20 factors so let's try this it's like 10 times 10 times 9 times 9 times 8 times 8. let's let's really do it if i run out of room let's see okay i did it 10 times i'm squaring the bottom 10 times 8 times 8. hmm actually i should put these in like descending order so i have like some tens over here and some eights over here so i actually have like four tens once i square it ten times ten times ten times 10 times 8 times 8 times 8 times 8 times 6 times 6 times 6 times 6 times 4 times 4 times 4 times 4 times 2 times two times two times two ah this is looking pretty good yeah because um oh okay this is gonna work but i think there's like a smarter way to do it um so we're hoping that this is now like oh wait we're hoping this is on the order of what 1 over n right let's see 2 to the n c to the n over 2 to the n we want to square it okay so we're hoping that this is like somehow approximately one over n okay i think the trick now is there's some telescoping wait how many terms do we have here one two three four five six seven eight nine ten we have twenty i thought that like i should have prepared this better you know i thought that this i thought that like um [Music] i thought that this turned out to have like 10 terms and like some nice telescoping would happen oh we can cross things out though [Music] let's try that so let's some of this cancels ten times ten eight times eight six times six four times four two times two two three four five that didn't cancel too much oh it's another six times six ten times ten six times six eight times eight four times four two times two okay done this in a messy way but let's see how this proceeds um maybe there's a faster way to get this 9 times 9 times 7 times 7 times 5 times 5 times 3 times 3 times 1 times 1 10 times 10 times 8 times 8 times 6 times 6 times 4 times 4 times 2 times 2. it's probably a faster way to get here than i just got here ah okay cool so now the trick that i remember just kind of come into play um all right so the trick for this now is let's prove an upper bound so let's say this is at most and here's like a cool trick we can do so what i'm going to do is say like this a half is at most two-thirds and this three quarters is at most four-fifths hopefully you see where this is going and this five-sixths is that must six-sevenths and this seven-eighths is at most 8 9 and i guess let's continue the pattern this what is the pattern here this 9 tenths is at most i guess 10 11. and then let's put in the other terms that we left let's just leave the nine tenths here this is like nine tenths this is seven eighths this is five sixths this is three-fourths and this is one-half ah hooray uh luckily this good thing happened and this is like cute right because like now everything cancels two cancels two three three oops three four four five five six six seven seven eight eight nine nine ten ten and we get like one over eleven okay and like clearly this one over eleven this is like in general this is one over n plus one okay so like in a roundabout way that i think we should now like go back and simplify um slightly we finally got to i mean clearly we got to c n over two to the n squared is at most 1 over n plus 1. okay now we can rearrange this if we want this implies that like c [Music] n over 2 to the n is at most square root 1 over n plus 1 and that implies that c n is at most 2 to the n over root of n plus 1. so it's actually very close to 2 to the n over root n and maybe in a later lecture we'll see like what to deal with if you want to like understand root of n plus 1 versus root n but that's that's pretty good uh okay so that was like the upper band and we would like to also have like a lower band um so let's see we had some exact expression oh right here we have a nice exact expression so let's go back to it perhaps you can see how to do a lower bound from this maybe can anybody can suggest it i'll think about it too we want to show that this is a at least something it's going to be like a similar telescoping trick i'm pretty sure can you simply like turn the turn the second 9 over 10 to 8 or 9 and like similarly yeah thanks let's see so my suggestion is to turn this one into eight over nine you said uh the second one oh the second one okay let's try that this one into like eight over nine so this is at least nine tenths is at least eight ninths yeah that's right it's like point nine and uh point eight eight eight eight eight eight eight so you're saying this is at least this is going to be good i think nine tenths we're gonna leave and then eight ninths we're gonna keep and then uh seven eighths will oh sorry we'll change seven eighths will keep the next one seven eighths we'll change this to six sevenths presumably yeah thanks six sevenths okay this is good that was that one okay let's finish it so like this is going to be all right you can see i guess where this going is like five but let's go to the end so we don't like screw up the end five six times oh man one two three four five okay we're at like five six okay so we're going to turn this like fourth or fifth i guess okay let's figure out what it ends on and then just go to that so like it's going to end on oh oh well you have to keep the last one yeah i have to keep the last one i was like don't change like one half to zero once because then you'll get a lower amount of zero which would be very poor um actually like yeah you can kind of see like you're really hurting yourself a lot at the when you change it at the end um it's probably where some of the sloppiness is gonna come in so anyway this one we're gonna change this is like the second last one right we're gonna change this one to two-thirds and then we're not gonna be like dumb and change this one to zero over one like that's a bad idea so uh let's go down until we get to like two-thirds four-fifths uh three-quarters uh two-thirds one-half and then we're going to leave this one as one-half i suppose i think i did that right uh great and then we're gonna get some nice it's like the multiplication version of telescoping i suppose um and what's left i guess we're left with a half and a tenth so like this is at least one-tenth times one-half which i guess clearly in general this is going to be like 1 over n times 1 over 2. cool so uh assuming i did that right we also learned and this thing we're studying cn over 2 to the n squared is at least one over n times one over two um oh actually okay let me finish this so uh therefore c n over two to the n is at least one over root n one over root two which implies c n is at least uh 2 to the n over root 2 times root n and actually i just realized uh you know we can make the above thing a little bit nicer i'm looking at this thing here uh it's got like divided by root of n plus one and uh i wish to compare this to like root n for simplicity but i guess like one over root of n plus one is actually smaller than one over root n right like one over root n plus one is less than or equal to one over root n you know like one over root three is smaller than one over root two um for example so i guess what i'm saying is like this is um at most 2 to the n over root n and that's like nicer because now we can more directly compare things so we got this is our upper bound and this is our lower bound okay that's pretty good i mean this is implies this thing that we're trying to say that c n is big theta of meaning up to a constant it's proportional to 2 to the n over root n and you know the constant factor difference is pretty good it's only a factor of like root two which is like 1.4 um you know as we saw earlier with our experimentation like the exact constant is like root two over pi which is pretty neat uh but we'll worry about that constant later and that constant like really comes up in cs theory i mean like this fact comes up like a ton and this is like a very good fact to know if you flip uh n coins the probability that you exact get exactly half heads is roughly like one over root n you know knowing the exact constant factor in the front is noticeably less important any questions about that okay um [Music] cool so uh one thing i was gonna do is now like type this up in tech to show a little bit of that but i think i can't like help but okay there's actually two things i want to do first i want to like you know i got myself a little bit confused here when we had like all these dot dot dots and so if we do type this up like i kind of want to get the most efficient way of getting from here to there so for example i suppose the the critical thing was like getting here then the the thing we kept using was this one right here like we want to get from here to here in like the most efficient fashion so what is like the most efficient way to do that i mean granted like what we've written here is only like three lines but like it kind of confused me a little bit so if somebody like spots like a nice brief way to get from here to there it'll be good i mean there's something that you should think about right if you're like now like oh maybe there's a homework you're gonna type it up i mean in principle like you could just copy exactly what's like written on your notes and you know it's mathematically correct and therefore you know probably the tas would be obligated to give you the points but you should you know take video on them a little bit and like try to present it in like like a nice fashion so you should like take a little bit of time to figure out like what is like the most elegant way for you to get from here to there oh sorry go ahead yeah i think one way that worked for me was um through the entire ratio as log and then so one of the uh um the log2 will factor out and then you have m plus a sum of logs and then for those sum of logs can be simplified in the same way that you did here um so i guess the way i'm doing is that so for each i in n yeah um so there will be a factor of 2i times 2i minus 1 over i times i let me see if i can write what you're saying so this is like the i mean uh the way of doing with logs which i said at the beginning is like a good suggestion i want to eventually just kind of go from here to here and like maybe uh we can do that in like one line but let's let's see what you're saying too i mean why not let's check it out so let me just try to write what you're saying so you start with like c uh all right i'll continue to write 10 for fun uh c10 over two okay so first of all with c10 did you write it as like did you write it as like this yellow box here or did you put in like all the factors on top and bottom it's a different way so it's like a 10 10 times nine over five times five and then eight times seven over four times four oh see ten times ten over five times five right oh sorry ten times nine over that oh right i see so you paired it up like uh you paired like in the numerator you put in like the natural yes yeah so descending order but like okay in the denominator you liked it okay so you did five times five four times four three times three two times two one times one and then like the numerator you put ten times nine times eight times seven six five four three two one okay that's natural and this will break uh break out nicely so um it's if we take the um block or like if we don't take the log so already the 10 over five eight over four six over three already gave you what two to the n that's okay good that's good you're saying like let's do like this one this is already giving you the two to the n uh yeah two over two i guess yeah uh oh yeah actually that's a little bit funny two to the n over two but okay so i mean yeah you can take these out and you get like two to the n over two for sure and then what are you left with you're left with let me just write this nine times five seven times four five times three right and each of them is upper bounded by two so you get an extra two to the n over two uh that's true each of those upper bounded by two so you could say like this is at most two two two two two that's your other n over two factors of two so this gives you two to the n which is good and the lower bound would be um you're changing this to be um like a over five and then um six over four and four over three and two over 2. let's see if i get that again so we do want to get something a little bit okay so this is good we do want to get something a little bit better than at most 2 to the n but we can come back to that so uh so for the lower round you said let's put this as what 8 over yes 8 or 5 and then six over four six over four right and then four over three so you can see like the eight and four and six and three and four and two cancel out two over two oh sorry what cancels out uh i think like the uh the first eight and the second four first eight so in the second four that's oh this gives you a two this gives you a two right and then six and three also cancel out so this gives you a two right oh actually okay so wait then this gives you a two so that's the rest of your twos yes yeah and then you have one over five yeah uh two to the n over uh and then the five is i guess probably in general like and by n yeah right so yeah this gives you okay so this gives you an upper bound of two to the n and like a lower bound of like two to the n over n maybe also over two so like with an extra two here right yeah actually this is uh nice this doesn't quite give us you know we wanted something like over root n and like over root n here but actually this leads me to i mean this is also uh nice and this gives uh um this actually leads me to like this is actually yeah this is like the the bounds that are most straightforward to get i think so let's see yeah um little bit yeah so i mean here there was like some trickiness to exactly get this like root end factor into the business um but these are already good i mean just even actually even already this is um pretty strong see we kind of a little bit already know it's at most two to the n because like this counts the number of strings that have of length n that have of h's and t's that have exactly half h's and like that's at most the total number of strings which is two to the n so that gives you like this upper bound but like it's really not like super obvious how to get like a decent lower bound to know that it's like i don't know at first you might be like oh maybe it's like exponentially smaller maybe it's like 1.9 to the n in in reality so this already gives you something pretty strong that shows that it's like that's practically almost two to the n um and let me even like okay so like this thing i showed you is like a sequence of like you know tricks that like in advance you can't really know like if this was gonna work or not um you know i went through it because i happened to know in advance that it was gonna work and there's like a way to like trickly telescope it to get a really good bound um but who knows right i mean like this is i mean what bingbin showed also is like a natural thing to do and like it doesn't get quite the same bound so um you never know exactly what's what's going to work i mean i guess like mess around if you're going to try with this let me actually tell you how i kind of really really think about this problem um and where i think like what i think is like the reason that like the answer is one of the root n like there should there should be some like intuitive reasoning that just doesn't come from like goofing around and uh this is how i think about it this will also get into some stuff we'll talk about in like the next couple of lectures some probability stuff so this c of n over two to the n uh is like roughly one over root n this is what we're interested in more using more notation like theta of 1 over root n um so why is this true so remember like this is this has a meaning as well this is the probability of getting exactly uh half n over two heads and flipping exactly n coins so it's small but it's like not incredibly small like if you flip a million coins the probability of getting exactly 500 000 heads and 500 000 tails is proportional to one over square root of a million that's like proportional to one over a thousand so you know it's rare but it could happen um yeah so exactly again you know for the upper bound is at most one because probabilities are at most ones and this is equivalent to the c of n being at most two to the n but what about a lower bound i mean if you uh what happens if you flip like a million coins and like i should really do this like in in uh maple or something but i'll just tell you uh let's say you flip like a uh like n coins like a million coins a lot of times you plot like a histogram of like how many times you got like each outcome um then it'll be like centered around like 500 000 or let's say n over two and you know anything is possible from zero one two up to n but if you like plot a histogram of this it'll turn out to look like this like unsurprisingly like the most common like number of heads you'll get is right in the middle 500 000 even though it's not actually that common um and it's like a little less likely to get like uh this would be n over two plus one again i'm assuming as even for simplicity here and then uh this is like n over oops this is like n over two minus one and um you know you may know that actually it's gonna kind of look like some sort of bell curve situation here um and uh if we normalize by like the number of trials and we'll kind of get like the empirical chance of getting any different number of heads in the outcome and so if you think of it like these like heights is like uh like the probability of getting if we plot here like if this is um a little x and this is like the probability of like getting x heads um well one thing that is almost was very intuitively clear and it's like not even that hard to prove is that um the most likely number of heads you'll get is exactly half like n over two which kind of tells you like the tallest bar in this like bar chart is the one that's like around n over two and um well there's like n plus one possibilities like there's uh there's n plus one like uh bars in this plot and so if like the tallest one is the one in the middle and there's like n uh plus one of them then this tallest one has to be at least one over n plus one just think of it as like one over n i mean whatever so like this is the tallest one it's already gotta be at least the probability here has to be at least one over n plus one because it's like intuitively the most likely one to occur um and that tells you i mean if you justify that which you can think about how to justify that let me skip it this tells you that like the probability of exactly uh n over two heads is at least one over n plus one so like in your mind think of this as luck roughly one over n and this is actually more or less the same thing as a bingman's bound like um this tells us that like c n is at least two to the n over n plus one um and this is not far too far from the truth i mean the truth is more like 2 to the n over square root n but like this is not so far from the truth that's already like a nice way to get a good lower bound um now suppose we want to like do better than this and let me as i said like try to explain like why i think uh how i personally like think about this um well there's like two things you have to improve about this bound right you have to show that like cn is at most something and cn is at least something um so which one should we work on first like c on n is at most something or c n is at least something uh i leave it to you no takers okay let's try to improve the lower bound so uh right now we're like maybe worried about the possibility that this is sort of the truth and like what if what does it mean if this is the truth or roughly the truth it would kind of mean that like all n of these bars i mean if this is the tallest one but it's already like kind of close to one over n then it would kind of mean that like almost all of these bars are like equally large or at least i don't know like a large number of them are um equally large like we're kind of trying to decide in our mind if we plot this like more carefully like perhaps you know they're all about the same height like if you go out like even like out here still pretty tall like this is like in the neighborhood of like 1 over n actually we might speculate that that's probably unlikely because we know like the probability of getting n heads like this height we know for sure this is like 2 to the minus n so it's like way smaller than one over n but like i don't know if we were like didn't know too much a priori it's also two to the minus n chance of getting zero heads we didn't know that like in pre-hurry we might be like oh i don't know like maybe like a large chunk of them have like a pretty large probability like one over n um and for the upper bound side we're worried about the opposite thing like oh maybe if you do this bar chart like carefully like the probability of getting like n over two is like super likely like maybe this is like you know thirty percent chance and like maybe getting something other than n over two is like super unlikely like these are really small and actually the true answer answer is like neither of these two things like you know we may we'll come to learn this later like we the true answer is that you know if you plot this in a histogram it looks like this like bell curve and it really starts to like flatten out in the neighborhood of um n over two plus or minus something like square root n it's like maybe n over two plus root n and like this is like maybe around like n over 2 minus root n and i'll explain why that is in a second but like it kind of means that like you know most of the probability is kind of in getting a number of heads that's like n over two the mean you know plus or minus square root n and therefore like all these bar heights are approximately the same and they constitute most of the probability so they're all you know there's about well there's like two times square root and many um possibilities in this range and they all have you know more or less the same probability and so those probabilities add up to around one or around some constant and so um each of those probabilities is in the neighborhood of one over square root n so that's sort of like what's truly going on and uh this is the reason why the probability of getting exactly n over two heads we're getting like exactly n over two plus one heads or n over two minus three heads these are all like in the neighborhood of one of our root men um and uh well again like i guess talk about this like later in the class but let me uh i guess i probably won't do much latexing in this uh that's okay um i guess the the let me just explain like why is that the case i'd like generally like you know the number of heads you get will be like n over two plus or minus something like square root n um this has to do with like probability so uh like spoilers for future lectures i think but let's say you do let's say you flip and coins and like let's say let x sub i be one if the ith coin is heads and zero if it's tails okay therefore like the total number of heads is x one plus x two dot dot dot plus x and it's like a classic uh you know programming thing you know you cast like a boolean to like zero or one and then you add these like up and get the total number of outcomes that occurred the right way and [Music] um so one thing we can know about this is the expectation of this let's call this h expectation of h we'll talk about this later in class is the expectation of x one plus dot dot dot plus expectation of x n um this is true for any random variables if you have a bunch of random variables the expectation of sum is the sum of the expectations this is called linearity of expectation and we know that each of these expectations is a half because there's a 50 chance you get one and 50 chance you get a zero so this adds up to n over two okay and this tells us something that we most likely knew that the expected or mean number of heads we're going to get is n over 2. um but the reason that you know the typical number is not like n over 2 but it's sort of in a neighborhood of n over 2 that's around square root n-ish is due to the standard deviation of h and that's the square root of the variance of h and let's figure out the variance of h so this is actually the variance of x one plus dot dot plus the variance of x n now this is not always true there's not like a linearity of variance but um it is true if these random variables are independent which they are okay so this is like a true fact about probability uh dependent if a bunch of independent random variables then the variance of their sum is the sum of the variances and these are all the same the variance of x 1 is the expected value of x 1 minus its mean squared okay and its mean is a half so this is like okay with probability of half you're getting one minus a half squared which is a quarter with probability a half you're getting zero minus half squared which is also a quarter so this is a quarter so each of these variances is a quarter so the total variance is um and over four so therefore the standard deviation h is the square root of this which is like root n times a half okay so uh that's great it tells us the standard deviation of the number of heads is like a half root n and the standard deviation is actually a pretty well named term like its name makes sense it's kind of like uh on a good day like the amount you expect the random variable to be different from its mean so it's kind of saying like okay if everything was like nicey nice then we actually you know do this experiment and like flip a million coins you know the mean that you expect is 500 000 but you also kind of expect to be on the order of a half square root n which is a half squared a million it's like a half times a thousand so 500 so like you kind of don't you wouldn't be surprised if you were like plus or minus 500 from the the mean 500 000. now as we'll talk about later um this is not always the case there are like um annoying bad random variables we don't like which don't have the property that they're like tend to be like you know one or two or zero standard deviations away from their mean um but nice random variables have this property this random variable actually turns out to be like a nice random variable h so it does like have the property that it's typically hanging out in the range between you know n over two plus or minus a few copies of the standard deviation um but as we'll see later in the course there is like a a tool and probability this sometimes called the second moment method or chebychev's inequality um which tells you like half of what you would hope to be true in general which is that um it's very unlikely h is like more than 10 standard deviations away from its mean and um very unlikely in this case means uh 1 over 10 squared and that's true for any value of 10 so there's like a 99 chance that h is within h is within okay so 10 standard deviations the standard deviation was like a half root n so 10 of those is 5 root n of its mean which is n over 2. okay what this fact means uh if you go back up to this disgusting histogram plot that i had before is let me try to sort of redraw it i'm thinking about this plot here what this kind of means is if okay this is like n over two and this is like n over two plus five root n this is like n over 2 minus 5 root n and like you're looking at the height of all these bars it means that like the total if you add up all these probabilities this adds up to at least you know 99 okay and there's basically like 10 root n different possibilities here and so in particular that means that [Music] the tallest bar which is this one has to be at least 0.99 over 10 root n again bringing it all back to where we started this implies that like cn is at least 0.99 over 10 root n times 2 to the n so this is the lower bound of the right shape this is basically like point one root at oops 0.1 over root n times two to the n you know our simple little argument with the telescoping even got us something better it was like one over root two times two n but um well you can like you can play with the numbers and mess around with the argument and try to do better but this in my mind is like the the real like reason why uh the lower bound at least is um one over a constant factor over root n for the probability of getting exactly half coin flips when you flip end coins um okay any questions all right well uh maybe i'll stop there then uh thanks for coming uh this one was like a bit unusual because i'll talk either about the lectures or the homework but we have neither lectures nor homework yet um but well this is actually this often happens it's like a little preview of like lecture stuff to come so by coming here you got like a leg up in the future part of the course uh okay well i'll see you all tomorrow in class\", metadata={'source': 'uqRvIlG8tNc'}),\n",
       " Document(page_content=\"okay hello everyone today's topic is gonna be more asymptotics we're gonna talk about factorials and binomial coefficients before we quite get into that I want to recap something that I did hurriedly last time so these factorials and bomyeo coefficients come up a lot in probability which is something that we'll be getting to soon in this course so I better recap this probability example we did last time in particular we'll a calculation we do here will come up later in the courses that later in the lecture as well so let me do a little bit more carefully so last time we're talking about this thing that is sometimes called the birthday paradox not really a paradox but it refers to the fact that you need surprisingly few people in a room before you're likely enough to at the same birthday so a slightly stater way to phrase it is in terms of balls and bins so let's imagine this setup you have M bins and you have n balls and each ball is thrown randomly into a bin there are 365 bins and n people then just like you imagine each one has a random birthday and we're gonna look at it's the probability that everybody every ball goes into a different bins so there's no bin with more than one ball in it so right P and M for the probability that there are no collusion collisions no no bins with more than one ball and it's not hard to write down the exact probability for this the probability that the first ball is by itself is one condition on that the probability the next ball is by itself is 1 minus 1 over m conditioned on that the probability that the next ball is in a bin by itself as 1 minus 2 over m and so forth as we mentioned last time and I guess this is right for all values of n but you see this become 0 once n is at least M and that makes sense because once you have more than more balls than bins and you're definitely gonna have a coalition ok so we'll assume that n is at most an so this is an exact formula but it's kind of not pleasant because it's a giant product and we don't really understand its asymptotics now the moer principle thing to do when you have a giant product is to take the logarithm to convert it into a giant sum and then deal with the giant sum because that's usually easier you know we're just going to leave it as a giant product so that we can illustrate this approximation I talked about last time which is that e to the X is approximately 1 plus X or small X in fact let's do something that's a little bit more precise you know we have expressions that look like 1 plus a small negative number here so let's look at negative X there's a little plot here this is a plot of e to the minus X I guess this is 1 0 this is X and here's a plot of the thing that it should be close to 1 minus X I guess it looks like this that's 1 1 minus X ok so one thing you can see is that each of the minus X is greater than or equal to 1 minus X for all X greater than or equal to 0 ok so that's not hard to prove in it's like a factual version of this one side of this approximate equality so we can use that directly here let's see forget that have it the wrong way in my notes e to the minus X is bigger than 1 minus X that looks pretty good so this should be bigger than the ah so this should be less than good P and M should be less than or equal to e to the minus 0 e to the minus 1 over m e to the minus 2 over m to e to the minus n minus 1 over m let me also reintroduce this notation X of T means each of the T okay so this is X minus 1 over m we'll factor that out like I did last time 0 plus 1 plus 2 plus alpha but plus n minus 1 okay and we know how to add up the first n minus 1 numbers this gives us X minus and n minus 1 over 2 ok so I did that last time and then I ran out of time and said well this is approximately a right it would even be nicer looking if we wrote it as X both minus N squared over 2m you have to be a little bit careful to do that but uh let's be a little bit more careful now so this will actually only gives us an upper bound even those turns out to be extremely accurate we would like to have a lower bound as well so how can we get a lower bound well I'll just leave this as an exercise for you that in the other direction 1 minus X is at least e to the minus X minus well we put epsilon here to emphasize I'm thinking of it as a small number see epsilon squared for all epsilon between 0 and 1 and some constancy okay so basically the the next term here is Big O of epsilon squared yep now sorry so this is for a universal C I was too lazy to write it but there exists a universal C probably like one or maybe two or something maybe even smaller than one such that this holds uniformly for all epsilon thanks good okay so this is like a bit annoying and grungy but see ya eventually you have to do things correctly so here we are so we can use this to get a lower bound for P okay and well these terms are the terms that we have here so they'll give us the exact same thing as before so we'll get the same thing as our upper bound but we'll get these extra quadratic terms that are a bit annoying multiplied together so we'll get times X let me factor out the minus C and we'll get like 1 squared over m squared plus 2 squared over N squared plus n minus 1 squared over m squared ok sorry the C's also look like parentheses but there we go okay and so this M squared can come out here so let me just write that this factor here is like X so we have minus some constant C over m squared and now we have the sum of the first n squares so what is that a big theta of yeah and cubes I mean it's not a 9 cubed over 6 or something but we're just gonna remember that it's an cubed ok so let's just put this C inside this theta and we can therefore conclude that this is well let me just leave it like this ok and now let's actually make it look more intelligible by using this one again I mean we have e to the minus some expression that we hope is small and it'll be more pleasant if we write it like this so therefore I will say that this piece is at least 1 minus Big O of n cubes over m squared okay yeah this one I used this lower bound on each of these factors in the same way that here I used this upper bound on Y minus X for each of the factors and you see that the upper bound I'm using is like e to the minus X and the lower bound I'm using here is like e to the minus x times e to the minus CX squared so all to all the e to the minus X stuff will give me exactly the same thing that it gave me here that's this factor and here I collected up to e to the minus CX squared stuff which is kind of like the gap or the error between like my upper bounds and what my lower bound is okay so let me summarize this here I'll write it a bit more clearly okay so what we concluded is that P and M is at least the same thing as our upper bound namely e the minus and n minus 1 over 2m times this annoying factor well there's got to be some air somewhere 1 minus Big O of n cubed over N squared which by the way actually this could be a terrible bound compared to this one this might even be negative if n cubed is bigger than M Squared so if n cubed is bigger than M Squared then maybe this was a dumb move but I'm mostly going to be worried about the case when and it's pretty small compared to M so let me just leave it at that okay let me just write that this is small this thing is small you know if and only if and is a lot less than M to the 2/3 okay so actually we could be done here we could just say well the answer is basically this up to a factor that you know it's close to 1 I mean it's tending to 1 at least just am it as long as n is not too big compared to M but also just for a little bit of exercise I want to go a little bit further in the same way I was talking about last time it's like kind of slightly annoying that you have n times n minus 1 here like it would be really cool if you just had N squared because then you could invert it and see that you know this equals 1/2 when M is root 2 long 2 times root N or something like we did last time uh so just for the sake of exercise let's let's worry about that so let's say I wanted to compare this factor with the simpler thing that just had N squared up here well the way you always do that is to factor out the piece that like you wish you had and see what junk is remaining so e to the minus and n minus 1 over 2 m I guess it equals speed to the minus N squared over 2 m that's the simple thing I wish I had and then the left over is plus 1 over 2 m okay and ok we can use the rules of exponents to say this is the thing we care about eat to the minus n squared over 2 M times X 1 over 2 m and let's see our favorite approximation about this thing 2 because 1 over 2m is a small number so this thing is at least 1 and it's at most 1 plus order 1 over n that looks better yeah good and over 2m thank you good and this fact is true because I mean we're using here that n is at most M so this is a number that's at most 1 good so therefore if we really want let me conclude it here we can put these things together and say that p and m equals this enjoyable term e to the minus n squared over 2m times 2 factors that are close to 1 this kind of slop and this slop so put in the slop first 1 plus order n over m and 1 minus order n cubed over m squared now this actually you know like a mildly subtle thing has happened here we have like two error terms and it's not even clear which one is dominating in fact neither of them always dominates sometimes this is bigger than this and sometimes this is bigger than this depends on how an M&M relate so let's in fact think about which term dominates here in terms of the error crossover it occurs well when they're equal so when we have and over em I mean up two constants and over M equals n cubes over m squared so let's solve that n squared equals M so N equals root M okay so we can say if we call this I don't know piece error because we could say that the error is 1 plus or minus order and cubed over m squared this one's bigger when and is small and is less than root M and order n over m if n is bigger and uh finally remember last time we were interested in like you know how big should n be as a function of n such that this probability is right in the middle like around 1/2 like it's neither really likely nor really unlikely to have collisions it's like 50/50 let me squeeze it on to let me squeeze it onto this board so we saw last time that you know X minus N squared over 2 m equals 1/2 well we can solve this easily enough if and only if n is like root 2 lon - hums root M okay or even just forgetting about that exactly root M okay so somehow like the most interesting phase transition occurs when the number of balls gets to be square root the number of Bin's that's where it kind of goes from likely to unlikely for there to be no collisions and actually it's kind of interesting that's where like the error terms become about the same they're both like order 1 over roots and okay so for example we could therefore conclude and we might actually even like to conclude since this is not literally an integer we can conclude that for we can check that for an equals this quantity root 2 onto root M if that P and M equals 1/2 plus or minus order 1 over root n actually I made it one little thing here but I don't want to dwell on it too long on one hand in this order 1 over root M comes from both these error terms when n is around root m and oh actually this is exactly true so I didn't made anything for this but even if you put in plus or minus 1 because or plus or minus order 1 because you want to imagine and it's an integer if you plug that into here you'll still also only get an error that's like this okay so that was the last thing I want to say about that any questions so portly I went into this some detail just to show you like one more example of like going into detail about I don't know doing the last details of some of these annoying approximations\", metadata={'source': 'o11c5ZeZOF4'}),\n",
       " Document(page_content=\"factorials and binomial coefficients so the plan for both of these is to just write them down and then first do some simple bounds and then well first do something like really naive bounds and then do some simple balance and then do some accurate bounds so we shall start with factorials so n factorial is n times n minus 1 times n minus 2 down 2 times 2 times 1 ok this comes up all the time and we would like to know the asymptotics for it so as I said you should always start with like the simplest possible thing so I mean our dream goal is to understand you know this is asymptotically equivalent to some function in standard form we'll eventually get there with Sterling's formula but let's start simple so the simplest upper bound is you can just say each of these terms is at most n so that most n times n times n times n times n and that one we can do that's n to the N okay so we have an upper bound and that's actually not even be too terrible of an upper bound even though it looks a little bit ridiculous uh as for a lower bound can somebody suggests a lower bound of almost equal simplicity I guess it's give me a little bit more complicated but yeah I'm gonna people have their hands up and some people said it out loud you can do that what's your name funny it's what do you say yeah thank you that's it because that's exactly the one I have in mind you could been or sophisticated but I was exactly hoping for this you could just do the trick of you know let me drop the lower half of the terms they're not helping me much and for the larger half of the terms I'll use that they're all at least n over two actually I'm going to cheat very mildly here because this lower bound kind of assumes that n is even but I'm too lazy to do it when n is odd so anyway we're going to see better bounce okay so this is N or two to the N over 2 actually I checked this is correct even when n is odd but only the little proof does not show it but if you're worried it's true okay so actually that's pretty good and we have here already two kind of matching bounds they're both sort of of the end to the end form so let's say we just wanted to under you know take these two bounds and kind of compare them here's another like sort of life tip that will encounter again and again it feels like some kind of asymptotic expression and you don't exactly know what's going on with it or you want to compare it to another asymptotic expression and like both of them are really big quantities then just keep taking the logs of the expressions on like both things you're trying to cook there until it looks more understandable so we can do that we can take the log of all three the quantities we have here and deduce that well let's take a lon just to be a little mathematical about it for some reason lon of n factorial is at most lon of n to the N is and London and it's at least okay this one was and over to lon and or two okay and now these are looking a little bit more easy to understand in fact I suppose oh how do I take in the base to log it would have been even easier to deal with this but in particular this is half and lon n plus a hat and over two lon a half but anyway that's a minus order and let's leave it at that say minus theta of n even okay and so this is easy to compare to this I mean okay this theta of n is asymptotically smaller than and loan n so just kind of ignore it and now we have n loan and versus I have been loaning okay so our two bounds for the logarithm of what we're going for are the same up to a factor of half that's pretty good it's only pretty good though because we really cared about the exponential of this which means that our two bounds are off by like a quadratic power actually which is still not terrible but we're going to try to do somewhat better but anyway I mean at this level what we can conclude is not even too bad we can exponentiate this and conclude that n factorial is exponential in or all right with two two to the theta and lon n log which is pretty good but it's not super grade because you know the theta is in the exponent which is like making it off by a power rather than off by a constant factor but anyway it's really quick and easy so that's a good start okay so now we would like to do better than this and shortly we will take the principled approach to that which is to take the log of this big product to convert it to a big sum and then we know some tricks for handling big sums in particular we're gonna use this integral trick again that we saw last time which on one hand I insisted didn't come up very much and then on the other hand it's come up for us like twice now but I think that'll be it but before we get to that I'm going to show you one more ad hoc bound which is still really simple and gets like a very good bound so this out of hoc bound will be based on our favorite Taylor series I know it's your favorite Taylor series namely the one for each of the X so this is a fact each of the X is 1 plus X plus x squared over 2 factorial if you wish plus X cubed over 3 factorial plus etc and this holds for all real X and depending on your math textbook it may even be the definition of what is e or what is e to the X okay then actually of course if you like you can even put to the 1 over 1 factorial here and this is X to the 0 over 0 factorial ok so I said this is like an ad hoc approach so I'll just show you this trick one thing that you can say is that well this is also true if we just stick with positive X's and in that case all the terms here are positive so it would only become smaller if we drop some of the terms so in fact let's just drop all of the terms except for one well keep the nth term and so on one hand we just dropped a ton of stuff but on the other it's going to be pretty good and what is the ends term it's X to the N over N factorial and therefore if we rearrange that we got n factorial you know it's not least X to the n over e to the X for all for any if you will not negative X great so it's only a lower bound but we're only going to get a lower bound out of this trick and that was actually great you can choose any X you want depending on n you could choose 2 or 3 or 4 or 6 and a half and there's a trade-off because the numerator gets bigger the bigger X is and the denominator also gets bigger so there's some kind of trade-off and you can use calculus to determine the best X which is then back X that maximizes this expression when you treat and as a constant I will not do the calculus exercise but amusingly when you do it it turns out the best X is in so we can now take X to be n which happens to be the best choice funnily enough and we conclude that n factorial is at least n to the N over each then and this actually turns out to be a pretty great lower bound in fact it's a lot closer to our upper bound than our previous lower bound was one way you can see that is to take the logs of both sides again let me just I can also write this as n to the E and over e to the N same difference and so if we take logs we get that lon of n factorial is at least and 1 and over E okay lon of n over e is lon n minus lon e also known as lon n minus 1 so this equals and lon and minus sign okay you should compare that to before where a lower bound for the log was like 1/2 times n lon n you know - something order n okay so it's much better it's quite close to the upper bound we have and so in fact this actually determines the logarithm of n factorial asymptotically so putting this lower bound together with that upper bound we get that lon and factorial is asymptotically equivalent to and lon N or if you want to exponentiate this you go the N factorial is approximately n to the end but approximately means up to a lower order term which could be something like constant to the end so it's still pretty good and so if we in fact we do exponentiate we have an upper bound for n factorial of n to the N and a lower bound of n to the N over e to the N ok so we're off by some factor that's between 1 and e to the N but we don't know is it more like n to the N is it more like n to the N over e to the N is it in between is it like n to the N over 2 to the N we don't know we'll find out though it's more like n to the N over e to the N as it turns out okay and I'm gonna continue to you know stall before getting into Sterling's formula which kind of reveals everything but sticking a little elementary let's see if we can figure out what n to the factorial isn't more like is it more like n to the N is it more like n to the N over e to then n to the N over two to the N what so let's define F of N by saying that well n factorial equals n to the N over F of n so like F of n is it kind of like this mystery that we're trying to figure out or it's defining it implicitly but I mean it's saying F of n is n to the N over N factorial so it's like our upper bound has f of N equals 1 our lower bound has f of n equals e to the N and we're kind of concerned like what is the real f of N and we can kind of heuristic we figure it out so the calculation I'm about to do will heuristic Li tell us the answer and if we speculate that F of n is of the form constant to the n then we can figure out that constant by looking at the ratio of F of n plus 1 to F of n let me just write that so I want to look at F of n plus 1 over F of n because this would be like C if F of n were our end to the sea so I see to that but you don't have to speculate we can just write down what it is because we have the definition of f of n right here so that's in the numerator n plus 1 to the n plus 1 over n plus 1 factorial that's f of n plus 1 f of n is n to the N over N factorial and now let's see some glorious calculation the cancellation happens this n factorial is really in the numerator this one's really in the denominator so we cancel them and we get like an N plus 1 in the denominator and actually then we can cancel that with this thing and now it's great because we have a to the power of n so this thing is actually n plus 1 over N to the N also known as 1 plus 1 over N to the N and tape us know that it tends to e as n goes to infinity as I hope you know sometimes that's the definition of E or if you like if you pretend that 1 plus 1 over N by our most favorite approximation is e to the 1 over N and this is like e to the 1 over N to the N which is like e to the 1 which is e so these calculations are all correct the heuristic aspect is it doesn't exactly maybe prove things this just says that in the limit it goes to e what this tells us that basically the true answer is that this f of n is like something like e to the N okay so kind of heuristic Li deduce that n factorial should be like something like n to the N over e to the N which is like close to our lower bounds well this is exactly a lower bound and so now like our next step is to go even further into this and get it even more tightly any questions right now okay so we have to do this because I mean why binomial coefficients come up all the time in probability and probability comes up all the time in TCS and like binomial coefficients have like factorials built right into them so got to do it got to know everything there is to know about them so now it's time to really do it let's take the definition and log both sides okay well I'm really saying I'm not gonna Cranmore login here everywhere so yes I'll just write it this is a long one plus lon 2 + lon 3 plus a but plus lon and okay I guess this one 0 so we don't worry about that and now it's time for the integral trick to try to understand what this is okay so the trick there as you may remember is to plot the function lon and try to compare this sum with the integral which we can figure out using calculus so let's draw a picture okay so here's my t axis and here's one I'll try to draw a line something like that that's lon T and let's see let's make this n okay and so we're gonna start making rectangles here so this is to 3/4 of a rectangle that goes up like this a rectangle that goes up like this a rectangle that goes up like this okay our last rectangle looks like this okay and this height is lawn to the side is lawn three pop thing is lawn and okay and the point is that this lawn of n factorial is equal to the area of the rectangles this we dropped that lawn one which is zero so the first rectangle should have area lon - and you know has width 1 and height lon - yeah this one I was with 1 in height lawn 3 and with 1 in height lawn for etc okay so the era that rectangles is more than the area under the curve so the naive thing will give us a lower bound for lawn n factorial so I'll me write like this less than or equal to area under the curve and we can get this with calculus it's the integral from 1 to N of lon T DT and then Oh y'all remember this one from calculus it's the FT derivative long T is T lon T - wait what is it - t thank you yeah if you don't believe me you can differentiate this and hopefully you'll get lon T ok and we need to evaluate this from T goes from like 1 up to n ok so I plug in into this and I get n lon n minus N and then I subtract from that what I get if I plug one in so if I plug one in here lon 1 is 0 and this is minus 1 so I subtract that and I get plus 1 so that's it that's my lower bound so you can see it very barely exceeds this lower bound that we got by this other method our old lower bound is n la n minus n now we got n lon n minus n plus 1 but actually that's going to be pretty pretty good it's actually consistent with our idea that the slower bound was like the type 1 this was the lower bound that was giving us approximately this e to the N and the denominator which is going to be accurate so let's save this as our lower bound I'll just I guess it's still true that this is greater than or equal to even though it's actually equal but I'll save it to remember that it's a lower bound huh good now what about the upper bound so ah for the upper bound what I want to say is this is like at most the same integral which gave us the lower bound plus the area of the curvy triangles okay so Plus this area actually this is equal I mean it's equal so now I would like to evaluate the area of these curvy triangles and how am I gonna do that I'm gonna do that by putting a wall here and like lining them all up against the wall so I'm gonna push all these curvy triangles over so this one pushes over to here I mean the point is this does not change the area it just moves it around [Applause] okay and then all the curvy triangles hope you can see it so well and highlight so this is excess or hanging out inside this rectangle good now let's return they were actually triangles they're actually they actually it curved in a little bit but let's pretend that they're actually triangles if there are actually triangles then you know by picture they would be taking up exactly half the area of this tall rectangle against the wall so actually taking a very slightly less than that because the curve is going you know this way which is good because we want an upper bound so I mean if I put less than or equal to here I can put like straightened triangles here and then we know exactly the area of the straight into triangles it's half the area of the rectangle okay so this this is therefore half okay a rectangle has width 1 and height lawn in so it's 1/2 1 did I do it all right I think so good so let me clear this up we have now an upper and lower bound for log and factorial which are very close they differ only by this straight and triangle thing which is a half long n which is really great because that's like the third order term here it's not the main term it's not even the error term is like that third order term so it's a real great bound so in fact let me just exponentiate everything so we get a bound on n factorial and we therefore deduce n factorial it's at least so the exponential the lower bound is a well I got n to the N over e to the N times e and to the N over e to the N times e and upper bound is the exact same thing is that n to the N over e to the N times e but plus the exponential of the straighten thing straight into triangles which is e to the 1/2 lon n is root n so great so this is this is a really sharp bound okay now we have n factorial pinned down and extremely well it's like n to the N is the main term e to then is the next term and then now we're just off by a factor of root n there's quite small that's great [Applause] so in particular we've shown that and factorial is going with the theta down we have to put a little tilt in because of the root end but it's theta tilde of n to the N over e to the N right this hides a factor which is logarithmic in this logarithmic poly log in this log of this is like n log N and root n is indeed like a constant power of n log n uh that's great and you know if you weren't tough you would just stop there that's pretty good but well well basically keep going a little bit and now ask ourselves any any questions about that by the way well will be like what about the next I mean what is it exactly is it like root N is that correct back there correct this factor correct if you think about it probably this one should be more correct right because like for our lower bound we just said like oh the original rectangles are bigger than the area into the curve for the other bound we like almost got it right except for the difference between straight and curvy triangles right that was the one that led to this one so this one's probably more right and indeed that is true and see the the gap of the upper bounds from the truth lawn in factorial is like the area of these slippers which are just the difference in is between curvy triangles and the straight triangles so it's like the area of these pieces that look like this Plus this Plus this this okay so if we could pin down the area of that difference exactly we'd have it exactly this is where I'm gonna kind of stop and not do that but I leave it as an exercise for you not a joke it's not that hard to show that the area the total area of the slivers yes at most order one and this is with respect to M going to infinity what I mean by that is actually even if you take an out to infinity these slivers are so small you add them up the total area is at most a constant in fact you know if you just mess around you should be able to get like at most I don't know 1/8 or something it's really quite small which means that the lower bound in truth is equal to our upper bound - Big O of 1 and when you exponentiate that it means that you know the thing you actually get is this right hand side times exponential in a constant which is just some other constant so once you do this exercise you can conclude that n factorial but this one is correct is up to a constant n to the N over e to the N times square root n and you very rarely need more than this so you can mentally stop here well actually sometimes you need more than this so again if you're really really really tough you like please tell me what the constant is and the constant is square root of 2pi but to prove that you need to start thinking about gaussian random variables which we will not see you until next time so let me just leave it by telling you that answer and with the constant you got something called Sterling's formula if Sterling's formula is like the asymptotic form of n factorial it says that n factorial is asymptotic to square root 2 pi square root n and to the N over each of n so it's great right PI and E together in one formula and what if you want more than that then you're really sick but sometimes he want it I'll tell you it it's this times 1 plus or minus order 1 over N so that's how close this is to the answer and like this constant o is like even 1/12 and you can figure out everything but this was really genuinely enough\", metadata={'source': 'TbazAJbw6RE'}),\n",
       " Document(page_content=\"this is the end of factorials and we're gonna go on to binomial coefficients okay so kind of in principle you know binomial coefficients and choose K it's like n factorial over K factorial times in the denominator and minus K factorial and you like well we just got Sterling's formula that gives us it incredibly precise formula for factorial so just plug it in and you're done and that's sort of true and well in fact do exactly that at the end of class there's two caches though one catch is like it's more annoying because there's two parameters in a binomial coefficient and in K and so somehow the asymptotics is more confusing because it depends so high okay depends on n like it's K like a constant like five its K like square we're done it's K like point 3n somehow the form of the asymptotics changes and the other reason we're gonna say if plugging in Sterling's formula to the end is because we're gonna follow this strategy of just starting with the most naive possible bounds so let us good let's start so what is in choose K well so you know it's in factorial over K factorial times n minus K factorial and usually psychologically you should think of K is kind of small and therefore like this is I mean most times so this is smallish but n minus K is kind of like n so this is like many more factors than this one so it's good to most the time to like cancel all these factors with these ones up here and only leave K terms on the top and K terms in the bottom what I'm saying is just that this is equal to n times n minus 1 times n minus 2 there's K terms up here and the denominator also has K terms it's K factorial but I'll just write k factorial I mean K times K minus 1 times I thought okay so this is an easier way to think about it and as I said what you want to treat this sort of depends on how big or small K is for example Oh like imagine K is 3 ok let's say K is 3 well then and choose 3 n n minus 1 n minus 2 over well 3 factorial is 6 okay and you know this is awesome - n cubed over 6 and that's it you're done and let's turn in general so for quote-unquote small k vinci's k is approximately and to the K over K factorial well remember this approximately sign has no meaning it's just you know it's heuristic way around this I mean and you will have to say better if we want to you know if K is really like 1 2 3 4 5 10 then now you're done like this if it's 10 it's 10 to the 10 over I don't know what 10 factorial is but it's some number good but let's do this more accurately some more accurately ah okay this expression look at what you should do just imagine there's like ten terms up here in the numerator and you're trying to make it look like and to the tenth over 10 factorial again what you should do is like factor out the term that you think is the real term and leave all the rest is like the junk so you would do it like this you would say this equals and to the K in general over K factorial and we're gonna factor it out now and what are we left with we're left with 1 minus 1 over n let's I mean we're gonna factor an N out of N and get 1 in fact they're an N out of n minus 1 and get 1 minus 1 over N we're gonna fact an N factor an end out of n minus 2 and get 1 minus 2 over N and so forth so we're going to end up by factoring an N out of n minus k plus 1 and get 1 minus K minus 1 over N so that's exact and now you may see why I bother to repeat the birthday paradox thing at the beginning because it's exactly the same we have this product here as like our junk so let me not be laborious but this piece this is like you're gonna write a little funny this is like 1 minus K squared over 2 and basically right I mean you saw you can convert these all to eat those multiplied them add up these quantities convert it back ok so these these things add and you get minus 1 over N times 1 plus 2 Plus 3 up to K which is like K squared over 2 ok but now I've gone back to being a little heuristic here but this is if K is small because all these approximations where you know this is approximately the same as e to the minus K over N it's only true if K over N is small and in fact if you work sort of bit more like this approximation is only accurate if this thing you're trying to imagine that this is the overall error term it's like this thing is small which is only small if like K is a lot smaller than root end ok so what I'd like to just say I'll conclude it over here sorry if we're doing this board a little in a weird order but the following is true and choose K and asymptotic to n to the K over K factorial provided hey if you think about it as like a function of n is little o of root n which is to say that like this is asymptotically small was there question okay so if you see something like entries - or entries ten or n choose and to the point one then this bound is asymptotically accurate now what if K is not smaller than brute int it turns out that it's actually kind of tricky and interesting so for most of the rest of the class I'm actually going to jump up to worrying about the case when K is proportional to proportional to n so K is like maybe 0.1 times n and part of the reason for that is as I said it's a little tricky like to understand the exact got some products there's like infinitely many different formulas there's like one formula if K is smaller than root n which is this one and then there's a formula that's accurate if K is between root N and n to the 2/3 and another one if K is between n to the 2/3 and n to the 3/4 and another one if K is between an end to the 3/4 an N to the 4/5 and so on and so forth these don't come up too often if you really need them there in this book well one place to find them is in this book Assam Tokyo by Spencer which I mentioned last time so if you want to find out really many more details in your life about asymptotically approximating binomial coefficients you can check it out there I'm going to kind of skip this intermediate regime and end in the last I don't know 25 minutes by discussing good bounds when K is proportional to n okay but I said I'm gonna do that at the end and I want to show you a few more bounds for n choose K which are simple and actually these simple bells I show you are gonna have the property that they hold for all N and all K there's no like is K big is okay small they hold for all N and okay you know the only caches they're not like super accurate but sometimes you know quick and dirty bound is all you need so let's see how can we get some very simple bounds for n choose K okay well we can start with this formula up there and do the most value thing to get an upper bound it's definitely at most n to the K over K factorial that's always an upper bound from that formula there but we want some kind of lower bounds now if you recall earlier in this class remember I was even right here on the board if you can read the faint chalk it's still up here in your mind we have this bound for n factorial that we got by looking at the Taylor series for e to the X dropping all the terms except for the enth one and then plugging in x equals N and that bound was that I'll write it with K instead of n that bound was from 30 minutes ago k factorial is at least K to the K over e to the K well it's kind of right there except that bound ignored this term in this term it's pretty good though okay so if you just plug this into this you get that I choose K is it most e and over K to the K and one great virtue of this bound is that this holds for all and all K there's no like blah blah blah blah asymptotically how big your smoke is it's just always true and this bound is pretty good this bound is often like all you need in life and it's it's great I mean it takes this thing that's got a very complicated definition and just gives you each of the K n to the K and K to the K okay so that's pretty good huh we may like to get an equally simple lower bounds let's do that by going back up to this formula and writing K factorial out is the product of K things so n choose K exactly equal to N and minus 1 n minus 2 K terms down to K K minus 1 K minus 2 also K terms and you might say like ok this part is n over K the next parts like also almost like n over K the next parts also almost like and over K it's not quite though and you're like is it working for me or against me well I don't know what's for or against but it turns out that let me write it over here but n minus I over K minus I it's actually always at least n over K for all I and I guess you can deduce that by cross-multiplying it's this true if and only if n K minus I K is greater than or equal to NK - I and just true if and only if I times and it's greater than or equal to I times okay which it is because we're assuming n is at least K so what this is saying is that each of these factors here is at least n R K and so therefore this is at least n over K to the K okay and that's a very pleasant lower bound that also holds for all and and all okay and it looks pretty similar its upper bound and I mean you see they're exactly the same up to a factor of e to the K which is really often not that big since if you think of K is kind of small this whole thing is like n to the K like if K is like 10 these are both like n to the 10 it's just squiggling over the constant factor is it e to the 10 or whatever so uh these are great like upper and lower bounds to just use in your life and often they're gonna be good enough let me effect in this little elementary segment by taking the log of everything so let me take the log of n choose K and okay I'll take the log of this from my lower bound it's at least K times lon and over K and it's at least or sorry it's at most K times lon en over K okay and I guess I can write this as K lon n over K plus K lon e also known as plus K okay so you can see it looks real great when you know you'll go through the log so I mean the log of the spinal coffee she ate it's like very close to K times lawn and over K it's off just by like an additive okay and it's interesting actually to imagine two cases here imagine you do okay to be n to the point nine nine so lawn of n Cheers and to the point nine nine well that's like you plug n to the point nine nine in here this ratio is n to the point O one log of n to the constant is like constant log n so this is Theta of n to the point nine nine log in [Applause] on the one hand if you plug in let's say K to be point one n so the lawn of n choose point one n there if K is point one end and the ends cancel and you get lon of 1 over point one you get lawn ten you get anyway you got a constant so you get a constant here in a constant there and K just remains 0.1 n so you get a constant times n okay and this is a general persist if you have n choose like n to a power it's sort of asymptotically like n to the K but that stops when K is linear and n you know n choose something that's linear in n is just exponential in n this log factor goes away okay um we're almost at the part where we like well off messing around let's just plug Sterling's formula in and see if it happens but I'll keep delaying that for one more elementary bound and now for this mindset well let me first say couple things first so I mentioned before and choose K it's the same as n choose n minus K it's like symmetric when you subtract the bottom guy from the top guy which means that if you want to understand this basically without loss of generality you can assume K is it most n over 2 if you ever have to evaluate and choose K where K is bigger than n over 2 then just swap k down to less than n over 2 by subtracting it from N and in fact as I said I now want to you know end by focusing on the case where K is proportional to n so I'm gonna introduce a new letter P and we're gonna write k as P times N or I want you to think of P as between 0 and 1/2 and strictly speaking you don't have to think of P as a constant independent of n like what I'm going to say is true for every P but in your mind think of P is like a constant like 0.1 or 0.4 or 0.5 or something like that okay so what I'm saying here is I really want to focus the rest of this lecture on things like how can I understand and choose PN for example and choose 0.1 n so okay so we're interested in trying to understand this and we're gonna tell you an elementary bound and it's just gonna be an upper bound so we won't get a lower bound out of it but we'll get it what turns out to be an extremely accurate upper bound and for this elementary ad-hoc trick I'm actually going to do you one better I'm going to bow up around not only this but actually this as well plus and choose K minus one plus plus and choose 2 plus n choose 1 plus n choose 0 honestly I'm not giving you really that much more because the binomial coefficients as a function of K increased super rapidly so like this thing is like almost always way bigger than this thing and this thing in this thing in this thing so this is this whole sum is not much different from this last term but anyway we'll take this whole thing and actually let me give a name to this you know how it ends you can call this V sub n ok for this binomial sum and this comes up a lot this sum of binomial coefficients because it's like the number of binary strings that have length N and at most K ones Hamming weight at most k and that's the sort of thing that I don't know comes up a lot and TCS and so you may care to know how many such strings there are or bound on such strings this is also sometimes called the chronicle volume of the Hamming ball of radius K which is why I chose the letter B for it okay so this is something that we may want to know an upper bound on and I'll show you this Idaho trick that works that's kind of similar to the way we bounded and factorial using the Taylor series for e to the X except we're going to bound V using the binomial theorem so I will recall for you the binomial theorem which says this 1 plus X to the N is literally exactly equal to 1 plus n choose 1 x plus n choose 2 x squared plus + and choose K X to the k plus + n cheese and xn ok lets the binomial theorem and let's assume X is non-negative and in that case actually let's assume that X is between 0 and 1 ok so think of X is between 0 & 1 mentally think of it is small so actually these powers of X are decreasing X x squared X cubed these are all getting smaller and smaller and smaller so what are we gonna do to make a lower bound first I'm gonna drop all these terms at the end I'm just gonna drop them they're positive so not negative so I can get rid of them and then I'm just going to say like well X okay this is really n to 0 X to the 0 X to the 0 is at least X to the K and X is at least X to the K and X squares at least X to the K all the powers of X are greater than or equal to the smallest One X to the K so then we can change all these X powers to X to the K and factor it out and we got X to the K and what we're left with is small n choose 0 plus n choose 1 + up to n choose K which is this V&K everybody catch that great so then we can just invert that and deduce that V and K is that most one plus X to the N over X to the K for any X so that we want between 0 & 1 think of anything K is fixed and I like X is something that we can choose it's similar to the thing with again you know where we bound it n factorial using the Taylor series for e to the X and then we chose our favorite X it's like mildly more annoying to figure out the best X to choose but what we're going to just do is choose the best X that mate between 0 1 that makes this expression as small as possible and get a good bound for the volume that way so let me just facilitate that a little bit this is 1 plus X the N over X to the P n because i calling K P times n looks good because now we have two things that are to the power of n so I can write it like this 1 plus X over X to the P to the N and yeah that's it so now we can choose our favorite X to try to make this not too big and we're going to bound this like some number to the power of n well you remember Sterling's formula okay so now uh there's a little calculus exercise to figure out what is the best X to choose and I will either do it or not depending on time mm-hmm what's not okay but just to recap so we have B and K or V and PN if you will is that most I'm just going to expand this one plus X over X to the P out as X to the minus P plus X to the 1 minus P to the N for our favorite X between 0 & 1 and it's now also gonna be convenient alright chill for 1 - P I know I'm introducing a lot of symbols but I will this this is literally just the same as this by the way 1 plus X over X to the P okay so by the way one reason for that looks like they're four inches K is n factorial over P n factorial Q n factorial okay so I'm gonna admit it omit it but calculus you know because calculus have the best X either one which minimizes this expression miraculously is x equals P over Q actually this is only true if P is at most a half what we're assuming that so in general like if P is like point one or something this this expression here happens to look like this [Applause] one zero and this is P over Q this is X to the minus P plus X to the Q okay so that's a fact that you can check by plotting in calculus if P is bigger than 1/2 the thing looks like this so the best X is just 1 and you don't get a very good bound okay but this is at most one using the fact that P is at most 1/2 which is good what it means we are allowed to plug it in in which case looks a little gross but something interesting is going to happen with X choosing X to be P over Q this expression in the base of the exponent X to the minus P plus X to the 1 minus P is back out X to the minus P 1 plus X ok which is X to the minus P 1 plus P over Q by a choice of X being the / - ok and one is also known as Q over Q and Q plus P is 1 so this is X to the minus P times 1 over Q and now I'll also remember that X is P over Q I'm just doing some arithmetic in front of you here so this thing is Q over P the minus P well I guess to the P times 1 over Q which is 1 over P this last step one key to the P times 1 or Q the Q she's like an oddly enjoyable looking expression so just I did arithmetic this is what you get if you plug this choice of X into here and therefore we finally conclude this bound that I'm trying to get to B and PN is a must this numerical value 1 over P to the P 1 over Q to the Q to the N okay okay so P is like a literally a number like 0.1 then you can like literally plot and therefore Q is 0.9 you like plug this into your calculator you'll get some number like 1.8 3 or something I just made that up and so you're bound would be like one point eight three to the end which is pretty good and actually it's a bit of a sanity check what if you take P to be 1/2 then Q is also 1/2 you're looking at n choose n over 2 or actually you're looking at the sum of all divine oil coefficients up to n choose n over 2 and you get square root of 1/2 inch square root of 1/2 reciprocal anyway you get the bound to to the end which is true all binomial coefficients are most through to the end but actually that's a very good bound because the sum of the first half of the binomial coefficients is 1/2 times 2 to the end so it's actually a very accurate bound so in general this bound is going to always be at most 2 to the N or it's going to be in general like some constant number to the N and a pretty accurate one in fact we generally write this bound not like this but as 2 to the log of this times N and we call the log of this h2 of P and N and H 2 of P is literally the log of this number it's called the binary entropy of P it's P log base 2 of 1 over P plus Q log base 2 1 over Q and maybe you seen it before it's the entropy of a biased coin that comes up heads with probability P and tails with probability Q so it's a number between 0 & 1 like if you plot it it looks like this that's a half it's symmetric that's one zero that's PE this thing is 1 this is H 2 of P okay so this pound is uh well it ranges from um well it goes from 0 to 1 and it gives you a bound of some number to the power of n which at the maximum when P is 1/2 is 2 to the N okay and wonderful thing about this bound it's just like no like error terms or anything like this found some of these binomial coefficients is that most exactly two to the entropy of P times n with nothing else okay well you have just enough time to plug in Stirling looks like what's gonna be horrible applying but it's gonna be real quick so let's do it oh I want to keep those assumptions which I just erased so Ange is PN is n factorial over P n factorial times Q n factorial this is or Q is 1 minus P and now he is sterling on all three terms all 3 factorials and so we get but this is asymptotic to the numerator we have good old sterling root 2 pi root n n to the n over e to the N denominator we have the same thing with P n and also with Q n root pn pn to the n over e to the pn times root 2 pi root qn q n the q n over e to the q n now let's see let's look at the powers of e pn plus q n is n so it cancels that's good now let's look at the power of n n to the this should be p n you have n to the pn and to the q n that cancels with n to the N so good ok some of the 2 PI's root 2 PI's cancel and what are we left with so we have left some constants root 2 pi and we got root P and rip-queue those are all constants in our minds and we get 1 over root and it's only one of the root ends canceled and what are we left with we're left with the exact same thing we're left with good old 1 over P to the P 1 over Q to the Q to the end oops so this the exact same thing is before one of a ruku pi root PQ 1 over root and times our entropy bound to to the H of P n ok so our upper bound was like miraculously closed it was only off by a factor of 1 of a ribbon and this is exactly asymptotic ly accurate for every constant P and Q and the last thing I want to say is the look at the very special case when P is 1/2 so Q is also 1/2 and this gives us the exact asymptotics of n choose n over 2 and if you just plug it in here you get that this is asymptotic to root 2 over PI 1 over root n times 2 to the N and if you divide both sides by 2 to the N you get this and what is this the probability if you flip and coins that exactly half of them are heads and it's the last fact I'm going to end on I want you remember because it's a good fact in life if you flip n coins what's the probability that they come up half heads half tails its asymptotic to 1 over root n always remember this the constant factor we'll see why this contact factor arises when we look at gaussians next time but the main thing to remember is that this odd event exactly have heads have tails has probability proportional to 1 over root n ok I'll see you on Thursday you\", metadata={'source': '_DgUgKOkPxo'}),\n",
       " Document(page_content=\"i think i got it fixed up all right let's see [Music] all right okay uh let's see let me share my screen here all right i think i finally did it um hey everyone just a reminder uh this is being recorded i'm probably going to put it online so if you don't want to show your face then don't i kind of recommend that like when we're interacting though i mean you can type in the chat but like i kind of suggest you nevertheless um just vocally pipe up because i don't know do you really care if your voice is in the video and it might be easier than typing but if you want to type that's fine too i got the chat up um so uh yeah how's it going does anybody have any question about cs theory oh you got everything solved could we talk about number three perhaps sure uh number three what's number three number three was oh we have a sequence let's see 1 equals a1 less than or equal to a2 less than or equal to a3 less than equal to a n and it satisfies a i plus 1 minus a i is at most square root a i and you want to bound some a i plus 1 minus a i over a i i goes from 1 to n minus 1 by log n uh this one yeah um yeah let's see what about it um yeah so i guess i started by assuming equality for the inequality uh okay that's reasonable let me just uh i haven't thought about this problem in a while so i always love this situation when like i myself don't remember how to do the homework because then i can just think about it right along with you um yeah that's natural uh now it's not super clear that you can do that but it's definitely a natural move so yeah let's imagine hypothetically say that a i plus 1 minus a i equals root ai i mean it's good to check that you're in okay shape in this case for sure yeah intuitively it seems like that should be the worst case but yeah hard to say well let's think about it later but like uh okay suppose this is the worst case uh-huh then then what then what are you thinking um well so i just started you can plug it right into the summation but oh yeah i guess you can just like literally compute i suppose i mean like a two would be two and then like a three would be like two plus root two a four would be like oh it's gonna get it's gonna get not too enjoyable okay yeah and then you can substitute in to the actual summation term oh but wait i guess it's not so bad right because like this thing here is this thing here so uh if you're assumed this is true then you have some i goes from one to n minus one of root a i over a i which okay that's like sum y goes from one to n minus one of one over root a i okay that seems not too bad hmm yeah this is where i'm this is where i'm quite stuck uh i don't really know what to do from here [Music] okay let's see well um yeah how could things go wrong for example [Music] well you might worry that like okay you might worry that like ai is i don't know less than two for all i that's not a for all now you'd be very sad because like this this quantity here would be like between one and root two so this whole fraction would be like i don't know some number it'd be at least like 0.7 or something and then the whole sum would be like 0.7 and which would be bad what can that happen i mean we're adding stuff so hopefully not but uh i don't know how to tell yet well must ai get large i mean i guess it was already like this big up here there's like two plus something here was like i don't know how much this is three point something i mean do you know like what ai is approximately like in this case oh wait there's chat we could try to plot yeah we could try to plot it let's do it uh it says the chat who wants to do it i guess i'm the only one like well somebody would i don't know is somebody like real cool and they can like share their screen like plotting it or is it gonna be like infinitely slower than if i do it let's all do it together you draw it on your computer i'll try it on my computer we'll do it together let's see i'm gonna have to like share my screen over here hold on let me get my favorite math program open aka canadian mathematica uh i'll share my screen one second do um cool okay so let's see um how do we do this i guess we're like let's make a list called a and we'll start it at one and then let's make like n 50 i don't know and then we'll go for i equals two two i guess it doesn't matter two oh wait you have to do this from two to n and then we'll be like a uh please add to yourself a new number and a new number should be a i minus one what is it plus no square root of is this plus square root of i minus one should have just made it go uh whoa okay i should not have done that let's do this as well let's make it go up to 10 so we don't go crazy okay okay that's plausible right is this is this plausible is this consistent somebody say like yes this is consistent with my experience uh oh it's good somebody's got it going i plugged into wolf from alpha and i gave the first five values and i didn't know how to get any more someone in chat says yeah on mathematica it's very slow as it fits you yes okay great yeah well okay yeah if you're using like mathematica or like maple or something like it's probably gonna be like oh i can compute this exactly and i'll have like you know like a thousand nested square roots which is i mean i should be able to handle that i mean come on it should toughen up but i can understand how it might get slow so you might want to like tell it to be like yo please do this with like floating point rather than trying to like literally compute it algebraically uh okay so now if i was good i would be able to just like plot this i think if you just like type this like it goes wrong you have to do this like annoying thing where you're like okay um let's just say points or like a sequence like i and a i i goes from one to n and i think we can now plot this okay so it looks like something let's let's beef up end now to like 100. actually let me just make this 1.0 because then it'll do everything in floating point not that really matters but okay hmm okay it's uh it's doing something uh i mean maybe it's a parabola how does anybody want to like eyeball this try to figure it out um probably if we like want to be like i mean does somebody have a suggestion on like a principled way to try to guess what the growth rate of this is actually you could just uh plot its log yeah that's a good idea so let's see let's do like we should probably okay let's like hypothesize it looks like a parabola i mean who knows it could be exponential for all we know yeah guess x to c and compare the ratio of the values oh that's a good idea yeah okay so let's say if it were if it were like uh you know a n to the c and then we logged it we would get c log n and then if we divided by log n we'd get c so let's like make the log version of the points where we're like hey please give me these points but let's let's be like let's log this and then divide it by whoops divided by log and that's going to be not so great when n is one so oh wait a minute this is a log i uh so let's just go from two it's fine oops i should have given that a different name let's restart this let's call this log points and let's plot this okay it seems to be flattening out which is pretty good i mean that's that's a good that's good news let's make it go up to like 10 000 why not let's definitely not print it out though oh it's taking a long time oh no it's not that long okay let's we can do this this okay well uh it's gone up to like 1.8 something uh i mean on the principle that like probably life is nice it's probably like two i mean it seems to be bounded by two so i suppose that we could speculate that it's at most like you know that a n is at most well let's say constant times n squared uh actually i suppose we could like okay if we believe that it's at most n squared let's plot the that's that kind of like overlay okay this is showing my bad knowledge i just want to like overlay two plots let me just say squares sequence this is not the right way to do it so i apologize but it's a fast way to do it let's just make the squares and let's plot is this gonna work okay i don't know which is which now so let me do color let's make the squares red and the other one's blue okay so it actually looks like it's less than i mean the squares seem to be beating the actual sequence um in fact like what if we do like instead of square we can do like point five i squared whoops seem to even be beating like 0.5 i squared well anyway i mean uh 0.4 i squared i wonder if it's like literally beating it for like every single n let's make n only go up to 20. um doesn't seem to be beating it for like right at the beginning the reason i'm i'm i'm thinking about like having it you know okay we know it's like big o of n square but like i'm imagining how you might prove that like if you really want to prove it's like big o of n squared it's convenient if you're like oh it's like literally at most you know 10 n squared for every single n because that could facilitate a proof by induction maybe oh but that's because i put this 0.4 in what if i put like literally maybe you guys can't tell what i'm doing anymore but uh aha so this one you put is like when the red is like literally n squared it seems kind of good so it seems to be like literally at most n squared let's see if we can figure that out prove that out let me share my screen over here hey i'm back um let's see what's going on um okay so we like kind of believe that uh okay so like conjecture is that like under this hypothesis uh star we kind of think that like maybe well i guess if it's a conjecture we don't have to write maybe uh so it's very speculative uh maybe a n is at most n squared seems pretty true i mean this was like 2 this was like three point something seems good um yeah any thoughts any idea how to prove that oh bernardo says something in the chat um maybe rona just like literally wrote a proof i'm not sure could be okay so we have one person like suggesting a proof one person is suggesting induction yeah i kind of imagine you can prove this by induction well it's not clear but it's what i would try and it might work so okay in lieu of me solving the whole problem let me just say perhaps you could prove this by induction i mean it's definitely true right i mean by virtue of our plotting it's definitely true so that's reassuring right like that's half the battle like now you know that's true um how are you going to prove it well hopefully induction works um okay let's let's move on for a little bit like let's hypothesize that it works hmm so is that good does anybody have any thoughts on whether this helps us in life i mean that would pretty much give it to us right because we could plug it back in and um we'd get a the harmonic sum uh yeah that's true well here's the weirdest thing he's going the wrong way like if if a yeah if a n were like let's say if a n yeah actually you want this right like if a n were like at least i don't know 0.1 n squared then then you know one over root a and would be see now it's in the denominator so you could resume like uh most one over square root point one n okay which is like some constant times one over n and then yeah then we know if you like sum this up to n you get log n right so huh okay so maybe we're like trying to do it the wrong way like maybe we wanted to show that a n is at least this which also seemed true right like it did seem to be growing quadratically maybe with like a smaller constant but then okay so like perhaps we could try to prove that too [Music] although then you see it's funny because then this like kind of maybe messes up your intuition that like this is all under the hypothesis that like the uh inequality i mean that the inequality was exact because there was some feeling that this would be the worst case and then it's but then like this kind of messes with your inequality because like there messes with this intuition because like we saw here we got sad if ai was too small somebody says with the given constraints all ais could be one and hence we can't expect such an inequality yeah that's a good point uh says person in the chat um [Music] if all the yeah the all the ais could be one and then [Music] the you would certainly not have the the nth one is at least point one over point one times n squared and then you might be like oh no i'm like dead because then you might say like oh but then like oh this thing like this thing the sum of one over root a i would be like sum of one and it would equal n which is way bigger than or n log or order log n and you'd be like oh no i'm dead but then you're not really dead because like this was not actually the thing we were summing we were something this and if like all the ai's are one then like the actual expressions are all zero so that's there's that um i kind of feel like it's never less like a good start like you know the things we did were not wasted in the sense that like i think by pushing this forward you could show that if this were literally true for all i uh then uh you're in good shape because then like the a's will like grow quadratically and then this sum will act like a harmonic sum and it won't be too large and [Music] then you have to say like well the only other case is like well the infinitely many cases when this does not hold exactly with equality but there is still some feeling right that like oh perhaps it's good in that case because i mean [Music] well it doesn't hold with equality then like this numerator is like smaller than it could have been yeah wow this one makes it a bit of a problem but uh something uh anybody who has not already solved it have a suggestion or alternatively we can just pause on this problem for a while and talk about something else uh a person in the chat who i know has solved it has said i think the inequality can get pretty tight even if the sequence a n converges to a limit l let's see [Music] yeah so i mean it could be that like you might worry that like a [Music] yeah it could be the okay so we kind of know with this inequality right that like a's can get bigger and bigger and bigger like we've kind of seen that like if you're trying to if you like hit the inequality like with exactly every time then the a start to go quadratically so you could have like the a's like kind of grow for a while but then you could decide like oh i'm going to stop hitting the inequality like maybe i'll have them like i don't know a bunch of a's in a row be the same which is consistent with the inequality um now if you have a bunch of a's and a row that are the same it's not like hurting you because this numerator will be the same every time or sorry this numerator would be zero every time yeah hmm yeah it's a funny thing right because like it's actually it's interesting uh uh it's i think it's an interesting illustration of like the greedy algorithm going wrong which you know can happen right like let's say like you know uh you looked at the sequence of a's for a while and they went up to some thing and like now you're on 8 27 and it's like 11. and then like you know your worst enemy gets to choose the next a and what would your like you know if you're worth something with being greedy what would they choose for the next a they'd be like okay like as i choose this next a like maybe i'm choosing a i plus one um you know this sum they've paid some amount already and now they're going to pay ai plus 1 minus ai divided by ai you're worse than me is like okay this ai has already been fixed this has already been fixed what's left over is my choice of a i plus one i may as well make a i plus one like as big as i can in order to like if i'm being greedy to like make the owner of the inequality pay like the most like right now and so that does suggest the intuition that like oh like things are worse for you the inequality prover if ai plus one is always like as big as it could be like the greedy thing um but like i think this is an example you write of like greedy going wrong or at least greedy maybe this is the worst case but like you see like it's not so clear you should be greedy because like if you make a i plus one like as big as possible yeah that works well but like in the next round like now the new like ai is like smaller so like in the future future u is gonna be able if you're the adversary like future u is gonna be able to force uh less gain that's tricky let's pause there for a moment we come back to this problem but like maybe does anybody have a question a different question uh yes i'm i'm having a some problem on 1.2 part b one point to part b allow me to remind myself what is 1.2 part b okay let's see so this was question i'll write it here this was question two um it was about oh you have like a read wants dnf what is this like it's like an or of like wide or i should really write these problems down before we start to save time uh what is this s and then like you have a bunch of ands here and these have width w and these are going down to variables x1 x2 dot dot x and where like n is s times w and okay so there's some problem exact probability that like this is true probability that the output is true okay even though this is like part a of the problem like i assume you can do this right so can somebody just tell me what the answer to this is i can see what i got so sure i got one minus and then between parentheses to the power of s 1 minus a half to the power of w that looks pretty right if that's not right then somebody pipe up in the chat but if everybody agrees then i guess it's correct okay and then part b is like oh we would like this to be as close to a half as possible without going over okay so we're interested in this thing being around a half which i guess is the same as like this thing being around a half uh okay and then for each w like let's say i take w to be 50 let s be the largest integer such this is true okay let's not stress for a moment about like things having to be integers because that's always very annoying uh i don't think it's the part of the problem uh oh yeah because it wants to compute s problem wants you to compute s okay so like given it's like given basically the problem is like given a fixed w you know what is s which is going to be a function of w such that this thing is about a half and okay maybe s has to be an integer but like the question is only asking you to get this up to plus or minus order one and so like rounding off a number to an integer changes it by less than one so we'll not worry about that so basically we have this problem like w is kind of fixed like w is fixed and uh we want this expression to equal a half and we're like what is that force upon s uh so far so good um okay so uh do you have a you have a particular question or no i'd say that from this point on uh my ideas didn't take me anywhere basically okay well what were your ideas um you know first it seemed like maybe i could make it in some way look like the limit definition of e but then the sign is changed and it would just mess things up see the limit definition of e okay let's see e probably you mean this one right like 1 plus 1 over n whoops yes exactly the n well it's not equal but it like this thing i've written in a funny way it goes to e um yeah well this is a good suggestion because it does it certainly resembles it at least uh somewhat right i mean you have like one minus something that's like small here you have like one plus something that's like small but um okay so somewhat and then you're like oh but if i raise this to this big power you know i want to like okay if you're like one minus small you square it it's still you know it's like .999 squared that's still close to one and then you cube it it's still close to one but like eventually as s gets bigger like one minus small to the power of s will get all the way down to a half and we kind of have like a similar situation here where if like if you have like one point zero zero zero zero one and u squared it's still close to one you cube it it's still close to one but eventually it'll like get away from uh one yeah suggestion in the chat is sulfur s and plot it we could do that right so in fact you know in some sense you can just solve for s but you know the question is asking you to like um the question is asking you to like kind of compute like a standard form asymptotic for s you could be like oh i could like log both sides uh i could take yeah i can be like s i mean okay if you really like um just re-forcing it you'd be like okay i'll take log both base 2 on both sides half to the w equals minus 1 and then like s equals minus 1 over log base 2 1 minus a half to the w and then you could be like well that's all well and good but that's kind of disgusting looking we're trying to show that you know s is like asymptotic to some like nice expression so the suggestion was yeah again we could like plot it and then try to figure out what the answer is and then that would be nice right it's always reassuring to like kind of feel you know what the answer is even if you're not sure how to prove it i kind of want to like go back to the original idea uh which was kind of make this look like this so i mean i know one thing one could do but uh yeah i think your suggestion was that like um uh unfortunately like this is like a minus and this is like a a plus i guess the uh i guess what you need to know is the following um yeah well do you know like any variations on this that could be uh helpful to us you mean other variations of the limit or uh yeah i we could tend towards the the series definition of e i guess but yeah but it still need that doesn't solve the problem of making the the negative thing positive this one you mean yeah but uh i'm not sure that helps i'm not sure i understood what you wanted uh well i would have something specific in mind per se um [Music] although uh well actually i mean [Music] there's like an observation uh if you take uh okay if you just say that like intuitively one plus one over n to the n is uh approximately e we can take the reciprocal of both sides so then like 1 over 1 plus 1 over n the n is approximately e to the minus 1. but this is like uh this is 1 over 1 plus 1 over n to the n and uh what is this approximately equal to i mean actually if you really want to be like sorry go ahead do one um i think it's minus the one over n plus z squared something like that yeah exactly yeah if you have like one over one plus small that's basically one minus small i mean that's the that's the first approximation you know one over one point zero zero zero one is about 0.999 actually i mean i guess you could literally say okay this is 1 over just clearing the denominators n plus 1 over n also known as n over n plus one and if you imagine oh this is close to one you're like oh it's like n plus one minus one over n plus one which is one minus sorry a one minus one over n plus one so indeed from here we have that like one minus one over n plus 1 to the n is approximately e to the minus 1. let's look a little closer actually because you know now you see how like oh here we have like 1 minus something raised to a large power this is you know approximately one over e i don't know what that is point six seven or something uh or no point three something um that's more similar um yeah i guess what i just did there was like kind of similar to just taking x to be minus x here so e to the minus x is like 1 minus x plus x squared over 2 factorial minus x cubed over three factorial plus dot dot um yeah so do you see how this might help yes no it seems closer to a solution seems closer right i mean your first instinct is like okay great so 1 minus x is approximately e to the minus x like this is kind of you know our favorite approximation in the world 1 plus x is approximately e to the x but you know i replaced x by minus x um so that may make you feel good here okay so there is the issue that like this is not literally correct so i mean we got to like come back to this but okay let's just press on as though this were true so then we'd be like one minus a half to the w to the s i mean is approximately one minus oops sorry it's like we're taking x to be this thing so it'd be like e to the minus a half to the w to the s and like the reason okay so like why were we messing around like the reason this turned out to be a good move is like now something like nice happens in the exponent because you know the laws of exponents we can say like oh this is literally equal to e to the minus uh s well times a half to the w let me just write that as over 2 to the w okay so albeit we made like an approximation here and we have to worry about what the error is uh we've got this like expression looking nicer so this is like another like it's kind of like life rule that like okay if you have like like the sum of some like maybe like one plus something small positive over a negative to a power that's kind of gross so like it's nice to make the base of the exponent look like e to the something because then if you take like a further power the exponents just multiply like this is what's happened here um okay so then you know if we believe that then we're trying to set this thing equal to a half well again we can brute force solve this but like it works better this time right because like okay i'll take the reciprocal of both sides get e to the s over 2 to the w equals 2. now since the base of the exponent here is e i should probably take the natural logarithm so i'll take the lawn of both sides and i get like s over two to the w equals lawn two which is some number point six or something and what we're doing solving for s oh we get like s equals two to the w times lawn two which is nicer so like this would count as like you know standard form like if we exactly had this you know it's just you know the parameter here is w it's like some constant times another constant to the w which is good now we we're still not done like we have to suffer because we're like oh well i mean we made an approximation here and we have to like worry about the extent to which that approximation is correct so okay like you have to basically the thing to do and like okay i won't like solve everything right now but like the thing to do is like um say like okay well this is around e to the whatever one over two w but like maybe plus or minus some error term and we gotta deal with that error term being there and actually now we have like the same phenomenon let me just repeat this like you know we got like e to the sum blah blah which made us happy and then we raised it to the s and we were like oh that's equal to e to the blah blah times s and we're like we're very happy but now we're like oh shoot there's like an error term you know like this wasn't really e to the whatever 1 over 2w 2 to the w it was some like nice thing plus maybe some error and like now we have this and we're like oh we're back to the same like annoying situation where we like we're all happy to like you know do this we got this stupid plus here now what well same thing oh uh a person asks can we replace the approximation with a less than or equal to yeah probably so like uh you know this like 1 plus x is approximately e to the x uh it has some uh i mean i guess like this is literally true for at least for positive x for negative x maybe i have to think about it well let's see e to the x looks like this like one plus x looks like this so i guess it's like literally true for all um x but you know you kind of need like a two-sided inequality because like we're trying to figure out the asymptotics of something so like um we kind of need to know that this like approximation is like not only not bigger than the truth but also not smaller than the truth by too much [Music] okay so all we're going to say here is like i mean i'm not going to solve the whole problem but like you want to like do a similar thing where you convert this plus to a times because you're exponentiating so you're like oh if only i could write this as like e to the something times like maybe one plus tiny and then if i raise this i mean this is maybe a step that one needs to do if i raise this to the power of s then we get e to the blah blah blah times s which makes us happy it's a nice expression times like 1 plus tiny to the s and now we might hope to show that like this is small which is basically true if tiny has to be like less than one over s um again you could be like oh this is roughly e to the tiny and so like but okay let me not quite solve everything uh yeah i mean we can come back to this um but that's how one can make progress alternatively like once you're here i mean all the things we've been doing are not too dissimilar from saying like oh like let's understand like the taylor expansion of log so like it's also true that like log of like one plus tiny is approximately tiny and that can make things simpler in your life too um but again you have this you're not quite done because you have to worry what does this approximation mean you know it's like up to like plus or minus tiny squared times a constant maybe i guess these are some of the ideas you'll need great um any other question how about another question or we can go back to something we've already talked about i bet there's another question i bet not everybody has solved every problem or you can ask about something like not on the homework about the lecture part e for one i'll thank you like otherwise i just have to be reduced to start chatting like how's your day going my day's going okay by the way uh part e on number one what is part e number one oh oh problem one was about the prime number theorem how nice let's see i'll just read it how many factors of three are there in 100 factorial 100 factorial well that's not the question it's like a rhetorical question that the problem starts with oh it's a hundred times 99 times 98 times two times one how many factors of three are there in here oh so like i guess like the way to figure this out is if you were to factorize this as like you know so many powers of two so many powers of three how many threes would you get well you have like a three here and then you have a six here which gives you another one and you have a nine here which gives you another two so there's like a like a non-annoying way to count this well sort of annoying way to count this 15 all the way up to like so you're like oh for the three i get one for the six i get another one for the nine i get two for the 12 i get 1. the 15 i get 1. for the 18 that's in there i get 2 because it's divisible by 9. this goes like this for a while then at some point there's like a 27 and you're like oh for this one i get three um yeah so like the uh the annoying way to count this would be like oh why did i write it backwards i should have written it from left to right yeah let me do that 100 factorials like 1 times 2 times 3 4 5 6 7 8. why not why not do this should have got to 18. anyway yeah the annoying way to count it is to be like oh it's like it's one plus one plus two plus one plus one plus two plus and then eventually you get like a 3 from the 27. that's the annoying way to count it but like the other way to count it is um i was doing it with like these underlines before which is actually kind of suggestive uh if you have like three i get one factor six i get a factor nine i get two factors but i'll draw it like that twelve i get two factors fifteen i got two factors 27 i got three factors but i'll draw it like this and then instead of you know counting like by columns you can count by rows and be like oh actually the total number of factors of three is like this many which honestly is basically like 100 over three right it's like stupid integer parts i mean it's like the floor or the ceiling i guess it's the floor of this um and then like count this row wait why did i put a dot here for nine that's you should have like a dot here for 18. this is where i'm counting the multiples of um nine which are giving me two factors uh so i get like extra for every multiple of nine so it's like basically a hundred over nine i guess it's exactly eleven and then like you get some even more counting the row for like this is like multiples of three of nine these are multiples of 27 and like you get around 100 over 27 of these it's really the floor of that it's what like three and you even get like the row for 81 like the multiples of 81 of which there's only one but like that's the one where you're getting like an extra factor of three i'd like this one is the count of four by like 100 over 81 four okay cool so uh does that make sense i guess this is just like taken for granted in the statement of the problem but yeah i guess anyway if you count it up if you can't have all the multiples of 3 you get like 100 over 3 plus 100 over 9 plus 100 over 27 plus 100 over 81. [Music] okay so if you weren't like lazy or if you were lazy and forgot about the stupid floors you'd be like oh this is like 33.3 repeating this is like 11 point very little this is like basically four was like three point eight or nine or something this is basically one if you're like add this all up you get like 49 ish but i guess the truth is actually 33 plus 11 plus 3 plus 1 48. that was close if you'd it's 48. okay so that means that if you write down like 100 factorial prime factorization it's like 2 to the power of something times 3 to the power of like literally 48 we just worked it out 5 to the power of something etcetera i guess the last line i could find that could be in here is what 97 uh oh okay so then in the chat any same strategy works for 50 or any other number yeah exactly yeah exactly uh so what was the point of this the point of the question is oh to similarly count the number of factors of 3 and 50. well let's do it it'll be good times so 50 factorial is 1 times 2 3 4 5 6 7. there's like a 9. uh there's like a 27 in here and like 50. so how many factors of three do we get it's like oh we got all the multiples of three and then we get also an extra one for all the multiples of nine an extra one for all the multiples of 27. which there's only one there's no 81 here so like these guys add up to basically 50 divided by 3. these guys add up to basically 50 divided by 9. these guys add up to basically 50 divided by 27. that's it so if we forgot about the fact that like you know you have to floor each of these like that's what is this 50 divided by 3 is 16 point something 16.7 this is going to be like what five and a half this is going to be two-ish a little bit less than two i don't know 1.9 so we add these up we get like 21 22.2 24.2 this is gonna be like 24.1 or something we added these numbers up 24.1 i just made that up somebody can do it with your calculator uh but i guess the actual truth is what is 450 over three 16. what is floor 50 over 9 it's 5 4 50 over 27 is one so if we add them up we get 22 so okay well this 24 was not so far off uh so that means that 50 factorial looks like 2 to the power of something times 3 to the power of literally 22 times 5 to the power of something okay what was the point of this question so then oh okay the point of this question was let's say you do n choose n over two where n is even just and even and then you look at the prime factorization of this this looks like 2 to the something times 3 to the something times 5 to the something times 7 to the something and the claim is that uh this number is at most n and like this number is at most n this number is at most n this number is at most n etc okay so this is the question uh okay well apparently all the stuff that was in the introductory part of the question must be relevant so that was about like powers of 3 in 100 factorial and 50 factorial so i guess the point was uh if eg n is 100 then what this is what 100 factorial over 50 factorial 50 factorial oh and we learned right like we learned how many powers of what was this how many powers of three were here it was back here this was like two to the question mark times three to the what was it 48 cool blue is horrible 2 to the question mark times 3 to the 48 times 5 to the something okay and then in the denominator we have 50 factorial okay it had 22. it was like 2 to the something times 3 to the 22 times 5 to the something times and then we have another 50 factorial so we get like 2 to the something 3 times the 22 times 5 something right so i guess the number of powers of 40 of of 3 are like here's 48 22 22. so the final answer is going to have like 2 to the something times 3 to the 48 minus 22 minus 22 times 5 to the something okay so that's like this is like 3 to the 4 which is 81. and hey presto that's supposed to be almost 100. yes it worked so yeah i guess the question is that a coincidence or what um a person who asked the question originally uh any thoughts or anybody else was it a miracle well hopefully not but um yeah i mean you can do the same thing for for any p right you can get exact terms for right you'll be in the numerator but i was having trouble dealing with the floors um when i was doing that let's see well what was your thinking well let me ask you this like um could you conceive that like okay here like you know we had 222's and when we added them up it got pretty close to 48. could you could conceive that like it'd be quite far away from 48 like if i was like oh i did the math and like uh you know i did all these things here and like this expression turned out and maybe i did with like some other numbers it turned out to be like you know 61 minus like 15 minus 15. yeah not not really because i guess the numerator only has like one extra term in its prime factorization um i guess you might lose some with the floors as well but yeah it shouldn't be uh they should be decently large what should be decently large the exponents in the denominator like the 22s i see um you only add 81 in the numerator everything else is still in the 50s er is accounting for in 50s uh sort of right like okay this was kind of a mess but like um let's see uh yeah so what's yeah maybe give me more on your thoughts here well uh let's go back up to where i think we had the 81 in the 100 term uh-huh um yeah so like that doesn't exist in the 50. uh i guess there's not really a corresponding term no that's true yeah and then um all the others do and we have two copies of the 50 so like that's just that was just my answer to why there should be let's see you're saying like corresponding to this we got like two copies of 50 over 27 floor yeah and corresponding to this we got like two copies of 100 oh 50 i guess i haven't raced here 50 over 9 floor so i mean all right what can you say about like two copies of 50 over nine floor versus one copy of 100 over nine floor is that they differ they're just at most one or is it almost one like two times this like two you're saying is like is 100 over 9 floor minus 2 times 50 over 9 floor at most one is this the question uh yeah in general not just for this case but yeah uh okay is it i'm not good with mental math but give me a second oh my god this is 11 point something clearly 50 divided by nine is like five point something so it's minus two times like five point something yeah it's true here it's like 11 minus two times five which is equal to one okay [Music] should we do another one let's do what's our favorite number 751 hey that's the class 751 should we do over nine it's over nine floor minus two times oh well it's not even okay let's make it 750 then what's half of 750 that's the hardest part 375. all right what is 750 divided by nine lord 83 83. could be i just plugged it in okay what's 375 over nine floored i hope it's 41. yeah yeah it is so yeah okay okay so why i mean well i'll leave this to you okay let's leave it here i'll leave it to you um but yeah uh presuming this is true then you're probably in good shape right because like if this difference is always at most one and that basically means like here uh wait it didn't even happen here what happened oh because i see yeah yeah this is not one but this is like the contribution of several things which are at most one right because like where does this 48 minus 22 etcetera come from like this 48 was like a bunch of was like i don't know like floor 100 over 9 a floor of 100 over 3 and so forth and we could compare that with the dots for the 50. okay so hopefully this is like the contribution of a few things which are at most one and it's therefore small but um yeah i guess let's leave it let's leave it there that's good thanks well yeah well i guess our time is more or less up are there any like parting thoughts or final quick questions let me uh let me stop the video now\", metadata={'source': 'Zy1eu1EVXUQ'}),\n",
       " Document(page_content=\"today we'll be talking about a few topics simultaneously in probability theory and it's gonna be great because I love these I love probability I love gaussians I love the central limit theorem you're gonna love them too it's gonna be great actually in some ways the the reason for this lecture besides you know covering these great topics is preparation for the next lecture when we're going to talk about Chernoff bounds which is like one of the most you most ubiquitous tools in theoretical computer science but I think you can't really probably appreciate the context for them unless you see some stuff about gaussians in the central limit theorem first so a very typical scenario in theoretical computer science is you have something like an algorithm that is probabilistic and it you know corn corn succeeds with some probability P and then you run it a bunch of times independently and you want to understand something about the total number of times this exceeds so that's the context for all these topics that we'll talk about today so let me write this down set up okay so let X 1 X 2 through X n the iid I'll remind you what this is in a second random variables okay so I ideas some like abbreviation is very popular in math let's so in computer science but since four identical I'm sorry independent and identically distributed so the main thing is that there are independent random variables and identically distributed means they all have the same distribution it's very common scenario and in this setup they're just going to take values that are either 0 or 1 where 1 is considered a success okay so they're gonna have these properties the probability that X I is 1 is P and as I mentioned they're only going to take the values 0 and 1 so therefore the probability that X I equals 0 is 1 minus P which is will call Q like last time actually or mathematically this is to say that each X I is a Bernoulli random variable it's probability success probability P and we're gonna let s to know their sum let me actually write S sub n so sort of remind ourselves that it's a sum of n copies of this random variable so sign is also a random variable and you know its meaning is it's the number of eyes such that X I is 1 okay and our goal is to try to understand this s random variable properties about it so let's start with the basics whenever you have any random variable at all like the very first thing you should strive to do is understand its expectation or its mean these are synonyms expectation and mean okay so somebody tell me the expectation of s s of n who said it yeah yeah what's your name yeah hey Sonny ah yeah PN that's right um okay the way to see that a good way to see that is by the rule linearity of expectation okay which says that the expectation of a sum is equal to the sum of the expectations and for this we don't even need independence you know I stipulated that they're independent random variables but this is true even linearity of expectation is true even when the random variables are not independent again each of these random variables has expectation P because it's one was probably P and zero otherwise that's great so now that we've got that the second thing you should always strive to do whenever you have a random variable is to compute its variance it's the next step up somebody know the variance of this random variable yeah and P Q right what's your name again he thinks and P Q that's true oh say why I mean maybe this is one that you don't necessarily immediately have in your head in fact let me review variance a little bit over here so what is the variance of a random variable and call the random variable Y here so it looks generic very it's a random variable Y is this expected squared deviation from the mean so it's this quantity where I'm writing mu for the mean or expectation of Y okay so let's somehow the meaning how much it deviates from its it's it's mean squared wise there's also a sort of a formula for it it's the expected value of y squared minus the expected value of y squared where those two things have different parentheses ation good so it's a measure of how much the random variable fluctuates and let's recall some basic facts about it one important basic fact which is the main fact you need to do this calculation is that variance is also additive but for this you need the assumption that the random variables are independent okay so in our situation s I mean extensive you know the sum of more than two random variables which is the sum of independent random variables it means the variance of s overall is the sum of the variances of the individual exercise so actually we can calculate that easily let's do that here the variance of an individual X I Wow let's say using this formula because it's easier each X I has a property that X I squared is the same as X I because it's just 1 or 0 it's the expected value of X I squared it's 1 with probability P and then we know it's expectation is P so this is P minus P squared which is P times 1 minus P or P Q ok so that's how we can derive this let me also just record a couple more facts about variants that we'll want to remember if you take a random variable and add a constant to it that does not change the variance variance this translation invariant I guess that sort of follows from this this one technically because a constant is a random variable that's independent of everything else and it has zero variance but it's good to just remember this one on its own that's a few add a constant to a random variable but if you multiply a random variable by a constant what happens to the variance yeah right what's your in Ian thanks Ian yeah it's a funny one it goes up by C squared as you can see from like this formula for example or the other formula ah good that's most of the properties of variance that I want to remember actually let's remember one more thing which is the standard deviation of a random variable it's just the square root of the variance okay and this is often denoted Sigma if this one is denoted mu which means we denote variance by Sigma squared so Sigma S stands for standard deviation mu M stands for mean and standard deviation we like because you know if Y is has units like it's Y dollars then the units of variance would be like dollars squared which is weird so you take the square root to get back to regular sort of units and you might ask why don't we just put the absolute value here or something in the first place but absolute various like a disgusting thing to work with this is a nice you square something that's a nice polynomial so it makes calculations nice so this is why things are defined the way they are okay great any questions okay so ah here's another sort of life pro tip if I may offer one you know as I said once you have a random variable you should figure out what its mean is you should try to figure out what its variance is to see we can and that if you can do both of these things you should standardize your random variable if you can and that means make it have me and zero and variance one like the best most enjoyable mean for a random variable to have is zero and the best and most enjoyable mean for a random variable to have sorry various for it to have is one you can kind of arrange for that by doing some trivial operations to the random variable which are and these operations are called standardizing it which I will explain to you just now so if you have a random variable like in our case S sub n I'll just look for this particular SMN the first thing is to get its have mean 0 you well you subtract the mean from it okay so this is in general the mean of the random variable this operation sometimes called centering okay that's a trivial transformation of the random variable but it makes the new random variable have mean 0 which is pleasant and then you should take this and get it to have variants 1 so actually subtracting this mean doesn't change the variance right by this bullet points and then you can mess around with the variance by multiplying the random variable by a constant which is a also a trivial transformation so what a constant should we multiply by to get the variance 1 yes yes very good what was your name Jin yeah you should multiply about 1 over the standard deviation or divide by the standard deviation okay so in our example here that was roots wealth of early roots npq okay in general this is the standard deviation okay and that makes a random variable which has mean 0 this also doesn't change the mean of course and variance 1 and therefore also standard deviation 1 which is really convenient okay so in general if you have any random variable you might define s you might define Z as the standardized version to be s minus mu over Sigma where these are the mean and standard deviation of s okay and that's great because it's a convenient meaning around it and variance to have and it doesn't lose any information at all about the original random variable so if you really care to understand everything there is to know about s it's necessary and sufficient to understand everything there is to know about Z just because I mean you can understand everything there is to know about s by understanding the probability that s is at most you for every value you that sort of completely defines the random variable s but s linear transformation of Z this is the same as the probability that I guess s is some mu plus sigma z CU and then we can invert this i guess this is the probability that z is that most u minus mu over Sigma okay a meal and Sigma are constants here so if you understand everything about the distribution of s then you also understand the distribution of at Z and vice-versa okay so let's go back to this example of our particular random variable s and which is the sum of n independent Bernoulli random variables that's zero one random variables with probability P of being 1 and let's even focus in on the most enjoyable all special case when P is 1/2 okay so this is like the case when you're flipping a fair coin n times let's consider the case of P is 1/2 which also means Q is 1/2 and in this case it's random variable SN like a random variable representing the number of heads let's say when you do n fair independent coin flips so it's mean as we see from up here is n over 2 and it's standard deviation well it's variance is n over 4 therefore a standard deviation is root n over 2 ok you should really for like non ridiculous random variables you know you should think of the standard deviation is like the amount by which the random variable it's sort of typically our standard deviates from its mean and this is true if you flip n coins you know you expect the number of heads to be like an over 2 but it should fluctuate it's an arranged it's like a few constant multiples of root n typically which is consistent with this fact we learned last time that if you flip and coins the probability that you get exactly n over two heads bang on when n is even is proportional to 1 over root n because if it's gonna be like fluctuating around in a range around n over 2 a few multiples of square root n maybe they should all be equally likely in fact probably then landing right in the middle is the most likely case so that should occupy like a 1 over root N or so fraction of the probability okay so now this sort of life protip told us that you know this SN is fine but we should really maybe standardize and look at I'll call it Z sub n which is S sub n minus n over 2 over root n over 2 okay let me multiply top and bottom here by 2 and factor out 1 over root N and actually let me not do that yeah let me multiply top and bottom I like to in fact throw out 1 over root n cos n minus n I do that right mm yeah and this is right like this 2 X 1 minus 1 plus 2 X 2 minus 1 oops plus 2 xn minus 1 okay that's sort of a funny way of writing this expression and the reason I do that is you see now look at these random variables here these are nice random variables to these random variables 2x I minus 1 half the time it's one half the time it's minus one okay so these are all like random variables which are like plus or minus 1 with equal probability such random variables are called Rademacher random variables by fancy people and this turns out to be like the more pleasant thing to study then the sum of n random variables that are 0 1 it's to study the sum of n random variables that are equally likely to be plus or minus 1 and also divided by root n that's the convenient normalization to put into because this big some will have mean 0 that makes sense if you're adding up random plus or minus 1 random variables and also variance 1 and as I said you know if you want to know anything about SN you'd suffice us to know the analogous thing about Zn so for example last time we were looking at the probability that SN is exactly n over 2 it's assuming as always that n is even the probability that you get exactly 50/50 heads and tails and which event about s Zn is this equivalent to yeah yeah what was your name thanks Chris it's equivalent to the probability that Z and is zero and as I mentioned last time this is asymptotic as it turns out to root two over pi times 1 over root n pi Sterling's formula or you know less precisely it's big theta of 1 over root n uh ok so one good thing about this ZN random variable is you can make like n larger and larger and larger and when you do that with SI and you get random variables that take larger and larger and larger values but ZM is like nicely set up so that always has variance 1 no matter what n is so nice things happen when you take Zn 2 when you take n to that larger and larger and larger and one thing you can do is plot you know the histogram of Z ends probabilities as n gets bigger and bigger and bigger and when you do that you'll find they look like you know this tell me you know they follow this trend that kind of looks like this okay and you know it's exciting to do this with an actual computer and look at it and you'll see as n gets really big it looks like this most famous bell-curve writes that this number up here seems to be something like 0.4 ish and you know it really goes down to zero very quickly actually the maximum possible value of Zn is root N and - we're done and you know Zn is taking on values that are integers divided by root n in fact there's a parity thing so like the width of these bars is gonna be like plus or minus well I guess the width of the bar is gonna be like to root over root n what's the width of one of these bars okay and well when you when you see this plot you're like clearly this is converging to like you know the Gaussian random variable okay it's converging to a real-valued random variable you know the end takes on like a finitely many discrete values but like it's obviously getting approximated by a random variable that's taking on real values on the real line and the fact that this is true is the subject of the famous central limit theorem which I will now state I sure I can leave this up okay so this is the central limit theorem or CLT now the word Center has a lot of technical meetings and math does anybody know why it's called the central limit theorem it's because polio who named it you know just thought it was like the most important limit theorem so it's the central limit theorem so it's like just a regular old English meaning of central there okay so central limit theorem concerns you have a sequence X 1 X 2 actually an unending sequence of the iid independent and identically distributed random variables okay so for example these Bernoulli coin flip random variables or any random variables that technically have a mean and a non zero variance then the Associated random variable Z n by which I mean the one you get by adding up the first n of them and then standardizing so that Zn will have mean 0 and variance 1 its distribution approaches in the limit as n goes to infinity that of Z which is a standard Gaussian random variable and I'll remind you what that is later but it's it's like the most enjoyable continuous random variable the the rehna variable this PDF is that bell curve and what is the sense in which you know these discrete random variables approach this continuous random variable in the sense that for all real numbers you the probability at Zn is that most u equal to we're gonna go to space here the probability that a Gaussian is at most u plus or minus little of 1 and this little of one let me put it right say it properly is with respect to and going to infinity okay so that's the famous and grand and central central limit theorem and one thing I'd like to remark about this theorem is it's basically useless so this is a remark okay not completely useless but I find it kind of hard to conceive of a scenario in in theoretical computer science where you could actually use this theorem and the reason is it doesn't give you any information at all about the rate of the error it just says that it's tending to zero as n goes to infinity and the statement does not say anything about it it's not like you know 1 over root n or 1 over log N or 1 over log star n you don't know anything from the statement of the theorem and in fact furthermore the way it stated this rate could actually depend on U which is kind of that would kind of stink so it gives you very little information there's not a lot you can do with it but by the end of the lecture I want to fix that by telling you the good version of this theorem the version that has actual error bounds and that person is called the berry Sen theorem so we'll get there eventually but we'll do a few things we'll talk about gaussians for a while first actually another deficiency of this theorem is that it only is true as thin for identically distributed random variables and that's a bit restrictive like you don't always have identically distributed random variables\", metadata={'source': 'r9S2fMQiP2E'}),\n",
       " Document(page_content=\"okay any questions right now alright so the next thing we're gonna do is talk about the greatness and gloriousness of gaussian random variables they're so great if you don't already love them you're gonna love them at first you like this the first time you see them you like these are horrible how could these be the greatest random variable but they really are oh it's gonna be so good once you once you come to love them so I should mention of course that they rise not only just in central limit theorem but like everywhere and like statistics and like machine learning and you know high dimensional geometry so you got it you got to know about gaussians for sure okay so yeah what's the Gaussian so we use this notation see this is not the asymptotically equivalent to backslash sim but it's the one that's like this random variable has the following distribution backslash sim and the notation for a standard Gaussian is capital n parentheses 0 1 ok so this capital n stands for normal and normal is like a complete synonym for Gaussian Gaussian so this is saying that Z is a standard I'll come back to that in a second Gaussian or normal random variable ok I might start using this abbreviation rv4 random variable and standard refers the fact that here we're saying a Z as a standard Gaussian which just refers to this it has mean 0 and variance 1 later we're talking about gaussians that don't have mean 0 and variance 1 but definitely the mean 0 variance ones are the best ones so we'll call them standard and that's what the 0 and 1 stand for here in general will you write the mean here and the variance here okay that's just terminology but what this means is that that Z is a continuous random variable which I hope you remember what it is or know kind of what it is a continuous random variables these are the good kind of real valued random variables that are not discrete but at least have the property that they have a probability density function if you don't have a probability density function and it's kind of annoying but luckily gaussians are so great that they do and their probability density function is where okay probability density functions PDF okay and then their PDF is this it's basically okay the notation is Phi of little Z it's a root okay so probably density function if you recall it's a non-negative function on the reals whose integral is one okay and like five Z represents basically the probability that this random variable will be in the range between Z and Z plus epsilon divided by Epsilon okay so the formula is it's basically this it's basically e to the minus Z squared that's basically it there's a few other parts to it I guess you're supposed to divide by two here and then you have to multiply by some number here which I guess is 1 over 2 root 2 pi but I'll write that it really tiny font for now so that you recognize like what are the most important parts and least important parts of this PDF are because this is like the maybe if you don't know Gaussian so well it's like the intimidating thing when you see like this very somewhat ugly looking PDF and you're like this this can't be a great random variable but it really is so the e to the minus Z squared is the most important part of this random variables PDF this is basically the plot of it by the way right this curve up here I mean ignore the rectangles and this axis this looks like a plot of e to the minus Z squared over 2 I guess times 1 over root 2 pi that's what this point for number was 1 over root 2 pi can you see that it's symmetric about the the z being positive or negative it takes the same values and as he gets big enough solute value of z squared gets like even bigger and then like e to the minus Z squared gets unbelievably small so this thing gets really really close to 0 rapidly as you go away from 0 right so even the minus Z squared is the important part and eventually we'll see it's important because you know if you multiply two Exponential's together the exponents add and so with multiple gaussians you start getting things like the sum of the squares of some numbers here and that's a great expression the sum of the squares of the entries of a vector is its length squared and we'll see that's the important feature of gaussians this divided by 2 is just stuck in there you don't have to do it but it's luck in there so that the random variable you get will have variance 1 if you didn't stick in or divide by 2 here you get a random variable with variance 1 over root 2 or something which would be slightly annoying so you put in the divided by 2 and then finally this is the least important part this has to be a PDF which means that when you integrate it it has to get equal to 1 and so this is the number that you need to stick out in front so that the integral of this expression is 1 or the area under the curve should be 1 and you know it's not hard to see like okay this when you plot it so I said it goes to like 0 like really really fast so like if you go out to like I don't know 2 and 1/2 it'll be real close to 0 it'll be like real close to 0 over here at like minus 2.5 and then you see if you like draw like a triangle up to here why don't you draw it like this you know the area under the this triangle is about the same as the area under the curve so that's supposed to be one so like the area of this triangle is like the base is five so it's like half times five times the height it's supposed to be one so that means the half height is supposed to be like about point four which it is I mean the accurate value is 1 over root 2 pi but it's a round point for it and as well see lately later it's actually slightly interesting why this is the right constant 1 over root 2 pi to put out here to make this integrate to 1 the fact that it's got this like root 2 pi is in fact the same reason that there's like root 2 pi and Sterling's formula you know it's connected to the fact that you know when you flip these coins the the resulting probability mass function tends to the gaussians PDF and this actually as I said a bit of an interesting fact that the integral of this is well this fact that the integral from 0 minus infinity to infinity of e to the minus d squared over 2 DZ equals root 2 pi which means that you have to divide by root 2 pi to make it integrate to 1 you might ask if you haven't seen before like why is this interesting just do this integral the reason you cannot sort of just do this integral is that this function does not have an antiderivative that you can express with you know good old normal functions so you can't just write down the antiderivative and subtract or anything I have to use some more smarts that said here's another tip to say so you can sort of can't do this integral explicitly but on the other hand very many integrals that look like it could go look some function of Z times this Phi of Z many functions like this you can do integrals like this you can do by doing integration by parts so for example if you put like Z or Z squared or any power of Z in here you can do this integral by integration by parts so you're not off the hook for computing it although you know you could probably just ask your computer to do it instead of doing it yourself okay any questions about this these definitions great okay I'm now gonna tell you the most important fact about gaussians in my opinion from which all other great properties of gaussians derive and the interesting thing about this is like like the most important fact about gaussians involves multiple gaussians so the most important fact is not about just like one Gaussian random variable but having a bunch of gaussian random variables so here's the fact that Z vector be z 1 through Z D what's a vector in Rd where the component it's like the coordinates are all independent and identically distributed gaussians okay so just fix a number D like two or three or ten and then pick D many gaussians independently and stack them into a vector so that gives you like a vector or a point in D dimensions and the fact is that then Z's distribution is rotationally symmetric no traffic sure in a second but think about for example the case of D equals two first so I'm telling you pick two Gaussian random variables from this distribution that first looks kind of bad and make a you know plot the pair in r2 and you can draw a like a vector then uh basically the angle is uniformly random it's equally likely cord equally likely to point in any direction I'm not saying anything about the length but the angle is uniformly random so let me draw a picture so here's our to imagine experiment where I first picked z1 to be a Gaussian and maybe it turns out to be I don't know positive 1.6 or something here's z1 and then I pick another Gaussian z2 and maybe it turns out to be positive I don't know point four or something there's z2 and then I'm like okay I consider this vector C hot so I'm always going to do this experiment and the point is it's equally likely to be pointing in any direction okay or for any circle centered at the origin your final point here is equally likely to be any point on that circle okay that's not a formal statement because it's continuous random variable about like you know what I mean right so I mean for any little segment of the circle it's equally likely to be in equal length segments okay or if you do this with three dimensions so you imagine like spheres like you know you take three independent gaussians and stack them in the coordinates it's equally likely to be anywhere on the sphere I don't really say much about how long it back there is but it's like rotationally symmetric it's even true when D is 1 but it's like a stupid statement when D is 1 when T is 1 it's just saying like the Gaussian is equally likely to be positive or negative which is true if you look at the PDF it's symmetric so this is the most important fact about gaussians and why is it true well it comes straight from the form of the PDF here I'm assuming you know a little bit about like continuous probability distributions but we're not going to rely on this information too much it's more for your edification so this is a random vector so it has a PDF which is a function from Rd into R and it's PDF a particular point like little Z hat vector well because I stipulated that the components are independent it means you just multiply the PDFs ok so it's 5 this 5 here 5 Z 1 times 5 Z 2 times 5 Z D ok the point is when you plug in you know the PDF for Gaussian this equals like some junk but basically e to the minus and you got Z 1 squared you know the exponents add so you get Z 1 squared Z 2 squared Z 3 squared summed up see you 1 square plus CD squared ok fine there's like an over to here which is not so important and there's some factor out here which is one of our root 2 pi to the D which is not so important but the thing that is important is this piece here is like the length squared of Z vector ok so you can sort of visually visibly see from this formula that the PDF of this random vector the amount of probability it puts like quote-unquote near the points little Z only depends on the length of Z the vector which is exactly what this is saying it's rotationally symmetric like the probability of masculine like a point only depends on its length like what spirits on and not like what angle it is okay one corollary of this fact that I'm gonna try to do rapidly because I don't want to get bogged down in calculations but you use this fact to deduce that this integral e to the minus Z squared over 2 DZ is the root 2 pi ok this fact that you need to know in order to figure out how to properly normalize the PDF so I'm just going to like sketch the proof of this this thing is root 2 pi and ya so there's a trick and it's illustrated by the fact let's just use the fact that its most basic incarnation with D being two so let's think about having two gaussians and therefore let's look at this function f of z 1 z 2 being the product of that integrand but for C 1 and C 2 so even as Z 1 squared plus Z 2 squared over 2 and okay this is going to go terribly I'm sure but I'm gonna try to plot this on the board in two dimensions so here's like the r2 plane oh man I'm alright so nervous so it kind of looks like a hump you know like it's it's its largest value is that Z 1 equals Z 2 equals 0 and I guess it's value there is um 1 and then somehow like it's like a hill you know it comes down like this sitting like this but there's like a 3d aspect to it right so somehow can't do it goes back down like this ok goes like that it's very rapidly decreasing to zero as you go out and yeah so first of all well this whole integral is equal to the area under this surface oh wait sorry what I want to say is this thing squared is equal to the area under the surface and that's cuz like well you know if you do the area under the surface it's like the double integral of this F Z 1 Z 2 DZ 1 DZ 2 and then like you undo this multiplication and the parts come out and you just get the square of this integral so we're trying to understand that this equals 2 pi or another way to understand the why like this is equal to this thing squared is sort of white like cutting this hump into like vertical slices so each vertical slice will look like of like a cross section that looks like the bell curve so you kind of compute the area under that but then it's like scaled by an amount which is also given by the bell curve so okay if you know some 3d calculus you'll understand that the area under this hump is equal to the square of the thing we care about but that's cutting it vertically you can measure the area under this hump or volume under this hump by cutting it horizontally as well and then you'll get like disk shapes right so the whole point is that this this expression is rotationally symmetric it's like circularly symmetric so like it's it's like level sets or if you cut it into slices it look like discs which is great because so you can like just think of it as like stacked up pancakes have like ever increasing or ever decreasing like radius and you can therefore sort of see that the area under the surface is also like the integral of like height h going from zero to well the peak here is f of 0 0 which is 1 1 of the area of the disk at that height which you know is PI R squared okay this R of H squared okay and what is the height how do the height and the radius relate well if you're at radius R squared that means you're at a point like z1 z2 with z1 squared plus z2 squared being R squared and then your height is this expression okay so the height is just a given radius R as e to the minus R squared over two okay and that means that R squared is two lon one over H so you can plug that in here so you've got the integral H goes from zero to one of pi times two lon 1 over H th I guess also known as negative lon H and we even did the integral of lon last time so this one you can just easily compute and well the integral of lon one over H from zero to one turns out to be one so this turns out to be 2 pi okay so that went a little fast and it's just like a math calculation that I wanted to show you sort of for my own amusement but it is one illustration of how like you know this key fact about rotational symmetry is like the key fact from which everything derives free gaussians any questions about that yeah yeah you can eat to the minus zero yeah okay I did a risky thing I put on this like unmovable board so like the next thing I'm gonna do is cover it up so really it's lost opportunity for a question okay okay let me tell you uh another very important fact about gaussians which is a straight corollary of this key fact and it's also this corollary is also the reason the central limit theorem is true so for Larry two of the fact is this I'll put in quotes because I'll explain it properly a little later but it's not the sum of gaussians is Gaussian well slightly more precisely the sum of independent gaussians is Gaussian okay I put any quotes because like so far literally speaking that cannot seem to be true because if you take two gaussians independent Gaussian they both have variance one you add them up you'll get a random variable with variance two and so far we only talked about gaussians having variance one but there's a more general notion of a Gaussian distribution it's like a you know shifted version of a Gaussian distribution and that's the kind of Gaussian you get when you add up independent Gaussian but the point is like the sum of independent Gaussian random variables will also have this like bell-shaped PDF which is Gaussian so I'll make some definitions about what is a non standard Gaussian and then we'll see why corollary two is a consequence of the fact but here's another nice idea about the central limit theorem well saying you believe that something like the central limit theorem should be true namely that if you add up a bunch of independent random variables the resulting some random variable should somehow converge to something maybe you didn't believe the central limits so far as to say that you believe that the sum of independent random variables converges to a Gaussian but you believe that converges to some kind of random variable then it follows from this fact that that rent variable has to be a Gaussian because or at least it has to have this property that gaussians have right because if some of random independent random variables converges to something then you can take a bunch of random variables there's some convergence do something take another bunch of random variables it converges to something and then add those two things together on one hand it's the sum of two some things but on the other hand it's a big sum of original random variables so it should also be something to see what like whatever is whatever kind of random variable sums converges to must have exactly this property the gaussians have that like the sum of two copies of this random variable again looks like the same shape random variable good and we'll see that it's like intimately related to the fact and that's sort of therefore why it's gotta be gaussians okay but as I said to make sense of this statement we have to talk about gaussians that don't have mean 0 + or variance 1 so let's do that ok so let's let Z be a standard Gaussian random variable and let mu and Sigma be numbers any real constant numbers and then let Y be this random variable we get by the translation suggested by the letters names mu plus Sigma times e you know like 3 plus 2 times e or something ok so the mean of this a new random variable Y is what you yes because this has me in zero this is mu and the standard deviation is Sigma oops right cuz you multiplied by a scalar it goes up by the squalor of that scalar but then when you square root it goes back down yes oh yeah very good actually great that's a mistake thank you I want you to be real and I want Sigma to be a positive real Wow let's say I'm not negative real thank you great should fix that okay then we call why a non-standard gaussian non-standard gaussian random variables okay so we're meeting some new kinds of variables to the Gaussian Club and we write Y Sim n for normal Mu Sigma squared as I mentioned before M stands for normal this first parameter stands for the me and the second one stands for the variance okay so this is like a just a Gaussian random variable with the different mean and so like it's scaled horizontally so I'm there how as well okay and PDF of Y is oh darn I ran out of space well as intended because I do not want to write down the PDF of Y you can but don't bother I mean okay if you really really want to you can write it down it's not so bad but instead you should just remember this life pro tip if you ever encounter like a random variable that has mean and variance different from 0 and 1 just try to standardize it so you get back to the situation about having mean 0 and variance 1 you know I already managed you but like remember the PDF of standard Gaussian which you should remember you know like I memorize it like it's a poem but don't worry about the PDF of a you know general translated Gaussian it's just annoying [Applause] great so know I can properly uh prove corollary to and we gonna prove it using the fact that I use it the fact just the fact with D equals two so this picture so yeah let's remember this picture but first let me properly state corollary to let me state it over here okay so let X be a normal random variable with any old mean mu1 and any old variance Sigma 1 squared and Y also be any old Gaussian with mean mu2 and variance Sigma 2 squared and let them be independent and then let Z be I came to take a linear combination of them ax + B Y where a and B are real numbers then the claim is that Z is a Gaussian and we even know which Gaussian it should be because we can tell in advance what is the mean and variance of Z so the mean of Z is going to be mu 1 plus B mu 2 by linearity of expectation and the variance using the fact that they're independent is going to be a a squared Sigma 1 squared plus B squared Sigma 2 squared okay so the content of that theorem is that Z normally has this meaning variance but it's Gaussian with this mean and variance good so that make sense so let's prove that and I claim for the proof you know I wrote like six different scalars on the board and I'm gonna get rid of almost all of them because I'm gonna claim that without loss of generality we can assume but x and y are both standard gaussians here I'll leave you to think about why that is the case but basically you know if x and y are not standard gaussians and you know you can think of x is actually or you let's say y you can think of it as being in this form like mu 2 plus sigma 2 times some standard Gaussian Y prime and you can think of X as being like mu 1 plus Sigma 1 times a standard Gaussian X prime and then Z is like some constants plus some other constant times some other constant times X prime plus some other constant times y prime so if you take care of ranging all that you find that actually suffice is to prove this theorem in the case that x and y are standard that didn't follow that then whatever I'll just show you this case but it's sufficient so therefore what we want to show is that for x and y standard Gaussian it's independent that Z being ax plus B Y it's supposed to be Gaussian with mean ok these guys I mean 0 then Z should have mean 0 and if these guys have variance 1 then Z should have variance a squared plus B squared okay so this is what I'm going to show if you take two independent standard Gaussian sex and why you form ax plus B Y you're gonna mean 0 variance a squared plus B squared Gaussian and conveniently I can cut is this exact same picture and let me explain why so here's what I'm gonna do a and B are constants right so I'm going to get out good old r-squared okay and I'm gonna plot the vector a comma B perhaps it's this vector here okay and now having written that down I'm go to imagine drawing X&Y and imagine drawing x and y and then I'll plot the pair X comma Y that's the thing we did so maybe it's like I don't know this if this is X my draw of X and this is my job why is X comma Y okay and the main thing I know about this procedure fix it this vector a B is that what I'm going to draw x and y I'm equally likely to get any angle it's gonna be circularly symmetric okay so I know for sure this 2d vector is like circularly symmetric you say rotationally symmetric and uh I said this is slightly out of order but see I'm interested in Z which is ax plus B Y so what is ax plus B Y in terms of this picture yeah this or that what's your name th-thanks yeah it's the dot product between these two vectors this fixed vector a comma B and this random vector X Y okay that's the quantity we're interested in and the dot product has a geometric meaning right of two vectors in two dimensions it's the product of the lengths times the cosine of the angle and therefore the product is also circle rotationally symmetric if you take two vectors with certain dot products and you rotate the two vectors simultaneously they have the same dot product because rotation doesn't change their length and doesn't change their angle so like so is not product okay so now put yourself in the mindset where like you drew a comma be down on the page but you haven't drawn the random vector XY yet and you're like okay here's a B and I'm gonna draw X Y in a second and then I'm gonna take the dot product with this fixed vector but I know that's why I'm gonna draw is like rotationally symmetric so nothing would change if I also like pre rotated a be any amount I want right cuz it's like the same as saying like oh I'm going to draw the Gaussian and then rotate it by like this like if I rotate a be some angle theta and then I also draw X Y and rotate it theta nothing changes to the dot product cuz I've rotated by the same amount but also nothing really changes with the drawing of the Gaussian because if you're like I'm gonna draw a Gaussian and pair and then rotate it a fixed amount it's rotationally symmetric anyway so it's not really doing anything and therefore you may as well pre rotate like a comma B to the x axis so therefore without loss of generality B to v4 the distribution of the distribution of the number Z I can rotate a B to the x axis and what vector does it become when you rotate to the x axis somebody over here said it yeah right it becomes a root N squared plus B squared comma zero like because a little length of this thing is root I squared plus B squared there it is here but now if you do that then but now Z is distributed as well root a squared plus B squared comma zero dot product XY and presto this gets really dependence on Y you got root a squared plus B squared times X so the distribution of Z is the same as that of this scalar times X which you know by definition is in this form with mu being 0 and Sigma being root a squared plus B squared okay so therefore Z has the distribution it's mean zero because we haven't translated it and it's variance is the square of this [Applause] okay any question about this not implicitly explicitly I mean yeah this is only true if they are independent they definitely have to be independent the fact is not true if they're not independent one kind of counter example is mmm okay yeah this is not like you've like it's a 100 percent compelling example but a counter example but like yeah like maybe Y is negative of X this is also Gaussian it's not independent of X and then X plus y is constantly zero which actually arguably is Gaussian random variable depending on whether you admit whether you decide to allow Sigma to be zero or not but like in any case it's not a Gaussian random variable whose variance is the sum of the two variances and you can actually get an example where it's it's not even Gaussian at all it's not just that the mean and variance are wrong okay good so that's it about the glory of the gaussians for now and now we're gonna go back to the central limit theorem and I'll tell you the good version of it which has explicit error bounds\", metadata={'source': 'hRqhf1edVIo'}),\n",
       " Document(page_content=\"okay uh so I will just state this theorem which you can use in your life it's called the berry s Ann theorem proved separately by berry Sen back in like 40s and 50s berries American Sen was Swedish so it's like the world war two days communication wasn't so easy back then uh okay so here here it is ah let X 1 through X and the independent random variables okay it's already first of all unlike in the central limit theorem they don't have to be identically distributed also it's not like there are its own ending sequence of them you just have n random variables that's the common situation in theoretical computer science and let's assume that each of them has me and zero this is basically without loss of generality because you know just we're gonna be interested in the sum of these random variables so you can just translate them all Center them all Tov mean zero and that just adds a constant to your sum which is easy to handle and they don't even have to have the same variances so we're going to introduce a separate letter for each of their variances so write Sigma I squared for the variance of the I f1 which equals also the expected value of x I squared since we're already pre assumed that the mean is zero for each of them and we're going to assume that the sum of these variances is one okay and this is also without loss of generality basically because if it's not one if it's like six then just multiply all the random variables by 1 over square root 6 and you'll achieve this thing and you've basically not really changed your problem just multiplied it by one of our root six okay and now let's ask be the random variable which is there some which is the random variable we're interested in understanding and what we want to say is the conclusion should be that s is similar to a Gaussian distribution s's distribution is similar to a Gaussian distribution and that's true I'm going to write a statement to that effect that'll look kind of weird but then we'll talk about it so then for all real numbers you the probability that s is at most you it's basically equal to the probability that a Gaussian is at most you okay this is Gaussian okay up to a small error so in the central limit theorem we just wrote like plus little of one but now I'll tell you an explicit error rate and I guess the easy way to write it is instead do this so the difference between these two numbers is at most something that's hopefully small so small and what you should think of is by the way you should think of this it's like a known quantity for every number you you're like okay this is just the probability that a Gaussian is at most you it's some explicit number that I can ask my computer to figure out and then you can be like haha this quantity is basically the same as that quantity okay let me now say what the error expression is it's at most some constant times beta as I said this is looking a little bit funny but we'll talk about it where beta is gonna be an expression where you'll be like it's not supposed to be small I don't even know but we'll see it's usually small where beta is this thing some I goes from 1 to n of the expected value the absolute value of X I cubed okay we'll talk about it but let me just say that this will usually be small which will make you happy and this cost and it depends on who proved it Barry and Sen I don't had some constant there was like 7 or 10 or there's some error in their calculation which made it worse but the ultimate champion of this constant game is Arena shevtsova and she got it to be 0.5 6 0 0 in 2010 it's not important but I just say it for amusements sake ok so it's a small constant times this beta going with this beta you might even think like wait maybe it's big right like it's in the expectation of like and random variable cubed isn't that bigger than expectation squared remember like our expectation Squared's add up to one's the expectation of like the random variable squared it's probably well if the number that's much smaller than one so maybe we're in the regime where X cube is usually even smaller so let's just do an example where you'll see that this is often small so ok so let's say we want to use this theorem to study coin flips let's say to study coin flips fair coin flips as we did at the beginning so to set it up we would lat X I be just like we did at the very beginning we decided it was good to look at instead of sums of zero one random variables look at sums of plus or minus one random variables and also divide by root n so we're going to let X I be the random variable which is 1 over root n with probability 1/2 and minus 1 over root n with probability 1/2 and that's good because it's set up for this hypotheses so it's like we flip and coins and instead of adding up zeros and ones to calculate the number of heads we add up plus as the plus 1 over root ends and minus 1 over root ends and the mean of each of these is 0 and here we have that Sigma I squared which is the expected value of x I squared well X Y squared is always one of our n no matter what so it's 1 over N and therefore this normalization condition is right these normalization conditions are exactly set up like the the mean of s is 0 and this is exactly set up to make the variance of s equal to 1 so this is a mean 0 variance one random variable for sure this is a mean 0 variance one random variable for sure and the content is saying that actually this one is close to the Gaussian and the standard Gaussian okay and great so now what is this thing expected value of absolute value of x I cubes well absolute value of X I cubed is X like cubed absolute value is always 1 over N to the three-halves so therefore theta which is the sum of this over all n random variables is 1 over root n ok which is small as I promised okay and therefore in this particular situation this particular coin flipping situation we get you know probability that s let me call it SN again SN is it most you minus the probability that a Gaussian is that most you it's real small it's at most point five six over root n okay and that's like a that's a lesson like knowledge you can really use unlike the central limit theorem okay and also like nicely it does not depend on you actually there's a version of this where you can put something here that depends on you and it's actually even better like the bigger you is but we'll just stick with this version okay and let's actually just use this to look back at the more familiar random variable which counts the number of heads if you flip n fair coins as opposed to this random variable which adds up these plus or minus ones so this SN as we've written it it's really if you think of flipping heads and tails it's the number of heads minus the number of tails if you think of heads it's plus one and tails us minus one divided by root n alright because you also have this root n factor [Applause] okay and that's like heads - tails of n - heads over root n ok so it's - heads over root n minus root n okay therefore mmm s is less than or equal to you it's equivalent to heads being less than or equal to n over 2 plus u times square root n over 2 I just invert this relationship between s and H and notice this is like the mean number of heads and this is the standard deviation number of heads so it sort of all makes sense ok so if you flip n coins the probability of this is very close the probability that a gaussian is at most U which is some numerical thing it's the integral from minus infinity to u of this e to the minus u squared thing the PDF and you get these numbers using like your computer or calculator computer I guess yep no they don't even have to be identically distributed they were in the example but they don't in general in general like this beta will be small if two things happen first the random variable should not be like really freaky like do weird things like occasionally be enormous but like otherwise be small like you know this is like a nice random variable that doesn't go crazy and the other thing that makes beta small is none of this Sigma Eyes is too big if the Sigma eyes are all small then that's gonna make beta small it's like one of the Sigma I is is like 0.99 then the sum is not going to look Gaussian because the variance will be dominated by like that one eye and probably the random variable will just look like that one X I so let me close by talking about this quantity because if you know you if a specific number in mind like negative three then you can ask your computer to compute this and you know it's this is the gaussian PDF this is v u it's you know the area under the curve up to you know be like point zero one if u is negative three in general this quantity is called the CDF of a gaussian the cumulative distribution function of a Gaussian and it's denoted capital Phi of you and the last thing I want to tell you about is okay well if you is some number like -3 or 1.5 or whatever then mm capital Phi of you is just some other number which you get a computer to tell you but what if you want to know the asymptotics of it like when you is really small or really big actually because we're used to like asymptotics when the parameter is getting big it's good to also define this thing called the complementary cdf of a gaussian to noted Phi bar of you it's the integral from u to infinity of v u which is the probability that a Gaussian is bigger than you somehow this is like nicer to work with it's basically the same thing by symmetry by of U is the same as Phi Phi bar of you is the same as Phi of minus u probability that Z is bigger than you is the same is the probability that Z is less than minus u um so yeah like that's we're looking at this quantity out here this piece here if this is U then this is the area here is v bar of U and sometimes it's important to know the asymptotics of it and since I don't really have time I'll just tell it to you and you can look at the notes for a justification but it's very similar to the asymptotics of the PDF itself so it turns out that this thing is asymptotic to Phi u over u okay well remember this is like e to the minus u squared over to 1 over root 2 pi okay so the probability that a Gaussian is bigger than you essentially basically equal to the gaussian PDF at u divided by u I'll see u gets large so you mean may need that in life and in fact in general like this is literally a strict upper bound and there's like a one-line proof of that which element but you'll see it in the notes it's real easy and there's a two line proof of this lower bound that it's like at least 1 over u minus 1 over u cubed times 5 u this is all for you non-negative okay so this is really accurate at the probability of a Gaussian being bigger than you is really really close to 5 u over you this is the fact you might want some time ok that's it so next time we'll talk about Chernoff bounds and such like things\", metadata={'source': 'Ig5TuZauhW4'}),\n",
       " Document(page_content=\"recording in progress all right hey everybody uh nice to see you or have you here at least uh let's talk about cs theory um does anybody have any questions i had i had a question about um 2d 2d let's see what is 2d i this time i came prepared with the homework right here i can even draw on it oh yeah 2d check it out oh yeah this is the like trickiest possible question not a possible question i think it's maybe like a tricky question this is asking us to explain why the constant this magic constant in sterling's formula is square root 2 pi and like somehow the truth of the matter is it like comes out of the fact that you know um if you flip a bunch of coin flips it looks like a gaussian and gaussian has like some square root twos and square root pies in it um but nailing it down is not super easy and i suggested you do it by invoking this powerful theorem that gives you the central limit here with error bounds baria san theorem so um it's still tricky though so yeah um what what's what's your what's the question what's on your mind i guess i'm thinking about how to interpret the hint um so what our group has been like doing so far is like comparing the probability that you get um like at most uh or the probability that you get like at least n over two heads with the probability that you get like at least n over two plus k like we're like we assumed that k was like one standard deviation out um okay and we tried the computation okay and tried to see what would happen and we also i guess like something else is um we tried using like the central binomial coefficient um as like an approximation for this like it'll give you i guess if you like subtract these uh the central binomial coefficient can give you like a lower bound and maybe an upper bound um if you like do stuff okay i guess like another thing is like i guess we noticed that like if you plug in like the supposed form of like the uh of like n factorial in like the previous problem into like the central binomial coefficient uh it's like rather nice like some things cancel out uh okay yeah so i guess uh go ahead we've been trying to like connect the dots from there okay uh well let's set some things up uh i mean what do you want to get into first like the binomial coefficients and the factorials or like the probabilities of certain numbers of heads or or what i guess it's more of like the various inside like how do you approximate this we using that theorem uh okay let's have a random variable what should we make the random variable be i guess just like the sum of the coins okay like maybe normalized uh yeah so uh normalized or not normalized um i guess like we computed um something like this like in class okay [Music] i guess was the result i think the result was like if you like flip and coins then like the probability that like the number of heads is less than or equal to um n over two plus like some standard deviations like multiplied by square root n over two right plus let's say t standard deviations and the standard deviation for the number of heads is um root n over two yeah and that's like approximately like the standard normal uh cdf plus or minus like 0.56 over root n yeah let's see plus or minus some constant i'll just write it as c over root n this is how i would like to write it to i mean like theoretically you should say like blah minus blah an absolute value is at most c over root n but i just like this slightly wrong notation plus or minus c root n as well um yeah true okay good so um um yeah should we apply this somehow and let's actually just draw like a little picture too to remind ourselves like this is the gaussian pdf such a great bell curve uh it's like phi of t uh yeah and so if like i don't know t is something um if this for example is t then this blue stuff is phi of t okay so did you want to like apply this uh for some values of t or yeah so i guess like if we're subtracting the two probabilities okay oh you want to subtract this one and this one yeah okay so let's see so this first one well we put greater than or equal to here i don't know should we put that's fine uh okay so that is like yeah i should say by the way that like okay this is like this is like gaussian land and then like if you actually like you know flip the coins and like uh did like a little histogram of the probability of getting like each exact value let's assume that n is well i guess you would get like some situation here where like this is like n over two and like you get some height here this would somehow be proportional to like 1 over root n i guess and then you get this like kind of discrete thing that sort of looks like a bell curve as well like you start to like maybe 10 down to uh you know um looking close to zero or looking a lot smaller in the range of rudin but like somehow these pictures are supposed to be similar okay so let's uh see so like uh i'm just trying to like it's not easy for you to like write on my screen so i'll try to write like maybe what you might write let's call this one and we'll come back to two so you're saying like okay for one um this is kind of like taking okay in here let's call this one star this is kind of like taking t equals zero in star right well you know one of the hand we have graded on that let's put a lesson here just because we had less than in star uh okay so i guess what's that saying is like probability that like the number of heads is less than n over two i guess barry sand tells us that like this is approximately phi of zero plus or minus some constant over root n and i guess five zero we know right five zero is what's somebody don't want to say just a half yeah it's a half okay plus or minus c over root n i guess we didn't really need barry sand for this right because like the probability of the number of heads being less than equal to n over two okay actually maybe we kind of well okay if n were odd then it would be n over two would not be an integer and be right in the middle so it'd be literally exactly half the probability this would be literally exactly half if n is odd then it's even then i guess like it's like a little bit bigger than a half because like n over two is an integer and i wrote less than or equal to here and so uh i guess this also sort of tells you like that well the probability of it being exactly n over two is probably on order of constant over root n like if variant is true because um there is a n is going to tell you like it's a half plus or minus like c over root n and like this error can only be coming from the probability of exactly hitting n over two i guess we already knew that when we studied the central binomial coefficient uh okay um so okay so then you're saying like in two well let's like make this like do you have like some value in mind for like here uh we were just using like one standard deviation okay so i guess i could down over two okay so like root n over two okay so then on two it's like t equals one so you get that the probability that the number of heads is less than or equal to n over two plus root n over two is like phi of one plus or minus c over root n five one is like uh well it's a number right i don't know what it is it's uh it's the probability of all this stuff in here uh it's like 60 i don't know if it's like 67 or 73 or something i don't know if you happen to know let's just invent a number it's probably not right but like let's say it's 70.73 i just made that up plus or minus c over root n so we could subtract and then on the one hand we'd have the probability that like the number of heads is let's see the opposite of this condition is like that you're bigger than n over two number of heads and that's n over two plus root n over two and then if we subtract these things we get uh well this fictional number 0.23 and then we should still say like plus or minus like maybe like 2c over root n because we're not being very careful about what way the errors are going so maybe they could add up unfavorably but this probably won't really matter that much um okay so this is nice so this is like telling us that uh if you look at this like discreet picture over two and you like add up literally all these heights of these bars you add up literally all the heights of these bars you know these bars are supposed to be like the probability of getting each individual number in the actual coin flipping experiment and this adds up to basically point two three or whatever that specific number is you know plus or minus like one over root n um okay all good deductions and indeed this is how you typically use barry sand to do stuff but now okay some somehow sterling's formula has to get into the picture right that's what this problem's about yeah like i guess it's kind of annoying that when you subtract you get this like 0.23 thing instead of like something related to like root two pi um yeah so where's the root two pie gonna come in in this picture i guess i mean it's the it's like the top of the curve on the gaussian side right yeah that's right this thing this height right here is i don't know i shouldn't have done that in green let's try sparkle color this height right here okay sparkle color is no good this is uh one over root two pi okay so that's a good clue like that's that's coming in there so um okay good so how can this help us i guess one idea that may or may not work is like you could like upper bound the difference of like the cdfs by like taking the rectangle instead yeah that's true so yeah so actually what you what we did here we kind of like said like this okay so actually this amount here was like this you know fictional number 0.23 well whatever it really is this is zero this is one and you're right it does i mean you could say that like oh it's sort of sort of related to this rectangle has height 1 over root 2 pi and width 1. so we know that 0.23 is uh at most this um yeah i don't know if it is one over root two pi over square root six well anyway uh yeah but i guess like the point is that like okay this blue curvy trapezoid or whatever it kind of looks like the rectangle so it kind of looks like one over root two pi but like there's a pretty chunky error here so it's not exactly great any and by the way like anybody else who's on the call here can also chip in this doesn't have to be like a one-on-one thing um any thoughts about that no no thoughts i guess like another thing we would think about is like uh like the complement of the cdf so like the other the other tale um i think we showed in class that it's like asymptotically like proportional to like the pdf um which involves like the routine pi thing so like true i don't know maybe like set k to be a maybe set like t to be like a larger value so that you can like use that yeah that's a creative idea you're saying that like if t is like quite big you look at like this probability we show that this is like five bar t we showed that's like asymptotic to five t which we know is like this it's got a root two pi in it too this is like e to minus t squared over two possible this is a good idea [Music] i got the factorials involved somewhere too no okay let's hypothetically put a pin in this one we'll come back to it later in this uh later in this uh recitation but let's let's just pause for now and maybe inspiration won't strike us um we'll come back to it for a moment yeah uh okay given that we've paused is there another question oh we got all these like friendly faces here maybe i should just call on somebody and say how's the homework going for you do you all like solve everything um all right i'll ask somebody i'll pick a random here we got seven people i'm picking a number between one and seven my head okay oh it came up to sg oh you were asking some questions on piazza in fact i think how's the homework going for you it's funny i think someone just left right before he randomly called um oh uh it's going all right i was also working on two um okay uh c and d uh so that was helpful i guess we i haven't spent that much time on 2.3 but we could look at 2.3 as well um um the integral is pretty confusing uh for me okay yeah let's take a look at problem 2.3 for a moment and then we'll return to 2.2 d um okay one thing is like okay so this is about some formula for expectations of things let's focus on like uh the k equals one as it said here so okay so this is like it's like a pretty relatively well used formula in in life so let's plug in k to be one so we're interested in this expectation of x is okay k is one so it's integral from zero to infinity looks mildly intimidating there and then t to the k minus one that's just one uh so we can ignore it then probability that x is greater than or equal to t dt okay so this is what we're supposed to prove uh let's just do an example that way we'll maybe be less um i don't know it looks intimidating so less intimidating let's have a random variable uh let's pick out let's pick our favorite well let's pick let's pick a random variable that's like not too complicated but like not too simple either so like say x is um um oh it's supposed to be non-negative two so okay so only give me some non-negative values so like let's say some value with probability something some value with probability something okay somebody give me a value for this you're on the call is one and zero too simple oh pretty simple well let's go for one though that's good okay so one with probability what should we give it two-thirds two-thirds okay well let's have three values okay so let's get let's have another value here um five thank you five great five color coded okay and what probability should five have we have one third left but don't make it one-third come on we just need to pick a number uh one sixth one sixth okay oh no one six left very symmetric let's round this down to point let's make this point seven we'll round this up to point two so that leaves us with point one okay we need one more value here can i say something non-integer yeah yeah please but don't make it like soup well go ahead uh 8.2 8.2 i love it okay great what a wonder wonderful random variable i'm loving it this is great okay 8.2 so first up let's all i can do this now here we go i'm going to calculate the expectation expectation of x i'm going to calculate the left side and the right side and like hopefully they'll be the same expectation of x i can do this it's 0.7 times 1 plus 0.2 okay we're scratching these up times 5. oh that's going to be nice plus eight point well point one times eight point two oh i can do this this is point seven this is literally one and this is a 0.82 so we get 2.52 oh great i can't that turned out great i'm loving this question so far okay so now we got to do this thing okay so you see this is um weirdly okay we're integrating we have something that looks like this right integral of blah dt so hopefully this should be like a function of t so it like matches up with our expectations for how functions work um and i guess it is right like for every value of t this is some number so like okay i'll ask you a question now if this is our x let's pick a nice value of t like um four so what is probability that x is greater than or equal to in fact let's call this f of t so let me ask you now what is f of four point three point three yep because uh these are the two cases where you're bigger than four so it's like point three and let's do some more like f of zero just shout it out one one f of a half one still one f of okay it's a bit annoying so let me say like f of 1.1 0.3 yeah f of okay i'll do a couple more three point three yeah f of six point one uh f of ten two okay great so we got a lot of things i hopefully get the gist of it so now let's plot f here's t here's the plot of f okay so for it starts out at zero it's one and then i guess it's like one for a while this is one [Music] and it goes one okay all this is like the cutoff points then it jumps down somewhere in the neighborhood of one it jumps down to like point three this is like point three [Music] and then it goes .3 for a while and then somewhere in the neighborhood of i guess 5 which is way out here jumps down to 0.1 0.1 and then somewhere neighborhood of 8.2 okay it jumps down to zero so let's finish this picture there's 8.2 as it jumps down to zero okay so this is like you know not like the most common function you see in calculus because it's like discontinuous but that's fine uh and then okay so that's f of t and then finally we're doing integral zero to infinity of f of t dt okay that's the area under this curve so that we can do i'll take over this part it's like the volume or sorry the area of all these rectangles well there's nothing under the curve there okay so this was a rectangle with width 1 and height 1 and this one is a rectangle with width 4 and height 0.3 and this one is a rectangle with width 3.2 and height 0.1 okay so now we have to add this up i can do that 1 plus 1.2 plus 0.32 which adds up to 2.52 oh thank god all right it's a miracle these two things are equal okay so it's funny though because like you know we added up three like quite different looking numbers here and three quite different looking numbers here and we got the same result like both times so that's neat uh yeah any thoughts on any comments on that or thoughts how did it happen i guess you can cut it like horizontally and vertically and that's like how you get the two different kinds of sums yeah that's true that's right this is uh this is a classic way to prove this it's exactly right you can be all sneaky and be like oh see this area under the curve that we computed i could do the rectangles this way it's like this rectangle mr yellow plus this like blue there's blue over here like red over here yeah and if you compute it this way see this one the height is 0.7 and the well wait a minute yeah the height is 0.7 and the width is 1. and then the blue the height is point three minus point one which is point two times uh the width is five and then the yellow guy is the height is 0.1 and the width is 8.2 aha right you see in this calculation is the same as this okay very good so in light of that you might say like ah okay that okay this is this one's not the coincidence so um probably you could take that and run with it and and and prove the result and the k equals one case actually as i said this k equals one case is the case that's by far most commonly used actually why is that like what's the whole point of this stupid formula um the whole point of this stupid formula is like sometimes in life okay usually if you have a random variable in life usually figuring out like expectation of x is not so bad and like figuring out like this is you know pretty good and figuring out the probability that x is bigger than t this is like pretty not so great this is what we've been talking about for a while like when we saw like turn off bounds we were like oh you know x is like a sum of a bunch of coin flips understanding if it's expectation piece of cake no problem but understanding like the probability it's like bigger than t this was hard we need to turn off bounds we did approximations and that's that's the general way of the world but sometimes it can happen that the reverse is true like not always but um sometimes or well maybe i'll try to give an example but uh um sometimes in life like weirdly like you kind of know some stuff about this one and you for whatever reason don't know about this one and you're like oh weirdly i have this information i wish i had this information and you can get it by this formula like like if you knew this like perfectly for every value of t then you'd be like oh i could just integrate it and i'll learn the expectation and like occasionally that like actually happens i can even give it like an example which may be relevant because maybe there'll be another homework problem in which you'll be asked to use this homework problem later so if you're watching this you may have a look at leg up potentially um what if i like asked you to um compute the expected number of heads in n coin flips but like times [Music] the uh indicator that the number of heads is at least n over two plus i don't know root n or something so in other words like you know i want you to calculate the you know flip a coin like n times but instead of like taking that like only take the number of heads if it's at least n over two plus root n and otherwise take zero so um that's like a slightly odd thing to do but like it comes up like uh maybe there's something now interesting like what's the expected number of heads like given that the number of heads was like at least one well two standard deviations above its mean we call this random variable x but it's not so easy to figure out like what is that expectation of x it's not super clear um but probability that like x is greater than or equal to t you have some like decent information about because um [Music] you know uh if t is you know less than n over two plus root n then this probability is uh zero well maybe you should put like greater than here um because you know x is zero if if um if the number of heads was less than n over two plus root n um and otherwise if t is bigger than this amount then we have like the probability that the number of coin flips is bigger than some number and we could use like berry sand for that so it's like a situation where like you might actually be able to reasonably estimate this and like this is the hard hard thing to figure out okay so that was like some weird digression about uh why you might be interested in this formula also if we return to this formula like it sometimes i've seen it called the layer cake formula so if you ever see this like in life layer cake formula it's referring to this thing and i guess it's somehow like this picture kind of looks like a layer cake maybe like this this is supposed to be like you know you're seeing like a cake and profile and like yeah like maybe you made the top layer with some delicious like red velvet cake and then the second bar was like some blueberry and it's like a lemon cake on the bottom and yeah you can compute the amount of delicious cake you have in two different ways um yeah but this uh okay so this was all about the case of k equals one q equals two is a little bit more interesting um but probably if you just like literally repeat this exercise exactly with k equals two and you're like okay now we're computing expected value of x squared so you're gonna be like all right i'm gonna have to like put a square here and a square here and a square here and uh i'm gonna do this calculation and like this integral you know apparently we've got to like stick a 2 out here and we also have to stick like a times t in here it's going to be kind of funkier but perhaps if we do that calculation we will become enlightened um neither of these things is actually like looking at the there's sort of like a hint but uh well anyway uh this would be like i mean especially if i didn't have a hand like this would be my natural inclination i'd be like all right well let's just try it out with k equals two and see why it could be true but uh well let's put a pin in that one too like maybe it's time to we can come back to that one as well one one quick question about the just like yeah so it's saying to interchange the sum uh well i think it's like an integral is like really the same thing as the sum i was just wondering like uh if we know we are allowed to interchange the integral in the sum i actually didn't know whether good question okay yeah that's a little bit okay good point so all right in 95 of the time in cs theory like you'll really have like two sums and if they're finite sums like you know it's like some i and some finite set s and some over j and some finite set t of love i and j then you can always interchange them um and it's like just like this weird fact of like cosmic fact of reality that if you're like doing math and like somehow you encounter like a double sum just immediately reverse it interchange it and like it'll make things better like i don't even know why it's kind of a miracle actually in some crazy way that's exactly what happened here right uh if i may go back to this picture it's gonna be hard because to draw on this picture but like you know the the integral the area under the curve is like you know the normal way you do it is like this way and you're like summing up all the heights of these like green vertical lines i'm drawing and sort of the way we made progress was by like uh interchanging it and like summing it up like horizontally but um okay that's never mind uh yeah in general okay we need some rules it's like it's either an infinite sum or an integral about like when you can interchange it and um like in calculus there's this like question of like again if you have like a double integral like when can you interchange it and there's this like theorem called like fubini's theorem and like you have to like sweat like some painful way to like check all its hypotheses and stuff um let me say like for example my homework like i'm not i'm not gonna like stress you about this i mean um it's fine generally um if all of the sums involved are finite even though they're infinite sums like the typical case when everything is fine is when everything is non-negative and the sums are finite despite being like the actual value of the sum is finite despite being um an infinite potential sum uh this is when it's okay i mean if we were in this picture for example like we might actually have a integral and a sum going on like somehow like maybe like uh let me try to draw with big we might try to draw with uh sorry you know here we have kind of like a summon an integral like there's like this piece plus this piece plus this piece so that's sort of like a sum of integrals right if you forget that this is like a flat thing you didn't know and you just thought like oh i have to use calculus to compute the area under this rectangle but now we have like a different piece and a different piece it's like a sum of three integrals and um yeah here uh i guess what i'm saying is like in this case i think you'll find that like everything is like very finite anyway like the things when you interchange you'll have like an integral because i let you assume that x is only takes on finitely many values like basically you're gonna have this feature that like things eventually stop like um you know you look at the maximum value of x and like nothing is going on beyond there so you can forget about it so like your integrals are like finite and like your sums only have finally many pieces so like this is definitely the k and everything is not negative so like this is definitely the case where it's going to be okay yeah that makes sense cool yeah that's a good question though because like you know i'm making you like sweat like some math things like actively making you sweat some math things and then maybe it's a little weird when i'm like well don't worry about this one but uh yeah this is what case it is okay and also i don't know i don't want to get like you know weird like measure theory or super advanced calculus in this in this course um okay uh any other like question i think it like going back to two point way problem 2d but uh if there's like another other question like [Music] oh this one here sorry yeah this one 2.3 oh yeah that's a good question um yeah okay so this one was about we were doing uh sterling's formula and um just to remind you somehow you end up looking at like the logarithm function it's like a terrible drawing uh okay this is lawn x and like some of the sterling's formula all boiled down to getting like an extremely like uh accurate um okay well actually you have to get this extremely accurate like sum of like these like rectangles right i was like literally sterling formula it's like you exponentiate but it's like basically about getting the exact area of these rectangles and we're like okay like first pass uh you could say it's like roughly the area under the curve up until this point and then we're like oh but like let's be more refined it's like the area under the curve plus like the volume of these like curvy triangles and so we had to like add up all these curvy triangles and then we were like oh this is like approximately the area of like the flat triangle which we could exactly add up and so like the only remaining uh inaccuracy in our estimation was like the area of these like slivers that you get by like superimposing the straight and flat ones so like for every if we like super zoom in here like for every one of these rectangles we get this like sliver here and like if we could figure out the area of all these slivers because i'm going to figure out the area of all these slivers exactly it's not a very informative drawing uh if we could add up all these like sliver areas all the way up to the last one just like this liver plus this liver plus this liver etc a lot of sliver then we would like exactly know n factorial like with no problem so we have some difficulty maybe we don't know the exact sum of the slivers from here to here or maybe we do do we from two point b again is smaller than one over eight oh yes that's right so the question has you show that uh wait what does the question have you show so we have one over eight [Music] oh yeah oh so wait uh yeah the question has you show something about like one over eight but did you like draw a conclusion for that about the number one over eight so what is your questions okay so yeah so i guess the the the question how do you show that like okay it shows you showed that like this first sliver has like area at most 1 8 seems like 1 minus a half or something right this one is like at most 1 8 a half minus a third and then this one is like at most one eighth third minus a quarter et cetera so did you draw like a conclusion from this yes right what is the conclusion so we know the sum of all silver is similar in one way oh yeah now when you say all the slivers which slivers do you mean formula first slowest to the ends slowest okay yeah so first sliver to the nth sliver adds to at most 1 8 times i guess like 1 minus 1 over n or maybe there could be a plus 1 here i'm not sure but something like this okay um [Music] neat so do you know the exact sum of the area of the slivers no okay do you need to know the exact number of the area of the slivers to solve the problem um i think no but the the control not enough i think sorry i think the [Music] cons uh a constant here is not enough for we get uh receives the same for both the upper bond and the lower bond right you so you're saying like okay we have okay yeah we have an upper bound on the okay so the area of the okay we have the upper bound on the area of slivers is 1 8 i guess and the lower bound is zero i mean uh you could maybe work even harder to get like some lower bound but um yeah so and you're like sad that these are not equal huh yes yes what if i told you well what if i hmm what if i told you is exactly uh 0.07359 then could you solve the problem um [Music] yes okay what if i told you it was exactly 0.09252 and could you solve the problem uh what is what does this number mean uh it's it's a number i just made off of the top of my head but like if i told you that was the exact sum of the area this delivers could you complete the problem yes okay uh well i think if we can get some low all the time it's okay it's also enough i think yeah actually uh it's a good question suppose like i i well yeah this is a bit funny suppose i suppose like some extremely like brilliant all-powerful person came down to you and said like you know i promise you this uh some of the slivers is like 0.01 exactly would you have a question for that person but you now you can like try to solve the problem but you might have can you solve the problem [Music] okay let's see uh okay what is the problem actually remind me of the problem [Music] oh i just mean i do not get your last sentence can you repeat oh well my last question sentence was like suppose somebody uh told you everything there is to know about the slivers okay could you solve the problem uh i think we need uh not an exact estimation for the servers but we need some some low bounce in the upper box true yeah i'm trying i'm being like a little bit annoying here because like i don't want to just sort of exactly tell you but like let's try it this way uh i will answer any question if you want about the slivers ask me anything about the slivers i'll just tell you the true answer but like you now have to use this ability which i'm granting you for the next few minutes to like solve the problem so uh ask me anything anybody else on the in the call to like ask me anything about the slivers i'll give you some answers if you know the exactly what they are then you know exactly what lawn event factorial is right uh is that your question i guess you know that's what i'm saying is the case right so uh what you could do can you repeat that so the uh lawn of n factorial is like the area of the rectangles plus the straight triangles minus the slivers and uh when you say the slivers let me dive into that for a second which slivers the sum of all of them when you say all what do you mean um like the one to n minus one where yeah okay yeah well i'm just saying if you know exactly what the slivers are then you know exactly what long of n factorial is yes good but okay so in the problem you don't have to exactly know you're not actually even asked to like exactly get like a formula for n factorial you're allowed some kind of error right yeah so it's 1 over n so that's why it was kind of confusing to me that we were ditching the the 1 8 times 1 minus 1 over n um it seems like we should keep that around maybe well okay so yeah so actually how accurately do you need to know the area of the slivers in order to solve the problem i'm not sure i guess just up to this order one over n turn yeah i think so i mean there's some like log in there you have to like take some care but like i think it does turn out to be like if you crunch the numbers it's like you need to figure out the some of these slivers the the blue light blue slivers inside this thing i'm outlining here up to like plus or minus order one over n i think that might not be right but yeah yeah i think it is right but yeah i agree that like we have to like try to like look into it for a few more seconds to nail it down but i think that is right so let's just hypothesize that that's right for now um so the question is but the funny thing is like okay you don't actually you yourself don't actually have to name the number which is such that okay so we want like area of slivers uh up you know between one and n it's like a funny question we like we need to like okay ideally we want to like be able to say like equals like some specific number which i'll just call alpha for the lack of a better level letter plus or minus order one over n the weird thing is we don't have to like literally explicitly name alpha like we have to be like it's 0.06 like we just have to show that there exists an alpha so like one could ask how could that be wrong could there could there not be such an alpha like for example imagine i ask you to literally prove the opposite of the question prove that there does not exist a constant c such that n factorial blah blah equals c times whatever plus or minus order one over n well i guess the only way it could not be true is if it was as if the area of the slivers is like huge uh otherwise you can express it as as alpha times this order one over n term right no matter what well yeah the thing we kind of know is when we have this area of the slivers we know it's between 0 and 1 8 1 minus 1 over n or something yeah so are you i guess you're just saying we need to be like a little more clever with the zero side uh no not really like what i'm saying is like here's okay yeah here's what could be the case perhaps perhaps the area of slivers up to um n equals a hundred is like .04 ish but the area up to n equals 200 is like point one two ish and like the area up to like n equals three hundred is point o ish and then in this case you wouldn't be able to say like oh yeah like there's one number alpha such that like this amount is like alpha plus or minus one over n like this information is all consistent with this because all these numbers are between 0.04 point 12 0.02 they're all between 0 and 8. but like is it really like this in fact there's like a clear reason why this is definitely like this this weird example i've just written here in the arrow is like obviously can't be what's going on well yeah i mean we have the if n is 300 then we already have the 200 slivers yeah so for example this could not be this could not be 0.02 it's obviously got to be more than um 0.12 so maybe i'm like okay the area of the first and slivers is point 13. oh well that's not less than point eight one eighth so let me say point one two one and then you might ask me well what's the area of the first 400 slivers and i'd be like uh it's 0.122 and then you might ask me what's the area of the first 500 slivers and i might be like 0.1223 like what question will you ask me next that's why i want to get in this like dialogue where like i'll answer any question you want about like the slivers area so like ask me another one i've just given you like five pieces of information oh i guess we could ask if it ever exceeds some value okay go ahead and ask i don't know okay one sec okay i only said that because the way you were suggesting the terms uh suggested it but uh i encourage you to literally ask me a literal question now to like follow up on this uh these answers these questions and answers try to pin me down okay so is it ever bigger than like 0.13 is it ever bigger than 1.13 ever bigger than 0.13 no next question okay well i mean uh doesn't that just give us an immediate error bound since the only error was um like with the zero we're not counting the silvers at all and with the other one we are um yeah and so if we know that that error is at most 0.13 then i guess we're in good shape no you have to get that you have to like get the area of the slivers for a given n up to like plus or minus one over order one over n oh yeah yeah so we're still not one thing to notice here is like i definitely had to say no i had to say no because you asked about point one three and point one three is bigger than one eighth oh oh you already told me that you already showed that like basically for every n it's at most one eighth so like i definitely had to say point i definitely had to say no there but ask me another one ask me a trickier one how about somebody else who's on the call how about let's see we got six people five people in here i'll pick one of you all at random around number between one and five all right ry also good those are some good initials do you have a do you have a question about the slivers [Music] i think i am kind of confused but um all right i guess based off of this uh get off about 600 okay what is what would it be for like any 600 for n equals 600 i'm telling you the area of the slivers is 0.12234 you're like paying me down a little bit here ask me another one what is n equals five step at seven hundred all right so 100 it's uh point one two two three four nine it was like we got up to like this sliver the sliver the sliver the sliver the sliver and like we've added it up from like zero all the way to like 700. okay so is this airbound something like them maximum minus something related to and i guess you say maximum oh by the max the upper bound that we found that was 1 8 times 1 minus 1 over n yeah you know that basically whatever you ask me i'm going to say a number that's one is ask me a really big one n equals a thousand oh you can do bigger than that n equals ten thousand uh sure n equals ten thousand it's uh it's uh it's those add up wow ten thousands way out here um let's add up two point one two two three four nine 005 six two now ask me like the biggest one you can imagine infinity yeah okay now now you're cooking what is it for n equals infinity first of all does that question make sense it does right okay i mean there's like infinitely many slivers one for each natural number and we do know that like if you add them all up it's still less than 1 8. right it cannot be bigger than 1a if somehow like at some point like at 1 billion 640 000 it got bigger than 1 8 that would be impossible right because we know even at like 1 billion blah blah blah it's still a mouth one eighth minus one over a billion so indeed like it's possible like you add up all these literally all the slivers and now i have to like now i'm really i have to tell you the kind of the answer right like i might say like oh it's like i don't know it's like some number called you know it's obviously gonna be like point one two two four nine oh oh five six two blah blah blah i don't know maybe it's finite rational maybe it's irrational i don't know it's some some number maybe some alpha so that's that's a very that's some good information you got out of me there so now you know if you add up all the slivers it adds up to this specific number now you might dial it back and be like oh what about the first trillion slivers that's a lot of slivers uh probably adds up to pretty close to this right in fact if you add up the first trillion slivers this is one trillion here it's pretty close to all the slivers i mean well in some sense like all the slivers is this like the only stuff you failed to add it up is like this but like this is very small this is what you or hope like you hope this is like super small in fact how small do you hope this is order one over n yeah very good exactly order one over n so if this was over obviously when n equals a trillion yeah then if you could show that life would be so great so okay let's leave it there i feel a little bad because we were going to come back to uh i mean patiently waiting so long for this like uh this one about sterling's formula and now it's like out of time so uh for those patiently waiting um let me let me give you some like direct hints because like yeah i don't want to like use up all your time here um so went all the way back to this problem uh first of all your your thoughts about using the like the central binomial coefficients is like very good so like focusing in on like n which is n over two like from sterling you know this is like exactly well it's very close to something i think it's like root two over pi times one over root n or maybe there's like some maybe the it's something involving root twos and pies with sterling you can work that out and that's exactly like this area it's literally if you i don't know if you just saw what i did there i like zoomed into the like the so-called discrete picture and like that's literally the height of whoops this bar here is like literal well it's extremely close to whatever this is like one over root n times this i don't know root two over pi or something and like these other bars are also probably pretty close to that height well the one's really close to n over and we want to compare that with like here we have this yeah problem that like you know fairy sand helped us figure out the volume of the area of this like curvy trapezoid and it was sort of related to the 1 over root two pi but that was like this thing so i'm actually trying to give you a hint before when i was like zooming in on it like zoom zoom zoom like maybe what we should do is like instead of going up to like one standard deviation maybe we should go out to like 0.000 oh it's a very large marker we should go to like 0.0001 standard deviations for a very small value of 0.001 and that's going to make some progress because see here like the triangle you'll get if you study that instead of one standard deviation or the rectangle you'll get this is like much more similar to uh the you know area under the curve from here to here like the area of the curve okay it's like a it's a trapezoid it's like a slightly curvy side but this is going to be like much much closer to 1 over root two pi times point what did i say 0.001 that's gonna be much much closer so on one hand okay barry sam will tell you that like this area well it'll tell you that like maybe if you go to like 0.01 standard deviations oh one standard deviations here it's very close to the area under the green curve just like here but like since 0.01 is so small like that's really close to the area of this like yellow rectangle which we know is exactly 1 over root 2 pi times 0.001 so maybe if we pursue that line of inquiry we might need to make 0.001 even smaller uh we can like you know try to get like a link between like the height of this bar which we kind of exactly know due to sterling and you know the area of the screen almost rectangle and therefore the area of this yellow rectangle which also has like a root two pi in it so thanks for waiting that's my my bunch of hints for uh for that problem um okay these are all great questions any other like quick questions even though it's a little bit over time okay well uh thanks for all for coming i'll see you tomorrow in class thank you\", metadata={'source': 'C3N3AHeoBsM'}),\n",
       " Document(page_content=\"welcome back I was in the elevator today and I saw professor guy block and he was like what do you teach in the semester and I said this cs3 - ok class we start making some jokes about like I should bring some power tools to class and so forth which is fine he's like yeah for example you know let's say you're in there teaching about like Chernoff bounds and it's like oh I gotta stop you right there cuz I'm literally teaching about you're not bounds today always just to say that like I guess you know feels like the most canonical example of like a CS theory toolkit topic so I'm glad we're doing it here today so topic with a lot of naming confusion and so maybe it's better to call these things like large deviation balance or tail bounds it's about trying to get a good balance on the probabilities that random variables are really large I think some theoretical computer scientists back in the early days like the 80s and whatever found some paper by turning off from the 50s and then I go that's exactly what I needed and this topic became known at Chernoff bounds but actually it's just some follow up you know that Chernov did to like some known research in the area example huffing prove you know more general bounds and turn off kind of in the 40s and Bernstein proved more general bounds than either of them in the 20s so it's not who clear who to name them after although in you know CS theory you'll just hear them call all called Chernoff bounds so that's what I've titled the lecture ok so let me give you an example of the sort of problem that compounds are good for and it's one we talked about last time so let's lat H be the random variable which is the number of heads in and fair coin flips okay look it's a Bernoulli or sorry binomial n comma 1/2 random variable and one thing that we saw last time is you could analyze you know the probability that this random variable is larger or smaller than something using this various an theorem which is a sort of version of the central limit theorem that actually gives you error bounds which is good so for example it would imply that a probability that this number of heads is at least n over 2 that's the mean plus square root Anna over to subscriber 10 over 2 times T this is the standard deviation remember so this is like saying the probability that the number of heads exceeds its mean by at least two standard deviations is approximately this complementary CDF of a Gaussian at four ammeter T remember the picture here this is the gaussian PDF and the probability mass out here from t to infinity is the probability that a gaussian exceeds t is written like this Phi bar of T and I mentioned last time that the asymptotics of this are that it's some product to the PDF of a Gaussian at T divided by T which let's just ignore the T because this is exponentially small in T so this divided by T is not really saving it that much this is at most 5 T assuming T is greater than or equal to 1 and will generally be considering quite large T and you know ignoring the 1 over root 2 pi this isn't most e to the minus T squared over 2 good so this looks pretty good and let me go over to this small board to continue the example let's say we were like great oh let's use this and try to get a really good bound and the probability that you get like way more heads than you expect so you might say okay let's say we take a pathetically 2e to be 10 roots lon and okay that's not too large as a function of n few multiples of root login okay if you do that then of course e to the minus T squared over 2 is 1 over and to the 50 which is oh great this is some tiny problem polynomial lower bound so you might say okay good so therefore we now know that the probability that the number of heads exceeds its mean and over 2 plus now let's put it in there so 5 root and learn is a mouse 1 over N to the 50 right well actually there's a problem with this deduction can everybody say what it is haven't written anything that's wrong but yes so I like and over to - oh that's true this is only asking about sort of the probability of getting way more heads than you expect it is symmetric yeah so we could get either way but that's I'm not that's true that we haven't looked at it but it's not wrong or anything I never anything wrong but I did something right something vague remember this symbol to me has no real meaning and um right actually the very essence theorem just says that this probability is really close to the analogous probability for a Gaussian which is this but there's an error I mean it's the error in the central limit theorem approximation and we looked at this error last time and okay the error what's up this is true up to plus or minus order 1 over root n which I wanted is a very good small error a lot of the time but actually this error one of our root n is much larger than this quantity we got here so this is like an invalid deduction at best you could deduce is that this is at most one over order 1 over root n but it's kind of disappointing because like our error sort of swamped but we thought the main term would be so that's bad and in general this very a SAN theorem is only going to give you like a really you know what you want when T is like kind of medium or small such that the actual value of this is you know larger than your error so this is great when T is like a number like 3 or 6 and it's even fine when T is well you know like a small multiple of root lawn and I guess but a big level 12 root lonna and you're getting to the situation where like the the main term gets swamped by the error term so this is like a small deviations found and we're gonna be talking today about like large deviation bound when T is like really large now it turns out actually this is true but we just haven't proved it so that will be what we'll eventually do in this lecture okay but before we do that we're going to spend some time proving simpler things because turn off bounds and there are related inequalities generally apply to the situation when you have a random variable which is the sum of many independent random variables which is the case in this coin flipping example and that's a very special kind of random variable a sum of many independent random variables and you use that specialness to get like excellent upper bounds and the probability that the random variable deviates from its mean but sometimes you just have like some random variable that is not as nice we have less information about it and you still want to bound the probability that it's far away from its mean so I will start this by going over or like you know techniques you can use to try to prove such things for general random variables and the general philosophy of this little introduction to you know bounding random variables is that like the more you know about the random variable the better bounds you can prove which is natural bounds you can prove so let's start with round 1 of this idea where you know almost nothing about the random variable you're trying to analyze you're trying to prove that it's rarely far from its mean when you literally only know it's mean okay and that's sometimes all you can get if you've like a complicated random variable and for this example or this technique well assume we have a random variable X which is always non-negative okay and you know something about its mean and maybe for some technical reason I'll assume that X is not the random variable which is always 0 it's a ridiculous random variable anyway so does somebody know the name of a tool or an inequality you can use in this situation where you have a non-negative random variable and you know it's mean yes right thank you what's your name Matthew thanks Matthew uh yeah for this you this situation is the perfect situation to use Markov inequality and I'm hoping you may have seen it before and what is Michaels inequality say it says the the probability but this non-negative random variable X its way bigger than its expectation is small ok well let's make that a little bit more precise it says for any parameter T think of T is large the probability that X is bigger than T times its expectation is that most 1 over T ok I guess this this bound is only interesting if T is at least it was bigger than 1 but I guess it's still true otherwise okay so if you just know the mean you could say well at least the probability it's like 10 times its mean or bigger is that most 10% then this can often be great let me give you two proofs of this simple inequality so proof one will be a words based proof actually before I get into the proof let me say that as we go to prove Markov inequality we can assume without loss of generality that this expectation of X is 1 why can we do that oh yeah yeah what's your name thanks Brian yeah otherwise I mean just take ax and divide by its mean that will make its mean have one and then this thing is like if you divide both sides if you divide X by a constant C its expectation goes down by a factor C so then you can cancel it on the same side and you're looking at the same event okay I guess it's expectation is not zero because of this assuming it's discrete okay so if we make this assumption that the expectation is one okay so we have a mean one non-negative random variable and what we want to show is that now that the probability that it's at least T it's at most one over T okay so now I'm ready to give the proof one which is in words and true to the name words I will not write anything I'll just say it to you okay we want to show this assume for the sake of contradiction that X is at least T with probability bigger than one over T so it's quite often bigger than T now think about like the expected value of x you know we already know that it's probably bigger than one over T it's at least T so that all right contributes more than one to the computation of its expectation and it's not like there's nothing negative to worry about because X is always non-negative so like you've already from the contradiction of this or the opposite of this deduce that its expectation is more than one a contradiction okay so hopefully you caught that if not then perhaps you'll prefer proof to which is pictures and let me get following this pictures proof even though it's kind of simple because we're gonna use this picture technique a few times in this lecture so here's the the picture is proof so when these pictures prove I'm always going to draw a plot of like a couple functions that's function of X and one function is going to be this function here which is a step function which I'll call f of X which steps up from zero to one at point t and why do I write that why do I introduce this step function well it's because I think we care about the probability that X is at least T it's literally the expected value of F applied to X okay this is 1 when X is at least T and otherwise it's zero we're using non negativity here yeah we're trying to shoot for an upper bound so let's put another function above this and we'll choose a very simple function a linear function call that G of X and what is this linear function which is above this step function I guess it's G of X should be x over T right because that'll be 0 at 0 and it'll be 1 at T and that's good ok so then we know for every outcome of X f of X is less than or equal to G of X so we can conclude that this isn't most expected value of G of X but now it's easy to compute expected value of G of X this is expected value of x over T which is expectation of x over T which wire assumption is 1 over T so in fact actually we didn't even need for this proof to assume without loss of generality that X's mean was 1 because we got to the right conclusion anyway but it's fine yep [Music] are we allowed to use the discrete function instead of well it's a good question regardless okay here's actually a secret most the time when I'm like doing a proof here I'll be giving a proof which is like valid for discrete random variables for sure once that only take on constantly many different values 99% of the time the proof I write down will also be perfectly fine for continuous random variables or even general random variables as long as like everything I write is well-defined I'm not gonna worry about some details of that too much because I don't you know want to turn this into like a measure Theory class where I like carefully state every assumption so yeah you should just watch out for that like one percent of the time maybe some things I say I'd be slightly wrong but it's still up to you to when you write your paper to get that one percent right so ask your measure theory taking friends just to check you I think this is all fine I mean so in general what the principle I'm using here is basically this principle that for any random variable as long as these expectations are well-defined which they I'm assuming they are you know that like if F is less than or equal to G point wise then this this this random variable is always less than or equal to this random variable point wise and therefore this expectation is at most this expectation I supposed like to make this proper right like I think also I should maybe make sure F is included here and not included here to be completely honest like sometimes I like forget the details about should this be greater than or equal to or strictly greater or less than or equal to I think what I'm writing is correct but you should always every check double check yourself on this when you're actually using it to get the right inequality but yeah is the I mean have I written anything definitely wrong here okay good fine watch me yeah if you worried like ask me let me know okay just an example the following thing is non Markov inequality but it's sort of like a similar fact and I'll just sort of put it up here as like a little bit of an exercise maybe an example of the idea behind Markov inequality so sometimes you'll see in a paper like oh they'll make a deduction they'll say this is by a Markov like inequality or it is by an averaging inequality or an averaging argument so here's a here's a fact let's say X is a random variable which is known to always be between 0 & 1 and say that you happen to know the mean of X is epsilon then you can deduce this the probability that X is at least epsilon over 2 is at least epsilon over 2 so it's mean is Epsilon you like to hope that like there's a good chance it's in the neighborhood of Epsilon well this is at least saying there's at least an epsilon over 2 chance that it's at least epsilon over 2 and let me give you like a words proof of this just like in the words proof before maybe this one's a little harder to do in your head but follow along let's see if I can do it suppose by way of contradiction that the chance that X is at least epsilon over 2 is smaller than epsilon over 2 and now we'll ask ourselves well just knowing that fact how big could X's expectation possibly be and we'll actually deduce that it its expectation will have to be less than epsilon which is a contradiction so why is that you'd say like all right well either X's at least epsilon over 2 but we're assuming that happens with probability less than epsilon over 2 now in that event that it's at least epsilon over 2 the biggest it could ever be is 1 okay so the contribution to the expectation assuming the opposite of this well it's at most epsilon over 2 times 1 from the cases when X is at least epsilon over 2 in other wise the cases when X is like less than epsilon over 2 well it's less than epsilon over 2 and even if that happens with full probability like 1 probability but still only contributing like 1 times epsilon over 2 to the expectation okay and these add up to less than Epsilon okay so looks like basically the same idea in like Markos proof and like you may see that somebody just casually making this deduction in a paper somewhere yep - could've been any K I mean I guess the point here is that these two things need to add up to epsilon so in fact on your homework there's like some other inequality I asked you to work on that's tries to get like better results for this knowing more about X about the chance that our random variables at least half its expectation somehow in general it's much harder to prove things like that I mean it's it's Markov tells you you know it's like knowing almost nothing about a random variable you can say it's very unlikely to be OK for non-negative random variables it's very unlikely to be like way bigger than its expectation you get pretty strong results but to show that something is always at least some 1% of its expectation that actually is kind of hard and you need like assumptions for it like here we needed to assume that X is at most 1 okay so that's a bullet point one like if the only thing you know that I ran a variable is its mean well you should try to use Markov inequality all right look point two let's say that you now know a little bit more you know the mean and the variance and this is not to rare scenario usually for a random variable like it's not too hard to figure out its mean and often the case it's often the case that you're like wow if I work pretty hard I can compute expectation of x squared and therefore deduce its variance so this is this is a case that's not too rare and it's the case that like makes you very happy because you can use like a much better tool in this case to found that probably of X being far from its mean somebody you know that the name yes thank you Chevy chefs in equality so chebyshev that nicole a is a great inequality because it gives you pretty strong results even a case where you know not too much about the random variable so it's chubby Chad's inequality say sometimes chebychev's inequality or the idea behind it is called the second moment method because you know the variance is related to the expected value of x squared which is called the second moment of X okay so chebyshev says the following no assumptions on X except that everything I write is well-defined Let X be a random variable and as usual we'll write its mean to be mu and we'll write its standard deviation to be Sigma okay and maybe to avoid trivialities I'll assume it's standard deviation is not 0 then the probability that X differs from its mean by more than T standard deviations is at most 1 over T squared that's what it says people write it in different ways but I really like to remember it like this then for all T at least 0 the probability that X differs from its mean by at least T or more stay deviations is that most one of our t-squared you know really think it's good some remember it like this because you know you expected to be near its mean hopefully and like the amount that should vary should be related to its standard deviation okay and this is saying the chance of going like five standard deviations above your mean or below your mean is that most one 425 okay so let's prove this and we'll prove its in two different ways again and for this proof we're getting going to start off with some without loss of generality so let me leave this as like a little exercise I suppose but I'll say some things about in words without loss of generality we can standardize X which basically means we get to a freely assume that the mean of X is 0 and the standard deviation is 1 okay in words it's because if you subtract the constant from X to make it it's me and 0 well I'll get subtracted from both places here so it doesn't really change what's going on and it doesn't change the variance and then having done that if you divide by the standard deviation to make the thing have standard deviation 1 again like both these sides scale by Sigma so again it doesn't really change the event okay but if you want to think about that a little bit more carefully on your own time then go for it okay and so now under these assumptions we now want something simpler we want the probability but the absolute value of x is at least T is that most 1 over T squared okay so you have a mean 0 random variable standard deviation one you want to show it's rare that it gets larger in absolute value than T and the bound is exactly one of our T squared okay so again I'll show you two proofs of this proof one is you use Markov inequality in a blackbox fashion so I'll put it here and this is another common technique in bounding random variables like you care about a certain random variable X and probably a bit doing something you look at a function of X use Markov saan that and somehow like magically get a better bound assuming you can compute the expected value of the function of X and the function is basically going to be x squared in this case so the idea is we use markov inequality on the random variable x squared okay one good thing about this is it's definitely a non-negative random variable so Markov applies and now the event that that we care about that X in absolute value is at least T another nice fact is that it's equivalent to an event involving x squared this event and now we can try to use Markov to bound the probability of this event well we know it's at most the expectation of the random variable here x squared over T squared okay that's looking pretty good because expected value of x squared it's basically you know related to the variance so we can just compute it so expected value of x squared is sort of by the definition of variance equal to the variance of X plus expectation of x squared which by our assumptions is a 1 squared minus 0 squared which is 1 okay so we get 1 over T squared here okay so that's a neat trick and in fact this will be the trick we use to also prove Chernoff bounds except instead of looking at x squared and using Markov we'll look at like something like e to the X and use Markov but before we get there let's do another proof fight pictures and it's really it's the exact same idea so let's see if I can uh hilariously draw it let's extend this axis here and let's remove this G of X because I'm not going to use a head I'll you something different but first thing I want to come up with a function such that the expected value of f of X is equal to the thing that I care about which is now probability that absolute value of X is at least one so what I'll do is I'll look at this you know indicate a function of being at least one now I'm going in a function on the whole real line let's run like this this T is now 1 and that's my f of X and so it's indeed true that you know now f is the indicator of absolute value of x being at least 1 and now I can fit literally any punk function I want G which is above this F function as long as I can compute its expectation of G of X then I can derive great results and so well again the main thing I know about X is I now know I can understand an expected value of x squared which leads me to want to fit a parabola here so let's do that let's let G of X be this parabola let's assume that I hit there and I guess this will be the parabola [Applause] well I guess I can just put 1 here G of X is equal to x squared fact if I just like set this to be t more generally I would directly prove chebyshev but let me leave it like this so then expected value G of X is expected value of x squared wait something funny is happening oh yeah this should be a T the thing we care about is not the probability that absolute value of x is at least one thing we care about is the probability the absolute value of x is at least T T standard deviations so therefore I have to retro actively change this to T and this should be X square and over T squared okay this is the parabola with vertex at 0 so it's 0 at 0 and at te it's 1 and at minus T its 1 so it's a deed above F now ok good so this should say expected value of x squared over T squared and then well it's just like here we computed the expected value of x squared is 1 so we get 1 over T squared [Applause] is it okay cool great so that's useful let me also just take this opportunity to mention like a problem which is on your homework which is basically you use chebyshev's inequality to prove this for X a non-negative random variable the probability that x equals 0 is that most the variance of x over the mean of x squared which is also at most the expected value of x squared over expected value of x squared it's weakening it and this is also sometimes called the second moment method usually you use this if like X is like a mildly complicated random variable that counts the numbers of something in a complicated experiment maybe you I don't do like some experiment like choosing a random graph and you want to know like how many triangles does it have in it that's a non-negative random variable but it's kind of complicated but so it's so complicated that you can't like you really compute the mean and the second moment and then if you can do that you can upper bound the probability that the graph has let's say no triangles in it using this technique okay so let's move on to scenario 3 so now let's assume we're even we're in a much better case where I'll write it over here so there you go three as I mentioned is the sort of Chernov scenario when you have a random variable X which is the sum of a bunch of random variables usually simple random variables that you know a lot about where the x i's are independent this is the perfect situation but sometimes you can also deal with the situation where the independent ish yeah which bounds oh yeah this bound is so always bigger than one oh yeah all right well you can use this one in fact is there something I meant to write here though yeah but uh that's true all right well it's not wrong but yeah don't use this one very good yeah you can use this one what's that I don't think so well you'll find on the homework anyway in fact you'll even see a better bound on the homework you better well definitely better than this but also better than this thank you though good okay uh great so this is a great example this uh this you know coin flip example let me just write it again H is the coin flips and X I is 0 or 1 with probability 1/2 each and we assume the coin flips are independent ok so now I won't actually for the sake of illustration really focus on this event just as an example how can we get a good bound and the probability that the number of heads these involve its mean by 10 Ravana and standard deviations erase this so you don't get confused don't rewrite it over here what can we possibly say about probability H is at least n over 2 plus 10 well let's write like this root down over 2 times 10 root lon in so ok we can try to use the tools that we've seen before so H is a non-negative random variable so theoretically we can use markov inequality that's very bad though if you use markov inequality directly you'll get like a ridiculous result like this probability is like at most 1 minus something that's like root lon an over n may be over root n this is ridiculous because I mean definitely this probably is less than 1/2 because you know by symmetry it's equally likely to be below or above and over 2 so this is ridiculous if you just use chebyshev you get something that's like not too bad so for chebyshev you should sort of rewrite this like this this is for the particular item variable H just asking about the probability that H minus its mean is at least 10 root 1 and standard deviations and so chebyshev tells you that this is at most 1 over 100 lawn in which is true and it's not terrible it goes to 0 as n goes to 0 all right go to infinity but it's not amazing because as I mentioned at some point yeah the truth is that it's more like 1 over n to the 50s so it actually goes to zero polynomial a pass not logarithmically slowly so you still might give this one a frowny face [Applause] however ah let me briefly mention some scenario in which a similar in which you would use chebyshev's inequality in this scenario we'll talk about like in lecture 11 or something when we come to talking about D randomization and pseudo randomness but there's a concept of random variables being pairwise independent I'll just write this and we'll talk about it properly later but chebyshev only needs pairwise independence in this kind of scenario and what that means is you don't in this scenario you don't need to assume that the coin flips are all mutually independent if they for some reason have the property that any two of them are independent even though they're not like all independent you can still use this chebyshev inequality so this will actually come up when we intentionally create like pseudo random variables which are not necessarily fully independent but one example of this if n is 3 if you like take 3 coin flips and you like conditioned on maybe an even number of them being heads then any pair of them will be independent but the three of them together not all independent and this stuff comes up sometimes and it turns out that this argument this mountain using chebyshev's inequality is fine even in that scenario and just to not write too much but say in words why that's true chebyshev really only relies on knowing the variance well in the mean of this random variable X oops and if you imagine like you know computing the variance of this random variable X it's like expected value of x squared minus the square of its expectation expected value of this thing squared you open it all out you get like a sum of expectation of X I XJ for like many pairs X I and XJ and as long as any pair of them are independent you can convert that expected value of x IX J into the product of the expectations and therefore the calculation would be the same as if they were all mutually independent and therefore okay we'll talk about this in more detail later but I just want to bring it up now and indents that chebyshev is like the best thing you can use in this scenario where you only assume pairwise independence to the random variables and these things like Chernoff bounds will actually rely on the fact that all the random variables are fully independent or mostly independent [Applause]\", metadata={'source': 'qqHHvOp5N6w'}),\n",
       " Document(page_content=\"so let's say we I mean motivated by this we want to like go even further and try to get a better bound than one over 100 lon end using chebyshev's inequality well in fact without going all the way we can try to take the idea of you know saying like well we can understand the expected value of x squared and we can put a parabola over our step function and get some better bound that way we can try to do that with like higher degree polynomials like put a fourth power polynomial above our step function so let me explain what I mean by that and to make things nicer let's switch now to the plus or minus 1 version of this scenario so let's switch to analyzing X sum and independent random variables which are Rademacher meaning they're plus 1 or minus 1 with probability 1/2 each okay this WP is with probability okay and let's try to do this picture thing again so now this thing that we care about is exactly the same as the probability that this new random variable X its absolute value is at least again 10 root lon and standard deviations and now the standard deviation of this x is root n okay so we're still trying to upper bound this and okay we can do this picture again okay so let's put T root n here in general let's put minus T root n here in general let's look at the function f which is 0 down here and jumps up to one out here one out here let's call this f of X okay so this probability it's literally equal to the expected value of f of X f is indicating the event that we care about and instead of pointing a parabola here let's put like a kornek okay so to look kind of flatter yes it's good because like it's doing like a tighter job of matching f of X in this range which is frankly the range that the random variable is likely to be in so this core tech I guess in G of x equals x to the fourth divided by okay we need to match up at this point so I guess it's t2 fourth times N squared okay so this is therefore at most expected value of G of X which is clearly expected value of X to the fourth over well T to the fourth N squared so now you can see if we could somehow compute expected value of x to the fourth we would get like a nice bound and uh if you look at it you're like well I guess I could do it and so let's do it yep oh yeah I need it okay so I used to I mean I didn't pull out of a hat I kind of knew something like this would work so I used some I guess some smarts knows the roughly a good idea so basically I wanted a function that was like X to the 4 but like I also wanted to like tightly in some sense upper bound this step function f of X so I just took X to the fourth divided by some constant where the constant is chosen just so that like G of X and f of X would match up here so if you plug in T Ruden here you get T to the fourth over and T to fourth N squared ok so let's do it that's slightly a drag and what I'm doing right now is sometimes called the fourth moment method and you can usually sometimes see you know to get better bounds even than chebyshev when you're feel like you can heroically compute expected value of x to the fourth where X is your random variable that's the point you got to do it like when we study analysis of boolean functions I'm going to show you a slicker way to do some of these computations but I'll do them like in the non-slick way right now so uh okay we're looking at X to the fourth which is sum I goes from 1 to n X to the fourth and uh let's do it let's expand it out it's gonna be a little bit gross but here we go since some of our I of X I to the fourth plus oh well we just saying some terms that look like X I cube to XJ and then some terms that look like X I squared X J squared and then like some terms that look like X I squared XJ XK and then some terms that look like X I XJ XK XL and we can work out the exact coefficients here but we're going to save time by not fully doing that and I'll show you why what we really care about is the expectation of this so by linear of expectation we can take the expectation of each piece so this one's easy because X is our plus or minus one right so X I to the fourth is always 1 so this is this term is always in okay now let's look at these terms we're gonna get something that looks like you know I expected value of x I cubed XJ and here I'll rely on the fact that these random variables are independent in fact I only need pairwise independence for this X I cubed times X so I've divided XJ and now look at expected value of XJ it's 5050 plus or minus 1 so this expectation is 0 actually so it's this expectation but never mind I'll just remember this fact and that's good because all these terms will have expectation 0 so we don't even need to count them it's all going to go to 0 let's go back to this one this one similar story right expectation of X I squared XJ XK by independence you can write it like this XJ XK okay and the expectation of XJ is 0 expectation of X K is 0 so this term is extra 0 and all these terms will also be 0 because there's at least one X here to the power of 1 great so a lot of stuff drops out that's nice the only thing we have left to do is this so okay well I guess X I squared X J squared because X sine X J are always plus or minus 1 this is just 1 okay so the contribution here we just have to count the number of terms there's nothing else to do so we not need to comb unit or X I will just look at my paper I guess the number of terms like this is n choose 2 times 4 choose 2 right you got choose 2 eyes and J is out of the end but also there's like for the 4th ways you can expand this out there's 4 choose 2 ways of arranging them anyway it's this good so just this term and this term survive which was convenient this is 3n squared minus 3n okay so in conclusion with some annoying work we deduced that the expected value of x to the fourth is 3n squared minus 2n I guess okay let's just say this at most 3 n squared okay we're not going to really bother to save this minus 2n great and now we can plug that into here and something pretty good happens we got that the probability that the absolute value of x is at least e set miles 300 over n squared T to the fourth which is 3 over T to the fourth and how do we use chebyshev if you remember we would have got like 1 over T squared okay so assuming T is large I mean this is like 1 over T to the fourth a constant over t to the fourth chebyshev is constant over t squared so that's a lot better and in general you know if we cared about the specific number 10 root lon n40 okay then we would get like constant over log squared n for our bound which is better than constant over N yep bloggin yes if you do it with better higher moments you'll get even better bounds but like the computations will become more annoying unless you do them in a slick way which would probably we'll eventually do but you could do them I mean you could tough them out your tough tough people and if you keep going and use like the the sixth moment method or whatever by the way we always use even powers because well to do this it's not a good idea to use an odd degree polynomial because you won't be able to keep it above the step function everywhere so you'll want to use an even power and it's a fact that expected value in this case of X to the two s when s is like a natural number is [Applause] at most well it's something like n to the S but there's a constant out front which is the annoying thing that depends on s like here was like three this computation we just did with s equals two you know we got this constant to be like three when you can plug it in and you can there for it to do so the probability that X is at least T root n you somos this constant depending on s over T to the two s and then if you like heroically like got a good handle on this constant then you can try to optimize s as a function of T because this gets better the bigger s is but this constant also gets bigger the bigger s is so you can optimize over s and if you do that it'll ultimately work out great and you'll get something that's basically the same as turn off bounds but it's a pain and now we'll show you the kernel bounds which is like a different slick way to do something like this okay let's erase this disgusting stuff okay so trip bones is gonna again be just like this but we're going to use a si sort of set at the beginning like an exponential function to instead of like this core tank let me erase this so it doesn't look so strange and it's gonna be convenient in the Chernoff bounds to only worry about the probability that the random variable is way bigger than its expectation you can do the analogous thing to worry about the probability it's way less than its expectation which means that we're gonna just think of this scenario let's quit T here and let's let F of T be this step function which is zero all the way out to T and then it's 1 when X is bigger than T and this curve that we're gonna put above this it's gonna look like something like e to the X so we could take this curve it's no longer part of a quartic polynomial it's gonna be like G of x equals we could take it to me like e to the X and then in order to make it like match equal 1 at x equals T we'd divide by e to the T but it's actually better to be more flexible and just say like well I'm gonna take G to be like some constant to the X not necessarily each of X and one way you can do that is stick with E in the base but just write e to the lambda X where you're like lambda is a parameter I'll name later okay so we're going to do this whole trick it's still set up for any lambda lambda is positive so that we have like an exponential like function divided by a constant sorry this should be T and it's set up so that they match at T equals x they're both 1 and now we're going to use like a Markov like trick okay so actually just so I don't mess this up let me call this U instead of T so I'm gonna focus just literally on this random variable X which is the sum of these plus or minus ones that are independent although Chernoff bounds are more general than this and I'm gonna say okay the event that X is at least U is equivalent to the event that e to the lambda X is at least e to the lambda u okay what's because e to the power of something is an increasing function I guess I'm not exactly using this picture I'm doing like the Markov version of this argument but anyway I think this picture helps okay and so therefore also this is a non-negative random variable so we can use Markov so more cause implies that the probability that this non-negative random variable is at least this number e to the lambda U is at most the expectation of the random variable e to the lambda X divided by the number key to land at you okay let me just make clear what what's going on here because I think I didn't explain it perfectly we have this random variable X we have some fixed number you and we care about what's the probability that X exceeds this you again we're saying that's the same event as this which looks a little bit weird but it's a trick and we're going to use Markov inequality to bound the probability of this happening and lambda is like a free parameter which I stuck in here I'm going to choose it advantageous ly later and it's going to depend on you yep well the thing that's gonna happen just now is so far we haven't this is true for any random variable at all even it's doesn't nothing none negative nothing but the the thing that we are going to use heavily in just a second to analyze this is the fact that the x i's are independent and then we're also going to use the fact that we know everything there is to know where we have good control over the exercise themselves but in this case they're very simple random variables but indeed uh we're gonna use the independence heavily in just a second because this quantity is aa this is actually it's maybe not so easy to tell this is a function of lambda for X fixed it's called the moment generating function of the random variable X because if you differentiate this function like K times you can easily extract the Kate moment of the random variable X for any random variable X so this trick of looking at e to the lambda X in expectation is a known trick in probability and what's great is that this thing is easy to compute using the fact that it's the X is the sum of independent random variables so their plan is to compute that and then just choose lambda effectively to make this right-hand side small [Applause] okay so again I guess well do a big old calculation on this board but this one is sort of entertaining okay so expected value of e to the lambda X is expected value of e to the lambda X 1 plus lambda X 2 plus double dot plus lambda X and should really switch to this expo tation so they don't have this have to write everything in the subscripts okay so this is you know by the properties of exponentiation this is expected I'll expose lambda X 1 times e to the lambda X okay and now here we will crucially use independence because all these random variables are independent X 1/4 X n so two are the random variables e to the lambda X 1 through each of the lambda X n and therefore we can say that the expectation of a product is the product of the expectations so this is expectation of X lambda x1 times expectation of X lambda X and ok and now these are n numbers we're multiplying together and they're all the same number because the fact that all these x i's have the same distribution they're all just plus or minus one random variables ok so we can just say this is equal to let's say an expected value of e to the lambda x1 the first such number to the power of n okay so that's stage one and now we'll literally explicitly kind of compute this as a function of lambda so let's do that down here expected value of e to the lambda x1 say well literally half the time x1 is 1 so we got it half e to the lambda and half the time it's minus 1 so we have 1/2 e to the minus lambda o some nerds would say this is like cos lambda or something but let's just use the Taylor expansion of e to the lambda so this is like 1 plus lambda plus lambda squared over 2 plus lambda cubed over 3 factorial plus da plus 1 minus lambda plus lambda squared over 2 factorial minus lambda cubed over 3 factorial plus stuff I thought o times halves so it's the average of these two things okay and that's cute because the odd powers cancel and the even powers are the same so this is just the even powers 1 plus lambda squared over 2 factorial plus lambda fourth over 4 factorial plus 1 okay and now here's the one like slightly weird move I claim but this is at most e to the lambda squared over two and why is this claim true well I'm gonna write the Taylor series for this thing down here it's 1 plus the first term is lambda squared over 2 it's looking good next term is like lambda squared over 2 squared 2 lambda fourth over two squared times two factorial and so far it's bigger right because here the denominator is bigger than this denominator because it's a 4 times 3 times 2 times 1 this is 2 times 2 times 2 times 1 let's just do one more term land a six over six factorial and the next thing we'll get here is lambda squared cubed which lambdas 6 over 2 cubed - 3 factorial and again here the denominator is bigger at 6 times 5 times 4 times 3 times 2 times 1 this is 2 times 2 times 2 times 3 times 2 times 1 so it's gonna be fine right I mean that's true so that shows the claim and we did it well we bound it in a nice way this quantity so we can put that back into the top right corner and get there for expected value of e to the lambda X is at most the nth power of that which is e to the lambda squared and over to now we can put this into that you can get therefore probability X is at least you it's the same as the probability that e to lambda X is at least e to lambda u then we use Markov the numerator here is at most e to the lambda squared n over to the denominator is e to the lambda you great so this is a true fact about every real number lambda maybe lambda should be positive actually and so we can choose our favorite lambda to try to make this as small as possible so we want to minimize this as a function of U and N and that's a very small exercise for you the best lambda is U over N and now if you plug in U over n you get that this is e to the minus u squared over two and okay so this box will put here and this box will put here so it was some work but we got a real great bound because this is like exponentially small once you gets bigger than root end but like Rudin is the standard deviation here so that's to be expected so for example with this bound we just finished doing if we take you to be 10 root lon n times root n we get probability that absolute value of x is at least this sorry just that X is at least this 10 root lawn n root n is that most exactly the thing that was supposed to be 1 over N to the 50\", metadata={'source': 'cLczU5-CW70'}),\n",
       " Document(page_content=\"oh this proof I've just shown you is like the simplest possible kind of case of Chernoff bounds or huffing bounds and sure enough huffing bands are always about sums of independent random variables and we use the independence crucial here but then we did use some more facts about the random variables when we literally did this computation here we use the fact that there are plus or minus one random variables with probability half each okay so if you have like different kinds of random variables like maybe of a random variable which is one was probably two-thirds and zero with probability one sorry one-third or some other like random variable and you're taking independent sums of it then you have to do a different calculation here and the ideas are the same and there's some general theory about like how to handle this situation but I'm just gonna skip all that and tell you the conclusion because this is the idea and now I will tell you the like optimized forms of the results that you should come to know and enjoy so yeah I'll now just state some theorems which are great theorems that you will use all the time in CS theory work okay so I'll say a couple of theorems another downside here is there's like look not like one like glorious or theorem that you just write down and you're just like I'll use this theorem every time like you always have to use like a slightly different theorem depending on the exact scenario and there's like many variants and if you look up you know Chernoff bounds and wikipedia is like incomprehensible so I'm trying to like give you like just a couple ones that should get you through like 85% of situations okay so let's x1 sorry let x be x1 plus Papa dot plus X and where the x i's are independent and they do not have to be identically distributed just nice they will try this enormous chalk we'll see how it goes and that's right Oh mu for the mean and generally assume you can either compute this or more or less compute this okay so the first banner I'll tell you is sometimes called huffing bound I don't really know what the standard Anglo pronunciation of his name is so we'll say something and here where a student will assume that X I is ticks values in a bounded range so we'll say it's between two real numbers AI and bi usually a usually all the A's are the same and usually all the B's are the same but just in case like you can take different bounds for each one okay so here you always need to assume like something about these independent random variables so they're not too crazy or you have some control over them in order to deduce a huff thing or turn off type bound okay so then thing says the following then the probability that X the sum is bigger than its mean by T or probability that X is less than its mean by an additive negative T these are both bounded by the same thing and the same thing is e to the minus 2t squared over the sum of BI minus AI Squared's we're all T greater than zero okay and this one's pretty powerful and you can use it in a lot of situations in particular you can use it it should give us back the result we just proved while I erased it or no I didn't erase it the result we just proved for this case where the X eyes are plus or minus one with 50% chance each and that's scenario we can take all the A's to be minus one all the B's to be plus 1 so this is 2 squared which is 4 so the denominator is 4 n cancel the twos you get T squared over 2 n okay well I called it u up there but it's T here and the MU and our special case up there was 0 okay so this one directly generalizes this one and it's nice there you don't need to see in these random variable or 5050 or anything they can just be any random variables at all they take values between let's say minus 1 and 1 or some bounded range ok so that's something now I'll show you like a similar but different one which is Chernoff bound which can be more effective let's say you're summing up zero one random variables and you know if these random variables where let's say 0 or 1 with constant probability each then the whole sum would be have mean proportional to n Charles Brown is effective when this mean is sort of a bit smaller than like n over 2 but maybe you put a little star next to turn off bound because if you have to remember like just one bound under this whole lecture maybe let it be this one try to try to memorize it I mean people will just when writing a paper say oh by trying I found this and you just got to get used to that get used to using this so the hypothesis here is that these random variables are between 0 & 1 you don't have that you can always you can typically scale your random variables so they have this property okay then for all parameters epsilon greater than zero probability that the random variable X is sort of you can think of this is like at most 99% its expectation okay so it's at most one minus epsilon times its mean this is upper bounded by e to the minus epsilon squared over 2 times mutants okay so a common scenario you know maybe epsilon is 0.01 mu is proportional to n okay and this will say that like the probability that this random variable is less than even point nine nine times its mean it's really small it's e to the some constant times and if the mean is proportional to n okay that's quite useful and on the other side all right something that's like almost perfectly symmetric but not sadly probability that it's bigger than mu it's mean by a factor of 1 plus Epsilon here's a mouse e to the minus epsilon squared over 2 mu plus epsilon in the denominator which is a little bit annoying when you're trying to memorize this bounce then you have to memorize this like hiccup and unfortunately you cannot remove this it's false if you don't put the plus epsilon in the denominator now this is really kind of a bit splitting hairs because you usually use this when epsilon is a very small number close to zero so there's basically no difference between two and two plus Epsilon but you know if you really want to write it really strong but true statement then you have to write this but it's very common to just see people use that like oh well this is at most e to the minus epsilon squared over 3 mu if epsilon is at most one which it almost always is you rarely care too much about this constant here but uh see here it doesn't make sense to even take epsilon bigger than one but here you might take some flown to be like five you're like oh I'm interested in the like rare case that the probability that X is bigger than its mean by a factor of six and then if you see actually this epsilon gets larger and larger this is no longer a quadratic in epsilon if epsilon is actually large then this becomes linear in epsilon so yeah I mean you know this is sort of like for large epsilon it's roughly X minus Epsilon oops times mu for a large Epsilon [Applause] okay so yeah memorize this if you can because you'll use it 100 or 1000 times in your life hopefully let me make a small comment here that is another true aspect of the Chernoff bound which people use like all the time without even mentioning it in fact I didn't even realize that like some subtlety is happening until I prepared like a version of this lecture the first time I taught it which is the following ah many times in this scenario you cannot actually know the mean exactly but you have some bound on the man so sometimes you might know something like oh I don't know the mean by no lower bound mu L on the mean I know the mean is at least something so it might seem intuitive that you could say oh well the probability that X is less than or equal to 1 minus epsilon times this found on the mean could also be like small e to the minus epsilon squared over 2 u L because you're like well this is even less than the mean so like this is even rarer than the event that X is at most 1 minus epsilon times the mean but I did something subtle here I put this lower bound in here oh actually that one's fine right I mean really this would be definitely true if I only put mu in here so am I eligible to put me UL in here yeah Wow actually yeah uh yeah so that's I guess trivial but on the other side hopefully it's not trivial or else this point is move let's say you know let's say you know mu is at most mu H high and low I guess can you say that probability that X is at least 1 plus Epsilon times mu H is at Mouse e to the minus epsilon squared or 2 plus epsilon times mu H the idea is you're like oh I'm going to use this mu H as a proxy for MU this should be even rarer than the event that you're more than 1 plus epsilon times your mean but now you did something sneaky because you made this right-hand side even smaller than what it truly should be if you're doing a black box appeal to the theorem but I'm here to tell you that this is still true so basically if you do this intuitive move like well I don't quite know the mean but I know like like a bound on the mean which should make the thing I'm trying to bound even smaller then you can plug that bound on the mean into the theorem and it's still true ok we close with a few things um okay so I'll tell you all by writing down one very simple corollary of Chernoff bounds it gets used all the time which is the bound you use when you're like estimating like a random variable from samples and you want to say that like the empirical average is really close to the true average there's a couple more versions of Chernoff bounds or huffing bounds that hold in situations where the random variables are not fully independent and these bounds that you can use in that case I'll leave them in the notes but enough time to go over them now let me just say in words a little bit about them so once an area where you have non independent random variables but like you somehow feel a journal found should still be true is when these random variables are like somehow negatively correlated you're trying to show that the probability of the sum of these random variables is not much bigger than its mean and they have some kind of like negatively correlated property which intuitively means that like if one of them is large the other one should be small then it feels like that should only make the Chernoff bounds like more true and that's true we have to carefully define what it means to have these random variables to have like negative dependents and the right way to do it is with this notion called negative associativity which I put some references to in the notes let me just give you like a couple of examples machen like you take a unit circle and you randomly and independently put down and points on it so that divides the unit circle into like arc lengths and you could these could be your x1 through xn and you could maybe take a these guys have the property that they always sum up to one so that actually proves that they're not independent and I guess it also means there's really no mystery about what's going on with their sum is a random variable well you could say like take the sum of the first n over two of them so they still wouldn't Bend n't but they still intuitively have the property that if I tell you you know x7 is big so like one arc segment is big kind of makes you feel like that while the other ones have to be small and you can indeed show that these random variables are negatively associated and therefore for any subset of them Chernoff bounds still hold that's an example of that another example when you don't have independent random variables if your command random variables and well let's say you have any independent random variables actually here they will be independent but you're not necessarily looking at the sum of them you're only looking at some function of them but this function is the property that if you change the value of one any one out of the N random variables it doesn't change F very much so f has some kind of like Lipchitz property then this is another case where like turn off type bounds holds and it's called um McDermid xin equality you can take a look at that and uh yeah well the last stairwell right on the board and then I'll be done this is sometimes called the sampling theorem it's a direct corollary of turn off bounds it says the following let's say you have a random variable which is bounded between 0 & 1 has mean mu and let's say you get an independent samples from it or you get like to see x1 through xn that all of the same distribution is this unknown X and they're independent you estimate the mean in the natural way mu as well the empirical average over N this should be close to the true Mew mean you had hope and it's a direct corollary of true pounds that for all parameters epsilon and Delta if n is bigger than 3 log 1 over Delta I guess is lon over epsilon squared then your estimate will be epsilen accurate except with probability at most Delta okay so this is the kind of thing that used like in polling right you want to say that Oh nineteen times out of 20 or a delta equals 0.05 my reported mean is within some tolerance epsilon of the true mean and turn off bound tells you how many samples you need to take in order to make that true and it's basically proportional to one over epsilon squared okay I'll stop there and see you on Thursday\", metadata={'source': 'zz4C-xECIp4'}),\n",
       " Document(page_content=\"okay hi everyone we've sort of finished our first unit in this course on asymptotics and probability and so forth there's a lot of math but this course is also about CS theory so today we're gonna start talking about some CS actually it's a bit of a one-off lecture today because the next few lectures we're going to be talking about things related to Fourier transforms but it will connect a little bit with what we're talking about today computational models when we talk about things like integer multiplication and quantum computation and so forth okay so let me state two like very very basic algorithms problems very very simple algorithms problems and I want to talk about the running time needed to solve them so here's problem one palindromes you're given a string you want to know if it's the same in Reverse so let's just give it an X of length n you know is X equal to the reverse of X okay that's a very pretty simple problem algorithmic problem and here's another one that you learn about in your cradle sorting you're the task is you know given the same n integers sort them okay so I what I want to ask is you know what's the running time needed to solve these two problems so I'll ask you first I mean what do you feel should be the running time for deciding if I string as a palindrome part of me and squared on a Turing machine maybe any other votes somebody know an algorithm for deciding if a string is a palindrome I'll do the first line for I equals 1 to N okay can somebody give me a couple more lines okay that's true that's over it like you know what to say say like you know hyper efficiency like don't over optimize before you finish let's get correctness first before we stop it in over to what found over to is not an integer okay if X of I equals how about not equals yeah just not equal X and plus one I guess minus I that I get to off by ones right yeah then return no and if you make it all the way to the end of the loop return yes all right we did it great yeah what's the running time without algorithm in computer science 101 you're so scared okay let me ask you this if you ran it on your own computer like implemented it see for like a strings of length a million and then ten million and then one hundred million and then a billion and plotted the running time what do you think it would look like yes I think this is it will look leer I mean if you actually do this I don't you have to think of an interesting test case for palindromes right which like oh where like it is a palindrome or you know almost as a palindrome so this loop will run to the end but I think if you do this I mean I think I even tried this once yeah the running time will look linear so this is I mean this algorithm you know this should be order n time right in fact what I would like to pause it you know is when we actually we're trying to model like what happens in our actual computers right if our computers you know just your regular old laptop you plot this and it's running an amount of time proportional to end then you want to like live in a world where the running times order n so that's good what else sorting we feel is the running time for sorting yes at least n log in good ok any other suggestions yes they have n log in okay so also order of n log in any other suggestions yes and squared well you can do it in n squared with like a not great algorithm I guess I'm talking about like the best algorithm yeah that's sorry what's that you mean order n yeah we're in hmm any any other boats yes lower bound of n okay good yeah we got a repo possibility up there great probably get thrown order N squared that's definitely true yeah there's a little bit of a tension here right because well these two are like contradicting each other so something funny is going on often you hear yeah I mean lower bound for sorting n log n I mean there's n factorial different possible arrangements of like Earth sort sort of orders to the numbers every time you do a comparison you gain like one bit of information but like log of n factorial is as we now know is theta n log of n log n so it would seem to suggest this as a lower bound on the other hand I mean it's going to turn out that in like the standard model of computation that is implicitly employed in papers about algorithms order n is the answer so how how can that be well as suggested by the title of the lecture the true answer for both these things up for the best running time is it depends on the model so I guess you probably also knew due to your reticence and answering my question so that's what I want to talk about today like what's going on with the computational models you know as we'll see this is true that you need like n log n time but only in like what it's arguably a strange or sometimes strange model of computation where it's not like the normal model of computation it's a model where like the only thing you can do is maybe compare two objects to see which is bigger or like that's the only thing you charge yourself for which you know when you're like I don't know programming and C and sorting integers is not actually true right you can look at the bits of the numbers and so you can try to use like radix sort will say what that is later and indeed in many models that gives you a linear time sorting algorithm but it's also mentioned even for palindromes like I mean well we'll talk about it but you know it depends if you're talking about on Turing machines or other models of computation so one thing I want to tell you is in this class we're gonna actually talk about three models of computation just in this lecture one is Turing machines or TMS two is circuits and three is something called word Ram model and uh just in brief the first two are greatly enjoyed by complexity theorists and so that will be a model that they often study and the third model is usually enjoyed by algorithm assists and also complexity theorists and this is a model of computation that like secretly underlies like every like algorithms text book that you read like CLRS or whatever they kind of refuse to exactly tell you what the model is and even if you take an algorithms course in undergrad they'll sometimes be a little bit cagey about like what actually is in the algorithms model they'll be like you know this is order n but there is a model that secretly people who actually do algorithms for a living yes and I will tell it to you today well maybe alright now it's okay but first I'll talk about Turing machines which have some advantages in discussing computation as I said there's sort of the base model of computation in like complexity theory the theory that you know kind of thinks deeply about time and space and on the other hand is not too concerned about the difference between a linear time algorithm or a quadratic time algorithm complexity theorist would just say all those things are P that's fine to us so I got to kind of soom you know what a Turing machine is like the basic one like has this like tape I like computes on you know the input comes on the tape and then it has this like finite control that has like some states in it and it's got this like read/write heads and like every time step depending on what's written here like the head will move left to right and it can change what's written here and change its internal state and this is a good model for computation in some ways the historical main advantage of this model for computation is that it's 100% clear given an algorithm like a Turing machine algorithm what its running time is and also what its space usage is okay so these are the two main resources that you know people care about in algorithmic theory at least basic algorithmic theory so you know on a given input the running time of the Turing machine is like literally the number of steps it does if you really want to keep you count it literally exactly so there's there's no ambiguity about like oh who is this order end or what you can you can get it exactly right and space which is I guess the complexity theory name for memory usage is also completely unambiguous it's like the number of tape cells that the algorithm ever touches actually generally you don't count the inputs against you so it's like the number of additional tape cells beyond the inputs that the Turing machine touches so okay that's nice and you can exactly say what running times are on a Turing machine and it's extremely precise the downside is that they're arguably not realistic and by realistic you know I don't mean that like this could not exist in reality but like it doesn't the running times a Turing machines give you often do not match like the empirical running times you kind of expect when you run a normal old algorithm on your normal old laptop and as an example as was mentioned way back in like 1965 a person called penny proved the following theorem solving the palindromes problem on a regular Turing machine which I'll call it one tape turing machine as we'll talk about multi tapes in a second requires quadratic time except if you think about like how to solve palindromes on a turing machine right the the things got to go like all the way to the end and back and back and forth till I compare the characters at the beginning the end and that's unsatisfactory if you're the kind of person who cares about the difference between quadratic time in linear time which most of us do so yeah generally considered not too realistic so there's also a traditional you know solution to this unrealism and complexity theory which you may have learned about which is multi tape turing machines so you can say oh well model of turing machines where now there's like two or more tapes but it turns out basically without loss of generality you may as well only have two tapes and you know now there's like a read/write head for each tape and this can allow like more efficiency for turing machines for example on multi tape Turing machine you can solve palindromes in order n time because basically the Turing machine can take the input on the first tape and like copy it and to refer on in reversal on to the second tape in linear time and then like compare the string and its reverse in linear time so the naive response is like great like we wanted palindromes to the order n time multi-state Turing machines solve palindromes in order n time so things are looking good but not so good I mean there's still several objections one could make one objection one could make is like well still doesn't feel like a realistic model like it still feels like it's like weird memory access system where like you got like lock the pointer all the way to the end of a string to find out its value I mean is it just like some luck that it could solve palindromes in order n time but for some more complicated problem that we still think should be order in time multi-tape turing machines couldn't do it in order n time that's another response actually in some sense it's I think about this sometimes it's some sense it's actually physically unrealistic even in the sense that this is a bit philosophical but like in real real life you know to decide what to do at each step right this control needs to get information from each of the tape heads and if the tape heads are like super far apart in physical space and like information can only be conveyed from the tape heads position back to the control at most the speed of light so arguably like it doesn't make physical sense to allow like in one step the Turing machine to like collate information from arbitrarily far apart now actually that assumption is going to get like even worse when we talk about more realistic models like you know the word Ram model but it's worth thinking about especially because we're gonna consider you know talking about things like quantum computation later and actually as you know computer chips get smaller and smaller the actual like time it takes like you know electrons to go from one side of a chip to another can start to make a difference but I digress yeah it does help I mean oh why doesn't any more tapes not help a good question there's a theorem that says anything you can do with K tapes and time T you can do with two tapes in time order T that's the theorem yep if you have infinitely many tapes I yes don't really give us uh it's I think it's not very different from having like okay I'm not sure but let's I'm not have percent sure but we can say it's not so realistic I mean yeah I mean they can I guess what I'm just saying it's the way a multi tape turing machine is supposed to work right if like n is a billion and like one tape head is here and one tape here is a billion centimeters away according to the theoretical model like in one step that like the control can find out what's written here find out what's written here make a decision in command the two tape heads does it go like left or right so we have to judge whether you feel that should take one time or should take time proportional to how far apart the tape heads are yes number of possible accommodations yeah I guess so yeah yeah you could have been like a lot of models I guess my guess is that like nothing especially interesting happens sadly when you like invent these models because sort of otherwise we would have heard about them I mean there's lots of ways you can go and yeah the reason though like eventually we're going to talk about you know random access memory and so forth we're like you can't just in time one like access any memory cell you want is because at least with our current computers it does seem to be the case that like the amount of time it takes to like look up one cell or one bit from anywhere is kind of negligible compared to I mean it doesn't make much of a difference where you look out from compared to the time it takes to do an instruction but these are all modeling issues that we're thinking about so yeah I mean you can go like arbitrarily far I mean actually is it something that like makes a huge difference in actual you know real-world code on your laptop it's like caching issues I mean you have like I show different kinds of memory and like this takes different amounts of time to access and so forth but we're gonna keep it relatively simple I guess for today actually there's yet another reason to be still sad about this solution which is that it's got the court of a right time bound order n but the wrong space bound so this solution which involves copying the string reverse onto another tape uses Omega and space again you should think of space as space beyond the original input and if you look at you know the you know the true algorithm we know and love for palindromes this one right it doesn't use a Omega n space these I don't know constant space stoller like I or something so we feel like the model should have the property that you need like order one space and order n time so you might ask well okay maybe you know just a smarter multi tape turing machine algorithm get order n time and order one space but that is not true there's another theorem that complexity theorist back in the olden days this was Cobham in 66 and Duras in khalil in 84 they showed that multi tape turing machines solving palindromes in time T you know of N and space s of n need T times s to be Omega N squared this is called time space trade-off result and it says well you can actually do it in quadratic time in constant space or maybe log space in a probably constant space or you can do it in order n time in order n space and in fact I think anywhere in between but multi tape turing machines cannot actually achieve this desired kind of time-space bound of linear time and constant space and it's all due to this you know memory access model for Turing machines right which is kind of weird or not too realistic that you know really to look up some bit of the input that's like the N fit of the employ it like takes you n time unless you're somehow clever about arranging things into your two tapes so the complexity theorists have a solution for this which is not talked about too often but it exists and it's called the random access Turing machine model or the random access memory RAM Turing machine [Applause] so this is like an even more advanced model of a Turing machine that you know complexity theorists like to hang on to Turing machines because they do appreciate the fact that there's like zero ambiguity about like what is the running time and so forth and it's a simple model but it's still like kind of like a funky model in that what is a random access memory Turing machine it's like Turing machines with augmented with a special like address tape let's say it's like odd one normal work tape and like one address tape and it has a special instruction or like you know the Turing machine can do this one move where in one step the pointer on the or the read/write head on the work tape jumps to the position written on the address tape in binary ok so you can like write an address in binary on the address tape you know it's like some other tape here and if you know this says like 1 0 1 1 I don't know if that is in binary 1 311 yeah yeah it would you know it does like one move it would jump this head to like the 11th spot [Music] and this is somehow like the really sophisticated you know complexity theorists like official model of choice for how they model you know algorithms in complexity theory and it's true that you know running times and space usage is on this model generally match those and like the more familiar up models that like algorithm assists work in and in particular these machines can basically implement this algorithm and they'll solve palindromes problem in linear time and well log in space they use log in space because for example to implement this you kind of need to like write down I to ranges between 1 and n and therefore needs log and binary digits to write down and that's you know charged against you in Turing machines but this is close to order in time and this is you know tilde one space so it's it's considered reasonable and you know complexity theory so they're before they're mainly like you know anything in polynomial time is kind of fine they're certainly not that into like quibbling over like log n factors but you know you should equivalent log 5 log in factors in life right I mean log n is probably 20 or 30 and like a normal scenario and if you can make your code run 30 times faster like you don't want to just be like oh whatever I mean we care about even these factors okay so in fact you know when you get into hard core algorithms you worry about all these things and so even this is like not a great model for you any questions right now okay it actually just doesn't like another reason to like you know make you say well the small is sort of still annoying here's like an another simple problem I'll just make it up called I'm gonna find Max and in this you're given and you know natural numbers let's say all at most N and you want to find the maximum pitch so you give it a list of numbers unsorted like which is the largest number like this is really a problem that screams out like I should be order n time right you just go through the numbers and remember which one is the biggest number and then you print it out but like on a Turing machine you know or in the Turing machine model that's impossible because the actual length of the input is n times log in right here technically when you write down the input it's like n numbers between 0 and n the input length is like n log N and you have to read all the input in order to make a correct decision so you cannot possibly solve this problem faster than n log n on a Turing machine and so you know from an algorithm mists point of view you're like this is annoying I don't like this model but at least it's like brutally you know explicit and correct okay so that's all I'll say about Turing machines if you love complexity theory then you can come to love Turing machines but otherwise you hopefully will not encounter Turing machines too much in your your life there are a little bit annoying\", metadata={'source': 'BwNzpttp2vw'}),\n",
       " Document(page_content=\"okay second model I want to talk about today which is also kind of beloved by complexity theorists and quite natural and kind of enjoyable is the model of boolean circuits and these are also really nice model for computation because again like Turing machines they're like 100% like concrete and unambiguous there's no like wishy-washy Nissa but like oh what is the complexity of this circuit I mean what a circuit is right it's like looks like this it's got some angei it's probably some or gates you know some more gates here I don't know whatever it's um not gay it's you have some the inputs x1 x2 x3 so this is the thing that computes functions that's probably valid it computes functions but map binary strings to binary strings I should say boolean functions okay we'll talk a lot about boolean functions a few lectures from now and it's like it's got a multiple outputs so you can have like circuits that like you know output long strings but often you only worry about outputting a single bits a yes or no answer I know as I said these are like a nice bowl of computation and they have like simple properties and you know you can do things like exactly count the number of gates in a given circuit and so forth so let me say a few definitional things about circuits just something to make sure on the same page so these are DAGs directed acyclic graphs and the nodes are called gates and when you're finding circuits you have to make a decision about like what you're allowed set of gates are and there's like I don't know two or three like reasonable choices and neither is more correct than the other so you just have to accept the fact that like every time you start talking about circuits if you want to be really careful you have to specify like you know what gate sent your loud so maybe one might write B for what's called the basis which is the set of allowed gates they let me continue I don't know over here okay actually besides you know these gates that are like and and or and on and so forth if these special gates at the bottom labeled x1 through xn which are called the input gates and usually there's something that's like marked as the output gate of course and it you know this object when you plug in bits for x1 through x4 or X and it computes a boolean function in a natural way and [Applause] unlike with algorithms like Turing machines you don't normally talk about like the time for a circuit or anything the to measuring main measures of complexity for circuits are size which is the number of usually defined to be the number of non input gates and it's somehow well let me just say one more thing the other popular measure of complexity is depth which is the longest input to output path length okay and it started actually a kind of reasonable model for parallel computation so like a good analogies between circuits and algorithms are to think that like the depth it's kind of like the parallel time for the algorithm in the size it's kind of like the parallel work or the total amount of work if you're not into parallelism then like the best comparison between just Turing machines and circuits is like size is kind of like time because you can evaluate a circuit and time roughly proportional to the number of gates and roughly speaking you can convert a Turing machine running in time T to a circuit of size T log T that does the same thing I guess just as one more small comment usually you're allowed like constantly zero and constantly one gates that are like free they don't cost you in size so like you know you can just have like a 1 here okay so let me say just a few things about popular choices for the set of loud gates the basis and I'll only mention three different choices here one popular choice is you know just yet not gates and you get binary and a nor gates okay binary means it has fan in two that's pretty common these are usually called or sometimes called DeMorgan circuits another possibility is you can allow for example all two big gates okay this you can allow like I know XOR of two bit gates and one more thing I'll mention it's similar to the first one but it has not gates and like any fan in and in or gates okay so I guess somehow I implicitly started using it here where this or gate has three wires fanning into it okay and one oh one nice thing about these two cases of they're simple every gate takes out in either one or two bits one nice thing about this case where you're allowed arbitrary fan in is that now you can actually do some interesting computations in concert with constant depth circuits which is like constant parallel time algorithms right you know if you only have constant depth then and your gates only look at two bits then you can't even have the circuit that like looks on all the input bits unless its depth is login well here you can actually do some interesting things like for example add to n bit numbers in constant depth okay so I'll give some quick examples here so if you want to compute the extremely simple function the and function that on n bits well you can do it in size one and let's call these models one two three in model three you just need what did you explicitly have a gate for that okay everybody in model one we just have fan in one and two what's the size and depth of like a good circuit for computing the nth yeah you can make a little tree of binary and gates and do it in size order n and that order log n okay as I said one like sort of similar to Turing machines and actually even more so than Turing machines one nice feature of circuits is that you can be like brutally explicit about their complexity like size and so forth so I'm just going to read you a fact without writing it on the board there's this theorem of Myer in stockmeyer to complexity theorists they proved in 1974 and it was about a certain problem called WS 1s which is the problem of deciding if a sentence in input sentence in a certain form of second-order logic over the integers is true or not I don't even know exactly what the definition is but it's some problem about logic whether a sentence and some form is true or not it's a decidable problem it's not like it's an undecidable problem or anything it has an algorithm but they showed that any circuit a actually in model two that solve this problem on sentences of input length 3660 needs to have size at least 10 to the power of 125 so you know you can actually prove explicit complexity results are kind of dramatic like that let's do one more little example just because somehow I got real excited about these two problems of palindromes and sorting starting a palindromes on circuit model so let's say we want to solve palindromes with the circuit what would we do well this is quite easy as well let me assume just to be simple that like n is even so even this is sort of going to be like implementing the like the actual algorithm where you compare the first in the last bit in the second and in the n minus 1 bit and so forth so one way to do it let's say is let me first on the side say that like if you want you can build like an equality gate let's say we're working in model one if you want you can build a little equality gate for two bits it's a little confusing by doing let's see let's do the or of either two bits are equal if either they're both true or if they're both false okay so there's some little gadget come in two bits using end or nut gates that checks if these two bits are equal or not or free and model two you can just be like oh I have an equality gate and then using this gadget we can check up a string like x1 to xn it's a palindrome by doing the following well I just wrote the bits in a funny order here cuz I'm gonna check if these are equal in pairs and then I'll actually let's say I'm working in the third model because I want to do the following I want to and these all together okay so here I use the power of unbounded fan-in and this solves palindromes with a number of gates that's gonna be like I don't know five n ish but anyway this will have size order n and this will have gaps maybe I should mention one thing another thing usually negation gates are also considered free in this model actually if you have and or and not gates you can actually push all the not gates down to the bottom by using like demorgan's laws and then one star at the bottom you kind of can just pretend that the inputs are like x1 through xn together with the negations of x1 through xn then you only have to worry about canada and and or gates and it's kind of convenient so in this case if you do that this will look like a layer of and or an so it'll have depth 3 this is another example of how you can solve you know a non-trivial problem in constant depth in this model where you have unbounded fanon and in or gates yep thank you yeah say might know how to do it in depth - don't think about it any solution here was a trick here we did this equality gate by doing an or of ends but you could also have done it with an and of ORS right built a tiny little CNF instead of building a tiny little DNF and then if you'd use that here these would all would be like ands of ORS but then you have ands feeding into a NAND gate so you can just merge them all into one giant and gate so you'd have like one giant and gate and some ORS at the bottom that's a little trick with circuits you know if you have like layers of the same type like all hands are all or as you can merge them together okay so now one thing about circuits which is like a little bit annoying but we should talk about it these are kind of contradictory I'll just put this here that we should talk about it is the way circuits are they're only designed to take inputs of a certain fixed length right I mean have this this circuit has like two n inputs whereas for like Turing machines or like a normal algorithm you assume that your input is just any old length so if you actually want to solve a problem where the input could be any old length you need not quite just a circuit but you need something called the circuit family and a circuit family is nothing more than like a bunch of circuits one for each input length where CN n has n inputs and such a circuit family you know decides a language hell or a problem l if you know the nth Circuit decides the language on like ends drinks okay so it's kind of annoying if you want to talk about the multiplication problem you need one circuit for multiplying or you want to talk about solving the palindromes problem really should have one circuit for solving palindromes on length one a different circuit for solving on length two on like three on like 4 and so forth ok so just because it's going to come up later i want to tell you the names of some different classes complexity classes classes of problems that are defined by how well circuits consult them and this being complexity theory these complexity classes have like terrible names you know analogous the terrible names like you know NP and L and all those other horrible class names I refuse to even tell you what the letters stand for it's so bad so the first complexity class I want to mention is called AC zero and this is all the languages or I'll gonna make decision problems decided decidable by a circuit family the main thing is constant depth order one and size poly n ok here I'm talking about being in life model 3 where you have and and or gates of any fan in so all the problems that these are all the problems that you can solve sort of efficiently in terms of work you know polynomial amount of overall gates or whatever but in like constant parallel time okay so these are like very efficiently solvable problems they include things like palindromes and although it's not quite a language that includes things like adding 2 n bit numbers can be done in this class a bigger class is called NC and this corresponds to the same story but poly log and depth and still polyanin sighs okay this kind of corresponds to problems that can be solved pretty efficiently in parallel so still like polynomial total amount of work but like you know wall time parallel time that's like poly log n yep okay once you get to here it doesn't matter whether or not you allow arbitrary fanon it's an excellent question I thought about this very question when making my notes it doesn't matter anymore because let's say you have a solution like this poly log and depth poly n size using arbitrary fan and and gates and then you're like you know I'd rather actually work with only binary gates just to be simplified yeah exactly you can take you know a law a big and gate with a fan in whatever poly N and you can convert it into a tree a binary gates and this will add that most like a log in factor to your depth and at most apology size to your circuit size yep yes it stands for Nick's class that's right good old Nick Pittenger and the last one I'll mention it's called P slash poly and this just means poly n sighs okay so things that can be solved quote-unquote officially with circuits and just to compare this with Turing machines its effect that it's proven by Pippin sure and Fisher like I said Pippin Jared Nick of Nick's class so maybe he deserves to get his name on the complexity class multi-tape turing machine that solves a problem in time I think I mentioned this actually T of n it's like stimuli ball by circuits of size roughly T of n it's order T of N log T min okay this is why there's sort of like a like an analogy between circuit size and algorithmic time you might ask about the reverse like okay conversely let's say I have a circuit of polynomial size does that mean let's say I have a circuit family of polynomial size that decides a language does that mean that there's a pawn real time algorithm for deciding that language and the answer is no but due to an annoying reason which I'll now discuss so it's the reason known as uniformity and it refers to this like weirdness that like circuits circuit families have like a different circuit for every input length and potentially that can allowed you some weird stupidities I bring this up because it's like an important point in complexity theory and it'll come up mildly when we discuss quantum computing so here's a fact to illustrate the the annoying weirdness I'll call it an exercise for you the halting problem is decidable by circuits decidable by circuit families of sighs I don't know tilde two to the N and that's unsatisfactory when you you know remember that Turing prove that the halting problem is undecidable it's kind of disturbing to think that there's a family of circuits that decides it but the reason is just you know every boolean function at all mapping n bits to one bit has a circuit can be computed by some circuit you know you can write down the whole truth table for the third circle and make like I don't know even a DNF that computes that truth table so in fact Shannon Claude Shannon in 1949 showed that like every function on n bits has a circuit of size in fact to order 2 to the N over N so around 2 to the N and therefore like in principle there exists you know the halting problem when restricted to the inputs of length 100 is just some function mapping a hundred bits to one bit so there exists a circuit with a hundred inputs that decides a halting problem on one hundred bit inputs and you know size 2 to the 100 or so and there exists one that solves it on in one hundred one length inputs and so forth and so on so there exists a family of circuits that solves the halting problem but this is kind of dumb right because now you see like well just saying that you know a circuit family decides a language if these appropriate circuits exist it's kind of stupid because like you're like ok please tell me these circuits so I can use them to solve the problem and you know their definitions responses like oh I only promised you they exist I'm not telling you how to get them so you can define that stupidity away and let me mention that a definition one says that circuit family CN is uniform if just there exists an efficient algorithm like a Turing machine over them or I don't know a Python algorithm or whatever that run sometime Paulie no.4 outputting the end circuit okay what it means you should have some algorithm that you tell it 100 and in poly of 100 time it prints out the circuit of length 100 okay which means you know like you have an efficient way to get the circuit and so if you always like insist that your circuits have this uniformity property then there's a real equivalence between circuit size and algorithmic time for example the poly sized circuits that are also uniform decide the same languages that are decidable in polynomial time it decides it's P yes oh yeah it actually depends on who you ask whether like when you write AC 0 does that mean uniform AC 0 or just non-uniform AC 0 I think a majority of complexity theorists would vote that it means non-uniform but since it's you know only it's contentious you should just say when you're making you're reading your paper which one you mean it makes a difference ok so I sent one reason I bring it up is actually when we talk about quantum computing and then one lecture on that it's extremely convenient to define quantum computing in terms of circuits and not in terms of like quantum Turing machines but then you want to make sure like you don't suddenly have like Oh quantum circuits can solve the halting problem for this dumb reason so now I can say oh they have to be uniform as well okay again as mentioned one nice thing about circuits and I mentioned this with that like you know lower bound about the WS 1s logic problem that requires size 10 to the 125 on inputs of length 3660 is that it is possible to sometimes show lower bounds for our circuit size and try and prove lower bounds on circuit size like the number of gates you need to solve some problem on a circuit was very popular in like the 1980s and then it kind of fizzled out because it turned out to be pretty hard after all yep ah the question was whether you know you can get some like annoying examples like this if you strict yourself to poly n sized circuits yeah you still really want to have it for example the halting problem in unary if you think if you remember all your complexity theory definitions I'll just bring it up is also undecidable by throwing machines but it has order n size non-uniform circuits so you can still engineer these idiots is even with the poly size restriction it's a good question ok so let me just mention to you some facts that are known one was proved by Shannon not only did he show that every in 1949 every function and bit function has a circuit of size order 2 to the N over N he showed that there exist functions on an bits that need this size let me just say needing circuit size Omega to the N over N ok so it's a non constructive argument it's a counting argument it just shows there exist functions boolean functions that require exponential size circuits so you know back in the 80s are like maybe we can find an explicit function quote/unquote explicit function that requires exponential size circuits like maybe we can show that solving stats on length and inputs requires exponential size circuits and in general like explicit usually means like in NP for example and turns out that's really hard people work super super hard to prove circuit lower bounds and here's the best that they could do for its general explicit functions this was proved by you wanna and more zoomy and 2002 building on a result of lucky Shiraz from oh one there exists a language L so like an explicit problem in fact it's even in P so it's not like sad or anything needing sized circuits of size 5 n minus little of n and this is when the basis is you know not and binary and and or and like that's the world record for an explicit problem best circuit lower bound five times n minus little Levin so it goes to show that yeah and complexity theory proving lower bounces pretty hard actually there's a very exciting result from 2016 but I find golovanov Hirsch and kulikov they consider the exact same problem but in the gate set where you can have like any binary gate so this even a more powerful circuit model there's more kinds of gates so it's even harder to prove circuit lower bounds and they got at least three and one eighty six and - little oven so I put those up as like a little joke to illustrate that proving circuit lower balance is very hard and one last thing I want to mention is you can consider the problem of sorting with circuits actually imagine sorting just single bit numbers really that's just counting the number of ones and the inputs and then outputting the count in unary and some special cases of that are outputting the least significant bits which is like are there an even or odd number of ones in the inputs and that's called the parity function and also outputting the most significant bits telling if there's more or less than half of the bits are one that's called the majority function and these are proven to not be in AC zero by first sex and sips are in 81 and then succession of improvements culminating with hosts Ida and eighty eight and another example is multiplying two n bit numbers okay so these are actually showing that these cannot be done by polynomial size circuits if the circuits have to have constant depth so it's some kind of lower bound with the constant depth is an extreme restriction you can compare this with adding to n bit numbers which actually can be done with a constant depth circuit\", metadata={'source': 'zrr9v8kmMGs'}),\n",
       " Document(page_content=\"questions so now I want to tell you about the third model of computation which is the word Ram model and as I said before these two models circuits and Turing machines are more beloved by complexity theorists but if you're a person that studies our limbs for a living you use the core code standard algorithmic model which for some odd reason they never really exactly want to tell you in basic textbooks but it's called the the word Ram model it's some kind of like model that sort of looks like you know C or modelling like C or assembly language or what-have-you so - like literally formally described it takes a while which is why you know it's not as pleasant as Turing machines but you could basically describe it in an understandable way so here in the word RAM model memory is not just bits but it's divided into words of W bits so like on your classic x86 you know chip think of like W is like 64 like it operate something like 64 bit chunks and for size and inputs you always assume that W is at least log in now the first time you see this it seems very stressful and weird that like oh what you assume that like the size of your word like your hardware actually depends on the length of the input to the machine it would like the problem that sounds weird but I claim that it's actually not weird because first of all like you do it when you study space complexity if you say oh this algorithm takes you know order and space you're kind of implicitly considering the case that like you might need more or less Hardware depending on the input size and the main reason you assume it is that you know when you actually want to like model like normal computation like our palindromes algorithm like for I equals 1 to N check if X is the same as X and minus I plus 1 and plus 1 minus I you know you want to assume that like I is just like you know it's like an int data type and see you know it's just like one number it should somehow cost you like space one like it should be an integer and like that's the kind of model we want to get into we don't want to say that like oh when I do I equals I plus 1 is storing a number between 1 and n so it's really log n bits so then maybe incrementing a log n bit number cost like log n time like that's the kind of like turning machine annoyances that we're trying to get away from right we want to be able to say like yeah I equals I plus 1 that's like one basic step cost one done but you know that's fine and this we're going to do and you want to set up a model where you know this via variable I can be stored in like one word but the typical thing like a variable is doing is like indexing into the inputs and if the input of size n you want it like I assume that you can store like a pointer in one word and that kind of means that your word size should be at least log in so even though like maybe it seems like weird at first like once you just accept it and get used to it I mean you can just take like every results about the word gram model it's starting up by saying like we assume that W is at least log in okay okay and as I said sort of the point of the model is that like costs time you know one by Fiat to do like a basic operation on words this is basically kind of how they ended like you know a classical undergrad algorithms textbook right then we really mention this but you know they say like okay you can add two integers in time one because that's like a basic operation but we can try to be a little more careful so here are actually to be honest all the basic operations like addition and subtraction so I usually look mod two to the W okay you're allowed to do that in time one you know bitwise and an or and not you know like the bar and the end and the tilde and C or whatever and left shifts and right shifts well you can just specify any amounts between you know one in W and say you know left shift this word by that number of bits or right shift it and uh one more possibility is multiplication but let me put a question mark here because I will come back to that because multiplication is a bit less basic than these ones so we have to think a little bit about whether we want to allow it yeah yeah comparisons see yeah so for example I guess one can add like you know test at zero or test of greater than zero so noun comparison is like a subtraction plus it yeah so comparisons you know other basic ones may be writable in terms of these many obvious thing I'm missing yeah Oh things like cosine or calculations no in fact I'm going to talk about that next class like more advanced calculations so in particular if you want to model real numbers here you better do it by hand by like storing like the numerator and the denominator and stuff like that so these are all these are like integer operations and then I mean there's instructions that have you know kind of I don't know like assembly plus Ram instructions these all take time one is well you know you have your like conditional jumps or whatever it's basically like modeling assembly language conditional jumps and in direct memory access by which I mean you know if you're storing like an integer in some word then like time 1 you can get the like say the integer is called ah you can get the eighth word and pull it out into whatever a register in time 1 in fact I guess it's this property that gives the name RAM to the RAM model Oh actually on this subject if W is much bigger than log n I'll talk about that in a second like let's say W is log squared n in your model then with these indirect memory accesses like in time one you can index into like super polynomially many different memory locations that's kind of considered tacky like and you shouldn't don't do that is what they suggest in this model I mean actually generally when you're you know if you're studying algorithms you probably are carrying about like polynomial time algorithms in fact you probably carrying about the difference between you know N squared log and time and N squared time so like it's considered not cool to use more than polynomial space in general you should probably try to stick to like linear space so for example you know we can take the problem of given an array of n numbers add them up ok this is really like algorithms 101 but just to like make a little commentary about it you could okay total equals zero for I equals 1 to n total equals total Plus you know a I okay if the input is a an array of n words okay and so finally in this model you have the situation that like everything is as you hope it's like order and time and this is order one space so you need like one word to store like I and like you know incrementing I implicitly in this loop takes time one and the go-to to do the loop takes time one this addition actually will come back to the addition in a second and this memory access takes time one one thing you may notice right is if you're getting really stickler these words if there are W bits are storing integers between zero and two to the W so the final sum could actually be n times two to the W which would need the log of that is like W plus log n bits so that actually wouldn't fit into one word W plus log n bits although it would fit into two words because we assume that W is at least log in okay so extremely technically like this addition is not just like adding two words together but you're like secretly implementing total by two words and like keeping track of that like carries because the final total could actually need to fit into two words but like that's a very low-level observation that like people think about once and then never mention again okay any questions yeah Carrie no formerly you'd allocate I guess like one word to store I and maybe two words to store total cause total sexual value of total as I will end up being at most n times the maximum value in AI which is like at most n times 2 to the W so finally the number of bits needed for total would be like log n plus W which is less than 2 W so that can fit into two words yeah exact appoint we're counting the number of words when we talk about space yeah excellent uh okay now in this model you don't make any assumptions about W other than it's at least log n I personally like to also assume that W is order log in because I find it kind of weird otherwise I mean I kind of think it's nice that your integers aren't gonna be numbers between 1 and poly and it's hard to imagine that you would I don't know want to use a different case you know in real life maybe W is 64 and so yeah and it's gonna be some number that's I don't know at most 2 to the 64 so yeah I like I think most people kind of vaguely assume that W is proportional to log in and they mainly work with that there are like some hardcore algorithmic lovers people who love algorithms and all the details that even like study they like like to focus on a model where W could be anything and you want a running time that does not depend on W and this believe it or not is called the trans dichotomous man model mention it partly for a joke but actually just actually people study it this means like running times only depend on n and it worked for Locke all W and this is an example right like even if the word sizes here are like square root end and it's fine like you know you would still you're adding up square root n bit numbers and you're storing them in square root and bit words so it doesn't really depend on W here but I'll see some examples in a second where it makes a difference the next detail commentary I would like to make is let me get back to this issue whether or not you allow multiplication and this you know this would be you know basically instruction that like in time one takes two words and you know does gives you the answer to their product which takes up two words I mean takes two one-word numbers and gives you the two-word product basically it's up to you whether you want to allow it you know if you're going to write a paper where it makes a difference you can say I'm allowing myself multiplication or you're like I'm doing it without multiplication like a champ so the pros and cons are for pros it's kind of realistic I mean honestly like the chips these days like they have a multiplication instruction which is very fast so that might seem fine and it turns out having multiplication is very useful even for problems that don't seem to have anything to do with arithmetic as we'll see I suppose one con is that if you want to be kind of principled about like what basic instructions you're allowing you know it to be time one I think a very nice and natural principal thing to do would be to allow any AC 0 operation you know constant parallel time operation I reminds you circuits it seems kind of nice and all these things like you know bitwise shifts adding two numbers subtracting two numbers and zan doors these are all in AC zero and so you could be like that's very nice my official model is and this is sometimes called the AC zero word Ram model or the practical ramp word Ram model like that's my model but you know some complexity theorist showed that multiplication is not an AC zero so then you wouldn't be allowed multiplication but as I said really at the end of the day it's up to you if you want to allow it or not okay so uh kind of explore the interesting aspects of the word RAM model I want to come back to the more sophisticated problem we talked about at the beginning which is sorting and what's actually kind of amazing is at the end of the day if you're really into like all the fine-grained aspects of algorithmic theory the best running time for sorting is an unknown research problem so we'll get to that so let's talk about the task of sorting integers and let's assume for the moment that we're in the waddle I like where W is like theta of log n ok so you're sorting log n bit integers now let's warm up by assuming that the integers you're sorting for now are actually literally log n bits in the sense that there are numbers between 0 and n n minus 1 [Applause] there's a very easy order and time sorting algorithm you're given n numbers between 0 and n so anybody you want to suggest it yeah Oh we'll get there it's even easier than that it's kind of like if you're sorting one bit numbers to see sort of the analogy yeah yeah I think what you're getting at is what I would call counting sort so when you do a counting sort you allocate an array for each you know he you know between 0 and n ok so that's order n space exactly n space and then you just go through the list and every time you see a number you increment the count for that key so like well count the number of occurrences of each number that's all around time and now you know the count for every number so it's pretty easy to reconstruct the sorted lists I mean you actually just print out 0 a number of times equal to the number of copies of 0 that you print out one a number of times equal to R of copies of 1 and so forth so to get the final sorted list you do this for I equals 0 up to n minus 1 for T equals 0 up to you know the counts of I let me just say print prints hi right and that will print out the sorted list and this is actually still time order and because if you think about it you basically touch each number in the input list like one time so it's proportional to the number of numbers in the list so that's great the overall time is order n which is awesome it's better than n log n there's one thing which is not super awesome though which is that the space usage here is also order n theta n just kind of like a semi frowny face because you have to allocate like an additional memory and maybe you're like okay space RN is not so bad my input array is also of size and but like imagine you're sorting three log n bit numbers in the same fashion well these numbers range between 0 and n cubes so if you do the counting sort you would need to allocate an array of size and cubed and that's pretty gross right like to use space and cube to sort some numbers ok and this these could easily you know this could easily be your word size 3 log n that's pretty gross in fact it's not even clear you can still do this in time order n right because to do this last step you want to like efficiently count skip over the counts that are zero and if they're like I'm cute I mean if you just go through them like naively it would take you n cube time to print it so then maybe you're like need like an efficient data structure to like remember with the non-zero counts R and then we get like super crazy I'm not even sure you could still do it in order n time so this looks good for like linear time sorting if you know you're sorting numbers between 0 and n but may you're sorting numbers between 0 and n cubes but there's a building on this there's like a better solution and the better idea for sorting which maybe you've seen before is called radix sort and the way radix sort works is the following first imagine you take your numbers and you first sort them all just in terms of their least significant bit and then you do another sort on the second least significant bit and then another sort and another sword until you're finally doing another sort based on the most significant bit if you think about it for like one minute if you never seen it before if you think about for one minute that actually correctly sorts the integers as long as it's maybe a stable sort that doesn't reverse two numbers that are tied okay so it takes a little thought and that would take time order and W because this is how many sorts you would have to do W one for each bits in your word and then each for each one you're sorting one bit number so you can definitely do counting sort in like order and time and space order one so that's good yeah this is required space order one I suppose well you know W is log in this is like n log n that's fine but we're trying to get order in here but we're know the thing you can do is actually instead of sorting them like bitwise you can sort them by twice right you could take sort them by least significant bytes secondly significant bytes and so forth so doing the bits and groups of n or eight and that would be mean you're counting sort would be sorting numbers between 0 and 256 which is still fine I mean that's fine counting sort will still be order n time starting such numbers and you'd say factor eight in the number of overall source you had to do that's good of course you should take that to the next level and sort not just by like chunks of eight bits but by some parameter K bits so in general if you use if you use this idea with a radix as it's called of K bits where K is you know at most your word size W then when you're doing your counting sort you'll be sorting numbers between zero and two to the K so you'll need this order two to the k space a but you'll get time which is order and W over K okay in particularly now you can take K to literally be like 1 times log n literally log base 2 of n what you can conclude is that radix sort over all sorts and words and order n space which is fine and order so I'm running out of actual space here and times W over log n time and that's nice because in the like you know usual setting I prefer where W is maybe 10 times log N or it's like order log n this will be linear time in linear space okay so this gives a very pleasant and basically you know ideal sorting algorithm order n space order n time for any word size that's at most Big O of log n any questions about that okay so I want to just read to you I won't write anything about read to you some improvements to this so in like a normal model or what I care about when W is order log n like this is fine but it's not trans dichotomous right because the running time depends on W so some people might ask well if W is log cubed n then this running time is n times log squared n which is worse than merge sort okay you can always new black merge sort or whatever and log n time no matter what W is so people are like oh can you get like can you still get order in time if W might be larger than log n that's the question so I'll now read to you what is known about this so in 1974 van M Tobias invented van m to boast priority queues and using them you can get n times log W time and in particular this is n times log log n time unless W is more than poly log n which is pretty weird so that's pretty good and then Kirkpatrick and Russian 84 got like n times log of W over log n which means it's always at least as good as radix sort okay then in 1990 Fredman and Willard got a trans economist result that just works for any W at all and it's n times log n over log log n so it's like a little bit better than n log n no matter what W is and in particular it heavily uses the multiplication instruction though so that's a you know arguably a downside then in the 90s anderson hager up Nelson Drummond invented signature sort which gives you an order in time whenever W is more than log squared n which basically covers all okay almost all cases it doesn't use multiplication but it's a randomized algorithm and then Hahn in 2004 gave a transect coterminous result and log log n for all W deterministic and no multiplication so all those Perfect's except it's n log log n and the last result to mention is Hahn and Thorpe from 2002 they got n times root log log in for all W but it's randomized and it uses multiplication so long story short we still do not know if you can get order and sorting in the Translate columnist model with or without multiplication or randomization so there you go sorting still an open problem okay I'll see you on Tuesday\", metadata={'source': 'yMcgAzeWDi0'}),\n",
       " Document(page_content=\"all right well we can get started um thanks for coming we'll see if there's uh maybe a better time to hold these uh yeah a lot of people can't make it i saw you send out the email yeah well anyway that's for now uh do you have any questions about the clown yeah i guess i was hoping there'd be other people asking questions first i think i'm doing well with the first two and okay i haven't thought about the third one that much um so i don't think i have any like great questions for the third one yet but if our only option is to just work then i guess we have to do the third one uh well yeah sure we can do it or we can think about it uh i mean have you looked at it yet yeah so i have looked at it um yeah i mean i did have like some very initial thoughts but um nothing too crazy so it's all one worst case right i think i saw in piazza that someone asked if we could do amortized but it should be worst case uh yeah that's the goal okay um uh yeah so i think well one obvious thing is just that we have to do something in initialization because um like no matter what if someone reads a word immediately after initialization no matter what they query we should be able to say that it's all zeros that's true yeah so somehow we need to set something up so that any index that's quarried says zeros um and yeah i i didn't even really know how to do that in constant time um so i guess that would be a good place to start oh yeah we want to make i want to like initialize an array well okay let's just remember the question somebody's going to come along and be like i want to initialize an array of length n and then maybe i don't know what does it say like right oh set whatever set you know spot five to number ten and then maybe read number three and so forth and so on but uh you know in this we're matching this model where like uh you know the memory it does not come he fills with zeros but it's got like some junk in it like 25 11 15 36 maybe it's got a 0 2 18 um yeah so i mean i guess if you don't know what you're doing um somebody says like oh initialize n well you can just block off some n amounts of space and then set position number five to ten i guess that's no problem either you can just be like all right position number five you are now ten but this is a difficulty i guess read number three because you know since you haven't set number three yet it's supposed to be zero but it's got this junk 15 in it yeah no you're sad like we could in the initialization we can like block off just a constant space of memory off to the side let's say it's like a dummy okay memory or something like that and that will at least tell us that if we so yeah i guess even that would have junk in it at the beginning so but we can edit that immediately um and then we yeah so we would have that marked or something and then in that case no matter what we read we would be able to at least say that it's zero zero zero uh to the w or whatever um um [Music] yeah well wait what did you have in mind or well actually i guess i don't even know how we would mark this but i was thinking like if you put like negative one in there or something like that um sure that's fine then we read and so now we read from this slot whenever we do a read um oh when we do a read okay you're gonna look in this spot instead yeah and so like yeah not that this is gonna end up working but if we read a negative one then we would know that like hopefully we would somehow know that whatever we're trying to read um uh should be all zeros true but then like we need to somehow use that index now and say that in the future um yeah we need we need to keep track of which ones we've visited somehow yeah i know the hint says a stack but i didn't really see how to use that yet yeah i don't know maybe maybe maybe it's just suggestive of like you just need to if you're at the level of computer science understanding you know what a stack is then you should be in okay shape i don't know yeah and this is a tough one to give hints on because i know it's like hence without just giving it away i can just give like generic hands though i get for every single problem sure the good news about that is like you can then you can give yourself the same hands later with that you don't even need me um i mean what for any kind of like algorithms problem like my generic hint is like how would you do it if you were like a human and you had to do this like let's go alright are you ready i'd like you to initialize an array of length 10 for me [Music] now set spot number one to five are you still doing it in your head yeah or you can have a piece of paper and you can have like you know uh now uh read spot number three uh so do you want me to like say it or not sure i guess you should tell me what's in spot number three okay well um i would like to say zeros but okay i would have to say 15 with our current setup well i mean no forget what our current setup just i mean think of how you would do it that's like the generic my generic advice how would you do it you're like the cleric person keeps coming to you saying like read the spot set the spot um like i mean that's the thing right like okay somebody comes in and they're like all right friend i'm gonna be like making all these like read and set commands to you um and yeah like maybe maybe you have like you know you have some like graph paper and you're like oh all right like i have this graph paper i'm all ready to like write down their things into these like slots now you're not gonna like preemptively write zero on like every single square in the whole page like that would just be so tedious i guess like luckily graph paper typically comes initialized to blank and like blank is like oh blank is so special i know like it hasn't been used before but maybe like i don't know some annoying person is like pre-written like numbers on it 25 32 112 or whatever and like you have an eraser and maybe you got like a couple of pages of graph paper that you're ready to use how would you do it like uh like this is all the tools you have like your own poor brain which can only remember like a small number of things you've got like all this like graph paper that somebody like stupidly filled numbers in and you have like your pencil and your eraser um yeah and somebody's like okay i need you to hold on to me like 100 numbers for me like in particular i want to store uh like 17 and slot uh five and you're like okay i could go over to slot five and like erase it and write seventeen i mean that would that would be my first instinct but then if they're like oh tell me what's in slot 64. well you're like well i kind of know like you haven't touched it yet so i should i should call it zero and maybe while i'm at it i could go over to slot like 64 and like write a zero in there hmm after a while i guess you know if they're like if they give you like 20 re commands and 20 set commands and then maybe they like i mean how are they going to be sneaky right like maybe they're going to like tell you to read something you already set and at that point you better be like oh i'm not gonna like say like that's just a zero and rewrite it like after somehow remember that you already said that one before yeah exactly so keeping track of uh it is tough let's see yeah uh don't think about it like you're you're there you've literally got the graph paper and somebody's already used in front of you um hey we got somebody else on the call by the way uh do you have any questions about the course yeah no i'm i'm i think i like have kind of the intuition i've just been working on the same problem but like um but yeah i mean i was just here just to see what you guys are chatting about probably um all right did you solve the other two problems i think some intuition for like one 3.1 but i mean my understanding of like three point one was just like i kind of like did it just using algebra and it worked out finally but i wasn't sure like i mean i was trying to put like a curve like there were there was one way where i was trying to put the curve above like the indicator function or or the step function um but that didn't work out so instead i just like use properties of math like problems there's like properties of like like x squared is greater than mu like just like squaring both like i'm normalizing x adding one squaring it and then showing that that's less than equal to one over t squared plus one and that kind of worked but like um i wasn't sure if like there was like a more intuitive way of like tackling it maybe that like using a step function is because does the um does does it work to potentially take a step function so so for the chebyshev inequality we took this like double-sided um step function or something and we put it um and we put a para parabola um underneath it right and then we found that like and we did like the mark called inequality of the parabola and that kind of worked and then my intuition was that for this one you could you could like for instance the left side step doesn't exist so you could potentially move that parabola a little bit to the left and then get some um and then get some [Music] benefit from there or you could like essentially push it but by by not having the left inequality you could potentially get that t squared plus one instead of just t squared does that make sense intuitively i don't know yeah so i mean let's see let's do like a special example so let's say we have x and it's mean let's do our favorite set up mean zero variance one yeah and i want you to prove probability that x is at least one is at most something right let's let's put the best possible thing we can here um so let's see like for example if we use chebyshev right now normal chebychev would tell you like the probability that x is at least well the absolute value of x is at least while this is like one standard deviation yeah like one times one is at most one over one squared which is one which kind of sucks um but maybe we can do better so yeah i guess the method is like we have this function this yellow function which is one right one and then it's like this and it's like this uh let's call this f of u my little x's and my big x's might look the same so it's called u yeah um so this is literally exactly a expected value of f of x yeah um okay and now yeah the idea is like okay from this you know that like expectation of x is zero and from this you know that expectation of x squared is one one um yeah you might call this like well let's leave it like that so the idea is like maybe as you say you could put like a parabola yeah above this well that should have been intersecting yeah well presumably it doesn't have to but like presumably it'll be like smartest if it does exactly exactly um this looks like a u squared plus b u plus c and let's call that g of u and okay in order to make progress we need like f of u is less than or equal to g of u all u yes and probably is equal to e g of q at one one right i mean presumably like okay as long as this is true then the following deduction is valid right right g of x and then this is good i mean now you're like oh thank god we chose g little g to be this quadratic because we can actually compute this this is expected value of a x squared plus b x plus c and then we know this is like a expectation of x squared plus b expectation of x plus c and this is one this is zero so we get a plus c yep yeah makes sense so now we actually have an optimization problem right right we have like this optimization problem on the side um do we want them yeah what it what is the optimization problem we want this to be as small as possible so we want to minimize a plus c subject to well we need this constraint so we need that like f of u is less than or equal to g of u really for all u in the real numbers right a bit annoying because it's like infinitely many constraints yeah but we could take i mean uh you know this is like if you ever heard about linear programming which by the end of the course this is like kind of like a linear program with three variables like a b and c and the objective function is a linear combination of the variables and like for every single value of u like this gives you a constraint so like for example you could take u to be zero then f of zero is zero so you get the constraint u is less than or equal to and g of u zero u zero is um c so you're like oh i got this constraint c has to be not negative i see delete this graph paper you could be like all right um i don't know there's like infinitely many constraints here we could choose okay like u to be one that tells you that like okay one is at least um g of one of one just like a plus b plus c yeah uh and you can choose u to be negative one and that gives you like negative no that gives you zero what's f of negative one yeah less than or equal to uh a plus minus b a minus b plus a plus c right yeah i see now yeah so you're like oh c has to be non-negative the sum has to be at least one a minus b plus c has to be at a plus c oh actually you see like a plus this is equivalent to a plus c is at least b that's kind of interesting actually because you know like oh that's what you're shooting for a plus c so whatever it is it has to be at least b right um [Music] and actually this means that like this is actually so this a plus b plus c we know that like b has to be at most a plus c so in a valid solution this is going to be less than or equal to a plus a plus c plus c yeah is 2a plus 2c so actually therefore if you divide by 2 you like learn that like a plus c has to be at least a half oh oh this is like a miracle i didn't expect that this is gonna work out so conveniently um but in particular that tells you that like oh well whatever you're shooting for here it's not gonna be better than half so you might wonder if it could be a half now this is like okay so you could keep going like this you could try to choose like more u's and get like more constraints um on the other hand at the end of the day like you kind of you can be a bit more smart about it like i mean think about like what properties this green parabola is going to need to have oh i mean ideally like if you want to be strict then it should uh if you want the inequality to be tight we want it to be equal to f of u at some point like at maybe the one point and maybe it like touches zero at the other point so that we can like like get some point there because that would like make it tighter as tight as it could be pretty clear that like you kind of goofed if you didn't arrange for the parabola to like meet this point here and also probably you goofed if you didn't make it touch zero at zero because otherwise you could probably get a better parabola right this might already give us some clues so okay so like touching uh well one at one that's this point let's suggest like a so just like a good a b c we'll have well a plus b plus c equals one right now what about this one we want like sort of the the what are they called the apex or the vertex or something the vertex of the parabola to be at zero as a minus b plus over 2a should be oh you know the stuff off the top of your head that's great what uh sorry what do you think i think it's minus b over 2a the minimum right like the you're saying the minimum is minus b over 2a over 2a yeah just just using calculus i'm using calculus wait this is like the u coordinate like the horizontal coordinate of where the minimum is i think so uh maybe i think it's maybe b over two just like i'm just doing the calculus like on the fly here yeah yeah yeah yeah let's do the calculus you can also do a quadratic formula let's do calculus so like the minimum occurs if i take the derivative i get like two a u plus b equals zero so like u equals as you say minus minus two a that's where the minimum occurs right right and then if we plug that in i mean i should it's like embarrassing not to know this off the top of my head but like whatever uh if we plug it in then like the minimum value like actually is yeah a well b this thing squared is like b squared over four a squared right minus a plus b so like minus b squared over 2a plus c right and that should be equal to zero this should equal zero it's not looking at oh okay so we like multiply both sides by a goes like this crosses that let's multiply both sides by 2a 4a uh 4a let's multiply both sides by 4a and we've got like b squared minus 2 2 b squared plus 4ac equals 0. hey i've heard this expression before b squared equals 4ac okay yeah so and we want this to be zero oh no sorry that is zero so if we okay yeah if we select the discriminant or something if this is true then the vertex will have parabola or the parabola vertex at zero right exactly so we want this and we want like a plus b plus c equals one right and we also now want to minimize a plus c yeah not so pleasant because it's not like a linear program because it's like b is got a square in it it's probably not so bad though right because okay so from this one we know that like we want a plus c equals one minus b i'll just rearrange so a plus c is the same as 1 minus b equivalently we want to minimize 1 minus b i don't know if this is helpful but it just occurred to me can you solve this i mean maybe this is a better way but could be oh yeah you could just just could you solve it like using like a lagrangian or something like that like or would that not work here because yeah both are convex i think so you could just use the convex constraints and this might be i don't know i don't know if that might work but i i assume like if i put this into like a computer it would solve it for me as well is or is that wrong uh no probably and actually one like good thing is i mean if you ever like writing a paper or something yeah you can like solve this however you want like by any like questionable means and if you like get some good values of a b and c like you're not really obligated to say where you came up with them you're just like behold i've chosen the following you know g of u to be like blah u squared plus blah u plus blah c or whatever and check out my proof oh i see okay so that's cool i mean we could try typing it into a computer and seeing what it says i think though we can just solve this ourselves it's not too bad like so like let's just try to as you say let's try to solve these two equations i mean i feel i feel somehow like b is playing like like a and c are playing a symmetric role so i really feel like saying from the second one b is one minus a plus c from this second one i feel like plugging that into this first one so then we'll get like 1 minus like a plus c squared equals 4ac and maybe this is going to be a bad idea i mean i can think of i could start to think of some tricks and stuff but like let's see if we just try to um solve this in the most simple way nah let's screw it let's solve it with a computer um switch over to my computer [Music] i'll share my screen yeah i also like sometimes you just like graph this and i use like desmos or something to like just play around with like because i know like x squared that has the zero at zero and that there's some like i think it's a that convert that that controls the curvature so you just like play around with that you can also kind of see how you could move them to like still maintain the upper bound but yeah that was just something i played with in the past yeah you can definitely do that so let's see like um yeah a screen here so we had like uh a what do we have like a plus b plus c equals one that's equation one and equation two is uh what was it you have a handy b squared equals four ac yeah yep that's it okay anyway yeah let me like do it like the slowpoke way i mean i could just like command like maple to solve this for me but let me just do the speed it up version of like what i was gonna try to do by hand uh which was like you know if i solved equation one for b then okay oh great it's like b is this so then i was going to like substitute b equals that into equation two okay so now i have like one equation and two variables but it's like quadratic so okay first of all let's just expand this and like simplify it and see if anything good happens not really okay let's go back to here so if we you know okay the most brutal thing to do would be to say like solve the following thing for you know i don't know a that's not too bad actually it's looking okay actually i now i mean maybe this wouldn't like jump out at you but like um i do see that's even simpler than it looks like this i think is like this is like a like a perfect square you know this is like uh yeah one plus square root c squared i think yeah um but okay so like this tells you this is uh this is what we solve for a so like our objective was like a plus c right yeah so we could substitute uh okay we got two solutions we could try both of them we substitute a equals solution one into the objective okay and now this is um what you would get so actually um see there's still like one free parameter and it kind of makes sense because i think like there's actually a variety of parabolas that you could yeah yeah that's right yeah yeah um and so like your objective problem is like your your optimization problem is kind of non-trivial like there is like one free parameter to play with and you know by making different choices you'll get like different values yeah and we just want to make sure that we pick the choice that would give us the bound that we care about well yeah give us the best bound so we could just like plot this and like i don't see any reason why c couldn't be anything so just plot it in a nice range uh we're trying to make this as small as possible right okay so it looks like one could save that so i could get as small as one which isn't great because we already know like chubby chev's inequality was giving us one and one is kind of a sucky bound but we all there were two solutions to this quadratic and we only used the first one here so we could try the second one it's got a subtraction in it that's looking maybe better and uh let's try to plot it aha this one's better it goes below one oh yeah okay cool but let's let's zoom in there i mean i think we could probably do this a little bit with our brains but like okay then we look at this and we're like uh pretty sure the answer is a half right right so um [Music] like the uh let's say the objective two is like this nine so honestly probably if you just i don't even know what will happen if i type this but like probably mabel can figure no i guess i can't figure this out i'm sure it has like some commando that will just tell me the answer but um how would you well okay i guess it's a calculus problem yeah um like if you really want to like be completely brain dead about it you could just be like all right i'll differentiate this thing with respect to c and then i'll solve this uh and it tells me oh i should take c to be a quarter right so then i could be like all right let me substitute c a quarter into this objective function and like huh didn't expect that oh wait square root 4 is known as 2. i don't know why he's being really about that thank you maple yes okay this is very promising i got a half and also like you're kind of convinced that this is the best solution right right um so we took c to b what did we take it to b quarter what and uh then a was gonna be oh it's called solution two or something no a was yeah oh yeah substitute c equals a quarter into solution 2. this is a this was the value for oh it's like this this is a value for a just being kind of silly about so like a is a quarter b is c is a quarter and then what's going on with b oh a plus b plus c is one yeah oh so it's zero um okay so cool so it's like saying the best solution is like you know a equals half b equals zero c equals a half and yeah i guess we could say like okay g is uh half times u squared plus zero times u which is nothing plus a half and uh yeah we could plot this okay how do i tell maple to do a piecewise function i guess we can just be like if u is at least one i'm just plotting both the f and g so let's say f is like if u is at least one then one l zero i'll tell the plot g of u and f of u you go from minus two to three and let's plot u g and red i always use red for the plot that's supposed to be above and like blue for the plot that's supposed to be below whoa wait a minute wait that's okay it's correctly above but it feels like there's it missed the zero yeah wait it felt like it missed zero something fishy happened on the other hand wait but hmm but it's still whole words oh no this is not a good solution right because like yeah a plus b plus c is one oh wait uh is it did we not hold the p squared is equal to 4ac though let's see and yeah wait a minute did we go wrong here what happened i thought this is all glorious um wait we chose c to be a quarter oh wait c was supposed to be a quarter i don't know why i started thinking it was supposed to be a half so wait a was supposed to be a half and c was supposed to be a quarter so then uh wait what does that mean for b a plus b plus c is supposed to be one so that means b should be a quarter quarter right it's not doing anything in this command but like so okay that means this should be yeah that's right that's right a quarter times u plus and this should be a quarter what that's still not right wait wait see c was going to be a quarter that looks good a was going to be uh i mean solution 2 was uh what a quarter oh what okay supposedly a should also be a quarter i don't know what i'm doing here okay let's try let's try this yeah wouldn't wouldn't that be that b would be plus b plus c yeah yeah okay let's try this i mean had i done this this looks like as though we're like you know experimentally trying things but we're just trying to like get correct the thing that we did okay that's looking good yeah there you go yeah there we go yeah we did it so the our trick was g of u to be this does this factor right yeah it's u plus one squared over four right and then this gives us the bound that we would want nicely oh yeah because like then we're gonna say we're gonna like substitute like u squared is 1 into g of u and then we're going to substitute u equals 0 into that and we get a right uh great we did it um yeah and then like you know the final proof this thought always isn't like math and then like if you were reading somebody like doing this they'd be like they'd just be like well consider that the step function uh right is less than or equal to you know this uh probably even like well maybe they just do like g of u is this expression yeah and then actually i guess you should prove that technically i wonder if that's obvious i mean you think you have to prove that this thing is the red curve is like bigger than blue right i mean but that shouldn't uh that that's is that gonna be hard to prove because i feel like you can you know that like at like the minimum that that they're equal at like minus one and then you know that like um parabolas are convex so it so obviously it'll be below it at that point and then like you know it touches it at one and then yeah and then that just like works right so yeah that might not be too hard to produce i mean i think since this is like literally a parabola like everybody you can say look the vertex is that it's occurring at minus one and it's zero and it's like hitting this point so it should be fine i mean if it's more complicated like if this was some like weird fourth degree polynomial or something then maybe you'd have to yeah that's right work but i guess it's not so bad i mean since f is like a step function you probably have to divide into cases like where u is bigger than one where u is less than one and then you'd have to prove that two certain polynomials were non-negative and yeah that can be done calculus but if you if you can show this then like then then everything works out and i think you can just use that so i guess obviously i just did this in case of t equals one well you can probably you could probably repeat this whole methodology with like t being a symbol like t yeah like repeat this whole like i'm not sure where like t equals one came in i guess right o is here yeah like you have to put like times t squared plus times t this would give some i think well you could just press all of these things that makes sense cool thank you very useful i feel like i feel like i i had done the math and shown something but i was like this is this is a lot more intuitive for me i can actually see what's going on here you're actually solving like a minimization problem which kind of fits my intuition better yeah if there's time i might show like a similar uh thing i had in mind um but maybe we could i don't know should we return to this uh array problem sure did you make any progress on it i was trying i was like going back and forth between working on it and like tuning into what you guys are doing because yeah yeah approach to um uh yeah well uh do you still have the picture of the graph paper papers yeah at least a little of it yeah so back to what i would just do if i was a human yeah i wait so in the word model we are allowed to like so in initialization i couldn't block off this whole other sheet of graph paper right but i can like jump to any spot in it is that yeah that's right i mean technically you have to like do i mean i guess i didn't say this like literally 100 detail but like definitely have to do all your memory management yourself so like the model is like you just get like i don't know let's say an indefinite number of cells or maybe you could say you get like i don't know yeah two to the w cells but like you get like an indefinite number of cells named like zero one two three and like that's it but like part of the word ram model is like if you have like x is a word um then you can just be like oh like read and write hot spot x and it'll just it'll just do it so like you know for example like if you're literally actually doing you know code which you would never do except maybe maybe one time in your life to like prove to yourself that everything is fine like you know you know maybe in your code you might have like some generic code you have in mind in english you'd have like a couple of arrays and like maybe like some array be like all right like this array a which needs to go from one to i don't know another two like i'm going gonna actually map it to like words you know i don't know n plus one to like n plus n over two or something in memory and so like every time i read and write from a like i have to like you know do a tiny little arithmetic calculation to figure out where it's like laid out like actually in memory um but uh i mean i wouldn't even like even for the purposes of like this problem i wouldn't worry about such like dumb things but like this is what's really going on under the hood um yeah so at first like i was thinking that we could just store like whether we visited things in the other sheet of graph paper but that runs into issues as well yeah that's the natural idea right like if somebody comes along and says like you know write in spot five something then you're tempted to be like you know what on the side i'm gonna like have like another array or something that like remembers everywhere we wrote so like five and then or you know it's wrote in spot five and then if they write and like spot like seven something you'll you'll actually maybe write into spot seven on some like actual array like maybe your first piece of graph paper but like on the second piece of graph paper you'll also be like oh i also put something to spot seven so then like when you get a read like say maybe somebody says like read from spot like 25 okay then what you'll actually do is like first like look through here and be like hey did i ever write into spot 25 because if i did then i'm going to like look it up from the real spot but if i didn't then i'll like i'll i'll return zero and maybe i'll put 25 into here to like know that like okay 25 is cool yeah i guess the issue is like that separate thing we're storing we'll also have junk in it um yeah that's not so bad though because you could say you know it's always gonna go from the start so okay so this could have this even has like junk in it squiggle squiggle squiggle but like if you just um you know have like another variable that says like it's just one variable that says like you know how many uh i don't know i call this page two how many things in page two you initialize this to zero and then when they write five you like stick a five in here and you like update this zero to one when they say seven you can just say like oh i'm on spot one so i can sort of instantly write a seven here and you could update this to two but then the reason this solution does not work well is then you know read 25 you know there's like two things okay you haven't done this yet there's like two things on this page too um but like any of them could potentially be 25 so you have to kind of like look through both of them to see if one of them is 25 um which is fine but if somebody puts in like you know 10 000 rights at the beginning and like you fill up like all the spaces where you've written something here and like this this counter now is like 10 000 in it just fine then like after 10 000 instructions they do some read and you know read and spot like 79 and you're like oh shoot like have i have i touched 79 yet the only obvious way to know is to like start looking through these first 10 000 slots to see if any of them say 79 and um that's going to take you like time ten thousand which it's not working out here so just to make sure i understand uh i was like uh i was just thinking that like when we go to read something we first like jump to this other sheet of graph paper and check whether it's been visited or not but yeah this approach is different than that it's saying um just like throw them in a stack is what you're doing right now great yeah i guess so i guess it's like even like a little high brow to call it a stack because it's not like you're ever even like popping you're only like pushing so but i guess so yeah like i mean it's like uh it's just like yeah like a list or that you're like appending to like all the places that i've touched before yeah okay so that's all this is um and like this is almost i mean this you know initialization of this i don't know list whatever is time one and um rights are also maybe time one like for a right and right in spot seventy five you could just directly write it on like the real array and then yeah then you're like okay one thing i could do is like stick 75 at the end here and like increment this and then that would be constant time which would be good now that might make you feel a little bit queasy because like maybe 75 is already on the list somewhere before so it's not really a problem to like put it on the list again but anyway like you have this difficulty that like reads don't seem to be constant time because like you do a read it seems like there's nothing you can do except like look through this whole list to see um if like 75 is in there or whatever if they want to read and spot 100 to see if spot 100 is in there so you're you're storing the indices that we've visited right not that yeah this is like an initial idea which yeah it's not a bad idea um to like store just in a list like every um array position that we've touched either read or written yeah so i think like another like natural computer science intuition is then like oh well yes you have this problem that like when you need to do a lookup in this list to see you know if they do like read you know in spot like 72 and like you want to know like have i ever read or written slot 72 i should look it up in this list i think a natural computer science intuition is like well it's very dumb to like keep this list it's just like an unordered long list and then when i look into up into it i have to like just scan it from left to right to see if it's got a 72 in it you might think like oh like maybe it should be like a sorted list or like maybe it should be like a binary search tree or like some kind of smart data structure yeah i guess that would get us to log in yeah that's right like that's certainly a much better idea and if you put in some like modestly smart data structure like a balanced binary search tree such as you learn about in undergraduate um then like these things generally have like you can um insert a key and uh i guess i'll never really need to delete a key but like insert a key and like look up a key in like log n time and then you're like okay that's actually pretty decent right like then the time to do a read and the time to do a write is like order login um because [Music] yeah so like basically you're maintaining like the real array here and when you know initialize you don't have to do much you have to also like maintain this like balanced search tree of like all the binary searches you've like all the places you've touched and then if somebody comes in and says like write you know into slot 25 some data then you actually write the data into slot 25 and also you like insert 25 into here so like know that okay we've touched 25 and then like same thing if they do a read like 17 so um you know the first thing you do is do like a lookup in here with like 17 and if 17's in there meaning you have like touch 17 before then you like go to slot 17 in the actual array and like whatever it is you return that to the user and if 17 is like not in here so you've never touched it before then you say okay i return 0 to the user and maybe i can also like literally write 0 in here because why not and i'll also insert 17. like it's basically as though like you had done a write of zero like you artificially stuck like a right you know into slot 17 like a zero and [Music] this would be like a valid solution a in a valid implementation and it would like have a cost of like login per read and write and i guess initialize would be order one and honestly that's pretty good but there is some like trick sort of the point of the problem there is some trick that is even better than this where you you can have order one time per read and write um go ahead well i was just gonna say like it's a little bit like hacky in the sense that like um it may like exploit some weird aspects a little bit of the word ram model in the sense that like from like a pure like extremely you know pure abstract data type point of view it seems hard to imagine that like you can get a solution that's better than this in some way right like yeah i think maybe it could somehow exploit the constant time operations of the wardrobe model like arithmetic or something like that but i did not know what to do with that approach yeah one thing i can say which is like an aside is there are like you know binary search tree type like data structures that like use like again like these tricks of like word ram and like if you're all all the keys are not just abstract keys but like they're really integers that fit into a word you can start like hacking and there's like things called like fibonacci heaps and stuff that gets some of these operations down to like log login and stuff in the same way that like you know in like this you know abstract pure world of like abstract data types sorting really does kind of require time and log n if the only thing you can do with your keys is compare them but like in this like hackier world where you know your keys are actual you know integers that fit into a word and you have these like you can like look into the bits of the word you can do you know radex sort and and sort in linear time um there's similar kinds of things with like more advanced data structures like these binary search trees yeah i guess i'm kind of curious how so i assume when you were saying like how would you do it as a human you were trying to clue me into the trick but really i mean i just always give myself i'll give well maybe but like this is advice that i always give for every algorithms problem no matter what but that was to just totally just to start off the problem not well because like if i was a human i probably wouldn't think of these like tricks using the word right model right well ah you never know i mean uh one like i think genuine like slight and i don't know is uh well the trouble is like you get some read instruction like please read from me like what's in spot like 700 and you feel a little stuck because you need to know like if 700 has been touched before like written to before or not you could like look into spot 700 and you look into spot 700 and it's got like some number in it like 12 and you're like is that the data or is that like just some junk that was already in spot 700 for the first time um that's tricky yeah it feels like we have to do some kind of bookkeeping and initialization but it just can't anything too crazy uh yeah i don't know i think i should just think about it more honestly um i think if josh has another question then uh or i guess we're running one time it's up to you to time but like yeah it's a good yeah i'll i'll meditate a little bit to see if i can think of like a good hint but um uh yeah is there another question oh no i was i was just like listening to you i think i think i have this one so oh really i don't wanna i don't i don't wanna um you got like a good hand uh i i think i think if i give give the hint like it would yeah maybe give it away yeah okay like okay let me tell you one let me give you like some small hands i think will help like one feature that's like a little bit different between like this like hacky i say hacky even though i like i love it but like hacky like a ram word ram model and maybe like you know how like a snooty like very like you know abstract data type persons would like to find such a model is the following like uh like a very pure like model of i don't know like data storage or something like if somebody commanded like the computer like tell me what's in like memory spot like 643 and like nobody had ever accessed like you know spot 643 before then like a very like you know pure system would just like give you an error message or like throw an exception or be like you've accessed like an invalid spot but like in the word ram model like it's like whatever like you can like look into spot 643 and like it's gonna have some it's a spot it's gonna have something in there like maybe it's junk but like you're still allowed to like access it and like see what's in there um and [Music] you have to like take advantage of this like somehow like to figure out like you see like an address maybe it's got some junk in it maybe it doesn't but at least you can like you can do stuff you can try that and see where it leads you i know this is quite a big hint but um no that that makes sense i i haven't thought enough about exploiting the actual model so that's a good hint yeah yeah let me just uh you know go i don't know say some random stuff that like occurred to me when we were talking about like question one maybe this is like just a bit of a life pro tip kind of thing there is a lesser known inequality which can help you in life called um this is really changing gears a lot back to problem one type stuff called paley sigmund inequality it's lesser known but i think it's actually kind of useful um when i think about the paley ziggman and equality i my first thought is always to like look up what it is on wikipedia it's very hard because you have to remember this stupid name um it's kind of like a fourth moment method thing let me just quickly tell you the idea the idea is like we talked a lot in class about showing like uh it's called concentration results which is like trying to show that like a random [Music] variable [Music] is like close to its expectation and you know trying to show things that like oh probability that x minus mu is large is small and 90 to 95 percent in time five percent of time in cs theory life this is what you're often doing with your random variables but like maybe five to ten percent of the time you're actually interested in anti-concentration which is about showing a random variable is um not likely to be too close to its expectation which um yeah you don't always um want to do but sometimes you want to do now again this could be like in general impossible like you have the random variable x which is like constantly five and then it's phi was probably a hundred percent and like there's nothing you can do about it it's always equal to expectation but let's say we have a random variable with like um mean five and variance uh one then that's like also saying a standard deviation is one and if you think about like the phrase standard deviation no it's supposed to sort of standardly deviate from its expectation by around one and in some sense that's true like the average squared deviation i mean this really says the expected value of x minus five its mean squared is one so you know you kind of feel like oh maybe typically it's around six or maybe it's typically around four or something like that and like if his variance is one you know it cannot constantly be five that has variance zero so you might like feel like you know this random variable maybe it does have like a reasonably good chance of not being close to its expectation um and you might want to prove that so you might want to show that probability x minus mu is at least medium is at least you know not too small uh now um this unfortunately is not always true even in this case there are some random variables where maybe their mean is five their standard deviation is one but like ninety-nine point nine nine nine nine nine nine nine nine percent of the time like one way for that to happen is like 99.99999 percent of the time like x equals 4.9999 and that's not very good because like almost always x is pretty much its expectation and like .0001 percent of the time obviously i'm making up these numbers like x is some huge value that you need it to be in order to compensate to like make the standard deviation one so i don't know like a million or something i'm really making up these numbers here and that's like the kind of way a random variable could have mean basically five and variance 1 but like it's really often close to its expectation um [Music] but this is the kind of random variable where um you know expectation of x to the fourth will like go crazy some huge amount um and we you know if you remember this lecture about like turn off bounds it's related to these things we talked about problem one like the more information you have like if you have like a good bound on a random variable's you know fourth moment like if you happen to know that like you know expected value of x to the fourth is at most three or something like we did in the turn off lecture then like you kind of feel that like this kind of thing can't happen and you might be able to prove an anti-concentration result like this and you can in fact try your hand at um directly you know here it's like we want to you know let's say if it was like mean zero and variance one you might want to show that um there was a good chance of um being out here so that would be involved like trying to put you know if this was f of u that involved trying to put like a g of u like maybe below this so that you know you would say oh the probability that like x is at least i don't know 1 is at least something this is your expectation of f of x you might want to say this at least expectation of h of x where that's something whose expectation you can complete compute that's below that um and that's a little different you might see that you kind of need like a degree four thing like a parabola you're not gonna have like any parabola that's like doing a good job of being below this indicator it's like i don't know this one kind of sucks i don't know um so yeah they're like theorems to these effect and uh you know this is like a keyword you can look up and this one really is just like kind of like applying chebychev to um the square of a random variable so like for example if we know this if we go back to the scenario where x is mean five and variance one we know this the expected value of x minus 5 squared is 1. you could say oh this is my random variable y i'm going to set to be x minus 5 squared this random variable is non-negative and it has mean one and you can also figure out the variance of y is the variance of y y is some quadratic expression x so this will be some degree 4 expression in x and so if you know something about the degree 4 expectation that it's like at most 3 or something then you might be able to bound the variance of y and then you could use chebyshev on y to say that you know it's unlikely that y will be far from its mean of one so if you use concentration on y like if you can somehow figure out how to use chebyshev on y we'll say that this random variable is typically kind of close to its mean and what does that mean it's saying that like oh the deviation of x from five is kind of typically in the neighborhood of one and that is like actually saying that like oh typically x isn't like exactly five it's sort of deviating from five by a decent amount so um that's just like another idea that comes up in in life it's connected to these like circle of ideas of approximating indicators by polynomials or like using chebyshev on uh not on the variable itself but if you have some fourth moment information you can use chebyshev on the variable that stands for the deviation and get like an ansi concentration kind of result so somehow like do we go ahead oh no i was just gonna ask if we know how to do this for uh arbitrary moments or does it get like super difficult yeah you can do it so like the basic philosophy is this um [Music] you kind of imagine always what happens if this is very vague but like you imagine like what you feel like kind of should happen if all the variables random variables were being reasonable and like not acting wacky like you know what i mean to say it's like oh if you have a random variable and you know it's mean you kind of think like you know if life were reasonable this random variable would usually be kind of close to its mean and like that's what concentration is trying to get at and you're like oh suppose i also know it's mean and a standard deviation and the standard deviation is one you say to yourself you know if life was reasonable like that would kind of mean that like typically the random variable was around plus or minus one ish from its mean and like all these beliefs that you believe a reasonable random variable ought to have um typically there exists random variables that suck and do not have these properties that they like you know confound these beliefs but the only way for that to happen is if they have like really large moments like this is like the hallmark of like an unreasonable random variable it's like fourth moment is huge or maybe like its mean and its variance and its fourth moment or normal but like its six moment is like a billion that's like the hallmark of like a random variable that's going to kind of like screw your expectations um sorry i'm just gonna say that like i'll take your question one second just to say that like it's a very common phenomenon like the more control you have over these moments like if you like no the expectation of x to the fourth is kind of small and x to the sixth is kind of small then that gives you like more ability to prove stuff about a random variable like behaving in a reasonable and expectable way yeah interest or josh can go ahead go ahead i was just i was just gonna ask like is it reasonable to think about like moments as like the taylor series of like a distribution or something where like when you think about a taylor series you think about like the first order terms to be relatively smooth and kind of like tell us something nice like oh it's like you know it's like only if the if the coefficients for the first order terms are good then like you kind of know that it's smooth and it's like kind of differentiable and good stuff happens but if like if you have like a sixth order like if the sixth star determined the taylor approximation is like big then you know that the the um the the the function could be doing something crazy is that like a reasonable kind of like analogy to make similar sort of heuristic yeah i mean um there are this is very rare in life but like extremely rare you could have like a random variable who's like first moment and second moment and fourth moment or like all like kind of normal looking numbers like you know one and two or three and then the sixth moment is like one trillion that's like actually like unbelievably uncommon in practice but like this would have the feature that like you know like many basic things then you could like use chebyshev on that random variable and get a good result or you could use like fourth moment method things on that and get a good result but like you wouldn't be able to get like a churn off like result for that random variable typically because turn off kind of needs like you know all moments to be behaving relatively nicely because i think like turn off like acts like laplacian right like to some extent like if you use the moment generating function then that's like e to the power like minus sx or something where yeah if if it's a polynomial if it's a complex number then it acts like a laplacian so it's kind of like the fourier decomposition of your distribution in some sense and like that seems somewhat related to like taylor series and other things as well yep yeah as you say there are like things you can look at like you know the moment generating function i forget the exact definition but like you look at e to like lambda x and you know this is expected value of 1 plus lambda x plus lambda squared x squared over 2 factorial plus and so this is like something like generating function it's kind of like a formal trick to sort of study all the moment means or sorry all the moments like simultaneously um you know the coefficient in this expression on lambda to the eight is like the eighth moment well maybe times divided by eight factorial or something and yeah indeed like we kind of saw on the proof of turn off that like um well the proof of turn off we kind of showed that like we could directly sort of show that this quantity is like not too insane um you know and turn off like we use the fact that this is x was like x1 plus x2 plus dot dot where like these were all independent and that literally let us like compute this exactly it was like e to the lambda minus e to the minus lambda over two or something and there's like some ends oh to the end and you know the fact that this was like not too insane it's kind of equivalent to the fact that like these moments are not growing too fast they're being like relatively reasonable right um so yeah like a lot of the same intuitions and heuristics are are at play right because i was like thinking like when you prove stuff about like like i was reading like half things and some of the other stuff and they use like sub gaussian so that sub gaussian kind of like does this it kind of like limits the higher order moments to some extent and that's how they were able to prove like kind of like with the impounded random variables and things like that yeah exactly right there's this notion of a random variable being sub-gaussian and uh at an intuitive level it's kind of saying like simultaneously all the moments are not too crazy they're not too high and instead of like making you know like a complicated definition where you're like oh every moment like expected value of x to the s should maybe be not much more than i don't know like s to the s over two or something like there's kind of like like a little bit of a trick where like if you study this expression like it kind of simultaneously gives you information about like all the moments and so the def like the official definition of subgaussian will have something to do with this expression but yeah it's like you know this expression being like not too large is kind of like simultaneously saying like all the moments are not too crazy right yeah and really like there's only so many different kinds of random variables like there are the super nice random variables like plus or minus one random variables like bounded random variables like 50 plus or minus one or like gaussians where like all the moments are like not too crazy and then like the canonical example of like a really terrible random variable is like you know it's it's uh like one over epsilon with probability epsilon and zero with probability one minus epsilon so this thing has like expectation i call this y the expectation of y is one um and so you're like oh this is a cool random variable it's expectation's one it's probably typically around one right but like no like it's almost always zero and like think of epsilon is one over a trillion like and then like occasionally it's like a trillion so like that's a very annoying random variable because like this fact is like quite misleading somehow and it manifests itself by the fact that the second moment is insane the second moment is like epsilon times one over epsilon squared which is one over epsilon and okay if epsilon is one over trillion then this second moment is like a trillion and uh that's an example of like an annoying random variable and uh this kind of random variable does show up in life and you've got to deal with it um the super rare thing is to find like a random variable where like oh expected value of y is one and expected value of y squared is like five and then expected random value of random white like higher ones like four is like you know 10 to the 100 like it can happen but like you almost have to like work hard to find a random variable that has this funny property yeah that's cool um okay well uh thanks for coming we'll wrap it up there and i might change the office hours time next time to get a better one but um i'll keep you posted\", metadata={'source': '7IDtr_owpTQ'}),\n",
       " Document(page_content=\"okay good afternoon everybody in today's lecture I'm going to tell you how you can multiply two n bit numbers in linear time it's even faster than it's written here of course how the root at running time for it depends on the model of computation and that's something we talked about last time so we'll get into it again this time it's actually this topic that's going to occupy us for the lecture fast multiplication it's not like the most important topping the world and like I don't inherently think it needs a whole lecture but it's a great topic for hanging several other subtopics on that I do care about including thinking a little bit more about computational models thinking about the complexity of basic arithmetic operations and arithmetic on really large numbers especially Fourier transforms will come up in this lecture and that'll play a major role in our next two lectures on analysis of boolean functions and quantum computation so it somehow it's like one interesting topic that connects a bunch of things together and so I think it makes a suitable thing to talk about today okay so as I said we're going to be talking in this lecture about you know the time complexity to multiply two numbers together two integers and bit integers but before we get there let's actually even warm up a little bit and ask ourselves this question what is the time complexity needed to add to n bit integers okay I guess I mean natural numbers here positive non-negative numbers and recall you know now that we're talking about algorithm we're gonna be working in the word RAM model like we studied last time and we're also we're gonna just make the assumption that the word size is proportional to log n which is sort of the standard assumption to make and here's the mental picture you should have in your head is that maybe n is 10,000 or 100,000 okay so imagine these two numbers are presented to you and to like files that are like on a 12 and a half kilobytes each okay you have to make another file that contains the the I don't know 13 kilobyte answer of the sum of these two numbers so you know the word Ram model if you have an integer that's between you know 1 and n or 1 and N squared you can fit it into like one word and or a couple words and then my definition you could just add those two numbers in constant time but weighed over a number between 1 and n we have like an N bit number right so if this is 100,000 and it's storing a number between you know basically 1 and 2 to the power of 100,000 now you might actually ask like why would you ever want to do that like what conceivable situation can you imagine where you want to add like 200 bit 100,000 bit numbers together and it's a reasonable question because you know 2 to the power of 100,000 does not represent any physical quantity right that's way bigger so it's not like you care about these numbers like in their role as counting things but the reason that you ever want to do this or to multiply to 10,000 bit integers or I don't take the greatest common divisor of two 10,000 bit integers is for cryptography so the one use of manipulating ridiculously long numbers is in cryptography I'm sure you've seen before like RSA you know the on which most encryption is based involves picking these you know 2,000 bits keys and multiplying them and factoring them and so forth so this is the mindset to be in for this discussion I supposed to be sides of cryptography aspect there's just the inherit I mean you know it's addition we may as well try to understand the exact complexity of it and multiplication and so forth but just bear in mind that these are more like almost like string manipulation problems and they are like actual arithmetic problems okay well I mean there's basically only real one way to algorithm for molting numbers right you just do it the way you learn to do it in school the numbers somehow looks like this and the second number looks like this I don't know okay those are the two inputs let's say they're both n bits long for simplicity the same length then okay you process them simultaneously from right to left and you do 1 plus 0 is 1 and 0 plus 1 is 1 this is 0 and there's a carry and then this is 0 and there's a carry and so forth okay and you get the answer and that's pretty clearly order and time now what I want to tell you actually is that you can almost do this problem and better time you can almost sort of do this in andover log n time anybody have like a clue what I'm getting out here yes yeah that's right the point is he on the warm model you can do like basic arithmetic operations in like you know one time step so like let me put a question mark here like if you you like repackaged these n bits be packaged and big into n over log n words you know each of length log in then you could add them in this order n over log n time and let's assume the word length is exactly like log base two of n in which case like you know packaging log n bits together is really no more than just writing the number in base n right so I mean here the numbers are assumed to be written in base two but a foligno hypothetically log n wherefore then you'd be like putting these together and these together and you'd be like oh this is actually the digit six and this is actually the digit 1 2 4 8 13 okay and you just be doing the same old addition algorithm but like on quote-unquote digits which would actually be you know numbers between 1 and n or 0 and n minus 1 well you know you can add two numbers that fit into a word in time 1 in this model and you can get the carry and so forth yeah yeah well so the reason that if you really have you know if the import is like two arrays each array elements storing a bit and they're the Rays of length n there's nothing you can do to beat like order n time you have to look at the whole input and this like repackaging takes you order n time but if someone like hopefully already did that step for you and took this like and bit long number or this number between these two numbers between 0 and 2 to the N and presented them to you as and of our log n digit numbers each digit being like in a word then you could do it in n over log n time okay so this learns like let's kind of like things that sometimes like you know called like bit tricks and like sometimes in like the word ran model you can save factors of like log N by doing neat tricks like this yeah any question about this okay so I think this up because it's good this same little trick is going to rise in the problem of multiplication which is like the more interesting task okay great so now we'll move on to the question of the day which is what's the time to multiply two and bit numbers okay and again and still imagine that these you know and it's like a hundred thousand or something and based on this trick you may as well like apply the same trick for my intradermal plication so whatever your future algorithm is gonna be you pretty much without loss of generality a step zero dance well repackage these numbers into you know and over login you know quote-unquote digit numbers each digit fitting in a word because these digits are between you know zero and n minus one I'm just having a word length is like let's say exactly log base two of N and this repackaging step is going to take you order n time and that's going to be fine because I assure you we're not going to try to multiply and bit numbers faster than order n time so we're willing to pay this for sure and so we may as well just do it and also for the rest of the lecture I'm gonna this is psychologically difficult buying a write capital n for n over log n it's it's painful because capital and suggests that it's bigger than little end but it's actually smaller than little n but allow me to do it okay so that's our task imagine you have to capital n digit capital n digit numbers and these numbers are written in base and so each digit is between base little ends so each digit is between zero and little n or any way it fits in a word each digit how are you gonna multiply them so ya and you can still do the the algorithm you learned in school if the number of digits is three you can be like okay 751 times 314 you know this algorithm right four zero there's a carry 30 whatever step in any way you multiply them and then you do some ads and you get the answer here okay so that's fine if these are capital and digit numbers then this algorithm the great school algorithm takes something like quadratic time well in fact quadratic time okay cuz basically this tableau of digits down here is something like capital N by capital N and filling each digit it takes constant time because you can do all these arithmetic operations in time one actually let's say that's assuming your word RAM has the multiplication instruction where we discussed this point last time we definitely wanted to allow ourselves to like add two words and like compare two words and cost a time and we said well what about multiplying two words it's sort of it's up to you whether or not you want to allow it so let's just say we allow it so that we can multiply two digits in like constant time and this then this will really be exactly like theta of capital N squared time I mean you also have to do the additions here and there's some carrying but this is also quadratic time so make sense any questions about the model issues okay uh great now you may already know that this is not the fastest algorithm at all for multiplication there are like clever algorithms for multiplying two really long numbers for example you may have seen like current subas algorithm or maybe not but this is some kind of like divide and conquer method involves like splitting the two end digit numbers into halves and somehow you need to do four multiplications but if you're clever you can do it with only three and you got a recursive algorithm that gets this down to like theta of and to the click log base two of three I think this is car at Suba okay that's like one point five eight or something in the exponent there and there's generalizations of this that are faster and faster and if you take more and more generalizations you can actually get algorithms that run in four multiplications that run in nearly linear time and rather than you know exploring those I'll sort of just jump to the the end the end of the story and tell you the ultimate divide and conquer algorithm for multiplying two long many digit integers and it's to use the discrete Fourier transform method okay so this will give us the opportunity to talk about Fourier transforms as well I guess that's DFT and what we'll see is actually that basically this discrete Fourier transform method for multiplying capital n bit integers lets you multiply them in an amount of time which is essentially not more than the amount of time it takes to do one discrete Fourier transform okay and so then it becomes a question of like how fast can you do discrete Fourier transform and as well see it's pretty fast so this idea was figured out by Strasse in 1968 and then developed further by Hagen Strasse in 71 and Pollard in 71 this reduction of multiplication to discrete Fourier transforms and then actually Gauss in 1805 came up with a really fast algorithm for doing this discrete Fourier transform on lengths and vectors he showed how to do it and order n log n arithmetic operations she's pretty cool I guess he was trying to like calculate something about the orbit of Juno which I guess is a moon of some planet and he's literally doing these calculations by hand he's like man I really wish I could speed this off from N squared steps to n log n steps and he did this is usually credit to like Cooley and Tookie from 1965 but then like some historian discovered that like Gauss had figured it out in 1805 actually before he did vented Fourier transforms not exactly sure how that works but anyway he figured out this algorithm back then and this fast algorithm is called the sport is really a mess but it's called the fast Fourier transform FFT okay so the thing that you're trying to do is the discrete Fourier transform the specific algorithm that does it quickly in this n log and many operations is called the fast Fourier transform and Gilbert Strang and 94 called it the most important numerical algorithm of our lifetime so I guess it's good that we get to see it great so that's good now I put in like a bit of a weasel word here I said the Gauss is fast Fourier transform shows how to do the discrete Fourier transform in order n log n arithmetic operations but I did not actually say time because well as you'll see these arithmetic operations are it's a little bit they're not arithmetic operations that are natively supported by the word Ram model okay although they look pretty simple as you'll see so a question becomes well what if you actually want to do these arithmetic operations unlike the official word Ram model how long does it take and canoes in like maybe early 70s show that this fast Fourier transform can actually be done in order and log n time on the word Ram model if you allow multiplication it sounds funny but it makes sense but I mean it is you love multiplication of two words in order one time which again as I said last time whether that you allow this is up to your own taste and I like to allow it why not I mean it's not an AC zero but you know our modern chips like have this multiplication instruction for two words so maybe it's a reasonable model so we figured out how you can actually do these arithmetic operations with sufficient precision and so forth that you actually get an order n log n time on the world grant model and therefore I mean putting it all together this reduction from as I mentioned this reduction from multiplying capital n bit integers to doing like one well actually two discrete Fourier transforms which we now know thanks to galas and canoes can be done in n log n time you put it all together and you get and also you put step zero together and you deduce that the time to multiply and bit integers to n bit integers on this word Ram model which is the sort of standard model is well we just plug in the capital n is n over log N and over log n times log of n over log n okay log of n over log n it's like log in so the log n scats all and this is order n which is great pretty awesome linear time you couldn't really hope for better than that and sometimes like a lesser known fact I don't know why I think it's because people don't admit they're using the word RAM model or something so you don't often see this fact like sighted but it's true it's like right there and the art of computer programming like volume 2 you know section four point three point three point C from like the 70s but yeah I guess like people were somehow I don't know worried about models or something so people have also studied like oh well what if you want to do into this integer multiplication on like circuits or like two tape turing machines circuit so i can kind of understand two tape turing machines seems a little sicko to me but so they studied it and there's a great progression of results so in nineteen seventy one should halt against stress in showed how to do it in n log n log log n time and this was like a very famous algorithm and usually you see people talk about the shin hawk of stress or an algorithm for multiplication and say it runs in this time really this thing is the shin Haga stress and multiplication algorithm just taking advantage of the fact that you have this word ram trick and then people are very excited in 2007 when Ferrer got it down to n times log n times 2 to the order log star n which is some like comically slow-growing function I guess is the number of times you need to do log to a number to get it down to 2 and then hey in 2014 in this paper Harvey and vanderhoven just got down to order and log in which people pretty much believe is the fastest you can do it in like circuit model or two tape turing machine model it seems like you can't really get rid of this log n and it sort of all like I don't kind of make sense like in this model it's n log n and if you go to the word random model with multiplications allowed where you can package things into n bit chunks and like manipulate sorry log in bit chunks and if you like them morally it sort of stands to reason you can get rid of this log n factor and indeed you K questions about this okay so uh why before we get into it and I will get into it how to do it sin what ask why obsess so much about integer multiplication you know it's just one it's just one problem one reason is that like the best-known running time for pretty much every arithmetic problem is like a direct function of the time for multiplication like almost every arithmetic problem somehow the best algorithms we just reduced to multiplication and so back in these days when like they weren't really sure like what should be the best running time for integer multiplication they just call it something like EMA van and say like here's my running time for like division or I don't know exponentiation or whatever so I'll just tell you quickly a few of these results they all appear in this great book by Brenton Zimmermann actually unlike the back cover or back page or something so let but M of n be time for multiplication okay so that's just em of n stands for order n if you're in the word wrap model or order n log n if you're in circuits or Turing machine model and you're following facts are known you can do integer division in time order and Levin it's not trivial if you want to compute like X divided by Y reduces to computing one over Y and then doing a multiplication with X and you compute one over Y by Newton's method where you like to solve the equation like Y Z equals 1 by Newton's method and it's somewhat sophisticated this reduction but it exists and in fact it was the this reduction and there's like the source of the Pentium bug from whatever 30 years ago there's some like mistake and like the lookup table for like where you should start your Newton's method when you're doing integer division okay you can also do K through so square roots cube roots etc all in order M of n time again with Newton's method you can do modular exponentiation to K power so if you course you cannot do like explanation exponentiation like a to the power of b and polynomial time at all because the answer is too large to fit a like a polynomial number amount of space but if you do like a to the power of b mod 2 to the W or mod any number actually to keep the answer size small then I'm sure you know you can do that efficiently and this takes time order K times M of n and this will be important into shorts quantum factoring algorithm you can do GCD in time order M of n times log in this does not use Euclid's algorithm Euclid's are going this quadratic time and Knuth was the first person to get like a near linear time algorithm for GCD which is totally different from Euclid's algorithm and an improved version on it gives this let me not write on them but you can like convert numbers between two bases in like order MN times log n time you can compute like long or X or sine or cosine of order M of n times log in time here the idea is imagine you're computing out like sine of an integer that's in one word and what you want is like n bits of precision in your real number answer yes yeah so here like imagine maybe it's like more complicated for here but like take the sine function right imagine just applying sine to an integer which is stored in I don't like a word say like computing sine of one and your task is to produce n bits of the output so what's like so for example similar to this is like if you want to compute like n digits of pi or n digits of e it's also M of n times log n and ya these things all use like something sophisticated called the a MGM iteration method which I don't know what it is but it sounds great and let's well let UNICEF prime ality testing you can test if an n digit number is prime in order M of n times n time what's a randomized algorithm though so that's like quadratic time there's a deterministic algorithm for primarily testing but it runs in some like discussing time like o tilde of n to the sixth or something you have a question yeah I guess here I'm not exactly sure what I mean I have to look it up I mean I guess this is probably like you be careful here right because this could be like a huge quantity it's probably like maybe you have like a rational number that's like n-bits law ednan rational number let's say I don't know I'm making this up a little bit but I can imagine it's something like you have a rational number between 1 and 2 expressed as a fraction where the integers and the numerator denominator are n bit integers and you want to get like n bit precision of the output something like this yeah I'm not sure I admit what I mean by this but you can check out this book but basically computing any of these like analytic functions like n bits of precision on an input is apparently this time yep\", metadata={'source': 'd1kBOUAlSRM'}),\n",
       " Document(page_content=\"so let me now tell you about this discrete Fourier transform method for multiplying integers and it is that multiplying integers is basically the same problem is multiplying polynomials so that's really the problem that we're gonna solve so recall we have to capital and digit numbers let me call them capital A and capital B and we want a times B and I can write their digits I connect them in like base n like this base little n like this I'm gonna should add one more example here there's a terrible choice ok that's with two generic numbers in base little n with capital and many digits look like okay the little A's and the little B's are the digits and we okay we want to compute the product of these as numbers you can see it really looks as though we're multiplying two polynomials so in fact a is equal to P of n where P is the polynomial the formal polynomial in an indeterminate X with integer coefficients you know a n minus 1 X to the capital n minus 1 just a 1 X plus a 0 and similarly you know this number B is Q of n where Q of X is also a polynomial but with like a B B's as coefficients okay so think of these is now formal polynomials and our goal in life is to compute a times B and that's R of n when R of X is defined to be the product of P of X and Q of X okay if we can formally you know think about the product of these two polynomials which is another polynomial in X of degree 2 capital n minus 2 and you know if we know what that polynomial is if we now plug in the number little n for X then we get P of n times Q of n which is a times P so what I'm saying here is and you know the input to the problem of multiplying two integers are is by the way these digits 0 through a n minus 1 and B 0 through BN minus 1 you know we're like to say is that suppose given that input we came up with you know the polynomial R of X which I'll write as C 2 n minus 2 X to the 2n minus 2 plus the dot plus C 1 X plus C 0 ok so now suppose we did this polynomial multiplication then these coefficients C i's are they say basically the digits the base n digits of the thing that we're trying to figure out a times B this number okay there's a mild problem with this claim what is this mild problem yeah yeah I mean this would be exactly true if all of these coefficients were between zero and and minus 1 but they're not there's a catch there's like carrying you we're like you know some coefficient might be like N squared so it like overflows a little bit in some sense when you're reducing it to base n like a few steps up so there's like a catch which is carrying but it's a very mild catch so I will leave the following things to you as exercises first each coefficient CJ will be at most n cubed and therefore it will fit into like three words I'm sort of assuming now that my words are as I said exactly log log two bit log base two of N and then it's like an exercise also for you that like you can do the sort of carrying and addition in order and time so like linear time which is negligible to what we're going for like basically what happens is you you get a picture that sort of looks like this like c0 is maybe this is some house c0 is maybe three words long and c1 is like maybe three words long and c2 is up to three words long okay you kind of have to like add these up and like do like some carries to get the actual digits that are like between 0 and n minus 1 like I mention that like you know a digit was actually a digit like a number between like 0 and 9 but like when you compute these coefficients you got like you know 17 and you know 36 ok then you have to like you know there's carriers and additions to like actually get the digits that are all between 0 & 9 okay so this is sort of recognized early on it actually makes it look like we've made the problem slightly harder but it's just a bit cleaner to like formalize the problem as multiplying two integer coefficient polynomials and getting an integer coefficient polynomial okay and then they're carrying business is like a little twist at the end okay so our goal is to do this now and that's why I want to Shh that's the many thing we're gonna show our goal is to multiply to degree and polynomials okay really their degree n minus one but never mind and and let me be a little big here and log n steps and continue to be a little bit vague these are integer coefficient polynomials but basically now let's sort of move to counting like steps or arithmetic operations I'll explain the little annoyances about that at the end but just assume you can do operations on like these coefficients in constant time for now okay so now there's only one parameter like capital n you have to capital n degree polynomials you want multiply them okay so how this is all like throat-clearing for the beginning of the real problem which is happening now okay so here's a critical idea for understanding the trick and this is like the most important fact about polynomials will come up to it again when we study polynomials in greater detail in a later lecture okay I suppose they're actually degree n minus 1 Oh remember that because I want to say this a degree n minus 1 polynomial P of X it is specified by its end coefficients or equally well it can be specified by its value values and distinct points ok this summer like the most famous fact or important fact about univariate polynomials you know two points determine a line and like three points determine a quadratic and so forth so what we know is we sort of the input is the end coefficients of P and the end coefficients of Q and what we are trying to get out or basically the 2n or so coefficients of P times Q now just going to show that if the representation of data makes a big difference suppose we could somehow say like well I know P's value or I know P's and coefficients but let me just somehow imagine I also knew P's value on endpoints and say we also got Q's value on endpoints let me even say we got them on two end points just for we're done in C sake okay so say we knew like hypothetically and for example we knew like p1 up through P to n and also q1 through Q to n then we'd get our of X's values on these same two end points really fast and let's say order and arithmetic operations because ours just P times Q so like if we knew these two that numbers then we knew these two n numbers then like R of one would be P of 1 times Q of 1 like we just do 1 multiply and we just multiply each of these pairwise and we know R's values on two endpoints so if somehow we could always be working with like a value this representation of polynomials rather than like a coefficients representation then multiplying two polynomials would be super easy we could just like multiply point wise it would be order n time or n steps now unfortunately we don't have these values representations I mean we're given the sort of coefficients representation the polynomials and we want to get the coefficients representation in the end but we are going to try to pass through the values representation to get this trick to do this like fast multiplication trick so in general let's say you have n coefficients of f of X let's say s of X is a degree I don't know n minus 1 polynomial you may have this or you may have like and values of s of X like hypothetically s 1 s 2 s 3 s of 1s of 2 s of 3 up to s of n and going from here to here it's just this is called evaluation you know I sort of have a polynomial explicitly written out I want to get its values you know I evaluate the polynomial and going in this direction it's called they met the interpolation you know if you have the values of kind of a mystery polynomial of degree n minus 1 on endpoints you know the end points and the end values figuring out the polynomial that achieves those values is called interpolation and the dream that will actually be fulfilled is to do both of these steps evaluation and woe evaluation and interpolation in order and log in again let me just say operations and if we could do this if we had some sort of way of doing this then we'd be in great shape because to multiply are to degree and polynomials in order n log n operations we first do we first evaluate them on to n values ok and that it's going to somehow miraculously take n log n time and that will have you know P's values on two endpoints Q's values on two endpoints just we'll multiply these together pairwise in order and operations to get ours value on two end points where RSP times Q and then we'll appeal to this again and take ours values on two endpoints to interpolation and n log n steps and get back our coefficients which is our goal what's that plan make sense okay so okay we just need to achieve this miracle and we're we're done we get multiplication in linear time now the trouble is if you try to do this naively it seems that this evaluation for example should take quadratic time I mean if I think you think it may have like a polynomial of degree N and I want you to find its values on n different points well to find its value on one point it seems you have to like I kind of plug it in that point in forex it like raised the powers and ads I should take at least n steps and then you've got to do it for like endpoints so that should probably and squared steps so it seems ambitious to think you're going to do it in n log n operations but you can if you choose the cleverest choice of points to do it on don't use like one through like 1 2 3 4 5 up to 2n that's not the cleverest choice see we have a freedom we can choose whatever points we want in this argument so we'll choose the cleverest ones and we're gonna shake choose some numbers to evaluate these polynomials on so in some sense there'll be a lot of like a lot of like cost-sharing when you're evaluating let's say a palm you'll pee on all these end points so this cost sharing will somehow allow you to do it much faster than N squared operations allow you to do in n log n operations ok so let's think about this and let's say we want to just do a valuation for now evaluations sort of the easier one to think about it so if we were to evaluate a polynomial at the points you know instead of 1 2 3 4 up to n let's just say Omega 0 Omega 1 Omega 2 up to Omega n minus 1 where these are some numbers to be chosen later and if you've seen this before it's not a coincidence that I used the letter Omega alright so if this polynomial P of X looks like this a n minus 1 X to the N minus 1 plus dot dot plus a 1 X 1 oops just X plus a 0 so they say this is our polynomial and we want P's values on these n numbers then to get those it's actually a matrix vector multiplication problem as I'll now illustrate what I claim is we can get the vector of answers P of Omega 0 P of Omega 1 down through P of Omega n minus 1 as this matrix vector product where we in the first row write the powers of Omega 0 and in the second row we write the powers of Omega 1 last row we write the powers of Omega n minus 1 okay and we multiply this big n by n matrix by the column of coefficients of P okay so looks a little bit complicated but all I'm saying is okay given these fixed s-- desired evaluation points if we make this matrix which contains all their powers 0 through n minus first powers which by the way is sometimes called the vandermonde matrix associated to these numbers I'll just say that for your own edification it's not important and you won't play that matrix by this calling the coefficient so you just see what you get I mean just you can visibly see like this row dot this column is P of Omega 0 and so forth yeah oh yeah we do eventually want to evaluate it to endpoints so if you have a degree and polynomial and you want to evaluate at two endpoints one thing you can do is just pretend that it's a degree 2 n polynomial where at the top and coefficients happen to be 0 and then do this all with and replace by 2n it's a good point it should have said that I'm actually doing the reverse I'm just saying that let's pretend that like it's always and the endpoints and degree and that we want to do although you're right it's actually gonna be actually 2n and the whole algorithm by just writing n here for simplicity good so the task of evaluating a polynomial on these points is just the task of doing this matrix multiplied by this vector and what's nice let me call this matrix just V the reverse task interpolation you know given the polynomials value on values on Omega 0 through Omega n minus 1 figure out its coefficients it's just multiplication by the matrix V inverse assuming V has an inverse which it will so that's good so I mean it mainly lets us focus on you know doing this kind of matrix multiply and the same problem with V inverse is the interpolation step now again I mean if you just have like a matrix n by n matrix times length n vector seemingly takes like N squared operations to do it but again as I said the the trick will be to choose like very clever values for these omegas such that there's like a lot of cost sharing and you can actually do this matrix multiply and n log n steps and okay just somewhat you know cut a long story short I mean one thing that would certainly help is if a lot of these numbers were the same these powers were the same okay so you want some numbers where like powers they're more like equal to each other you know long story short the the clever choice of Omega is our complex roots of unity okay so we ought to bring complex numbers into the picture but that's okay they're enjoyable so let's do that so I'll remind you these are the complex numbers the circle represents all the complex numbers of magnitudes one that's one that's I etc huh and what you do is you take Omega J the J's number just to be this is gonna be a little bit confusing but Omega to the J where Omega is like a primitive capital nth root of unity and just to be consistent with like everybody's else's notation actually mean this one normally this one is like consider the primitive nth root of unity but just to be consistent with everybody's notation I'll choose the other one so this e to the minus two pi over two and okay so this is Omega and like this is Omega squared this is Omega cubed this is Omega to the fourth I guess in my example here that I'm drawing n is 16 so five six seven eight nine ten eleven twelve thirteen fourteen fifteen and then Omega to the 16 is one which is also Omega to the zero great so with this choice of omegas I mean so you fix n then you let omega just plain omega be the nth root of unity and then you let your Jeff interpolation or evaluation point B the the J power of this Omega those will be your end points and when you plug them into this matrix with this specific choice the resulting matrix is called the discrete Fourier transform matrix while the N by N one and so really write this is okay so Omega 0 is just 1 so actually when we do this the first row is all ones okay then the next row like Omega 1 is just Omega so it's like Omega Omega squared up to Omega n minus 1 the next row is like the powers of Omega square it so it's 1 Omega squared see if I got this right I'll make a fourth Omega 6 etc okay and in general the K elf entry of this matrix is Omega to the K to the L so Omega to the K times L and you can also take this modulo n right because the powers of Omega like repeat once you go Oh mod and once you go like pass to n okay that's actually kind of nice because like a priori there could be like N squared different numbers in this matrix but since you can reduce all the exponents mod n there's that most like n different numbers in this matrix which is how you potentially can get some like at least start to imagine getting some like cost savings and doing this matrix vector multiply yep oh this is just like the math way to write this complex number so if you don't like this or if you forget about complex numbers it's just the complex number which is has like magnitude one and it's angle from the origin is negative 1 over negative 2 PI over 2 n like - wait there should be over N all right maybe this was your question should be over N so it's like thanks it's like one end of the way around the circle thank you that was confusing ok so I tried to write an explicit example here this is the N equals 8 except it was I didn't want to write like all the Omegas this is DFT sub 8 so really like when I write 6 here it's really Omega to the 6 and when I write like 4 here it's really Omega to the 4 and this is when Omega is like an 8 fruit immunity so Omega is this it's that like negative 45 degrees from the real axis so make out Omega squared is minus I and so forth and what we're going to ventually see I'll come to this soon enough but what we're going to see is that with this beautiful choice of numbers there exists an algorithm the fast Fourier transform algorithm for doing this matrix vector multiply in n log n steps rather than N squared steps\", metadata={'source': 'rRNS5qIxqgE'}),\n",
       " Document(page_content=\"but before we get there I just want to bask in some of the glorious properties of this matrix it's like a a Fourier transform matrix and the next lecture will encounter a different Fourier transform matrix but they're all belong to these like Fourier transform families and as I mentioned these will play an important role in for example quantum computation in some sense actually the only power that quantum computation has over classical computation is its ability to do implicitly do extremely large Fourier transforms efficiently ok but that's an aside let me talk now about properties of this matrix mainly only one property is critical and the main property is that this matrix is basically like an orthogonal matrix or a unitary matrix if you know what this means basically any two columns of this matrix are orthogonal to each other or they have inner product 0 so this is like basically the key property of the matrix it's the inner product of any two columns while distinct columns is zero and that's that's the critical property of the matrix now in order to illustrate this I have to like remind you at a very very annoying fact which you probably tried to forget from linear algebra class which is that when you're doing linear algebra over the complex numbers and you're doing the dot product you can't just like multiply the numbers together and add them up you have to complex conjugate one of the sets of numbers that's the deal you gotta do it okay so you have to bear that in mind when we're doing this calculation so for example let me look here I'd like column 1 inner product column 3 and I guess this is what I mean by column 1 and this is what I mean by column 3 uh let me really start to do this so let's let's interpret these two columns together okay so really like this is the Omega to the zero times Omega to the zero I'll put the complex conjugates in in a second plus Omega to the 1 times Omega to the three plus Omega to the 2 times Omega to the 6 should I really do this sure four times will make it to the four plus Omega to the 5 times Omega 7 you can watch this a 2 times feed my X 2 plus Omega to the 7 times Omega to the 5 ok that would be like your friendly dot product of column 1 and column 3 what is complex numbers you got a complex conjugate let's say the first column now how do you complex conjugate like I'll make to the J oh well luckily it's the same thing as Omega to the minus J that's convenient so basically it just means you need to stick a negative on like the first exponent everywhere ok and then this is equal to Omega to the 0 plus Omega squared plus Omega to the fourth plus Omega squared plus Omega to the 0 plus Omega squared plus Omega to the fourth plus Omega squared and that's supposedly zero and it's true because you see we have each of Omega and 0 Omega 2 Omega 4 something has gone a little funky oh there's some sixes this is gonna be like dramatic oh this would be minus 4 that's 4 to 0 which one should be what should be okay grant which is 6 there should be another one that's 6 thank you great good thing you're all here good so we have I'm going to the 0 2 4 & 6 each occurring twice so this one's will make it to the zero this one's Omega squared this one's Omega to the fourth this one's Omega to the 6 and to say that the sum of some things is zero is the same as to say their average is zero and it's clear like visually that the average of these four vectors is zero so the add up to zero oh okay I guess that was just an illustration but a similar thing happens for any two columns I'll do the proof in a second but conversely the inner product of a column with itself is n oh well I guess it's easier to see with this column with itself this is omega to the 0 which is 1 so these are all ones if you dot product they all ones vector with itself you get n okay great so in some sense that columns are n orthogonal vectors and they all have the same length which is square root N and maybe I'll write a fast proof of these fact it's fits columns if you're interpreting columns K 1 and K 2 which may or may not be distinct then you're looking at the psalm is J goes from 0 to n minus 1 of Omega the first thing is minus JK 1 and then plus J k2 I was kind of what was happening here okay which is J goes from 0 to n minus 1 of Omega to the K 2 minus K 1 all ^ J and on one hand this is equals and pretty clearly if K 2 equals K 1 because then this exponent is zero and so mega to the zero is 1 1 to the J is 1 so you're having up one end times you get n otherwise if this K 2 minus K 1 is not 0 so if K 2 and K 1 are distinct well this is a geometric series with common ratio equal to this thing so you can use the formula for that you know else it's well the common ratio Omega n to the K 2 minus K 1 the power of n minus 1 over the same common ratio minus 1 and I guess this is the same sorry for this ugly board usage as Omega to the N to the K 2 minus K 1 if I don't make to the N is 1 so this whole thing is 1 and so the numerator here is 0 and it doesn't matter what the denominator is ok that was just in case you didn't believe me okay any questions okay so this is a wonderful pair of things to be true about it's Maitre matrix basically all of the columns are orthogonal and interpretive themself is n the most wonderful thing would be if the interproximal themselves was one so they're all unit vectors need be like oh this is like the columns are like a orthonormal basis so you can fix that if you would really want that to happen by dividing the matrix by 1 over root n we won't do that but if we did that this would have this property where the columns are unit vectors that are orthogonal to each other and that's called being a unitary matrix you remember your unit linear algebra and basically with the way to think about it is just it means it's like a rotation maybe plus a reflection I mean forget about the fact that there's complex numbers just imagine like n-dimensional real space when you have like a matrix where the columns are orthogonal unit vectors it's like a rotation and possibly a reflection so it's like a transformation which preserves the lengths of vectors this will come up a lot when we study quantum well in any case let me ignore this little observation for now and just observe that another thing you could observe is that what we basically shown is if you take this matrix and multiply it on the left by basically the transpose of the matrix and this dagger means both take the transpose and take the complex conjugate about each entry then I mean if you imagine like you know rotating this matrix and multiplying on the left then and then doing the matrix multiply that like exactly computes all these N squared pairwise column inner products that we've been talking about and therefore you get like ends on the diagonal and zeros elsewhere and so for example if we now put like factor 1 over n here and we can change these two ones and now this is just saying that like this matrix times this matrix is the identity matrix and so we conclude that the inverse matrix for DFT is basically the again okay I say basically because there's a few hiccups there's this stupid factor of 1 over N but that's just a scalar and this is like the conjugate sorry this is like the conjugate transpose of the matrix so you like take the transpose and you also complex conjugate it ah thinking the transpose of this matrix actually doesn't do anything and then taking a complex conjugate just sticks a negative everywhere remember because the complex conjugate of Omega to the J is Omega to the negative J so like the inverse of this matrix is what you would get if you just stuck negatives everywhere here really this scheme remembers this too means Omega to the two so be Omega to the minus 2 and you multiply the outside by 1 over N I bring that up because you know the final thing will do to bring this all home is show how you can you know do this step evaluation on these roots of unity and n log n steps operations this multiplication by DFT and an interpolation is multiplication by DFT and inverse and 50 n inverse is basically the same as the ft so I mean if I show you the the fast algorithm for multiplying my DFT you immediately get the fast algorithm for multiplying by the inverse [Applause] so the final thing that we need to do to bring it all home is indeed show how you can do it's still on the sport or not yeah for a given input vector a how to multiply it by this DFT matrix and get the answer in n log n you know operations and then we'll have everything yes no I don't think so ah it looks that way but yeah you're saying like this this looks very suspicious it looks singular because this is what's wrong call them they're all zeros but remember this is my shorthand it's Omega to the zero so in the DFT the first row and the first column are all the real number one yeah okay okay so the final I mean crucial property that's not too nice is the fast Fourier transform algorithm which is just saying that you could do this discrete Fourier transform and by n times you know like and vector and order n log n arithmetic operations actually this proof is only easy and I'll definitely only tell it to you in the case the capital N is a power of two it's I think extremely hard if capital n is not a power two but I believe it can be done with something called the chirp Z transform but I just read about that this morning but it's okay for our problem of inserter multiplication to assume without loss of generality that capital N is a power of two because you know you can just I don't pad things out by a little bit and it's only going to affect the constants in your big O's okay so we'll definitely rely on the fact that capital N is a power of two and the idea is this is going to be a recursive algorithm and what I'll try to illustrate to you is that you can do multiplication by this DFT matrix by reducing it to two applications of like the 1/2 is big problem the ft n over 2 plus order and additional work or operations okay and once you see this then you're like okay it's we're pretty much done we get a recursion for the complexity recursion is like T of n is two times T of n over 2 plus order and this is like the most famous recursion in computer science it's like the recursion for merge sort or whatever so this goes down to order n log n ok so this is the last thing I need to explain how to do I should have put it on the other board okay well right this is hard to explain over the board and I really should have like brought in like some animation slides but here we go so what I want to tell you how to do is multiply this matrix remember you have the Omegas in there this matrix times this vector a 0 a 1 a 2 a 3 a 4 a 5 a 6 a 7 ok equals whatever and we should do this with two applications of DFT for plus like order 8 you know extra work ok what I'm saying is I'm gonna illustrate it to you in the case where capital n is 8 and you will get the picture I'll try to illustrate it ok so we're multiplying a matrix against the vector to get a vector and one way to think about that is you want this vector times a 0 Plus this vector times a 1 plus you know this vector times a 2 a 3 a 4 a 5 a 6 a 7 okay the answer that we want to get is a linear combination of these columns where the coefficient on the Cullens the coefficient in the columns are a0 a1 a2 etc okay that's how a matrix vector multiplication works and I'm gonna do it in I'm gonna get that answer in two steps first I'm going to get the sums for the even columns and then I get the sums for the odd columns and then I will add them up okay the adding up is like adding two vectors of length n so that can go into the order n work here so let me figure out how do you get getting the vector sum for the even columns by which I mean well this is the 0th column so this Plus this Plus this Plus this okay so I won first compute zero times this plus 82 times this etc when I manage that then I'll tell you how to get the odd column stuff it's very hard to see how I can tell well one thing that you can see is there are only even exponents in these four columns which is interesting and in fact let me just start writing out the entries in these columns squished together from top to bottom so the first four are like zero zero zero zero and then I guess it's zero two four six and I guess it's if you can read this zero six four probably - oh yeah the third one is zero four zero four thank you know our pies what's the fourth one zero six four two okay and then there's four more rows here but actually the four more rows are the same if I've done it right we get zero zero zero zero again and zero two four six again and zero four zero four again then zero six four two again so it's kind of this plus like ditto so actually if I can just compute like this a 0 a 1 sorry 8 2 a 4 and a 6 times like this matrix if I can compute remember there's still make is in here this matrix times this vector and get like a height for vector then the thing I'm actually trying to get is just that height for vector stacked on top of itself so I really just need to get that height for vector and then with like order and more work I can like stack a copy of it on top of itself ok so I can actually get all this stuff if I can just do this 4 by 4 matrix times this but you probably will not be surprised to find or learn that this is just exactly DFT for this matrix because really Omega is like an 8 through to the unity and so we have like Omega squared I'll make a fourth I'll make a sixth here but like if we define let's say sy to be Omega squared then this is a 4th root of unity and then this is like you know 5 side to the 0 to the 1 to the 2 to the 3 0 to 0 2 and 0 3 2 1 so you could tell I'm doing an example but it surely works out in general so this is a DFT for okay and so indeed with like one application of DFT for and then like you know re stacking I can get these 4 column sums and now I just need to get the other four column sums and this is the funny trick look at the sum of odd columns suppose I just did literally exactly this again but with a 1 a 3 a 5 a seven then what I would get is you know a 1 times this a 3 times this a five times this a 7 times this but I really wanted it shifted by one but let's say I did that and sort of got the quote-unquote wrong answer I'm just going to multiply I'll do that anyway which will again be one application of DFT and order and work and then I'll multiply this entry by Omega to the zero this entry by Omega to the one this entry by Omega squared finally this Omega to the n minus one and like somehow that fixes everything because like for example like in this row the thing you want is like Omega times the sky to its left there's Omega times the guys to its left and in this row like the thing you want is Omega squared times the thing to its left and so forth so it's basically the same thing except you have to put this little twist on the end but okay you can just do this twist in order n operations to I mean it's just one more multiply okay so that was like a little bit fast but I think it's pretty believable and we're basically done I will just conclude by saying some things in words indeed this fast fourier transform algorithm is recursive algorithm achieves this reduces the FCN to two applications of the N over two sized problem plus order n work recursively it's n log n operations so now if you're a real stickler you're like that's true but we were really trying to do this in like n log n time on the word Ram model and it seems like a bit of a drag because if you want to like literally implement this you like well first I have to like compute these nth roots of unity which you know are like transcendental numbers or something and then we have to do all these like complex number of multiplications which you know the word Ram model does not support this so what gives one observation to make is it's a funny thing where you're solving a problem where the input is integers the output is integers but you're passing through complex numbers so you're thinking about it like even just imagine the the polynomial multiplication problem you have two polynomials with integer coefficients you're multiplying them you're gonna get a polynomial with integer coefficients you're doing it this by like you know discrete Fourier transform multiply together a bunch of complex numbers discrete Fourier transform inverse and if you did it all with like infinite perfect precision you'd go from integers to integers so you can kind of at least imagine that if you didn't do it with full precision but you did it with like a lot of precision then it would probably okay because then you could just round the numbers two integers at the end because you know the answer has to be an integer at the very end and so that should at least give you some confidence that like there should be some sufficient amount of precision with which you can do all these operations such that you do it with this amount of precision and then just round to integer at the end and it should be working you just need like sort of error tolerance 0.49 for the final answer because that will let you identify the right integer okay and then long story short it's like you know some boring but straightforward calculations so like figure out how many bits of precision you need to like compute all these complex roots of unity to to make it all work and like you know Knuth does it and like a few pages of the art of computer programming I'm not gonna do it but he doesn't and yeah hooray like it turns out that like order log n bits of precision are sufficient so you can do it and you can put them all into like a constant number of like word RAM model words and do all the arithmetic and all the arithmetic steps in like order one time yep no they're into your operations so I mean you're like maintaining everything is like you know rational numbers which are like fractions of integers and so forth so yeah of course I'm going to skip that but Knuth heroically did it for us yep that's oh sure because like you the general thing you're computing here is like I guess like an integer linear combination of powers of Omega so I think that would probably be as expensive as doing the quadratic thing there is like at least for some of these like n log and log log n time algorithms that are like purely integer versions of them that use like some kind of modular arithmetic or something but I don't know about how they work\", metadata={'source': 'j77uFXx8tAQ'}),\n",
       " Document(page_content=\"okay everybody today's lecture is about analysis of boolean functions one of my favorite topics sometimes called Fourier analysis of boolean functions so that word connects it a little bit with yesterday's or the last lecture so this is a sort of mathematical tools associated with studying boolean functions and in this lecture I'm going to kind of tell you all the basic formalism and the definitions and so forth and then at the end we probably won't have time to really do some applications so I'll just tell you some applications then it's a little bit hard to specify you know when these tools are useful I tried to like write an extremely vague a set of criteria here like you know if you have a boolean function in your hands or a subset of the boolean cube it's the same thing and like somehow flipping input bits or like XOR input bits is relevant to your problem and maybe somehow counting input strings or the uniform probability distribution on strings is relevant to your problem that you may be able to use these tools from for analysis of boolean functions absolutely hard to say it's like you know asking when should you try to use combinatorics to solve a problem I don't know if you're counting something so we ran out of boolean functions the major tool in quite a few areas of CS theory both areas that we're going to talk about like literally in this course like quantum computing communication complexity learning theory hardest approximation PCP pseudo randomness also other topics that we're not going to talk about in this course like concrete complexity random graph theory additive combinatorics metric spaces social choice lots of different areas so I hope you'll find some occasion to use this set of tools in your life ok so as it says right there in the title this topic is concerned with boolean functions like what more basic object is there in computer science really then the boolean function now here's how you're most used to writing a classic boolean function the function mapping n bit strings into one bit and one thing that's both kind of annoying about this topic but something you got to get used to right away is it's very important to be flexible about how you represent bits so normally you represent bits are like true and false with 0 and 1 in computer science but there's a couple of different ways that I'm going to prefer to represent them in this class and I mean you're gonna make different choices for the domain and the range which is a little bit weird but you just gotta get used to it so for the domain sometimes I'm going to think about it as instead of just thinking about it is you know binary strings I'll write it as f2 to the N really I just mean here as a fancy way to say vectors of length n where the you know elements are numbers mod 2 okay and that'll be someone important because you know when you add two numbers mod 2 that's the same as exhorting them right and as I said somehow like X or Inge flipping bits plays an important role in this the start topic but also sometimes I'll change my mind and instead write it as like this I'll use plus or minus 1 instead of 0 and 1 and when I think of these two numbers plus or minus 1 as real numbers and maybe that's so weird to use plus or minus 1 instead of 0 and 1 for bits and one reason we're gonna like to do this is again if you think of plus or minus 1 as real numbers and you think of and this is distressfully weird if you think of like negative 1 is representing true and plus 1 is representing false then again multiplying becomes the same as XOR right I guess maybe yeah so this will be like very convenient so you also have to get used to accepting that and as for the range well especially in this particular lecture I'm always going to use or often going to use this a plus or minus 1 notation to represent the to possible output bits and I'm also often going to generalize to range R which is also a little bit funny but this is a hallmark of analysis of boolean functions we're not just going to think of functions mapping n bit strings to bits but in general functions that math and bit boolean strings into real numbers and you know the case of main interest is when those two real numbers happen to be plus or minus 1 or 0 & 1 but it's gonna be very useful to allow this generalization okay so the upshot is you know when we start studying boolean functions here I'll often say something like ok let's say we have a boolean function and so I may just write it like this snapping and plus or minus ones into plus or minus one or I might write this it's mapping vectors of length n of numbers 0 n 1 month 2 into plus or minus 1 or these might even in general be mapping into the real numbers [Applause] okay so really this analysis of boolean functions boils down to representing a given boolean function that you're interested in as a polynomial you know usually you think of a boolean function as being given to you maybe by a truth table that's the most explicit way to get it and this topic analysis of boolean functions involves finding a different representation a polynomial representation for it and indeed sort of just like in last lecture we're gonna be talking about the sometimes called the Walsh Fourier transform or the Hadamard Fourier transform I'll just call it the Fourier transform here and just like last time it's gonna be all about interpolating s values with the polynomial okay so in fact the main difference from last time is that last time we were talking about univariate polynomials and in this lecture we're gonna be talking about an variate polynomials what are you start with a simple example so like here's a very very simple boolean function say F is the majority of three function okay so it Maps three bits which I'll write as plus or minus 1 to the 3 into plus or minus 1 okay and the output on a bit string of length 3 is just the most the more frequent truth value okay so one way to represent this function is with a like a truth table X 1 X 2 X 3 and the majority of 3 just gonna be some table of height 8 so the first row plus 1 plus 1 plus 1 the majority is plus 1 X row plus 1 plus 1 plus 1 minus 1 majority is still plus 1 okay five more rows and then the last row is minus one minus one minus one and the majority of that is minus one okay that's a very simple boolean function and so I stressed a couple times here it's good to think of these plus or minus ones as real numbers and you can therefore think of this as with the geometric picture you can think of these as three dimensional vectors or like eight points the corners of a cube in space and this is like a labeling of these points by some values so sort of the geometric picture of this boolean function is like a labeling of the three-dimensional cube where each vertex is labeled by the function values let me try to draw this okay so those should have been - it anyway this is our three and these eight points here are like the eight points with boolean coordinates plus 1 plus 1 plus 1 this is minus 1 minus 1 minus 1 maybe this is plus 1 plus 1 minus 1 well I don't know I've drawn the axes in a weird way but they're labeled in this way and uh actually let me try to make it less ridiculous let's call this foot back here plus and minus minus plus try to make these - ok so somehow this way is - and this way is plus okay and the function majority of three labels each vertex by a number so like it's sort of labels this one by the majority of these three things which is minus one put that in a square this one gets labeled by -1 this one gets labeled by +1 and so forth and what we'd like to do as I said is instead of you know just taking this this truth table view point on the function or this label data of viewpoints on the function what do you fit a polynomial to this function it's gonna be a three variable polynomial that interpolates this function values and it's pretty easy trick to do this when you're finding polynomials that fit data on the corners of a cube or a hypercube these boolean values and the trick is to bring into the picture some like indicator polynomials for each of the vertices of the hypercube and that won't be too tricky so what I want to say is okay the majority of X and we're good usually use this notation X stands for X the vector x1 through xn this in general is going to be an x1 through xn in our example n is 3 and these X's are going to be we're interested in them taking on plus or minus 1 values okay so what I want to do is again I kind of want to you find the interpolate Amiel or a polynomial that fits this data like one vertex at a time so I'm going to start with this lower left vertex minus 1 minus 1 minus 1 where the label is supposed to be minus 1 so I'm gonna start by writing this 1/2 minus 1/2 X 1 times 1/2 minus 1/2 X 2 times 1/2 minus 1/2 X 3 that's some kind of polynomial and what have I got on my hand so far you see if you plug in minus 1 minus 1 minus 1 here then all these factors become 1 1 1 they multiply to 1 okay if you plug in any other of the 7 coordinates the corners of the cube well at least one of them will be plus 1 and for the Associated factor it'll become 0 and the whole thing will become 0 ok so this little expression is one exactly on this lower left corner of the cube and 0 elsewhere so what we can do now is like multiply this polynomial by sort of the EPS label for that corner of the cube which happened to be minus 1 okay so times well minus 1 and now we sort of got the function a polynomial which gets the function right on minus 1 minus 1 minus 1 and it's Co 0 elsewhere so we just have to do this same operation on the other seven points and add everything up so let's do that I'll move on to that second point there minus minus plus and well similarly you know 1/2 minus 1/2 X 1 minus 1/2 X 2 plus 1/2 X 3 ok and you can see that this expression here the second expression is 1 at minus 1 minus 1 plus 1 all the factors are 1 and it's 0 on any you know sign choice of signs for X 1 X 2 X 3 which is not this minus minus plus which is good so it's sort of the indicator for the kind of bottom right corner and we multiply this by what x value is down there which happens to be minus 1 again in our case and it's good now this polynomial so far is getting the right answer on the bottom right kind of point and it didn't mess up thought it was getting the right answer here either because this polynomial is 0 everywhere else besides the bottom right points okay so we can do this for all the other points and the last one will be 1/2 plus 1/2 X 1 the top right guy 1/2 plus 1/2 X 2 plus 1/2 X 3 times the label at the top right point the function value majority 3 on all pluses which is plus 1 and now finally we're done so this should be a polynomial in X 1 X 2 X 3 which you know computes the majority function on all boolean strings of length yep Oh hang in there yeah it's gonna be well it's gonna be much better it is kind of tedious well I'll give you a clue if you write capital an to be to to the little n this task will take you n square capital n square time but you should be able to do it in capital n log in time later but yeah we'll start with this method just to assure you that it can be done sort of for any boolean function eventually if you do this and expand it all out or more accurately have your computer expand it all out for you you will get something that's less ugly you'll get a 1/2 X 1 plus a 1/2 X 2 plus a 1/2 X 3 minus 1/2 X 1 X 2 X 3 so in fact like tons of cancellation happens and you get this and now you can check this right it's actually not so bad like if you plug let's say one one one into this you'll get 1/2 plus 1/2 plus 1/2 minus 1/2 which is 1 which is the majority of 1 1 1 if you plug in like plus 1 plus 1 minus 1 you'll get like plus 1/2 plus 1/2 minus 1/2 but then this will be plus 1/2 so you'll get 1 again which is correct and you can check the other six cases so good so we got a nice polynomial it's simplified out and this is a polynomial that sort of computes the majority of three function in this plus or minus one notation let's try to do another example but I'll leave it to you let's consider the function I'll call it parity sub 3 or you might call it X or sub 3 x1 x2 x3 it should be the XOR of these three truth values or the the parity of the 3 truth values maybe under this interpretation anybody know the polynomial that computes this yes yeah it's x1 x2 x3 it's exactly cooked up and then you know the niceness of the plus or minus 1 notation is you have this very simple polynomial it's just a monomial which competes the parody or the xor of all the bits ah great and I should emphasize that this is you know heavily using this fact that we're representing the range by real numbers plus or minus 1 I will not talk about in this class representing functions that map you know n bit vectors mod 2 into numbers mod 2 these can also be represented by polynomials and in this world parody sub 3 of X 1 X 2 X 3 is X 1 plus X 2 X plus X 3 mod 2 so that's confusing but well forget about this for this lecture we're gonna be sticking with this you know real value plus or minus 1 notation great so I like to point out one other thing here this is not just any polynomial both these polynomials happen to be multi linear multilinear just means like the highest power of any variable is 1 so there's like no X I Squared's or higher powers of X I and you can see if you use this recipe you will never get any squares or anything and it could have makes sense that you would never need to use a square like X I squared if you're trying to build a polynomial to compute a boolean function where the bits are plus or minus 1 because if you only care about when X is are plus or minus 1 then X I squared will always be 1 so if you had an X I square you could just erase it or convert it to a one so I claimed by this example that you should agree to the following theorem that well basically every boolean function can be represented as a multi linear polynomial okay you just do this interpolation trick for you know that and to the N vertices of a n bits cube just multiply all the function values against these indicator polynomials uh-huh the polynomial it Maps everything to true is the polynomial f of X you know 1 through X n is 1 or like s minus 1 whichever and yeah this is a multi linear polynomial it just means that there are no X I Squared's there no X is either but that's okay it's a degree 0 polynomial okay so sort of the fundamental theorem I guess of analysis of boolean functions is this every function f mapping n bit strings to and now I was gonna write you know bits because we're used to talking about boolean functions but you see actually it would be perfectly fine if I did this trick for functions whose range happen to be our you know functions that like gave a real value label to every corner of the cube because you know there's no trouble and be putting like any any real numbers here I would still get like a multi linear polynomial so I can extend to this more general case is expressible as a multi linear polynomial okay we also throw in the word uniquely here I've not really proved that but it will become clear or that it's unique by the end of the lecturer or you can think about now why the answer is unique yep I mean a polynomial you know what a polynomial is where the highest power of any variable is one so like these are all multilinear and like you know three plus X 1 plus 2x squared X XOR X 2 x squared X 5 is not multilinear because of this two uh I should throw it real here okay so is that theorem maybe aside from the uniquely part sound okay we've kind of proven it by construction okay now what is the general form of a multi linear polynomial in n variables multi-layer polynomial in n variables can have how many different terms yeah yeah exactly that's right so there's totally a general like if I just want to write down a general multi linear polynomial it'll be like a linear combination of monomials like this and each monomial you know every variable either appears in it or doesn't or has power 0 or 1 in it yeah so there's like one possible monomial for every subset capital S of the N variables right so the most you know a general form of a multi linear polynomial looks like this sum over subsets s of this notation is from your homework brackets n means 1 2 3 up to n some real coefficients e sub s and n times the Associated monomial product over I and s of X I okay by the way product over I in empty sets X I stands for one by convention okay so like the constant coefficient here is C sub empty set so great every boolean function can be represented by a polynomial that looks like this and I want to change the rotation a little bit one thing that's not too nice about this notation is it doesn't show the dependence of the these are the coefficients these are real numbers doesn't show the dependence of these coefficients on the function f so the usual notation is not CS but it's this F hat s okay so this is a real number and I'll rewrite it in here sit okay show a question okay so this that looks a little funny it's got some funny symbols in it this is called the Fourier expansion of f but as I said it's really just you know writing F as a polynomial and these coefficients here against the monomials are called the Fourier coefficients and I want to make one more observation while I'm looking at this all these monomials here if you think about this monomial just by itself it's also a function of the excise it's the product of the excise where I is in this set subset s of coordinates and actually even nicer it's a bullion valued function for every input X this monomial outputs plus or minus 1 and as we talked about here it even has like a like a a meaning it's the parity or the XOR of the bits in X from the set s so like these monomials here as you vary capital S or like these two to the N different monomials are all the functions that are like parity of a certain subset of the input bits and this is really like writing app is like a linear combination of these functions so somehow like majority of three function is like you take 1/2 times the while the parity of the first bit half plus the parity the second bit half plus the parity of the third bit and then subtract half the parity of all the bits ok where so if you're used to you know from calculus you know Fourier series like representing functions is like linear combinations of sines and cosines this is the same thing except it's representing boolean functions of linear combinations of XOR functions okay so let's get a little bit used to this notation okay we're still going to use this majority of three function as an example I can leave it up so let's figure out what its Fourier coefficients are so what is this quantity majority sub three hat of the singleton set one with that ha yeah this is this whole thing represents a number it represents the number 1/2 it's just saying that the coefficient up there on monomial that just includes x1 is a half and that's also a majority some 3 hat of 2 and it's also majorities of three hat 3 and we also have majority some three hats 1 2 3 is minus 1/2 and all the other Fourier coefficients are 0 ok so you know majority sub 3 hat s is sort of the the coefficient on the parody of the s bits function in this linear combination or similarly if we think of this function parody of all the bits all right 1 2 3 this is 1 and all the other Fourier coefficients are 0 that's this second one actually let's do the third one that's also up there let's say the function which always outputs true f of X is always negative one if you want to write this as a polynomial at x1 through xn well you've already done it this is the polynomial right here minus one so we have F hat empty set is minus 1 and F hat s is zero otherwise okay any questions yeah how do you mean what are the set of Fourier coefficients about what are the set of values the Fourier coefficients can take great question so if you're concerned with functions who map n bit strings to reals then the Fourier coefficients can be completely arbitrary real numbers and indeed this gives you like a sense of this uniqueness right like every real valued boolean function is specified by just 2 to the n real numbers and every multi linear polynomial it's also just specified by 2 to the n real numbers the coefficients so this lets not of proof that like there's a you know one-to-one correspondence but it's a it's a pretty good indication on the other hand if you restrict attention to the functions that we really care about the ones whose range or plus or minus 1 then it's true that not all coefficients become possible it's hard to say it's not easy to say an exact characterization of which ones become possible but I'll say one true thing that we'll get to at the end of the lecture if you have a boolean value function the sum of the squares of the Fourier coefficients is always 1 that's not you know that's necessary but not sufficient so you can see here you get half squared plus 1/2 squared plus 1/2 squared plus negative 1/2 squared is 1 1 squared plus a bunch of 0 squares is 1 this should have said minus 1 nobody caught me on that you should speak up when I make like such an obvious mistake negative 1 squared plus a bunch of 0 squares is 1 and so forth okay so yeah I want to talk a little bit more about your earlier question namely you know how do we go about computing these coefficients from let's say the truth table and like a less annoying way if possible and you may also wonder why do I even want to do this at all like why why do I want to compute these coefficients it turns out and hopefully this would be to be a bit of a recurring theme in this lecture if you can compute these coefficients for a boolean function that interests you it's usually great because mmm they encode a lot of interesting information about the function so a lot of interesting combinatorial properties about the boolean function can be read off from these coefficients okay so I'll try to give some examples of that throughout the lecture\", metadata={'source': 'lXJP-UkTl-4'}),\n",
       " Document(page_content=\"so on the subject of how to kind of compute these coefficients I want to you know be annoying and switch notation a little bit and go back to this notation so yeah this is the Fourier expansion of a function f of X equals the sum over S sub set of n F hi s then we can also write here you know as I said you know parity sub s of X and this is well product I in s X I when X is plus or minus 1 today I want to go back to now thinking though not of this representation but I want to think of X as being a a vector of zeros and ones mod 2 ok and then I need like kind of like a new formula for this it's quite simple so if I think of acts like this and I think of app is mapping such vectors 0 1 bits into the real numbers then I'm going to write PI sub s for this parity function that map's an n-bit vector into plus or minus 1 in the same way but then I'll have to you know slightly change the formula Chi sub s of X I'll write it as product I an ass of negative 1 to the X I ok so this is really the same as the parity function the on the bits s except now that I'm switch T's in like 0 1 notation I have to write minus 1 to the X I rather than just X I okay and in fact once you kind of do this and you start thinking about okay how do you actually go from the the values of function to the coefficients and a story expansion or vice versa it bears a very strong analogy to what we did last time with the discrete Fourier transform and I don't really have to tell you much time to dwell on that analogy but perhaps you'll pick up some of it as we go along in fact in a case where little n is one it's exactly the same thing of the discrete Fourier transform that's when n is two somehow it's because minus one is the tooth root of unity so those two a a picture like we were doing last time in fact we've been thinking so far of interpolation let's think about the evaluation question again let's say I gave you a bunch of coefficients for a multi linear polynomial and I want to know okay what's the boolean function that this is representing or like what is the truth table associated to this multi linear polynomial well it sometimes you just to plug in all possible boolean strings the polynomial and find out what the two to the N values are but let's do that using matrices again so let's say I have the Fourier coefficients or the coefficients of some multi linear polynomial F hat empty set F hat one F hat to F had s in general maybe the last one is f hat of everything and I want to get out the values of the function so it's like F on all zero string F on see what 0 0 0 1 F on X in general F on all ones ok let's switch to this 0 1 F 2 to the N notation again uh well it's again just a matrix vector multiply how do I get F at all zeros string if I have I guess the Fourier expansion now will look like this ok so this is my Fourier expansion of a boolean function f it's a linear combination with these coefficients of these like kwang-ho parity functions or XOR functions and I get F on the all 0 string by just plugging in all zeroes here and this sum will exactly be given by putting a I empty set of all zeros here and then cayenne theis archive 1 of all zeros get through the last one okay if I dot that row with that column well exactly again F 1 all zeros by this formula okay and then similarly here okay and the general term here is going to be pi sub s X this is in the X throw and this is gonna be in the S column and this is exactly like last time this is evaluation going from coefficients of a multi linear polynomial to the boolean function that it's a sort of the truth table of the function and this is just some explicit matrix here and in our previous scenario we called this the DFT matrix and in this case it's called the it's called the Walsh Fourier transform where the height of our Fourier transform and we'll just call it the Hana mard matrix and it's notation as H sub capital n where capital n is two two the little end [Applause] actually let me write over here what is the entries of this in the X throw and the s column well it's Chi of s of X which we know is products over I and s of minus 1 to the X I and actually another way you can write this is well we can interchange these we get minus 1 to the sum over I and s of X I this is sort of computing the parity of the bits in X in the subset s mod 2 and then we're taking minus 1 to that and you might even write this like this minus 1 to the sum over all I 1 to the N X I s I mod 2 where I kind of made up some notation here si standing for 1 if I is in the set s and 0 if it's not in the set s if you use that notation then you actually it's nice you see that it's a it's a symmetric matrix actually okay this is all looking a little bit simple heavy so let's as we did last time like do an actual example when little n is too [Music] so when n is 2 what is this sort of boolean Fourier transform matrix we got H 2 to the 2 or H 4 okay so it's a four by four matrix and it's columns are indexed by the subsets of the two numbers 1 & 2 and it's rows are indexed by the all the binary strings of length 2 okay and the entries are going to be plus or minus 1 and what you need to do to compute an entry is you look at what subset of the bits are talking about here and compute the parity of the bits of the string and that subsets and then kind of convert to minus 1 plus 1 notation okay one thing is a little bit annoying here the nice way to do it is consider this to be the first column and consider this to be the second column I know that's a little backwards but the thing you should really really do is like instead of indexing your binary strings like position 1 2 3 4 5 up to n you should really do n minus 1 n minus 2 down to 0 like as if they were like you know binary digits that's to annoying us too far as step for me but that's justifying like why you really ought to like reverse these columns anyway so let's go with it so in this column we take the empty set so the empty set contains nothing so this column is always plus 1 okay here we just look at the first column which is this one so 0 goes to plus 1 1 goes to minus 1 0 goes to plus 1 1 goes to minus 1 now we look at the second column which is this one we get plus 1 plus 1 minus 1 minus 1 and now we look at both columns together so the parity of these two things is 0 which is goes to plus 1 parity of these two things is 1 which goes to minus 1 get minus 1 and then these things add up to 0 mod 2 so we get plus 1 ok so finally this is the matrix H 4 this plus plus plus plus plus minus plus minus etc ok this is a actually a fairly well-known matrix I have more matrix it comes up in many contexts other than analysis of boolean functions so it's good to get to know it but it's exactly the function or sorry the matrix which Maps coefficients of a multi linear polynomial to like all the values on the boolean strings and it's quite analogous to the DFT metrics we saw last time except it's even simpler has an even much simpler recursive structure and I leave these to you to check maybe as exercises again if you let H to be the smallest example which turns out to be plus 1 plus 1 plus 1 minus 1 this is the N equals 1 case this is also DFT to ironically then in general h 2 to the little n is what's called the Kronecker products of n copies of this little two-by-two matrix okay this thing is some kind of matrix product called Kronecker product which you should look up on Wikipedia basically means you take this matrix if you just want a Kronecker product with itself once to get here you take this matrix and like multiply it into this entry and you get plus plus plus minus and then you multiply into this entry and you get like plus plus plus minus and the bottom left entry and get plus plus plus minus and you also multiply in into this entry and you know everything gets - so you get minus minus minus plus okay so as I said it has like very I mean very beautiful property is very simple recursive structure and so in fact just like last time and even more simply if you want to do this task of multiplying this matrix it's kind of marred matrix against some vector of coefficients see empty set see one crew see everything even though this is a capital N by capital n matrix you can do this matrix vector multiplication and n log n time by some simple divide and conquer method okay so this gives you how like a way to like simultaneously evaluate a polynomial and all the boolean strings 2 to the N billion strings and you know time that's nearly linear in the number of strings I'll be at that's 2 to the N strings to the little n strings this is like it's called the fast Walsh Hadamard transform but obviously we don't usually we're not usually doing this because to the end is usually pretty large unlike the DFT we're usually actually doing the DFT in practice I mean it's more for a theoretical interest okay so let's prove some more great properties of this matrix so in fact just like last time the key property it's not any two distinct columns have inner product zero on the dot products here oh and I guess two same columns inner products and two to the little end this one's obvious right because this matrix has only plus or minus 1 entries if you got part of a column with itself you just add up one capital n times and I'll show you this property in a second it's also quite easy but once we have these two properties we're actually in the exact same position we were in last time with the discrete Fourier transform and we can make all the same kind of observations so in particular that means like 1 over root and HN is a unitary matrix just remind you means it's like a rotation or reflection it doesn't change the lengths of any vectors you might already think about why that shows for a boolean value function the sum of the squares the Fourier coefficients is always 1 another thing this implies is that if you look at you know H and transpose times H n that's basically the matrix of all the dot products I think they stick a 1 over N in there you get the identity matrix again ok this fact it's basically identical to this proposition and so just as before I mean not worrying too much about the scalar 1 over N this means that like the inverse of this matrix is the transpose of this matrix and it's even nicer now there's no complex numbers and no complex conjugates and in fact this matrix is symmetric it's it's a easy to show so I mentioned it sort of follows from this fact so this matrix is its own transpose so in fact you know H and inverse which is the interpolation matrix or remind you at that in a second is well 1 over N times H n ok so this is the matrix you use if you have just like the kind of like truth table of a boolean function it's 2 to the N values listed down in a vector and you now you say oh I want to get the Fourier coefficients again you just multiply by this special matrix H sub N and divide by 1 over it yep a much more elegant generalization of the output space yeah if you were considering three values in your output then it might be nice to represent them by like complex third roots of unity for example yeah I mean there's a common generalization of both this set up and discrete Fourier transform set up when you have a function this is like an aside a function from like on a-z M to the N into the complex numbers and yeah there's a common generalization that involves roots of unity this is like the special case where m is 2 and so forth because the the the second roots of unity are plus or minus 1 then everything is a lot nicer in it it's just plus or minus ones and can take place over the reals and so forth ok and so in fact let me use this deduction to get like a formula for the Fourier coefficients so if you imagine just putting the truth values here or the function values here you get the Fourier coefficients out you also need to multiply by 1 over n you deduce that F hat s equals well this 1 over N times the sum over all X of f of X times the matrix entry Chi SX and actually I prefer to write this is follow C this is the same as 2 to the minus little end here we're like summing over all X and also multiplying by the 1 over the number of all possible strings so this is like taking an expectation this is the same as taking an expectation over a uniformly random input string of f of X times the sky s of X and this is kind of nice because this is a function that you care about this is a boolean function f you care about remember this is like the boolean function which gives you the parity of the s bits of a string X and this is kind of like saying when you take expectation and multiply it's sort of like the correlation between these two functions so saying that the Fourier coefficient f hat s remember we have this formula oh maybe it's still up there yeah this is the writing f as a polynomial f hat s is like the coefficient when you write F is like a linear combination of parity functions F had s is the coefficient on the parity of the s bits function in saying that coefficient is sort of the correlation between F and that parity function okay any questions okay let me prove this proposition real quickly because it'll give us a chance to use this expectation notation okay so the inner product of the s and T columns is well the sum over all rows of the entry products X is x times Chi TX and we kind of want to show that this is 0 when X does not equal T well you can see some examples of it here but let's prove it now if you want to show that some number equals 0 it's okay to multiply the left-hand side by some constant and in fact let's multiply it by 2 to the minus n that doesn't change the question hey I'll do that because it converts us to this nice expectation notation so this is true if and only if the expectation for a uniformly random X of the parody of the estimates of X finds the parody of the t bits of x equals 0 now let's say this is all the coordinates and let's say this is s we'll say this is T two subsets consider the coordinates I in the intersection these coordinates will show up both here and here and so you'll get a minus 1 to the X I twice okay so for I in the intersection you get these factors that look like minus 1 to the 2x I and well regardless of whether X is 0 or 1 this is minus 1/2 something even so it's just 1 which means that you can take out all these things in the intersection of s and T and it doesn't change this product so what you're left with is uh well the symmetric difference of s and T is the expectation of our random boolean string of the product over i and the symmetric difference s symmetric difference T of minus 1 to the X I okay and now there are two cases depending on whether or not this set is empty or not okay so if s equals T which is the same as s symmetric difference T being the empty set then here you have the empty products the empty product is by definition one so you have the expectation of just the one or it's like saying you know if s equal T like everything would cancel out and you just have a product of a bunch of ones here so it'd give you one so then you'd non randomly have one okay and that's actually consistent with the second bullet point there because I put in this factor of 2 to the minus n or one of our capital n okay otherwise you have at least one term here and here we're choosing a uniformly random boolean string and an equivalent way to do that is to just choose each of the bits independently in uniformly they flip a coin for each bit which is to say that the the values the bits whether they're 0 or 1 these are independent events and so the expectation of a product of independent random variables is the product of the expectation so you can use that this becomes it's the product I in this symmetric difference of the expected value over a random string X of minus 1 to the X I and now I mean for a given I X I is 0 1 with 50-50 chance each so this is minus 1 or plus 1 with 50% chance each so the expected value if it's 50/50 plus or minus 1 is 0 so actually each of these expectations is 0 so the product is extra 0 so it's 0 whenever s doesn't equal T ok good ok so I gotta give you a little bit more like notation and stuff and then I'll finally take 15 or 20 minutes to tell you about some applications of how all this like representing a boolean function by a polynomial is useful ok so this notation and it's sort of inspired by what's on the far right board two boolean functions I'm gonna write something that looks like there inner products and it's gonna basically be like their product but divided by two to the end so what I mean is it's this 2 to the minus n sum over all X of f of X times G of X it's like if you line up their truth tables together dot product to them and divided by 2 to the minus n or to use this probabilistic notation and it's the expected value of f of X times G of X again this is some kind of like correlation between these two boolean functions so this proposition we just finished proving actually showed that two distinct parity functions have this in a correlation being 0 and it's 1 if they're the same that's actually what this is saying and this formula over here for the boolean for the Fourier coefficients we can also use the same notation for this this is saying that f hat s is the sort of inner product or correlation between app and the s parity function and actually it's an example of this if I take s to be the empty set I get that F hat empty set just remember this is the coefficient in the polynomial expansion sitting next to the empty monomial in other words it's the constant coefficient this equals the inner product of F with the constantly one function which is the expected value of f of X or in other words the average value of the boolean function and I sometimes like to think of this like the simplest example of a Fourier coefficient telling you something interesting about a boolean function the Fourier coefficient associated to the empty set is the average value of the function average of the functions values so if you have a like a really boolean valued function that's like plus or minus one valued for example this Fourier coefficient tells you how biased it is towards plus values or minus values and in particular is 0 if the function is equally often plus or minus 1 so that's nice the other thing is this I look at f inner product G and I replace these two guys by their Fourier expansions just like an inner product this is this is a linear combination of vectors is the linear combination of vectors I can use linearity if you don't follow this it's okay you can trust me that it's true this is sum over st f hat s + g hat t inner product is kaity and using this fact this expression is usually 0 unless they're the same in which case it's 1 so most of these terms in the double sum drop out and the only thing we're left with is sum over s f ha s G hi s this is a pretty cool formula - this is called parcel Vols formula or a plant reals formula it's kind of nice it says you know if you take two boolean functions F and G and sort of stack them up in truth tables and look at their correlation by basically taking a dot product it's the same actually is stacking up their vector of Fourier coefficients and taking the dot product so how similar the Fourier transforms are istic somehow exactly the same as how similar the functions are good okay and a corollary of this if you take the inner product of F with itself well by definition this is the average value of f of X is the average of the square of the function and this formula tells you that this is also the sum of the squares of the Fourier coefficients and in particular that's one sorry this thing is one if f is a boolean valued function let's remember these are the functions we care about most if we think of F as being a function that actually takes on boolean values then f of x squared is always 1 so this is always 1 regardless of what boolean function f is and this justifies this like neat fact I told you before that for a boolean valued function the squares of the coefficients always add up to 1 [Music] hmm I'm sure there's one other than neat thing you can do here remember F hat empty set is the expectation of F this is the expectation of F squared so if I take this and subtract the square of this I can conclude expectation over X of F of x squared minus expectation of f of x squared which is the variance of X F F how does this formula it's the sum I take this whole sum when I subtract f hat empty set squared since the sum over all the non constant coefficients of the squares of the Fourier coefficients okay so I bring this up just to say it's like another example of how you can like learn something about like the truth table of the function or functions values like it's variance by a simple formula of the Fourier coefficients\", metadata={'source': 'TIqqO9PsHPg'}),\n",
       " Document(page_content=\"I want to tell you some applications I have time to proof any of them but I want to tell you some applications I don't have too much time so let me tell you some applications to a topic that's not even exactly computer science I'll tell you in applications to area called social choice just like the theory of decision making and voting so let's say applications so how do you boolean functions have anything to do with social choice boolean functions are can be a model for an election rule so a function a boolean function mapping n plus or minus 1 values to a plus or minus 1 value can be thought of as a voting rule in a n voter to candidate election ok the two candidates are named plus or minus one and so all the end voters vote for a plus or minus one and so their votes consist of a boolean string and then F tells you given the votes who the winner is so a popular and democratic choice is the majority function but that's not the only one used in practice so like to elect a president in America they use some like Electoral College thing which is like a two-level weighted majority of the votes right so it's like a two layer neural network applied to the votes and in some other countries they use an election scheme like this f of X 1 through X n equals x I let's call the dictatorship there's only one person's vote makes a difference you might think about the voting role f of x equals negative x i as well okay so let's uh an interpretation of boolean functions in terms of voting and now let me tell you another example of how like boolean sorry Fourier coefficients can tell you things about your voting scheme so there's a concept and analysis of boolean functions called the influence of the eigth coordinate or vote or if you will on an election rule f and in the social choice literature it's called the bonds off power index it's some attempt to measure how important eyes vote is so useful eyes vote is to an election scheme because sometimes like you have the friend voting powers right like in electoral college with different sized states like you're voting power kind of depends on whether you're in a big state or a small state so sometimes you have a symmetric scheme like majority where everybody has the same influence of it seem sometimes not so the definition of this is it's the basically the probability that your vote made the difference or that the ice boat is pivotal okay so the probability that like if you compare the outcome when X is one way versus when X hive oats the other way and everybody else vote remains the same probability that flips the outcome of the election you might say what's the probability over here it's over assuming random boats what's four minus one which might be a little bit weird to think about assuming that people vote randomly but like in the social choice literature this is a known assumption called the impartial culture assumption it's like some baseline to I don't use when you're trying to I don't understand something about an election rule in otherwise a vacuum so natural concept it's actually invented in like the 40s snuck by bonds off by an earlier person Penrose and yeah here's the formula it's not hard to prove but yeah enough time for it but in five minutes we could prove this the influences I Fodor on an election is the sum over all subsets that contain coordinate I of F hat s squared and she's very nice right in some sense this even particularly nice because right this somehow measure is like oh just keep me the Fourier coefficients next to monomials that involve the ithe motor and I'll square those and add them up so this one also looks like the importance of I to the Fourier expansion and it's exactly equal to the influence of voter I on the outcome scheme on the election scheme and you can also see this is a number between 0 & 1 right because we know that the sum of the squares of all the Fourier coefficients adds up to 1 so that's kind of nice and in fact you can prove Oh quite a few very interesting theorems in like mathematical social choice using analysis of boolean functions let me just mention two so using analysis of boolean functions you can prove arrows impossibility theorem so this is some it's like I'm of the most famous theorem I guess in social choice Ken arrow got like the Nobel Prize in Economics for in the 50s basically says if you have any three candidate or three candidate or higher election any and you try to cut with a voting rule for picking a winner out of three candidates that satisfies a bunch of natural properties that you would consider desirable the only voting rule that has the properties is dictatorship those consider like a negative result for like having up a nice voting rule when there are three or more candidates and there's very nice proof maybe you'll see it on the homework of this theorem using analysis of boolean functions and it uses this fact I'll just write this down without really explaining it if you have a three candidate election and you let people vote at random and you try to aggregate their votes in a pairwise fashion then you can get this thing called Condor size paradox which is that the voters seem to prefer candidate aid to pick can it be and they prefer B to C and they prefer C to a which seems paradoxical but can happen if you're using like a voting scheme where you like you compare a and B using like a two candidate rule F and then you compare B and C using a two candidate rule F and you compare a and C using a two candidate rule F so you get asked what's the probability of this paradox occurring if you use a boolean function f and again it's this it's a Fourier formula it's 1/4 plus 3/4 sum over s negative 1/3 to cardinality of s f hat s squared can only pull out there because it's kind of cool right I mean this this interesting thing about voting has this like very kind of elegant like formula involving the Fourier coefficients ok this is basically what you use to prove arrows theorem another one that I'll just tell you in words I don't have time to write it it's probably the most famous theorem and purely an analysis of boolean functions it's called KCAL theorem after conchal i and lineal who proved it in 1988 and you can stave it in terms of voting like this if you have any two candidate voting rule like this f and it also has this property expectation of f of X equals 0 this is actually extremely natural property right it says that f has 50% plus one values and 50% minus 1 values ok so it's not like inherently biased towards one of the candidates for any reelection rule like that you can always find a little o of one fraction of the voters like a sub linear fraction of voters it's actually like n over log n of the voters such that if you bribe them like fix their values to either plus 1 or minus 1 that essentially fixes the outcome of the election in the sense that if everybody else votes randomly there's a one- little of 1 chance that the outcome of the vote will be equal to the bribe value this is also a negative result for voting but it's interesting result in like it can only be proven in this one actually using like quite sophisticated analysis of boolean function techniques in fact it uses a like the main non elementary fact in analysis of boolean functions which is called typer contractivity just say that long word to scare you but the last a thing I want to mention is one more application and analysis of boolean functions that uses this hyper contractivity property and it's like a generalization of Chernov huffing theorems which I quite like let me mention this last application so if you remember turn off huffing theorems the set up was like you know you're adding up a bunch of independent random variables you want to say the resulting thing is very close to its mean with high probability so let me write down something which is essentially how things count let capital X 1 through capital xn be iid plus or minus 150 50 random variables let P of little X 1 through little xn be any linear polynomial a 0 plus a 1 X 1 plus dot dot plus a and X n the AIS are real numbers and let Y be what you get if you plug in the random plus or minus 1 bits to P ok so it's basically going on here is you have these fixed coefficients and you're adding them up with random plus or minus 1 signs ok a hopping now tells you the probability that Y is far from its mean by like T standard deviations is that most e to the minus T squared over 2 okay so the final thing I'll tell you it's not using analysis of boolean functions you can prove a generalization of this to higher degree polynomials so just the theorem is that same set up but if P has degree at most K same conclusion but with a horse bound it's still an exponentially strong bound it's like e to the minus T to the 2 over K and there's over some constant here ok so if K is 1 it matches and you know still K is 2 or 3 or 4 you still get some exponentially fast decay of the probability of being far away from your meeting yep great question the question was whether what happens if voters in some of these results of voters are voting anything other than 50/50 basically if the voters vote probabilities are independent like they're all voting one way with probably P or even pi/4 different p is you can generally use fourier analysis to get similar results but sometimes there's a quantitative dependence on P P is going to 0 actually a lot of interesting work like research work and like you know these current years is like trying to get versions of these theorems that don't have any dependence on P\", metadata={'source': '2CTu1Qsw6m8'}),\n",
       " Document(page_content=\"okay hi everybody uh here we are again uh i hope this time is a little bit better for some people um so yeah uh nice to see you and uh i have any questions about the course yeah um i was wondering if you can go over 4.3 c i'm having trouble gaining like intuition really for the question what's 4.3c about the switching level and like proving the bound on your fourier coefficients for a circuit with uh like log d minus one for a polylog and depth okay um well what tell me your thoughts on the problem sure so um i guess i was thinking here that if the average if there's like a bounded uh like a bounds to the number of gates and the that uh so um if there's a bound to the number of gates and like there's a certain depth then it kind of makes sense that the average sensitivity is founded as a function of those two things because if you have like a low depth then you can't have such high average sensitivity because that would imply that like multiple bits are able to flipping multiple bits output change the output of the circuit but you can't really have that if you can't differentiate between bits so well if you have a lower depth circuit um that's possibly true i mean that intuition is actually quite good for showing the average sensitivity of a decision tree is not that high but this problem doesn't ask you to prove that every sensitivity is low it like tells you that the average sensitivity is low sure okay so um yeah perhaps that might be some intuition for why you find like this fact believable that i try to draw here that if uh that if you know f is computable by a small circuit then it has low average sensitivity but uh that's all i mean all well and good to think about but i mean that's that's given to you i mean this is what we gotta prove fisher or i mean uh so that thing like the prevalence of like xor functions above a certain size is small in the decision tree which else kind of makes sense uh i don't think there are any decision trees in this problem sorry uh circuit um i mean in the fourier expansion yeah i mean i guess so yeah [Music] get started um okay well i mean uh i mean i got to figure out exactly what you have and what you want to prove is like a good first step and then um uh sometimes there's like only so many pieces to put together and that um you know we we only know like so many so many facts about uh fourier coefficients and uh you know there's some reason why this is like a part c of a problem involving part like a and b as well so a little bit like you kind of have to just like stir the known ingredients together on this one but um [Music] give it a give everything for a little bit maybe we'll come back to this one uh if you have another question i see there's like another question in the chat we look at part a first i was having trouble dealing with the summation on the right um sure let's look at part a drawing keep drawing white on white it's not the best idea just to follow the following identity from lecture influence on i f is the sum or resets s contain i i like to write this s contains i it's the reverse of uh the element sign you can use backslash knee in um the tech for that it's like you know backslash n spelled backwards uh of f hat s squared um okay yeah we want to prove this i suppose it wouldn't hurt to remember the definition of the left-hand side this is uh maybe exactly how i wrote it but like probability uh if you choose x uniformly at random from all the binary strings that f of like x is different from f of x with the i fit flipped i mean i think this is how i wrote it in class um it's all right yeah so i mean uh person who asked the question do you have any any thoughts to start on this yeah i was trying to work like from both sides of it so i guess i did start with it so um i guess the obvious first step is to just use the polynomial definition the f true you want to plug that into here yeah um okay simplifying from there okay so it's the probability that x is drawn from plus or minus one to the n yeah i mean i guess the point uh is that f of x sort of by definition it looks like this it's the sum over all the monomials indexed by s of this coefficient f hat s times um well it's like the monomial also just write like this product i and s of x oops i don't know why i wrote there x i yeah so that's f of x and how should we write this bit well so that's where in the next step um like this will be an if and only if with just checking the subsets that i is uh contained in because they're only not like the terms won't change that i is not contained in since we flip only flip the i bit i think i know what you're saying but can you just yeah a bit more like um okay well i guess i wrote i i just wrote the same exact thing for this step on the right hand side and i just said like it's just like a little bit notationally difficult like it'd be like all right this and now this but like what should i write here yeah um i guess it's probably also a bad form of me like just sorry to interrupt but like okay yeah we we chose the special letter i here i i i i it's probably like i mean this is a good life pro tip too it's probably a bad idea to write lie as kind of like the dummy variable here oh yeah let me write that as um j yeah my js are not too different from my eyes but okay yeah but so then what should i write here in this blue spot um well so i guess if we change the summation to being over i in s then we can put the whole right hand side as the negation of the left-hand side because each of those terms will be just have the i spit flip so the parody will be flipped i see what you're saying so you're saying like um well i think i see what you're saying you're saying like okay like what i was trying to say is like it's kind of inconvenient here you want like product over j and s i don't know why i'm doing this in yellow like product over like j and s of like like these things like the j coordinate of these things like if you call this whole string x twiddle you want to say like x twiddle j here yeah and so i guess that will be the same if i is in that set and right left otherwise good good good so i agree so uh i see what you're saying so you like kind of cancel out all the terms on both sides that have an i not don't have an i in them and you'll say so this is like you're saying this is the same as sum over s containing i f hat s product j and s x j different from wait oh oh negation of sum over s contained i f-hat s product j and s yeah exactly but then yeah i guess that's getting closer to what we want because it has the but then i didn't really know what to do from there because this whole thing is like a probability right so i didn't really know how to think about this condition yes that's a valid difficulty um i suppose like one thing that's like a little bit funny is this okay what i'm about to say is gonna sound weird but like this is a real number and um see back before we did what you said uh this is also like a real number we knew it was either plus one or minus one because like f is a function whose range is plus or minus one uh i hope that was specified yeah um [Music] but now like we took this but like it's it's funny because like this is like a bunch of uh numbers that add up to plus or minus one but kind of for like a weird reason and now we like kind of deleted some of them we deleted the terms that had an i in them and so we don't really know so much about what kind of numbers this can be and then it's like a little bit weird because like if this is just like some real number and you're comparing it whether it's equal to another real number that's kind of like a fragile thing like you know i feel like a computer program it's like a bad idea to check if like two floats are equal because you know what is what is uh equality for for floating point numbers so i have to like worry about this like a little bit um maybe we can even do an example like uh i mean my favorite example that i know off the top of my head is this like majority of three function x um i've got like some blurriness here see if i can undo that um oh anyway uh this happens i happen to know this one is like a half x1 plus a half x2 plus a half x3 minus a half x1 x2 x3 and kind of like i was trying to say like it's a little bit funny like this is you know it's like some halves in it so like it's like a it's like a mild miracle that like when you plug in like plus or minus one values here for the x's that this always ends up as plus or minus one um and indeed let's say we're uh let's say we're concerned about the influence of the first coordinate on uh the majority of three function that's like the probability in a little mini election with three people um governed by majority that the first person's vote like makes a difference um you know this expression here where we only take the terms that include i i guess that just would be here i is one so it'd be like a half x one minus a half x one x two x three uh i guess this side is um well it's the minus of that so what function is this like does it have a semantic meaning i'm not sure yet i'm thinking well we can make a truth table x1 x2 x3 this green thing it's a mystery quantity let's see plus plus plus plus minus minus minus minus plus plus minus minus plus plus minus minus plus minus plus minus plus minus plus minus now i have to plug it in let's see uh doing like x1 x2 x3 here that's uh let me make a column for that let's do that x1 x2 x3 that's like the xor of all three of them that's going to be plus minus minus plus minus plus plus minus now we're doing like half of x1 minus that so we're kind of taking like this column subtract this column and then divide by two so let's see like this minus this is 0 so that's 0. this minus this is plus minus minus 1 is 2 divided by 2 is 1. plus take away minus is two again divided by two is one plus take away plus a zero minus take away minus is zero minus take away plus is minus two [Music] this is uh minus one minus take away plus is minus one and um i say y minus is zero well okay i guess one thing is like it only yeah it's zero one or minus one um let's see when is it zero i mean just looking at the truth table when the other two things are uh are the same yeah i would seem right yeah i mean the zero rows i'll just circle them i think what you noticed here are zero these are the same these are the same these are the same and these are the same okay when's it one when they're different and x1 is plus uh yeah when they're different okay what's left over is when they're different and like somehow it's like yeah these ones x1 is plus these ones minus um [Music] okay so actually maybe we can figure it out even for this specific thing like this green box thing is like literally this expression this is and this is like the negative of that so like when is like a number equal to it's negative when when does this happen zero yeah the number is zero so somehow this condition is the same as like this green box being zero which we saw well see like now here's something we saw hey presto that occurred precisely when x2 equal to x3 um so the probability of that green box being zero i guess is the same as the probability that like x2 equals x3 which is actually the same as like the probability that like i mean if you think there's a little three-way voting scheme right that's when the first voter has influence when the other two people vote the same way because you're like the tie breaker now so i guess that's like illustrating the result in action um yeah i suppose well i suppose you have to think about like whether or not it was like a miracle or like there's some kind of reason why this expression that you got well it kind of it was it was sort of zero exactly when on those inputs where x1 vote made a difference or it was non-zero so maybe that i don't know perhaps that holds in general um yes that's good i'll i'll think about that yeah i think about that i think about that for a bit think about it um let's see somebody else have a question they want to ask another question about 4.2 c 4.2 c that's this one let's see what's going on in this one okay you have a decision tree oh you it's about decision trees and about maybe either fourier expansions and trying to learn them [Music] let's see under the promise i remember this problem yeah okay what are your what are your thoughts okay so i was thinking that we could do something like uh what's described in part b where we like to generate a bunch of random examples and plug them in f um and try to with high probability compute each of the um coefficients in our polynomial um or essentially each of our fourier coefficients um except that ice and like pick out appropriate delta so that we can union-bound it and hope that you know all of them are not wrong um except that i think that i ended up with a choice of data that makes it poly um rather than into the o log n um which seems not good wait wait sorry um uh [Music] yeah you wait there's a twist in that little story where you like started describing you know your thoughts for the algorithm and i was like okay but then like it ended with a technical question about delta so um yeah so it's all fine but like so wait you said i mean should we address the delta business you said that what was i think like we needed to pick a delta for when we were doing our choosing like random examples and plugging them into f in order to like make sure that in the end with high probability all of our coefficients are good um and i think that might be the part that i'm messing up on because uh when i try to do that i come up with a delta that gets the wrong uh like time bound basically uh well what delta were you using i was using delta as o of one over n to the c delta equals of one over n to the c okay that doesn't seem like too small is that bad i'm not sure i'm psych this feels like um then we'll draw like um four to the h log into the c examples which is like up about n log n ish and to the c oh this is like c 4 to the h log n yeah and we know h is like c log n as well okay uh okay oh you're saying h is c log n so what 4 to the c log n is 2 to the log n is and so the n to the 2 c yeah c times log n it doesn't seem too bad c is a constant right this is polynomial yeah um you're even allowing yourself more than polynomial time yeah so my worry is that i've apparently made a strictly polynomial number i apparently need a strictly polynomial number of answers which is an open problem and i know oh you think you did it too good um i see um well uh now i mean i suppose i have to find out what you're how you're solving c so like you're saying you think you can do it with like n to the consonant okay okay well i guess that's what i thought um [Music] all right great yeah well um i am a little skeptical but uh i'm so skeptical okay so i think i was so we we do the thing in part b for every all of n to the c um or like four to the h distinct um s's because from part a we know that the coefficient is not zero for at most four of the age distinct masses okay so yeah let's assume we've done a and b okay um and then like we can use the union bound and say okay if each of them is wrong with some delta then all of them are wrong with at most like the sum of a bunch of deltas yep um and there's unto the c-ish deltas so that comes out to like delta n to the c and if we want this to be like a constant then we end up with like well delta equals one over n to the c so that delta m to the c is like a constant okay um i'm seeing a sound giving me a correction in the track you don't know which for the age to choose uh that makes sense yeah i think i had like the same exact approach but like we know that they're non-zero for most sport of the h but we don't know which ones like to try i guess we only know like the ones we don't have to try are the ones where the size of s is bigger than h true um yes that is a that is true uh it's part of the difficulty yeah that might be the source of my [Music] too good answer okay yeah yeah that is part of the difficulty um it's like there's not too many things you have to find but you don't know exactly where they are um it's part of the problem so yeah i mean if you imagine i mean you know it says you know describe an algorithm like yeah think about that algorithm like what is step one of that algorithm what is step two and i don't know perhaps if you try to write it down you will yeah see maybe that you will get stuck um yeah props perhaps you can think a little bit about that um maybe we could take another question for now or cycle back to something yeah uh i was looking through it i think i kind of i'm almost there but there's still something kind of weird before it's like not bounded by epsilon but it's not about epsilon times the number of things you're summing together so you're talking about like part c of 4.3 i think sorry i think i just like uh because i just figured out like how you kind of relate like the f s square to the average sensitivity uh okay well yeah it's you do it like this oh uh i guess i wasn't really using that so much as like the probability of f hat of s squared being zero is going to be like the probability that uh wait sorry can i can i pause it for a second oh sure probably with respect to what random oh sorry uh respect to like some sort of with respect to like an s where s is greater than uh that like c prime divided by epsilon sorry i still don't get it um if you're gonna say probability then like you know you're rolling some coins or flipping some dice or choosing something at random so like what thing are you choosing at random sure so some s subset of like all the possible uh like uh set of n okay where uh s is bigger than c prime divided by epsilon times log star of t minus one time so like you're choosing it at random yeah choosing one of those random so basically like the expected value it's like obviously using that like get the expected value of f of s squared [Music] uh still how do you mean like i know it's a little hard because we're like we don't have like a shared board so easily but like what's the math what's the math fact i can write down and stare at sure it's like the probability that um so like the probability that f hat of s squared equals zero where s is taken from like all possible subsets of uh like n yeah but s also has to be bigger than like c prime um is then going to be basically probably that like um for those uh then if you oh wait right which is like basically equivalent to the probability that there's going to be some i guess the way that i was writing that was that i actually expand out the definition of that square squad sna and then i basically look at the probability that like the sensitivity of like every x [Music] within [Music] what's the probability what's the expected value of f of s squared and then you can get that by writing out its definition [Music] and then looking at what's the probability of like i think that's right a bit more okay i mean there's a lot of uh symbols in this problem um but some of them are a little bit irrelevant i mean like this whole like blob of stuff oops i'm still drawing i mean that's a lot of symbols but it sort of just like recurs here um without the epsilon uh like um uh we can also for example okay let's do our let's do our same like little favorite example as always f is the majority of three uh so what is the average sensitivity of this 1.5 yeah it's 1.5 that's true so um let's take epsilon to be let's take epsilon to be um uh let's take it to be point eight um you may imagine that like perhaps we're wondering whether the sum over s greater than or equal to 1.5 over point eight less than or equal to point oops of f hat s squared is less than or equal to point eight um well is it i'm going to say yes because the problem says uh that's true but can you say yes because of some like numerical facts what is this number this is a number what is it so i guess the size of s is like first has an s of two or three and then that's going to be 1 minus since the sum of f squared where s inside of s equals one um true uh i guess you're using the fact that like it's zero when s is empty but that's true well i may not put you completely on the spot does anybody else know what this number is well this 1.5 divided by 0.8 is 1.875 so sure that's what this number is i mean like this whole number oh isn't it like i mean if if the size of s is greater than equal to 1.875 then there's only one in the majority algorithm there's only there's only one um there's only one like set which has a positive coefficient with um a nonzero coefficient which is likely three so it's like a quarter then yeah that's right it's uh i'm just staring at this expression down here there's only this one and yeah so you get this one it's a quarter so this is a quarter and uh well great yeah that's less than 0.8 um so it worked um yeah so that's good how did you know by the way alive how did you know that this was 1.5 uh because like it takes 1.5 like one point basically the majority is determined by one like more than 1.5 uh like this being that's one or negative one not so happy with that uh yeah i was kind of like just kind of guess oh all right well uh i guess uh you can review the definition of the average sensitivity then um did not just use the part b for that uh that's a good idea yes yeah just like if you just use part b then it'd be like a quarter times three times a quarter times three plus um a quarter uh sorry like it's because because like there's one two and like that there are three coefficients for like s of size one which are which have coefficient quarter uh yeah and then like one of size three which has coefficient minus half that's right yeah i mean you can compute it in two different ways you can compute it i mean by the definition just like average sensitivity yellow is not easy to see but you're saying you can also compute it using this formula where you like square this times one square this times one square this times one square this times three and add that all up and indeed i think that will give you one point five oh sure okay yeah i didn't see the formula down there oh um yeah oh let's take a break from this one uh we can come back to it um is there another question um sure 2b yeah i guess everybody solved number one huh impressive i thought that's like tricky um well anyway uh yeah sure 2b what's your question uh so for now what i'm thinking the algorithm is like uh so the actual f hat of x is like a one over n and sum over all the x and that f of x times like x s of x that's like the actual i'm over sorry sum over x i'm just writing something over here uh what did you say here uh f of x times like uh i guess this right yeah yeah that thing yes true so what i was thinking is like if we don't use all the x's here if we only use a random examples we drawn like to estimate ahead of x yes uh but so i think we're supposed to use like i've had of x is like an integer multiple of two two like two to the minus h or something yeah this does help yes yeah yeah and like so if i estimate like i've had a best that way i don't really know how to like bound the probability um i hear you um let me ask you this what's your favorite value of s um empty set great that's mine too i was i was hoping you'd say that uh so let's let's if s is the empty set and let me just plug it into what you wrote like f empty set is one over n sum over x f of x and then i guess this is one so that's it um okay let me i'm just going to scroll back and look at the question here uh describe and analyze a simple algorithm like you in any set s so yeah we decided it was gonna be the empty set uh that's good i mean we better do it at least when s is empty set draws some blah blah samples and outputs a number which is very likely to equal f hat after set okay that's cool so let me go back to where i was over here so yes we have so-called random examples from f and we're trying to pretty much figure out with high probability um this quantity on the right yeah um so yeah uh let's play a game i'll give you i'll be the i'll be you'll be you be you you'll be like the learner and you don't really have too many things you can ask you can do other than like ask for a random example but do you want a random example uh f of zero sure well you don't get to pick but like you can just say like an example okay yeah oh sure yeah well here's my example and i don't know if you knew but n happened to be um um 10 and there was like an h as well and maybe h is like i don't know three or something let me make a four um cool uh yeah x is uh zero zero one oh ten is a lot of digits well okay i'll do it one two three four five one two three four five okay and for this x f of x is uh plus one okay what next do you want another example yeah okay cool so then uh maybe like i'm just picking this at random and for this one f of x is um plus one let's say i'll give you another example why not like let's say you asked for another example okay this one x is minus one what do you think so far if i was like the machine broke like you got to stop and guess right now what is f hat empty set what would you guess uh not easy but i mean if you had to guess or you can have some more examples if you want you want some more examples i don't really see how more examples would help here oh maybe [Music] but we have to figure out this i guess n is 2 to the power of 10 that's like 10 24. this is uh all the strings of length 10. trying to figure this out anybody else want to do another step of the algorithm can you take the average off the values because which ones uh b plus yeah yeah what's the average of those i guess it's one-third so if you had to stop now and like you know like you know you were on a game show and like you had to do your best guess for what f-hat empty set is um should we guess a third one half or one fourth yeah one third is not like your smartest guess because i mean it's on one hand it seems like a good idea but on the other hand you kind of actually there's a reason to know it's 100 definitely not one third do you know why i mean sometimes for sure it's not a third right i mean like look at this number it's like the sum of a bunch of plus or minus ones divided by 10 24 like there ain't no way that could be one third actually uh it could be close to one third but it's i mean uh um yeah so do you have a gas let's see oh yeah sorry i don't want my chat like right open in front of me let me get it out here in the chat zoom troubles oh it says used to be a multiple of one over two to the four yeah true yeah i mean i think that was like on a previous um fact or something it has to be a multiple of one over two to the four like a multiple of one over what is that one over sixteen so yeah so like if you really had to guess i mean probably you probably want a few more examples but if you really had to guess what what do you think would be a good guess five over 16. yeah that's what i was going to say if i had to guess this was like the life is on the line or maybe to put it positively i if i could win a million dollars if i guess correctly i would probably guess five over 16. cross my fingers but um yeah maybe if i got like another let's say i was like oh the machine came back online i was like oh great give me like two more examples and i was like okay zero zero zero zero zero one one zero zero that one is um minus one maybe the next one is like zero zero one one zero one zero zero one f of x is like minus one who answered gabe what's your best what's your if you had to guess right now what was this expression what would you guess um let's see what would i guess yeah hold on it's like close to [Music] negative one fifth which is like yeah i find like absolutely sixteen yeah if i'd like okay if i didn't know anything about decision trees or whatever i'd be like maybe it's minus a fifth but then if i did know something about decision trees i guess i'd be like uh maybe minus 3 over 16 would be my best guess um i don't know if this is making sense to the original person who asked the question but um uh i mean let me put it this way like if one could ask the following what could say like if i gave you how many examples do you think you would need before you'd like really really like be you know you'd bet a hundred dollars against a prize of a million dollars that you'd get the right answer if i give you like one billion examples you'd probably be happy right oh can the random examples like repeat they can repeat yeah i mean it's literally every time you like hit that button like one out of the 1024 possibilities for this string is chosen at random and then whatever it turns out to be like you get the true value of f of that string so yeah like say i was being like friendly and i gave you one billion draw so like it would be like wow this would be a very long list obviously there's only 1024 different possibilities so actually after a while after a while you'd probably be like you know what we can stop this trade like probably you'll get into a situation where you're like literally you're no longer giving me any new information but give me more examples um uh do you see one that will happen i mean it'll definitely start happening once you've seen all the 10-digit binary strings yeah true i mean uh at that point you'll literally know the entire truth table of f and you'll be like i can answer any question you want you want me to figure out this no problem i'll know it exactly um yeah um now a question of like um yeah actually let's even i'll i don't know let me let me do one more kind of weird example let's say it's a decision tree of height one um the decision tree of height one is that a good example i'll say there's a decision tree of height zero so this entry of height like zero just uh i mean like a normal decision tree it's like x1 x3 x2 x5 blah blah blah and then there's some leaves like if you get down here it's like plus 1 is the answer and then if you get down here like minus 1 is the answer plus 1 is the answer minus 1 is the answer plus one minus one something like this this is a decision tree x2 century of just a height zero is just like there are no nodes like the whole thing is the answer is right here so it's either plus one or minus one um [Music] yeah in this case how many random examples would you want in order to bet a million dollars that you know f i think i just made one yeah i think just one example would be good enough here this is actually like a case i mean you kind of know well you know f hat empty set is either plus one or minus one so i don't know if this is actually very helpful this is probably not a helpful digression but um yeah but uh i guess probably the most helpful digression was maybe back in this discussing game where we've got a few examples and we maybe thought our best guess was like um five sixteens and then we got like a few more examples and we thought okay maybe our best guess was like minus 3 16. i mean obviously you should guess like some integer over over 16 and this h equals 4ks um [Music] just give you some idea or maybe you still have a question god no response all right well i'll leave it open then um is there another question or a follow-up question maybe i can ask 4.1 since no one else asks that okay yeah sure 4.1 4.1 oh this thing yeah yeah so um i mean i kind of know how to do to add relatively easily um i mean maybe i'm wrong but but basically if you multiply all the integers by two and then subtract t then you're looking for the two integers that um are one is negative of the other one in some sense uh-huh let's see wait okay so you've got a bunch of numbers and numbers between zero and n minus one yeah and you're also given c you want to know are there two numbers that add up to t right so uh let's see so like for example oops like maybe the input list is like uh two six four eight two zero one maybe twenty and t is 5. so there's like two numbers in this list i guess n is whatever seven here oh well maybe it's not valid because the the maximum number here exceeded all right so fine i'll make this um whatever seven and like uh six and like another eight that should be okay uh yeah so what was your idea for for solving this well um at least like if if you if you don't have the bound then you can just multiply all the numbers by two and divide by two point and like uh subtract 2.5 at t over 2 oh sorry um multiply by 2 and subtract by t and then if the two number if you have two numbers that are um negative of each other right then you would get the you you would know that that would be true i see so then you're subtracting t you get like negative 1 7 3 11 by negative 1 negative 5 9 7 11. wait so you want your point oh and you're looking for two they're like the negatives of each other yeah they're saying that would happen that would mean like like if this one was uh a and this one is b then i would be like two a minus t is the negative of 2 b minus t let me just uh figure this out so that's like minus 2 b plus t so you get like two a plus two b equals two t t equals a plus b okay cool yeah how do you find out if two numbers are the negative of each other though uh you sort by absolute and then you um using radix sort and then you just compare in a list good good so okay so this linear time this linear time now you're saying you like sort these by absolute value that's even linear time but even if you're not being overly sophisticated it's order n log n and i think the problem allows that or maybe it wants linear time then it's like time so you know okay all right so fine so linear time so you sort them good and then okay in this case you would get like minus one minus one three seven seven and nine eleven eleven right then you like start scanning them this way yeah you could like secondarily sort on their sign or whatever and like i guess you could i guess in some way you could like do a linear pass and figure out the answer i i agree exactly yeah okay okay so so i i have that okay i think yeah and i mean i assume that like there's some way to do this with uh fft but like i'm not sure exactly how like i'd be like trying and playing around trying to play around with it where i'm like um is there some way in which you can represent this as a polynomial like this array as a polynomial in some sense and then take the fft of it and then have some like and some some properties of the coefficients are sorry i guess this is the coefficient representation but the sub property of the value representation gives us information about how the um how how numbers are added or how what the sum of like f two numbers are for three numbers or four numbers or something like that but yeah that's that's just like my high level intuition i'm not exactly i i'm not sure how to like actually tell that from the n value representation of um the using the fft yeah definitely i mean there's a problem where like if i were you like you kind of want a metagame a little bit like yeah um you know like it asks you to do something in like n log n time you know after a lecture we talked about like fourier transforms and the word ram model there's only so many algorithms that are n log n time you know sorting or fft so i can understand that i guess you should possibly slash probably also speculate that like there doesn't seem to be any like inherent relationship between like the number five and the answer yeah so therefore you should probably also work for like six or four i mean exactly so there's that uh so these are like you know meta clues you can get um it's okay so like we've got a list of numbers what do you want to like figure out if like some five of them add up to this target um or ksdn in general yeah sure again yeah so you said something about polynomials yeah i mean i guess you can represent this as a polynomial i mean that's just uh you i guess if if you have a polynomial um yeah i was somewhat unclear but like my intuition is maybe maybe like if you have polynomials where there's zero like they could take on values of like zero or one that the um x's can take on values of zero one or something then when like five of them are one and the sum is equal to um that then the polynomial should evaluate to t or something something like that i wasn't exactly sure though like how well what a good polynomial representation would be of this yeah hmm i mean [Music] i do remember how to solve this problem so my you know humming is kind of fake but like do you like discover the solution to this problem um i mean [Music] well okay what does the fft like let you do an n log n time uh if you have coefficients of polynomials it gives you the um it gives you the evaluations of the function on the nth roots of unity okay or an endpoint so [Music] does that seem like the sort of thing that will help uh i guess if you're all adding so so it would be like you'd have the n thirds of unity multiplied by each of the numbers in the array and then [Music] yeah i'm not maybe not entirely sure of that but yeah maybe anything else is good for the fft uh i mean you know like the first value tells me the average but that doesn't really help i think yeah that's true i guess you can work out the average and linear time oh let's see [Music] naively i would just like try all five combinations but obviously that's like two yeah that's gonna be like end to fifth time yeah maybe i'll take some inspiration like i have no idea what i'm about to say is uh right or not but like maybe i'll take some inspiration from like the whole like counting sort aspect of radix like what if all the numbers were like zero or one would that help like i give you this long list you know i always imagine like okay what would i do if i'm there with my piece of paper another thing to imagine is like what if like whatever n is you make a small value of it but here like it's a bit funny because n represents two things it represents like the number of elements in the list and also represents oh right yeah maybe maybe that's sorry i maybe did not take advantage of the fact that you're actually given the range i i just assumed that the range was unlimited i'm pretty sure again like from a meta gaming points of view like the fact that it's specified might mean that it's yeah relevant so actually what if we shrunk that like if let's say everyone was either zero or one i mean well we could ask okay yeah that's about the smallest possible value i could imagine so now if i give you t is um it's hard to make it hard uh five uh yeah how would you do it could you do that in uh n log n time you could do that in end times i think yeah you could just sum everything and see if it's greater than five but actually it's already a little bit interesting what if it like you know zero one or two uh oh yeah yeah like two two now yeah you could do radix or sorry a counting sort and like count the number of zeros count the number of ones count the numbers maybe actually that's true although wait a minute um actually now that you bring that up it's a little bit funny uh let's say you did that and now you're like okay there's like um well there's a number of zeros is two and like the number of ones is 10 and like the number of twos is one two three four four and like i don't know t is um let's say t is um um seven uh yeah and what exactly like what exactly are we trying to figure out if t is seven where there's some combination of the zeros ones and twos and like that we have you can add them together you get seven but you can here just using ones um wait no what what exactly is the problem asking uh whether you can whether there's a total of k right like whether there's k five five okay so whether there's five um the sum of five of these digits will add up to seven okay so uh is there yes you can use how would you do it uh you you can use two uh five right so you can use i i guess three ones yeah and two twos three ones and two twos yep what if it was uh this let me give you some more examples what if it was uh this uh what does this add up to 16 what if it was like um nine zero and seven could you do it no true just give me another one sorry what are we gonna say just because seven is odd yeah true what if it's this one fourteen one one i can you do it no really wait 14 zeros right yeah i think you can't really really uh am i missing something here because you only have two twos and one one the total you can make is three maximum you can make by adding digits as three right um [Music] does everybody agree to that you can reuse them right oh i didn't i didn't see that oh yes yeah ah that makes sense so okay can you do it though oh with or sorry what was the yeah this was it there was like 14 zeros one one and one two yeah yeah you can see you can still as long as you have one one and two one two you're you're good you can do it yeah that's actually interesting because like now imagine you were like producing this vector you were like going through doing your counting sword or whatever you're like okay zero so you're like initialize things to zero zero zero because i don't know if something in this problem like imagine you know in advance it's only zero going to twos you're like zero it's like cool like you know plus one i have a zero that didn't really help that much one okay then you're like cool i've got a one then you're like one okay cool now i've got two ones that's actually there's something a little bit funny about saying that um yeah it doesn't really matter that's true it doesn't really matter like once you've got that first one you're like okay great i got a one yeah um yeah once you got this two you're like oh i have one of everything like i don't even need to look at the rest of the list i'm done yeah exactly that makes sense yeah i think this observation doesn't help solve the problem very much but i think it helps solve it a little bit okay thank you yeah yeah sure i'll all think i'll meditate on it yeah i mean uh one thing at least yeah one thing at least to know is that like it doesn't help you to have more than one copy of a given number right right right not necessarily distinct helps thank you okay i guess we're over time so uh i'll pretty much wrap it up but if there's like any like 20-second question no all right and i'll see you in class tomorrow\", metadata={'source': 'Nt7PHupBu-A'}),\n",
       " Document(page_content=\"okay everyone today's lecture is about quantum computation comes in a bit of a funny time in the course it seems a little early but we had been talking about models of computation so that makes some sense and as well see it's closely connected to Fourier transforms which we've also been talking about so let me set the stage for you back in the 70s people are really excited they had a great idea for making computers better they had the idea of like let's add probable ism randomness to computation hey you know just the ability to take regular old computer program and add like coin flip as an instruction or random bits as an instruction and surprisingly this leads this idea led to faster algorithms and previously known for some basic fundamental tasks assuming you allow for a small probability of error which you must when you're talking about randomized computation let me give you some examples here's a basic task you know you're given an N bit number and you want to know if it's a prime number or not we've talked about this a little bit before earlier lecture the best-known deterministic algorithm for this problem well no deterministic efficient algorithm make was known until 2002 but since then the best one known takes something like and the six steps which is quite terrible but the fastest known probabilistic algorithm is much better it takes only quadratic time probabilistic so that's pretty great and in practice people only use the probabilistic algorithm just one more funny example if you want to compute the minimum spanning tree all right over here if you don't compute the middle of the spanning tree and an end vertex graph the fastest node probabilistic algorithm is linear time and it's not known whether there's a linear time deterministic algorithm the fastest known algorithm is order n times alpha and where alpha is the inverse Ackermann function some very slow-growing function so very exciting great I mean fantastic we can in practice you know speed up certain computational problems by using randomness knows how exciting can this possibly get and I'll tell you what is no or sorry I'll tell you what's believed about the ability of probabilistic computation to speed up over over deterministic computation so one belief and all of these beliefs are backed up by well understood complexity theory assumptions believe number one is that randomness speeds up kind of basic you know compute a function type tasks like these ones all right what's the polynomial amount so it may be the case that there are some problems that you know we don't sound like we don't like lower bounds for any of these problems but it might be that you know prime allottee cannot be determined in our cubic time for example with the deterministic algorithm but it can be with the quadratic time it's seems possible and in fact this we know situations similar to that and there's some complexity theory evidence however that you'll never get like some dramatic speed-up that's bigger than a polynomial gap okay so roughly speaking this is known to be true if for example sat requires circuits of size 2 to the Omega n which is pretty believable then this becomes true that's the theorem of improbably it's a weakness in in others another belief that's kind of look like negative sort of negative statement about the power of randomized computation to do amazing things in speaking of Sat you know we believe that the set problem is extremely hard and seems to require time basically to to the end for any algorithm and it's generally believed that like you know randomness does not help with this especially that if I allow you have a randomized algorithm first sad it's still going to take you like to to the end time so no belief is that lets say the Sat problem cannot be solved in like order 1.999 to the end time even with the probabilistic algorithm okay so there are many problems well you know where randomness seems to afford a pretty good speed-up over what we know that deterministic but generally we believe it's not like some amazing superpower that's going to give exponential speed ups for problems okay so I bring this up because it's a great analogy to quantum computing so then like 20 years later in the 90s some other people had like a similar idea they're like let's take regular old computation probabilistic computation and add quantum particles to it now the laws of quantum mechanics have been like well-known for like I guess close to a hundred years now and one thing that's well-known and surprising about them is that if nature wants to keep track of the quote unquote state of n particles like photons or electrons or something it requires her to do calculations on two to the N numbers and so that's kind of surprising people have the idea if you could somehow like hack into this amazing computational power of nature as my sponsor only likes to say then perhaps you could do use it to do something like exponentially awesome if you could just get these particles to do some kind of computation through the transformation of their state and people thought about it and defined them all of quantum computing which in principle is realizable in practice although you know the engineers are still working on it but here's some things analogous to these speed ups that we know there's two very famous ones first of all there's a factoring algorithm by sure from 94 so on he showed that on a quantum computer you could factor and basically N squared time this is for factoring n bit integers okay compare this with the best known classical algorithm by the way throughout the adjective classical means don't want them so the best-known classical algorithm seems to take two to the n to the one third time actually this is only conjecture all the best-known proven bound is like two to the O tilde of root end time but okay it's it's pretty much believed it takes this time which is exponential time but the course of algorithm speeds it up to quadratic time which is amazing okay so once Shore discovered this everybody seemed to they had to sit up and take notice about quantum computing and there's one other famous quantum algorithm which is called Grover's search algorithm and it's from 96 and it's not normally saved this way but I think the best way to state it or to think about it is that he showed that the Sat problem from over there is in time Hotel the square root two to the end ton okay so you can solve it is something like one point four or one to the power of n time which is you know only a polynomial speed-up over two to the end time in some sense you know this is about the square root of this but it's still kind of amazing to solve sad which naively seems like it should take two to the N time and this much much smaller amount of time one point four to the N or square root 2 to the n time uh so that's pretty great now a little bit sadly like unlike randomized computation where we actually know like you know dozens of examples of like using randomness to speed up basic computational problems where random doesn't seem to be involved I think it's still fair to say in quantum computation after 25 years we just know like two problems like these two where quantum computers have some kind of amazing speed up but still that's pretty good right I mean these are still pretty awesome I mean sat you know the most famous problem in computational complexity and factoring like the basis for most of cryptography these are pretty important problems so today I'm gonna try to give you the superfast introduction to quantum computation and tell you how this one works go research okay so what is the main way in which quantum computers get an advantage over classical computers and will tell you how to tell you in one high-level shot and maybe it won't make perfect sense and then I'll like back up the truck and explain quantum computing a little bit more thoroughly so this is the main power that quantum computers have or n capital n being too thin a little n the quantum computer if it gets like an physical like you know subatomic particles together and can control them and manipulate them appropriately according to the well-established laws of quantum mechanics then it can do the following so it can essentially get some kind of access to the Fourier transform of capital and data points as long as those data points are implicitly representable that was a complicated statement so let me try to say more imagine you had some boolean function such as we were talking about yesterday or last time so boolean function when you think about it's a truth table is like just a list of capital and in this case numbers and think of this is like a data vector if you will and the point is that this function that map's the the index into this like array into the actual value should be efficiently computable okay and these numbers in this this data vector can be complex numbers if you like so this should be efficiently computable by a classical algorithm this is what I mean where you have like an exponential amount of data but somehow it's implicitly represented by like a classical object okay think of it like a circuit or a little algorithm that computes f now imagine oh and so what what what goes on here is that if f is efficiently computable like this then in principle you can efficiently basically coax and particles like n photons to have a state which is equivalent to this vector again I'll explain this a bit more later but that's sort of the set up and then what a quantum computer can do by you know running those particles through some fixed obstacle course cleverly designed of I don't know lasers and mirrors and so forth it can transform the data according to Fourier transform either the DFT matrix from two lectures ago or the this is sort of the Fourier transform for functions on numbers once around or the Fourier transform for boolean functions the Hadamard matrix we talked about last time and this will actually correspond to some polynomial in little n size physical apparatus and okay when you do this you'll get out some vector sometimes you think of as the coefficients of a polynomial which represents this data if it's a boolean valued function or the coefficients when this data is interpolated over the roots of unity [Applause] okay and in quota computing you sort of set up your particles in this stay then your quantum computer does this Fourier transform now unfortunately you don't just get to look at this outcome I mean this outcome does not like exist anywhere in nature because it's a vector of length two to the little N and you know it's maybe a thousand so that's way too huge to exist in nature but what happens is that the algorithm can receive a string s with probability proportional to this entry squared okay so for example if there happens to be one coefficient here which is really big then the algorithm will learn which coefficient is really big with high probability okay so somehow and we'll get into this in a bit more detail this is the power this is like the unique power of quantum computers if you have some like implicitly representable data you can efficiently with like a quantum apparatus Nature will like take its Fourier transform and then you don't get to see the results but you can sort of sample in some way from the results so this is a like a weird power but it's gonna be enough to do these two things\", metadata={'source': 'JmFeg_Hpsqk'}),\n",
       " Document(page_content=\"Oh to back up a little bit in order to tell you more about how this works I have to tell you just a teeny bit about quantum mechanics now I personally know a teeny bit about quantum mechanics but it's fine it turns out like that's just enough to know too you can learn very little and get started doing quantum computation so there's basically just a few like axioms of quantum mechanics from which everything derives and I can just tell them to you all right so quantum mechanics axiom one as I said these are like you know well understood laws of physics that have been like unimpeachable for 100 years so even if they look strange to you there's a I mean there's nothing to worry about okay so I actually want to sort this the state of physical qubit I'll come back to this term in a second is a unit vector in two dimensions okay so in particular okay you know a vector in two dimensions looks like this a B it's in well I would normally like to write r2 here but technically the the entries of this vector are allowed to be complex numbers but I always like to emphasize don't get too stressed about complex numbers if you don't like them it's without loss of generality and quantum computation to only worry about real numbers but anyway quantum complex numbers and the fact that it's the unit vector means they they're squares add up to one okay I'll come back to this in just a second but let me continue a little bit so when you see the this state it's sometimes written like this just as a vector like we always write like a B it's also sometimes written like this a times this thing zero written in weird asymmetric brackets this is called a ket plus B times this weird thing one written in asymmetric brackets but I'm just here to tell you that this literally is shorthand for this vector 1 0 and this literally is shorthand for this vector 0 1 ok so physicists invented this notation called Dirac notation it's really actually when you get to know it it's really awesome it's very lovable but first it's annoying to you but don't worry about it it's not gonna be a big deal in this class but as you can see under this you know terminology or notation I mean this is of course equal to this ok and these a and B here are called amplitudes okay but what it means but what does this really mean right so well you know when you're building like an old-fashioned normal computer you use like some physical thing to represent zero and one right you have like a wire and it's got like a high voltage and that's one I think and the low voltage is zero okay and I don't know high and low mean but the off like some physical object they can essentially you have two basic states and you're like great I have a physical object with two basic States I'm gonna call one zero or false and one true or one and a same story in quantum like they're different kinds of like you know subatomic particles that have sort of two different basic States like for example if you have a photon like one basic state is it's like horizontally polarized and that means it can go through one of the lenses in the movie 3d glasses and the other basic state is vertically polarized and that means it can go through the other lens and the movie glasses or you know if you've like an atom with one electron like one basic state is like the atom or the electrons like near the nucleus and the other basic state is like it's in like some farther radius I don't really know a lot about physics but this is what they tell me and so you know wherever you have such a physical object that has like two basic states then you can be like great this is my qubit I'm gonna call one of the state zero and one of the states one but like the principle or axiom of quantum mechanics it's like whenever you have this physical object with two like perfectly distinguishable basic States the actual state it can be is any linear combination of those two basic states okay so if you can have a quantum particle in two different states then it can also be in a linear combination of those states that's sort of the axiom and right so you call those two basic state 0 and 1 and then the actual state of your your qubit is represented by a dimension 2 vector okay and this is like some normalization property ok so just in the same way I was like you know the you know the state of like a hockey puck that can slide down like a long track is you know represented by like one real number all right the state of a photons polarization is represented by some 2d vector okay actually although it's traditional to write these amplitudes is like a and B or like alpha 0 and alpha 1 for the purposes of today and in light of what we've done previously I'm going to write them as F of 0 and F of 1 I'm gonna call the amplitudes like this okay what these are just two complex numbers so like one qubit state well look like this just a vector F of 0 F of 1 these are two complex numbers whose squares add up to 1 ok any question ok so that's what the state of like one particle with two basic states is like a folk one Photon well let me extend this a little bit to you know what I call like axiom 1b which is basically if you have n photons or 2 and Q bits together their state is a unit vector in two to the N dimensions it's sort of more generally you're the state of and physical qubits it's a vector a unit vector in two design dimensions okay and today's notation so you might guess as I alluded to earlier instead of calling it you know a 1 a 2 a 3 up to a 2 to the end I'm going to call the amplitudes F of all zeros through F of all ones okay this is a complex vector of length 2 to the N and the normalization condition the unit condition is that the sum of all these numbers squared is 1 ok you can also write this using this physics C notation as you know the number F on zeros times this object plus the next number times this object plus the dot but this object you know this angular brackets object just stands for the vector that is a 1 in the first place and zeros elsewhere this stands for the vector that has a 1 in the second place and zeros elsewhere and in general you know angle brackets with the necks inside it stands for the length 2 to the N vector that is a 1 in exactly 1 position is the position that when written in base 2 and 0 indexing is X okay and one thing from like a practical point of view are from an engineering point of view when you're talking about building quantum computers to manipulate you know n qubits according to your will one thing you generally assume from like a computational engineering point of view is that you know when you get your end photons together earlier and qubits together you can initialize them to a sort of starting state which is just you know the simplest state like one followed by all zeros okay so we assume that you can initialize to this state 1 0 0 which is also known as this which I guess is also known by the function f which map's X to the nor of X in this notation now puts 1 if they're all 0 and it outputs 0 otherwise okay great so now you know about what can be the state of a end particles okay so just like a normal computing you know you don't just get your end bits and your end wires into some state of zeros and ones and leave them there you put them through like a circuit and manipulate them so now we need to know like what are the rules in quantum mechanics for how states can change okay so this is axiom two of course the mechanics it says that like a physical change to a qubits these are like equivalent to linear transformations of states so the leer algebra concept in particular or some physical separatist changes the state of particles that go into it it will change them according to a linear transformation actually conversely for any linear transformation on two to the N dimensional vectors there in principle exists a physical you know contraption that will effect that transformation so you know a physical systems like a it's gonna be like a quantum circuit it's it's action on and qubits is to just multiply the the state which is a vector by a capital n dimensional vector sorry and by n matrix okay so any physical operation that transforms the state of some and qubits together its action on States is that of multiplying by a matrix but one thing you can further deduce from kind of axiom one is it cannot just be any old matrix example it cannot be the matrix of all zeros because that would map the incoming state to the all zeros vector but the all zeros vector is not a legal state because it does not have the property that the amplitude Squared's add up to one okay so the for it to be like a legal matrix multiply it must have the property that it always Maps unit vectors to unit vectors and there's a name for that in linear algebra we've seen it before it means it has to be a unitary matrix so it must map unit vectors unit vectors or in other words it must preserve the length of vectors and as we said last time this means it's what the linear algebra people call a unitary matrix but basically in your head just think of this as a matrix which is like a rotation or a reflection okay these are like the objects in space or the transformations of space I'm imagining real space in my head which preserve the length of vectors yep nothing actually everything is always complex but like sometimes as a crutch in my head like I imagine real numbers actually one interesting fact is like they really these transformations are occurring continuously on time and for rotations that like kind of makes sense you can imagine like a physical operation that's sort of changing a state in time is like slowly rotating it well you might think like how do you like slowly reflect the vector and they're like that's possible but you need complex numbers for that but never never mind that's like some weird digression that just occurred to me yeah so just I'm saying you mentally in your head I mean a good viewpoint is to think that like the physical transformations are like rotations okay Oh spits okay and so like in principle you can for any like rotation matrix or unitary matrix you can build like a physical apparatus whose effects on particles like when you fire the particles into this apparatus of mirrors and lasers and whatnot is to rotate the state vector this capital a dimensional state vector in the way you desired okay so uh well over the last few lectures we saw a couple of unitary matrices namely the basically the Fourier transform matrices you always had to divide by 1 over root capital n if you recall watts all right for example here's a little too much for example this matrix 1 over root 2 times plus 1 plus 1 plus 1 minus 1 this happened by the way if you recall to be both h2 and DFT two but anyway it's a it's a this happens to be a unitary matrix and it's if you think about it acting on two dimensional real vectors it's the rotate by 40 degrees matrix 45 degrees counterclockwise matrix although it operates more generally on complex vectors okay you know it's a transform so that's a particle who stayed is a b2 well whatever a over root 2 plus B over root 2 a over root 2 minus V over root 2 ok so what I'm trying to say is like you know if you're representing your photons by or your qubits by like photons polarization state you can build like a little device that like takes in a photon and like applies this to its state another example is this matrix is also emit 2x2 matrix which means it's suitable for one qubit and it's 0 1 1 0 let's say unitary matrix in fact you can easily see that it preserves lengths because if you apply it to a qubit in state a B you get out ba ok so that does not change the sum of the squares of the entries and in particular if you apply this to the vector called ket 0 which remember is 1 0 and maybe this means like horizontally polarized if you're using photons then you get out 0 1 which is what we called 1 and vice versa so this is actually like a quantum not gate but it it operates not just on these two vectors but it actually operates on any state by switching the two amplitudes and as I said as we saw in previous lectures more generally for example this matrix the capital N by capital n Hadamard matrix which looks like some sort of generalization of this two-by-two matrix when capital n is 2 to the little n is a unitary matrix which means our in principle there's a physical apparatus that will affect multiplication by this matrix on the states of qubits [Applause] so I said in principle but we have to talk a little bit about in practice so well on that subject there's like a simple analogy with classical computing so like in classical computing or electrical engineering what's the story you get together like you want to operate on n bits so you get together n physical objects usually called like wires and you say okay they're gonna have voltages which are rather high and low and I'm just gonna call that 0 and 1 so like each wire represents some voltage and then you are like compute with them right so you put them on into like some like red board or something some physical apparatus that like manipulates them and does some physical transformation to them okay and these wires maybe come out and so in principle for this see you can actually build anything I guess that would take any in that would implement any mapping between n bit strings and n bit strings I mean we could ask ourselves you know does there exists like a physical see implementing any like boolean function not being n bit strings - n bit strings and we actually talked about this before the answer is yes right I mean any boolean function can be computed by a logical boolean circuit which can be in principle implemented by a physical boolean circuit but you know that doesn't just mean quanta or classical computation is trivial because what we normally do is say like yeah but you have to like build your circuit out of little like 1 or 2 bit gates and like what you should do is count the complexity of your circuit against yourself like you should charge yourself for how many gates you used that's like the fair way to measure computational complexity ok so it's true yes but you charge yourself for like the number of one and two bits or one and two and three bit gates used and when it comes to quantum computation it's exactly the same thing it's most convenient to like model quantum computation with quantum circuits and you know i similarly said okay in principle for any capital N by capital n matrix that's unitary it can be physically implemented but what you really do is you know you try to physically implement it by having a little like one qubit or two qubit gates that you string together each of them does like a little unitary transformation on the one or two qubits on which it operates and together collectively the whole of them like implements like one big unitary transformation but you you know you charge yourself for like how many little gates you used ok so again you imagine that the physical circuits are built from like 1 & 2 qubit gates that each do like 2x2 or 4x4 unitary transformations on some piece of the vector space and you charge yourself for how many gates used okay and just like with classical computation it doesn't really matter like which like gates you allow for yourself like in classical computation if you just say oh you can use any like two-bit gates then it's not a big deal from a computational complexity perspective just changes the number of gates needed by a constant factor same is true for quantum you can just assume that like any like little one qubit two by two rotation matrix or two qubit 4x4 you know rotation matrix is allowed ok any questions about this yeah yeah the question is like why do we constrain them to debase one into qubit gates I guess sort of for the same reason we do classically actually I guess classically sometimes we allow ourselves like oh well have like a NAND gate that takes in all of the wires but I guess there's like some physical complexity to like putting you know a thousand wires into one gates and for a practical point of view yeah I mean it's constrained this way because especially challenging in real life to like build these physical devices that manipulate the quantum state of like electrons and photons and things and so you really want to keep them directly as simple as possible because you got to try to implement them in real life so that's the reason we understand for like most quantum computers these days there's many different options for like what sort of physical system you'll use to try to realize your qubits but I think generally these days they're really excellent at making one qubit gates they can pretty much make like a one qubit gate that does whatever you want and they're like really bad at making two qubit gates we're just kind of believable right you gotta get like two photons together and like delicately manipulate them and together and so forth so like you know they're working on making like high-quality two qubit gates yeah okay good so that's a valid question it was like do we need like an uncountable number of gates to represent every unitary transformation yes that's true theoretically if you want to say that like every unitary transformation to be built up from you know 1 and 2 cubed gates well you know there's uncountably many you know complex numbers even need like a kind of Li many gates it would seem however in fact well not in practice it's like in theory even you don't really need to have uncountably many gates for the purposes for all like you know purposes of quantum computation it's sufficient to be able to like approximate a target in unitary matrix like to some desired accuracy Epsilon and there are theorems for the effect that like if you just want to be able to approximate every like n bit unitary transformation to accuracy Epsilon there's a fixed set of you know like 4 1 & 2 qubit gates or 3 or 5 or something such that the accuracy epsilon you know only need like poly log 1 over epsilon blow-up in the number of you know lead probably log 1 over epsilon multiplicative number of gates I didn't say that perfectly well but I think maybe you get the gist of it it's exactly 100% analogous to a problem that's in probabilistic computing where you're like oh you're like a lot of coin flipping Zin you're in your computational model right like so do you get to flip it like a pea biased coin for like every possible real number P between 0 and 1 and that's like uncountably many different like instructions you're adding to your your instruction set like flip one of our PI biased coin and so forth well you kind of know for practical purposes I mean if you're gonna be running a 1 million step computation it's probably fine for your coin flips bias to be like bounded by you know 1 million bits of precision and you can simulate that just by like a fair coin extremely accurately so it's like exactly the same situation with the quantum gates and unitary transformations but I mean excellent point ah great so ah now you know if we had like ten more lectures about quantum computation or I would just get into this as well and like what are the gates that are allowed and how do you put them together and what kind of things can you build but I'm just gonna cut that story a little bit short and say one thing is so like sort of is like a little bit of it like a life rule this is more of a guideline but like if you ever have like a capital N by capital and unitary matrix that's kind of like simple and like explicit or you have like a nice formula for it then probably or possibly you can efficiently build it with like a poly little n number of quantum gates okay this is like a life suggestion and anyway as I alluded to before some of the main big unitary transformations we want to do are the Fourier transforms and I will just tell you those can be done by like very efficiently with only a small like as a function of little n number of quantum gates so these will just be facts that I shall tell you okay so for capital n being to to the little land as usual this one the sort of boolean Fourier transform we talked about in last class how to mark transformation it's implementable with literally exactly N one qubit gates and how do I really fully told you the rules of like you know how qubit gates work like locally to transform space then it turns out real easy to see that you literally just do this transformation separately to each of your N photons or whatever and globally and collectively it effects this transformation okay but anyway take my word for it that you can build like a little n gate circuit that does the boolean Fourier transformation so that's wonderful and we had a different kind of Fourier transformation the discrete Fourier transform that we talked about in the context of make integer multiplication it's a little bit more complex but it's implementable with N squared 1 and 2 cubed games ok like proving that takes like twenty minutes once you know the model the actual like gate diagram is not completely dissimilar from like the recursive fast Fourier transform algorithm that we talked about two lectures ago but you'll just have to take my word for it that it this one is also not hard to do great it turns out like this is the one we'll use to talk about Grover's algorithm today and this is the one that you use for shorts factoring algorithm now there's one more kind of quantum circuit that can be efficiently built that I sort of need to tell you about which is what I will call it's not standard terminology but what I'll call like quantifying quantum of fiying a classical circuit so here's one more fact let C be like a little classical circuit with like an input gates and one output gate that's we consider efficient so let it be a classical circuit with T gaze hey then I'll just tell you it's easy to like look at the circuit diagram for this and write down a circuit diagram for a quantum circuit which does some quantification quantum of vacation so it's easy to construct a quantum circuit which I'll call QC of order T gates or T little quantum gates doing the following unitary this will be very simple kind of view tere transformation I shall draw it way over here it's a unitary whose matrix is a diagonal matrix and on the diagonals are just plus or minus ones and the pattern of plus or minus ones is governed by the output of the circuit so what I mean is here it's minus 1/2 the circuit on the all-0 string and then minus 1/2 the circuit on the next boolean string and the last one is minus 1/2 the circuit on the all one string ok so if you just actually think of see the circuit is natively outputting plus or minus 1 instead of 0 and 1 then basically like C's truth table is on the diagonal and if you think about like how does this a matrix act well if you're multiplying it against a state matrix here which is like f on the all-0 string state vector through F on the all ones string you see what does it do it just negates some of the amplitudes ok these are the to learn to Deline amplitudes and this operation negates some of them a certain subset of them that's definitely unitary right cuz it doesn't change the squares of any amplitudes so it preserves the fact that the squares add up to 1 and which ones is it negate it negates exactly the ones the amplitudes indexed by strings where the circuit outputs 1 right so it keeping it does is it negates amplitudes f of X for those X where the circuit outputs 1 okay so you can basically have like a officially built of quantum circuit that negates your favorite set of amplitudes as long as the set of amplitudes are like describable y deficient classical algorithm any question about that a little bit confusing yep this classical circuit yeah yeah okay so now I've told you like okay I said you know we in this quantum complexity model you imagine these quantum transformations are built up from little one and two cubic quantum transformations and I've just sort of told you that like here are three things that you can do efficiently like the two kinds of Fourier transforms we've seen and also this like quantum efficacious of see now you might say well can you do and actually I'm here to tell you that without loss of generality this is all you need to know so it's actually a theorem maybe first proof by like Bremner and his thesis may be kind of a bit folkloric that like if all you knew were that you could do these things then these are enough to let you do everything that you can do with any efficient quantum computer yep oh it's it's a constant factor blow up so if C has T gates then the quantum version of it has order T gates so actually when I learned that fact at some point I was like very happy because I was like oh that's great I mean in some sense you can forget about a lot of quantum computation and just remember that like okay you can like allocate and qubits and their initial state is the vector like one zero zero zero zero zero and then you can do these three things to them and then you can do the one more thing which I'll now tell you about so yeah what is this one more thing I've told you about sort of how you compute a question now is that like well how do you get your answer and the sad thing is like once you've built your quantum contraption that manipulates States and some clever way you cannot just say like okay great qubits now please tell me your state this is definitely impossible because as I mentioned you know maybe you have 100 cubits you know hundred photons together that's nothing special but their state is a vector of length 2 to the 100 which is you know more than the number of particles in the universe so there's no physical way you could just get there there's whole state vector but rather oh what happens if you want to know something about the state is the subject of axiom 3 of course some mechanics which is that when you measure and qubits I'll come back to this briefly what this means in this state let me write this state like this sum over all boolean strings of the amplitude f of X times X or this just means the vector that has exactly 1 1 in the x position you receive a classical string I mean just a regular old binary string X with probability F of X square well and then the state I'll put this in quotes two collapses okay so this is also an axiom this is like a bit controversial in quantum mechanics but basically in practice you can build measuring devices which like take in quantum particles and they have like you know a regular old readout on them like the meter or like a LED readout and you know this measuring device reports classical information to you and when you you know do this to end qubits whose state is given by these amplitudes then you get back some classical information about the state and its probabilistic you get back the string X with this probability and remember by like 1 or axioms these numbers are not well these are definitely non-negative numbers and one of our axioms says they always add up to 1 so this makes sense and this business basically just means that after you do the measurement the state changes to like a vector that is mostly zeros and just has a 1 in the x position where x is the thing that you read and basically this means you may as well throw the end photons in the garbage at this point because you know their state has basically been reset to some state that you know so you know you can reuse them in your computation physically if you want but like you can't remember anything about this cool state that they used to be in ok so basically you have like one shot to like learn something about the amplitudes by measuring them yeah no and it's some sense like it's not like you learned the number either like it's not like you know if your state was like 0.8 let's say it's just a single qubit and it was like this 0.8 in the first call home and minus 0.6 I in the second column okay if you have like one qubit like this and you like fire it into like a little like polarization measure error that has like an LED readout on this side that says either horizontal or vertical than with probability 64% it'll read out horizontal and then this new state will be like all amplitude on horizontal and with 36 percent it'll read out like vertical and it'll go into this all vertical state and it's true that for example if you were to multiply this by negative 1 or if you were to multiply it by I or in fact any complex number of magnitude 1 then that exact same set of words I just said would be equally true so in some sense like and you only get like one shot at this so you can't like you know keep measuring it and like estimate that like this is something like point eight times the magnitude one like magnitude point six like you just get one shot so somehow you have to like cook up your whole like quantum circuit your obstacle course for these photons such that ideally like the final state vector has like a lot of amplitudes on the string which represents the correct answer to your problem so that when you measure like that'll probably be this the string that you receive so some wave it's all order and maybe that's why you know there's only so many like known amazing quantum algorithms\", metadata={'source': 'Pi7lbpNslSo'}),\n",
       " Document(page_content=\"[Applause] okay so now you know everything there is to know about quantum computation great and now I'll tell you how Grover's algorithm works which is this algorithm which as I said before all I think a good way to think about it is it solves the Sat problem and something like square root two to the power of n time okay so this is sat in about Oh twiddle root two to the end time okay so really I mean like the circuits at problem so here the input is a boolean circuit a classical boolean circuit C and normally the goal let's say oh and inputs one output okay then what is the normal goal in circuit set you give it a circuit you want to decide yes or no is there some input string that makes the circuit output one that's the task and generally also when the answer is yes you would like to find such a string that makes it output one more correctly output that the answer is there's no such string let me make a few simplifications here really if you're like into complexity theory there's like two parameters to worry about the number of inputs and to the circuit and also the number of gates in the circuit itself but let's just assume that C has like it's not a big deal it's just a so we only have like one parameter assume C has like poly n gates which is the reasonable case okay and then we could just have one parameter n okay and in this case we still believe that solving the Sat problem should take to to the end time like we don't really know much better algorithm than just enumerate all the two to the N strings plug them in to see you can evaluate C on a given string in like poly end time so that's considered no and see if any of them output one I'm also going to make one simplifying assumption which is not necessary for Grover's algorithm but I'm just making it because you know we only have whatever 21 minutes left in this lecture so it assumed that either C of X is 1 on a unique string which I'll call X star or it's always output 0 okay so technically this makes the problem easier because I'm promising you either like either there's exactly one string that makes it C output 1 or they all output 0 but like intuitively this is like the hardest case like when there's just one string that makes it output 1 and like you got to worry about that possibility and indeed in some sense this is really the hardest case you can reduce the general case to this case at least with some randomization okay but this will make the presentation of grow grow this algorithm simpler so far so good so now let me now we made those assumptions this assumption mainly let me say another thing that we can say without loss of generality we may as well focus on the task where we assume it's this case where there isn't a unique X star and our goal is to find that next star and we don't to worry about what the algorithm does when there is no X star because let's say I come up with a you know great algorithm that runs in a certain amount of time with the property that you can always find the X star that makes C output 1 provided that X star exists okay so in the case where it does exist great the algorithm you know finds it as desired what happened to the case where it doesn't exist well we don't know anything in principle but like the algorithm definitely cannot find a string that makes the output 1 because there's no such string so if you're like fails to find such a string we can just say oh we must have been in this case when there are no such strings okay so I'm relying on the prop pretty here that like and can check a potential candidate like once it's wrong it's like if any string is going to make C output 1 it's this X star it's this X you know it can then it's like a final step plug that in to see and see if indeed that makes it output 1 okay so all which is to finally say for this scrubber problem what we're going to focus on is finding this X star assuming it exists okay so finally the problem that boils down to I give you a boolean circuit with any inputs like think of em as a thousand in your head one output and I promise you there's exactly one string from among the two to the thousands input possibilities that makes this circuit output one like please find it of course there's the brute-force algorithm of just trial strings until you have this X star that takes time like 2 to the N times paulien ok and Grover's gonna do it in root 2 to the N times poly n time ok any question about that there's one more thing let me say we're gonna find it with high probability this is uh you know quantum algorithms are sort of inherently probabilistic in fact we're playing with high probability will find it with not low probability so like I don't know probability at least 1% and this is also fine because if you have an algorithm that finds X star with probability at least 1% then you can just like run it 100 times or like 500 times and for every string of potentially outputs you can check whether that's indeed the string that makes the output 1 and if it is great if it's not you're like well I'm going to run this 499 more times ok and therefore this is good enough to you because by running it like a constant number here I really mean a constant like 1 percent I running it like a hundred times or 200 times you can raise this 1% up to a really high probability okay great so now we're set up to actually do drover and I will tell you Grover's algorithm so yeah let's do it over here okay so the other has a few preliminary steps okay so preliminary step one we like you know allocate and cubics or get our and photons together okay and we initialize them to state as I said before well this trivial state that has like 1 0 0 0 0 0 okay and the next sort of preliminary step is like BAM we do this how do our transformation right away okay if you remember what is this how do our matrix look like hopefully you remember from last time the first column is all plus ones it has some other entries but I bring this up because well we know what the new state will be when you multiply this matrix against this vector it'll just turn into this column okay so the new state is now the amplitudes are all plus 1 over root n okay so you can think of them as they're represented by the function f of X equals plus 1 over root n for all X okay we have like an equal superposition of all the two to the little end positions in the vector and the less of preliminary thing is to build the quantification quantum efficacious of the input circuit seat okay so take the circuit C and let's do this thing I'm telling you about on the top board which is build the quantum version of it okay this takes like paulien little end time and it's a poly n size little n side circuit so this part is efficient remember we're shooting for an algorithm that's like time 1.4 to the end so poly n is perfectly fine and remember what does this QC quantum circuit do to States well we remember we're now assuming C outputs zero on all but one string X star where two outputs one so this matrix here is gonna have like all ones on the diagonal except for one negative 1/4 in the X star position so what does this quantum circuit do it it's whole thing that does is it negates a single amplitude of the state which amplitude the one in the X star position which bear in mind the algorithm does not know but like it has now has this like mystery box they can do to the state and the only thing I notice is that will negate the x star amplitude I mean f of X start so for example uh if you know little n is 2 and capital n is therefore 4 and therefore the plot of f initially looks like this you know there's four values 0 0 0 1 1 0 1 1 ok initially after step 2 the amplitudes look like 1/4 1/4 this is some kind of histogram 1/4 1/4 no half half half half okay this is height 1/2 okay and the algorithm like knows that the amplitudes look like this half half half half half and then it can apply this QC and that will like negate one of the amplitudes perhaps if this is x star it'll turn this to like minus 1/2 okay the out really doesn't know this was X star now girl knows that the pattern looks something like this but doesn't know which ones - ok now here's one thing algorithm should not do the algorithm should not now say alright I'm going to measure the state because if you measure the state now all of the f hat square or that Squared's are all 1/4 it doesn't matter that you - - this one they're all 1/4 so you would just get back each of the two bit strings with equal probability it would be useless you did not learn any information about which one is X star okay plus okay this is just the primaries so now here comes the exciting bit so [Music] Grover thought up a cleverer sequence of like three moves that will greatly improve this situation here they are I'll call them collectively like the Grover move step one take their state and apply this boolean for you transform to it one of our route and HN okay step two apply the quantum ultima fication of the or function of this of a circuit that computes the or of n bits okay that's an extremely simple circuit the or circuit it has like n classical gates so you can quote them with fayed and get like an order n gate quantum circuit which also does like a very simple thing as I'll mention in a second well let me just mention it now I mean by looking at this right this is the matrix that has a 1 here and negative 1 everywhere else okay so this is the matrix which negates all the amplitudes except for the zero with amplitude and 3 apply this honimaru transformation again okay these three moves which are all collectively efficient is like the grover move and we're gonna analyze what it does to like pictures that look like this one quick thing I'll say is like these are all linear transformations so like it's equivalent if I move this one of our route and down here and I do this because this is kind of like the boolean Fourier transform this is kind of like the inverse boolean transform so what do these three moves collectively do you have some that when you apply them to a vector of amplitudes think of what happens if you apply these to a vector of amplitudes in some sense like what does this step do it transforms this vector of like the functions values the truth table of a function to the coefficients representation it transforms it to the coefficients of the multi linear polynomial representation of F so one kind of gives you the vector F hat empty set F hat one you know F hat and we're like we talked about last time these coefficients are the coefficients of writing F as a polynomial I guess I should reuse this notation for the monomial okay and what is to do I just told you it negates all the entries except for the first one so it basically takes this polynomial it's multi-layer polynomial and negates all the coefficients except the zeroth one which is the like the empty set coefficient the constant coefficient so if you think of F is like a multi-layer polynomial - you know negates all coefficients except for the constant coefficient and what is the constant if you're running out from last time this is the expected value of F the average value okay so in other words this - f of X now sort of after it becomes the original constant coefficient - all the other terms let me write this expectation as mu or this average is mu this is mu minus f of X minus mu right I like took away the constant coefficient mu negated it and then I added the classic coefficient back in so like two changes the coefficients of the polynomial in this way and then three just like puts it back to the truth tables okay so the overall effect of Rover's move this victor of values F X values are flipped across their average all right this sounds like look at all the four what's the new value of f of X look at how much the whole old value different from the average of all the old values and then call the opposite direction from the average okay so if you think about it for one second maybe that's going a little bit fast but for a collection of numbers to transform them in this way is to like reflect all of them across their average okay so this is the if overall effect of the grover move so in particular like let's go back to this little example [Applause] if I were in this position and I did the Grover move what would the new four values be well the values are half half half and minus 1/2 the average of which is 1/4 right which is here that's new 1/4 and like think of it like a mirror okay so afterwards you reflect all of them through the average so like this goes down to zero this goes down to zero this goes up this distance is 1/2 3/4 so you have to go 3/4 above here she actually is to one and then this one goes to zero okay so actually after one Grover move the new amplitudes are zero zero one zero Oh tada now if I measured I would get X star with a hundred percent probability which is great now that's the case little N equals two but it's going to be pretty good in general case so we're almost done let me just sketch the general case of this okay so in general how does it go okay so this is some kind of weird plot where the x axis is the boolean strings like you know 0 to the N 0 then 1 and then all the way to the 1 to the N there's like some X star out here which we don't know and the initial amplitudes are all 1 over root n well this one's actually minus 1 over root n okay so we will happen to find out what happens with one Grover move well first we have to say what is the average of these amplitudes the average is basically one of a root end okay this is like kind of negligible the average is going to be like super-close maybe just a tiny bit less than one over root n okay so when we now do the Grove remove these numbers all get reflected across the average so basically all the non-text are ones barely change but this one it's like distance two over a root and away so it becomes distance 2 over root oh and the way this way so it becomes 3 over root n ok now we do QC again what does Q see you again this QC do it negates the X star amplitude which we don't know which it is but it just does so we do QC again and this becomes minus 3 over root n okay this is gonna be a little animation on the board here now we do Grover again what's the new average the new average is still basically 1 over root n so when we do the Grover move all these other amplitudes basically don't change and this one is now 4 over root n away so it shoots up to 5 over root n okay and now we do QC again it goes down to minus 5 over root n we do Grover again it goes up to 7 over root n okay so up to repeated Grover ring you know f of X star becomes like 3 over root n 5 over root n 7 over root n 9 over root n etc okay basically because the average is not literally exactly 1 of a root n but the average remains like 1 over root n even once this gets as large as like 0.1 and once this gets is largest point one then we are in good shape we measure and we receive X star with probability point one squared which is one percent which is our goal and how many times do we have to do this to get it up to point one well something like root n divided by point one times maybe divided by two so therefore after like order root n which is order root two to the little n which is the same as root 2 to the N funnily enough it's like X percent of Y is the same as one percent of X you know that little funny trick it's like square root of a to the B is the same as square root of a to the B anyway yeah after this many steps you know the amplitude gets up to a constant we measure and we learn X star with like one percent chance\", metadata={'source': '9KD1AVHnZKY'}),\n",
       " Document(page_content=\"so last time we ended our sort of little unit on computational models and things to do with Fourier transforms and now we'll start a new one that's related to some algebraic topics so today I'm going to tell you about fields and polynomials and then we'll talk about error correcting codes and D randomization and a few other things so fields of polynomials are important topic to know about in computer science theory they come up a lot and I want to tell you about them today and about how you can actually work with them algorithmically because many times you know like a paper will just be like okay let's choose a field of size two to the N and then start manipulating some polynomials and derive some conclusions and so today we'll talk about how all that works I've put some things up on the board already so in case you don't know a field is like a system of numbers where you have the usual arithmetic operations addition and subtraction multiplication and division by not 0 so some popular examples are the rationals and the reals and the complexes but not the integers because they don't have division but there are also some fields that have only finitely many elements and that's what we'll be mainly talking about today so once we know about fields we can talk about polynomials and polynomials are a very important topic and the reason to introduce fields is that polynomials like to have their coefficients be from fields ok so that makes things nice so to start at least we're going to talk about polynomials as formal expressions don't necessarily think of them as functions just think of them as formal expressions that look like this where you add up some monomials which are products of variables or indeterminate which we'll call X I and where the coefficients come from a particular field ok and I also put down here some common notation if F is a field and x1 through xn are the names of some indeterminate just symbols then this F square brackets followed by the list of the indeterminates means the set of all polynomials like this where x1 through xn are the variables or the in determinants and the coefficients are from the field F okay I also put up on the board like pretty much everything you need to know about finite fields and polynomials for CSTR purposes so actually after reading this you can just all leave if you want but I will try to expand on these facts for the remainder of the lecture so they go over them quite quickly first fact is that if you have a prime number P and you look at the integers modulo P then this is a field the main trick here being the fact that there is division here this set of integers is closed under division or reciprocals and this is usually written like a boldface or double struck F sub P however there are other fields of finite size in fact there are finite fields with Q elements whenever Q is a power of a prime but it's not as simple when it's not just a prime but it is a fact that for every prime and every positive natural number L there's also a field with P to the L elements in it and this is particularly important and useful in the case when P is 2 this is like the most common case since the theoretical computer science relying on the fact that for every power of two there's a field of that size furthermore first of all it doesn't just exist it's actually unique so I mean for every prime power Q there's basically one field with Q elements upto renaming elements so that's fine well it means that there aren't any more fields which is maybe a shame but you at least get one for every prime power and not only that not only do they exist but like you can actually like get them and like work with them and do addition and subtraction and multiplication division efficiently and what is the efficiently mean here well if you have a field with Q elements in it then you would naturally be able to write each one down with a name that's like log Q bits long ok and so you would strive to make all your operations efficient in the sense that they're poly log Q time ok and indeed you can get this I'll be at there's a very minor asterisk to do with in certain rare cases you need some randomization but basically it's it's fine so that's fields and now to polynomials there's really only one super key fact it's this key fact that if you have a degree D polynomial it has at most D roots where root is also called a zero it's an input that makes the polynomial output one I should add a key word here which is univariate meaning having just one variable and there's also generalization of this fact appalling we also have more than one variable which is called the Schwarz Sippel lemma and it's basically this if you have a multivariable polynomial and it has degree at most D and you pick random values for each of the inputs then it's unlikely to output 0 and what is unlikely mean it means probably at most D over Q where Q is the size of the field so if it's a big field so you have lots of choices for these different A's then the polynomial is unlikely to output 0 I should also add the words here P should not be the all zeros polynomial then it's very likely to output 0\", metadata={'source': '3btIy6vv3mE'}),\n",
       " Document(page_content=\"okay so as I said these are all the things that well this is like 95 percent of what you need to know on this topic but now I'll talk about it in more detail okay any questions all right well let's start with this fact which you may already know that if P is a prime then the integers mod P make a field but let's think for a moment about why that's true I mean the thing you need to check is that the integers mod P are closed under these operations addition subtraction multiplication and division for the first three you know and that they follow the normal rules of arithmetic and so forth the first three are fine the main interesting part is why are the wise the integers mod P closed under division so Assasin's question for P a prime why is this said the integers mod P a field okay and the the thing we need to answer is you know how come you can do a divided by B mod P and really the only owners need to understand how can you do 1 divided by B mod P like you need to understand like what is the reciprocal of B because if you have that then you can get a divided by B by multiplying a by the reciprocal so the main question here is you know the answer is because you know any B which is not equal to 0 mod P has a reciprocal okay that means it's some number mod P such that B times B inverse is one now that's all well and good but if you actually want to like do the arithmetic like you're like okay I will now like work with this finite field of size P and you've got some number B you like now I need to know what is the reciprocal of B so does anybody know how you find the reciprocal of B yes Fermat's little theorem yes you can do it with Fermat's little theorem wasn't what I was thinking do you know no another way Euclid's algorithm yes that's right that's what I was going for so given such B you can find it efficiently okay in so efficiently means poly log P time because like the numbers mod P our number is that you can write down with like log P bits so poly log P means efficient using like Euclid's algorithm or the extended GCD algorithm and let me just remind you what that is what is the extended GCD algorithm if you're given two numbers let's say given any two numbers B and P integers well they can be any old numbers but I've called them B and P for reasons you'll see in a second what does this algorithm do not only does it compute the GCD of B and P but it also finds additional information it finds like an integer or linear combination of them that adds up to the GCD so I'll write that it finds some integers C and D and these are like smaller than basically B and P max of B and P such that you know C times B plus D times P equals the GCD of B and P okay so it finds the GCD and it also finds C and D and this is very nice when P is a prime so if P is prime then this GCD between B and P is 1 because we're assuming B is not 0 it's not a multiple of P and so now once this you have this what is B inverse or a reciprocal of B yeah it's C so this is be reciprocal mod P because this is a multiple of P so you can ignore it mod P and now it's saying that this number times B is 1 mod P ok so C is the reciprocal you're looking for ok and because of this you know computation in SP is efficient you know you can you know add and multiply these log P bit numbers and poly log P time and reduce the mod P and so forth and this is the main additional thing you need actually let me make a couple of additional comments here this is efficient and like the classical sense its polynomial time but actually a very interesting open question is whether this has a highly efficient parallel algorithm so this is actually a sort of famous open problem in computer science theory can you compute the GCD of two numbers in the complexity class NC which remember is like a poly log parallel time or you know uniform circuits with poly log depth and polynomial size so this is open and you know it's a little bit of a sadness about the complexity of arithmetic that we don't know that's one observation also if you think about this a little bit longer you can see why if you take the integers mod M where m is not a prime it's not a field and it's because the numbers that make a GCD with M that's not one will be zero divisors they won't have a reciprocal mod M ok any questions about this there is actually one catch if you know of this discussion where I try to tell you that okay you know working with this field of prime size you can do it efficiently and this catch is like well how do you actually get the prime so one thing you're often doing in computer science Theory applications is working with fields that are sort of exponentially large like if you're doing an algorithm where the parameter is n you often want a field that's a size exponentially large and n which is fine I mean it'll be write down a bowl with n bits and sometimes you don't really care that much about the exact size of the field you just need it to be sort of big enough and so a field of size you know something like exponential and n is it's fine for you know you don't care the exact size but if these are the only fields you know about the fields of size equal to a prime then that means in order to do this you need to find a prime number of sighs you know exponential and N or you need to find a prime number with n bits and that's actually an interesting question how do you get given n how do you try to efficiently get an N bit prime number okay so you know maybe you don't care exactly how big this thing is you just want it to be let's say n bits long so it's you know between two to the N and two to the n plus one but how can we get such a prime efficiently does anybody know the algorithm for this yeah okay sure oh yeah we'll get to that so actually in many applications you're like oh it's actually going to be more convenient for me to just fix two SP which I know is a prime and work with this field of size like 2 to the L for a convenient choice of L but um well they're like a little bit more complicated these fuels I mean it's not so easy just immediately say oh it's the integers mod P like it's not just the integers mod P to the hell so uh yeah but usually you probably would do that in practice but I'm bringing this up because it's also like kind of a fun question sometimes you want Prime's of N and around n bits for non filled related purposes uh yeah yeah that's exactly right so when I first learned this I was like so disappointed with computer science like I was really mad but this is how you do it you just pick a random number and you're like I hope it's prime and then you test that and if it's not you pick another one so this is the algorithm and it's like the best algorithm it's one of the only efficient algorithm known really so just repeat just pick a random n bit number and then check if it's fine and as we mention a few times this last step you can do with either this deterministic algorithm of Agarwal Kyle and Saxena from o2 which runs in like o tilde n to the sixth time or since you're already using randomness you can use like the Miller Rabin algorithm which is basically quadratic time it's a randomized algorithm for prime ality testing and it's a surprising an interesting fact that there's no known deterministic algorithm for solving this problem efficiently and you might just think coming if you're a little bit confused you might just think like well I mean why don't I just test if two to the N is prime and then if well it's a multiple of two so two to the n plus one is prime to the N is plus three is prime to the n plus five is prime and keep going until I hit a prime unfortunately we don't probably know that that will quickly lead you to a prime actually there's a conjecture a number theory called Cromer's conjecture which is pretty well believed by the number theorists from what I understand and it implies that if you do this algorithm try to the N 2 to the n plus 1 2 to the n plus 2 to the n plus 3 and so forth you'll get to a prime always before you get to 2 to the n plus order N squared and therefore if you believe crew Myers conjecture then it's fine you can do this algorithm just start at 2 to the N and keep going up checking from prime allottee and after you know N squared many or so many tries you'll find a prime but that conjecture is unproven and the best proven fact is that if you go up to 2 to the N plus 2 to the point 5 3 5 n then you'll get a prime but this will take you you know exponential and many tries so we don't have that but this algorithm works fine and to understand that you need to know the fact that the primes are not too rare among all n bit numbers but I know you know this because you had like a homework problem about it considering the prime number theorem which basically says the fraction of n bit numbers which are prime is big Omega of 1 over n so actually more precisely if you really want the details so you might remember from that homework there's this famous prime number theorem which is proven and it says the following and is very nearly equivalent to the following the fraction of n bit numbers that are prime is like 1 over 2 lon 2 times n ok and I think on your homework you showed that it's at least like 1 over 2n but anyway I mean in turn this implies that like after order and trials you'll find a prime with high probability this is a comment that has nothing to do with fields and polynomials but you'll often see this phrase whp just means with high probability ok good and actually for some applications for which you'd like a large prime for example cryptography you actually often want a large random prime so actually sometimes you're like perfectly fine and even happy that your algorithm works by like picking a random number until it finds a prime but if you just want any old prime which sometimes is all you want this is still the only efficient provably efficient deterministic algorithm Sariah provably efficient algorithm we know we don't know a provably efficient deterministic one yeah just a few more comments this is a randomized algorithm that has like a zero sided error like this never wrongly gives you a prime it's just you know sometimes if you run for a while it'll come back and say like well I don't have any any answer for you but it's a kind of randomized algorithm where it never gets wrong answer is it just might take a long time before you get a right answer okay any questions okay good so that's all I have to say on the subject of fields with a prime number elements so now let's start talking about fields that have a prime power number of elements and just as a bit of a teaser for them I'll tell you what the field with nine elements is the field with nine elements or F sub three squared you can write it as all the numbers that look like a plus B times I where a and B are integers mod three and where I is the square root of negative one so it's not super obvious that this makes a field that like well you can multiply an add and subtract these numbers using like int mod three arithmetic on the the real and complex part but it's not obvious that like every number like this has a reciprocal that looks like this but it's true and so that's a representation of the field of size nine but on the other hand if you try to get the field of size 25 by just changing all the threes to fives it doesn't work 1 plus 2i times 1 plus 3i if you do the coefficients mod 5 I think this is 0 and that's bad in a field relation of two non 0 multiplying to give you 0\", metadata={'source': 'LoH-08xEZg0'}),\n",
       " Document(page_content=\"okay so that's a little teaser but in order to understand these finite fields of non-prime size interestingly you also start talking about polynomials so we're gonna switch to talking about polynomials now you have to at least understand univariate polynomials one-variable polynomials yeah okay so let's talk about univariate polynomials okay and remember the notation here IFIF it's a field and X is a symbol then this F square brackets X means all the polynomials with coefficients in this field ya over variable X okay and as I said before for now you should just think about polynomials symbolically later we'll actually talk about plugging numbers into polynomials let me also mention a fact that I guess you know this number n is called the degree of the polynomial well assuming CN is not zero okay all polynomials have a degree a except for the constantly zero polynomial that has no well-defined degree you can just leave it undefined or maybe call it negative infinity or something but the all zeroes polynomial is always like a special case okay but you know if the consonant coefficient is like three the polynomial that just has constant coefficient of three that's a polynomial of degree zero ah okay now this set of polynomials is actually not a field because in general you know polynomials don't have reciprocals they do have plus minus and times though which means that the math people would call them a ring so just like the integers they can at least support plus minus and times and in fact there's many many analogies between the set of univariate polynomials of a field and the set of integers they share many properties so actually I'll just mention some of these properties briefly first of all a fact is that there's like a there is some notion of division for our univariate polynomials but it's just like the notion of division for integers where you have a quotient and a remainder okay so if you have a polynomial B of X you can divide it by a of X and you'll get some other polynomial Q of X which is the quotient and a remainder R of X and the property of the remainder is that it's one quote smaller than the thing you're dividing by so the degree of R is less than the degree of a always okay what this means is well that B of X is a of x times Q of X plus R of X and I guess probably in some math course you've probably even seen the algorithm for this like polynomial division I mean it's like the usual division algorithm somehow times it's called synthetic division or something but that's an important property of polynomials that this notion of division makes sense with the remainder which is smaller than the thing you're dividing by and actually one consequence of that is that Euclid's algorithm also makes perfect sense for polynomials over a field and yeah you could do Euclid's algorithm to compute like the greatest common divisor of two polynomials where greatest is with respect to degree and you can also try to factorize polynomials just like you try to factorize integers where like you you know take the thing and see if there's like a non you can write it as a product in a non-trivial way meaning a product of two things that have degree bigger than zero and if you cannot write a polynomial as a product of two things of smaller degree then you call that polynomial a prime or the preferred to numerology I guess is irreducible but you can also call it prime if you like so let me put this up as a definition upon toil P of X is irreducible or prime if you know it can't be factored as the product of two lower degree polynomials uh and also I guess like with prime numbers and you should have degree at least two okay and just as we saw before if you take the integers mod a prime you get a field if you take the polynomials mod a prime polynomial you also get a field and the proof is like literally exactly the same it's the same like GCD Euclid's algorithm business so let me write that because this sort of tells you some sense what these fields are the set of all polynomials f of X mod P of X is a field of size cardinality of the coefficient field F to the power degree of P if P is follow me f prime or irreducible ok what does this mean same thing with integers right leave all the polynomials but you like kind of treat P of x equals 0 as like a formal rule so if you ever have like a polynomial which is a multiple of P of X you're like oh this is just zero and so you can like start subtracting away multiples of P of X from any polynomial and it's like the same mod P in particular it's not hard to show that you can keep doing this given any polynomial of any degree you can like start subtracting away multiples of P of X to kind of make the degree go down until the degree of your starting polynomial is less than the degree of P then you can no longer like take multiples of P to like cancel out the high degree terms so I'm particularly like this set is like all to polynomials of degree less than the degree of P and that's why I mean these are representatives for the set and that's why the number of elements in this set is size of F to the degree of P right because that's the number of polynomials that's the number of way of choosing coefficients for each possible power of the indeterminate X up to degree P minus 1 and you know similarly to the integers mod of Prime right I mean you know if you just add two representatives together you don't have to do anything special but when you multiply two of them together you could get something that's of degree bigger than the degree of P I know you have to like subtract off some like multiples of P to make it go but down to degree less than the degree of P so sort of just like with the integers good so actually whenever you have field F and in irreducible polynomial which is like a prime you can make a field of this F to the degree and as we'll see in a minute there for every degree L there's always at least one irreducible of degree L and that implies the existence of all these fields okay the fact that they're like unique and that you effectively get the same field if you use different irreducible ziz being a much harder theorem but you can just take it as a fact so as an example of this let's consider the field to be F 3 the integers mod 3 and then you can check that this polynomial x squared plus 1 is irreducible you know in F 3 just like an brute-force check that there's like no to polynomials like linear polynomials mod three that multiplied to this guy I mean there's only I don't know nine polynomials of degree up to one so you can just try multiplying them all together and see that none of them equals this okay and therefore by this theorem you know the set of all polynomials sort of mod x squared plus one is a field well it's F 9 right this is all the things that look like a plus B X subject to the rule x squared plus one equals zero this rule comes in when you like multiply two of them together you can always consider x squared plus one equals zero actually computationally an easier way to do that is to always consider x squared equal to minus one so wherever you see an x squared you're just like oh I'm going to turn this into a minus one and that's how you reduce degrees back down to linear or most to you cancel out all X Squared's like this and as you can see this is exactly the same thing I told you before where I told you oh it's like a plus B times I I mean here it's just there recalled I the number whose square is minus one or like here we're calling X the symbol whose square is minus one what it's the same thing okay and this fact that I used to show that doesn't work mod five is because x squared plus one is not irreducible mod five because one plus two x times one plus three X is a factorization of x squared plus 1 mod five okay so as a corollary of knowing this we have a similar situation where if you're somehow given an irreducible polynomial of degree L mod P you can do arithmetic in this field F pizza L still ball in efficiently and like L well polynomial and L log P time okay so to write down like 1 field element you know you have to write down all the coefficients for degree 0 up to degree L minus 1 so it's like L numbers and each of them is a number mod p so it's like log P bits so you basically need like L log P bits to write down a field elements okay and then addition is basically vector addition mod p and subtraction and well for multiplication and especially division and you need like Euclid's algorithm to compute inverses okay any questions about this yep great so the question was are there efficient algorithms for checking your reducible bility yes there is that's great so let's just exactly return to this question how do you get like and degree L irreducible log P okay let's so let's assume you know P and you know piece of prime and you're like oh I wish I had this field of size P to the L so I have this target number L and now you want like a degree l0 DuSable mod P you can actually do the exact same thing so you can pick a random polynomial of degree less than Al just by picking all its coefficients uniformly at random and then you want to check if it's irreducible and luckily there's an efficient algorithm for this oh yeah I forgot too it's due to but it's definitely much older than the prime allottee testing algorithm so this is doable in poly L log P time deterministically and you suit to Shoop no well I'm not sure who it's due to but it's it's easier in fact all the algorithms for polynomials are easier then there are algorithms for integer analogues so for example you know it's famous cell phone problem whether you can fact there are integers efficiently but you can factor polynomials efficiently this is a famous result of Birla camp from like 60s it's actually a randomized algorithm we don't know how to dear an demise it but you know you're given any univariate polynomial over a field FQ Berlin camps algorithm factorizes it efficiently so particularly tells you it can tell if it's irreducible or not as I said Berlin camps algorithm is randomized and if you just wanna know if it's irreducible or not there's a deterministic algorithm for that great so then you might need to know what about is there like a prime number theorem for irreducible z' yes there is and a it's easier it's much easier than the one for numbers ok smell called irreducible theorem but anyway it's like the prime number theorem for irreducible x' and it says exactly the probability this random polynomial is irreducible is between 1 over to owl and 1 over L for any P and for any L ok that's great so it means again like if you just pick a random polynomial after like order L trials you'll get an irreducible high probability and it's actually asymptotic to 1 over L if you care about the exact constant so that's great but it's actually even better because there ok so it's again true that there's no known efficient deterministic algorithm for finding a degree L irreducible mod of prime there's an efficient randomized algorithm but there is an efficient deterministic algorithm well its efficient as a function of L but not as a function of P but that's often good enough so this is a theorem of Shoop there exists the poly L&P not log P but just P time deterministic algorithm to get an irreducible in FP X of the greedy a degree L I should mention by the way this is in particular it proves that like an irreducible degree L always exists so this is not as I said fully efficient in the sense that it's not polynomial in law l + log PE but often you only care about P being a constant particularly - okay so often you're like oh I run it like a field of exponential size I'll just try to get the wand of size 2 to the L that's very convenient and then P is 2 so it doesn't matter if this says you know P or log P it's just it's a poly l time algorithm it's deterministic and it gets you an irreducible mod - of degree L ok and so that's super great that's all you would want in life it's actually quite nice by the way I mean this mod 2 version I mean the polynomials sorry the elements of this field F 2 to the L ok on one hand you think of them as polynomials of degree L minus 1 whose coefficients are numbers mod 2 well those are just bits 0 & 1 so the field elements are just bit strings of length L and to add to feel elements you just add these bit strings mod 2 which is great and same with subtraction well how you multiply them is more complicated but they have some nice properties finally let me just mention this last factor because I like it if you like well it's too complicated for me to go into like soups book and like look up this algorithm I wish it was even simpler then you're kind of in luck there's this theorem of Van lint which gives you super explicit irreducible x' but not of every degree just one third of the degrees so vanillin showed that X squared plus X plus one is irreducible mod to let me just write this irreducible and f2 X and also X to the sixth plus X cubed plus one and X to the 18 plus X to the 9 plus 1 etc so you just proved us a theorem all these things that look like you know 1 plus X to a power of 3 plus X to the twice that power of 3 they're all irreducible ok so if you're not super picky about the degree and you're willing to change the degree L by a factor of up to 3 then you can just take one of these as your irreducible\", metadata={'source': 'SLYHziGGcBs'}),\n",
       " Document(page_content=\"any question okay good so it's basically the last thing I'm gonna say about fields and now let's move to talking about polynomials more generally I'll start out writing over here so this thing iRacing is saying that a polynomial over a field is like a formula expression we have indeterminate pyscho officiants but okay as you now know you can also think of it as a function - because you can plug in field values for those in determinants and it like computes the number given an input number so we'll often do that and there's a very simple fact about this which our local expert on fields and polynomials everything professor Franco guruswami calls the degree mantra because basically because he feels that like every fact about polynomials that you ever need to know is the rival from this one fact which is as I mention before just a nonzero univariate polynomial of degree at most D has at most D roots or zeros which means inputs where when you plug in that input the output is zero and the fact is super easy to prove well first of all it relies on the fact that the coefficient the coefficients and the numbers are a field and this fact is super easy to prove basically you take it your polynomial of degree D and you try to factor it into irreducible x' as much as possible and whenever you get a factor that looks like X minus alpha then alpha the number alpha is a root of P I mean if you plug in alpha x equals alpha you'll get zero and conversely if you have a root alpha then X minus alpha will divide your polynomial that's a consequence of the division algorithm and yeah so basically you factor a polynomial down to irreducible as much as possible you'll get factors that are like linear like this times some may be quadratic or higher things and all the linear terms give you a root and that's you can of course have at most D of these great so it seems super simple but let's see a basic use of this fact in a basic application of polynomials in theoretical computer science so this application comes from a major area of CS theory called communication complexity perhaps you've heard about it we might mention it a little bit more in this course and communication complexity is like the study of computing when two parties are computing functions and you don't care about their computational cost you don't care about their memory you don't care about time the thing that you measure is how much they need to communicate in order to compute the function so here's a picture to have in mind juice on the base most basic set up for communication complexity there are two parties and so as usual we call them Alice and Bob and we'll imagine that they're somehow spatially separated and they each have their own inputs okay which is usually a boolean string so Alice has string little a of n bits Bob is string a little B of n bits and now the game is there have some function of a and B which they're trying to jointly compute but Alice doesn't know B and Bob doesn't know a so the model is that they can send messages back and forth to each other based on their inputs and about based on what they saw before this is the commute ocation and at the end they're supposed to let's say both know the function of a and B that they're trying to compute that's a simple example in the example that I'll talk about let's look at the Equality function this is the problem where they just want to know are there strings identical or not identical and the thing that you're trying to you know minimize be economical with respect to is this communication it's obviously given two strings it's trivial time wise to check if they're the same or not but if Alice and Bob are separated it's interesting to try to understand how many bits they need to communicate to check whether these strings are the same okay well an exercise for you it might seem as though there's not a lot they can do for example Alice can just send her whole string to Bob and then Bob can know whether the same or not and maybe can report back one bit were they the same or not I also know we're like we're going on a team here there's nothing adversarial that uses like n plus 1 bits of communication and you know Lois might seem if you haven't seen it before there's nothing else they can do and that's actually true any deterministic algorithm solving this problem needs to communicate and +1 even bits of communication ok so there's literally nothing they can do better than like the absolute trivial algorithm of one of the party it sends the whole string to the other party and the party tells if they're the same or knots and reports back to one bit answer and it's not very hard to prove however like a lot of things in theoretical computer science if you allow randomness it becomes much more interesting and indeed if you allow even just one-sided error and the error can be very small let's say at most one in n you can make it even smaller than this if you like you can do it while only communicating order log n bits and just a quick comment and what I mean by randomness here the strings the input strings a and B are not random so this algorithm which I'm going to tell you works for every possible string a and B they could be the same string they could differ on exactly one bit they could differ on all the bits doesn't matter the only randomness comes from the fact that like Alice and Bob use randomness to decide what messages they're gonna communicate and this order log n bits is also optimal you can't do better than that okay we'll see how to do it using fields in polynomials okay if you've never seen it before like a good way to imagine how this couldn't work maybe is Alice like pick some kind of like random hash function or like hashes a sends the hash to B maybe it's a lot shorter Bob checks if the hash of B is the same as the hash of a that he just received and if a and B are really the same then the hashes will be the same and he'll say yes correctly and like hopefully if a and B are different even in one bit then they'll have like a different hash and then like hopefully that'll cause Bob to realize they're different strings but you know to make that precise you're like well I mean you can't just like imagine there's some kind of random hash function you have to like do it somehow and we'll say yet one way to do it in a way using fields and polynomials there's multiple ways to solve this problem but the fields and polynomials way is kind of nice so let me tell you how they do it I'll tell you the protocol okay so first thing is alice fixes a big field field FQ and she doesn't really care exactly what Q is as long as it's around and square size actually it's even fine if it's bigger but let's just say you know give it an she tries to find a prime power Q that's between N squared and 2n squared and she tells q to Bob okay this takes log Q bits or so what Q is like N squared so this is like order log n bits okay we're fine to communicate order log n bits so she can do this if that's under budget and actually there's a variety of ways to do this since we don't care actually about like time complexity or space complexity like you can't just say Oh Alice tries every number of you know between N squared and 2 N squared until she finds a prime or finds a prime power or you know you can do any of the things we said before you can use like van'll int if you're willing to make this between like N squared and n to the 6th which is also fine but we can just say like Alice you know uses up all her time to find a prime between N squared and 2 N squared and sends that to Bob okay so now listen Bob by both thinking about the same field and now they also just think about their input string as a polynomial so Alice in her head forms this polynomial P sub a of X just by using her bits as coefficients so a and X n plus a n minus 1 X to the N minus 1 goes down to plus a1x that's it you can save on one if you use the constant coefficient but it doesn't there's no point to it so for simplicity I just used a I odd as the coefficient of x to the I but also Bob forms in his head this polynomial piece of V of X that uses his string okay now the problem okay so these they think of these as polynomials and F Q to the X that's a bit funny because the coefficients are 0 and 1 but 0 and 1 are numbers in every field so it's it's fine and their goal now you know formerly it was to test if these strings were identical now they're trying to test if these polynomials are identical or another way to write this goal is they want to know is the difference polynomial PA minus PB is this the constantly 0 polynomial okay it just seems like we're making the problem more complicated than it was previously but it's gonna help us solve the problem and now Alice does a very very simple thing let's just choose no random number and computes her polynomial on that number let's call her number alpha she chooses at random from FQ she computes her polynomial on this number she's also an FQ and she sends both to Bob if you like maybe it's best to assume accuse a prime so these are really just integers mod Q and this is like basically two log base two qubits which again is order log n bits so that's under budget and okay so basically Alice has picked a random alpha and said Bob my polynomials value in alpha is this and Bob down as alpha so Bob can compute his polynomials value on that alpha he be of alpha and checks if they're the same okay this number he knows and now basically if it's the same he'll decide okay we think the strings a and B are equal and he can send a one bit message back to Alice saying that and if these two numbers are different then he'll say oh I think the strings a and B are different and that's definitely true actually so so Bob says yes if and only if P of a alpha equals P of B alpha which is the same as saying that P of a minus P of B of alpha equals zero but now this is great if a equals B and Bob will always say yes which is wonderful that's as desired and what happens if a does not equal B well the key point is in this case PA minus PB is not the zero polynomial that's the only deduction we're making it's not the zero polynomial and also we know its degree is that most n clearly and so therefore we know there's that this this polynomial here it's not the zero polynomial its degree is at most n so there's at most n different alphas which could make it output zero but we're choosing alpha from among at least n squared possibilities so therefore the probability that Bob computes these things to be the same value which is the probability over alpha that PA minus PB of alpha is zero is that most and the number of roots of this polynomial whatever it is divided by N squared the number of choices which is at most one over N okay and that shows that he's unlikely to falsely believe that a equals B\", metadata={'source': 'orOJRhlqrvk'}),\n",
       " Document(page_content=\"uh okay now I want to talk about multivariate polynomials and I also need to bring up a confusing point that you know it's confused me for a while which is that it's very important to distinguish between formal polynomials and the functions that they compute so every polynomial and I'm gonna start talking about multivariate polynomials although this admonishment also holds for univariate polynomials compute some function mapping and field elements into a single field elements and it is a fact that it's not hard to show that every function is computable lisent like this is computable by some polynomial maná uses the interpolation method that we did last time well when we're talking about analysis of boolean functions it's not quite the same thing there because we're doing it for like plus or minus 1 values over the reals but the idea is the same you can easily write down a polynomial which computes the value 1 at your favorite tuple here and 0 elsewhere and then by taking a suitable linear combination of them you can get a polynomial that computes your favorite function mapping and field elements to one feel element but check this out how many different functions like this are there you can do this part of me not Q to the end that's the number of elements here right there exists Q to the Q to the N such functions and how many polynomials are there over the field FQ in n variables infinitely many that's right all right because the polynomials just you know like a list of coefficients and like the degree can be as large as you want okay that just clearly shows that there are some functions that are computable by multiple polynomials actually all of them are computable multi blowing almost but there's some function that has let's say two different polynomials that compute it and now if you subtract those two polynomials you'll get another polynomial and it computes the function which is constantly zero but it's not the constantly zero polynomial because well I imagined you took two distinct polynomials so what I'm trying to say is there exist like formally nonzero polynomials that compute the all zeros function okay it's this this is why you have to really be careful when you talk about in some sense the zero polynomial because there are nonzero polynomials that compute zero everywhere and here's a simple example x squared minus X a and F 2 X okay this is a non zero polynomial like it's of degree two it's got coefficients one minus 1 and zero but if you plug in any number from F two there's only two to plug in zero and one they both output zero okay so this is like a non zero polynomial computing the zero function or more generally X Q minus X and F Q X these are just univariate examples even the fact that this is always zero when Q is a prime is Fermat's little theorem but this is even true in the non prime fields so that's something to watch out for but on the other hand there's some nice observation we can make using just this fact this fact tells us that in FQ X to the Q always computes the same number as X so what this is saying to say that this is always zero so it means if you have a polynomial even a multivariate polynomial where the field is FQ if you ever see X to the Q you can just replace it by X and you'll be computing the same function or if you see X to the Q + n you can replace it by X to the 10 you'll be computing the same function ok so more generally whenever you have a multivariate polynomial with FQ being the field you can reduce all the powers on the variables down to below Q by you know repeatedly reviewing this rule that X to the Q computes the same thing as X okay so that's a sort of corollary of this observation given any polynomial P with coefficients in FQ you can reduce each like X I to the M to X I to the M mod Q without changing the function is computed you certainly change the polynomial but not the function of confuse and once you do this all the individual degrees are less than Q let me say that most Q minus 1 same thing good by the way one sec um it to be a little bit careful what you mean when you talk about the degree of a multivariate polynomial usually degree means total degree which means the degree of the highest monomial the sum of the powers in there like the monomial that has the highest sum of powers but here I'm talking about individual degree like you look at all the variables and like every variable itself will have a degree that's at most Q minus 1 but the monomials can have degree larger than that you had a question uh yeah yep no I don't think so like if Q is 2 then you're saying you can always reduce the exponents mod 2 which is like saying oh wait maybe it should be actually yeah let's see here yeah okay okay oh good so okay so yeah so therefore the number of reduced monomials is like here to the end if they're n variables so for example if Q is 2 it means you can reduce everything to a multi linear polynomial all the powers are at most 1 so as we've seen before there's like 2 to the N possible monomials in that case and therefore the number of like reduced polynomials well you have a choice of one coefficient between well in the field FQ for each monomial so it's Q to the Q to the N and this is the same as the number of functions so actually this implies that every function is computed by a unique reduced polynomial okay in particular if you have like a reduced polynomial this means again one where all the individual degrees are at most Q minus one it computes the zero function if and only if it's the zero polynomial for example okay so there's a generalization as I mentioned before of this degree mantra fact to multivariate polynomials and it comes up a lot in theoretical computer science and it's called the Schwarz Sippel in them it's basically about the fact that like a nonzero polynomial cannot be cannot take on the value 0 too often so this was proved by Schwarz zip all or demillo and Lipton but somehow only Schwartz and Zippo got their name on it ok and this is the following let's say you have a polynomial in n variables feel this FQ and let's say it's a reduced polynomial so we you know got all the individual degrees down to Q minus 1 or less and it should not be the 0 polynomial and let's say it's total degree is that most D then if you pick random inputs for it the result is rarely 0 so like I got a pick and uniformly random inputs from the field alpha 1 to alpha N and I'm going to plug it into P and I'm going to tell you that this is not 0 with at least some probability and let me just say what is this probability it's some certain function which I'll tell you eventually of Q and D ok but the idea is that this polynomial is somehow often nonzero so before telling you what this function is let me tell you what it is in some cases so case one which sometimes you need is when Q is two so you have like a multivariate polynomial on the pick a random input for it in this case this function is 1 over 2 to the D okay so it says there's at least a little probability you'll get a nonzero answer it's at least 1 over 2 to the D this is the least main less lesser case this is the main case so I'll put like a big star case 2 it's when you have a big field this is like a big degree small field now we're gonna talk about big field small degree so we'll talk about Q being bigger than the degree and in this case the function that you can put on the right-hand side is 1 minus D over Q in which case you know you feel like a better way to write it is that the probability that the polynomial does output 0 is at most D over Q okay a particular if Q is way bigger than D this is like a small number and so in a very big field a degree D multivariable polynomial rarely outputs 0 also this fact doesn't make sense if the field is an infinite field like the rational numbers or something but there's a version of it which makes sense in any infinite field so in fact also for any subset of your field if you pick your alphas just randomly from this subset then you can put D over the size of the subset here okay so you don't even have to pick your field elements from the whole field you can just pick them from like a big enough subset and it'll still be true and you can use this if like you know the field is the rationals or the reals or something okay the reason I didn't write the function in general is it's like kind of complicated and you may never need it but in case you ever need it here is the function itself it's 1 over Q to the floor D over Q minus 1 times 1 minus D I I don't remember this D mod Q minus 1 over Q so you can check that this is the same as these two things in the special cases I told you that then you use this if you know Q is like 3 or 5 or something or 4 it gives you 4 small Q it gives you some kind of like generalization of this fact that like it's there's a reasonable chance it's nonzero and one may ask how do you prove such a thing and it's not that hard maybe it'll be on the homework maybe not but the univariate case the N equals 1 case is basically the same as the degree mantra and then the general case is like a boring but straightforward induction on n okay so let me end by telling you an application of short simple and this application is the problem of finding perfect matchings in a bipartite graph okay so I'll give you a bipartite graph the N vertices and M edges you want to know is there a bipartite or is there a perfect matching in it and if so you'd like to find it so it's a very easy and old fact that you can do this in order M and time it's not very hard probably you've seen it before or you basically do DFS like n times but what's open still is can you do it efficiently in parallel okay so is there like a poly log and time parallel algorithm for it and this is still open but lot C live offs and like a 79 showed that the answer is yes if you allow randomness okay and in two minutes I'll show you how to do that by the way it was recently a big result from 2016 Fenner gorgeo and Thoreau that you can do it deterministically with poly log depth circuits but quasi-polynomial sized circuits so they got in deterministic quasi NC okay so what's the Lobos algorithm hmm you know what just to excite you I'm gonna not do it because I only like two minutes left and I don't want to go away over time but it'll be in the notes you can see it the story is given the graph you constructs matrix with indeterminate entries and it's easy to show that the determinant of this matrix is zero if and only if there's no perfect matching or the determine is a nonzero polynomial if and only if there is a perfect matching and so to find out if there's a perfect matching you just start plugging random numbers in for the variables in this matrix and take the determinants which Victoria chunky showed in like 1976 does have an efficient parallel algorithm taking a numerical determinant and then the Schwarz Impuls lemma says that if this is like a nonzero polynomial which is equivalent to there being a perfect matching in the graph there's like a high chance when you plug in a random number you'll get a nonzero answer for the determinant so the whole algorithm is just formed its matrix plug-in a random number compute the determinant in parallel and if it's nonzero the graph has the perfect matching and if it's zero the graph very likely doesn't have a perfect matching okay so you can see the details of that in the notes and we'll talk about error correcting codes next time\", metadata={'source': 'Rzmj5OA-QqU'}),\n",
       " Document(page_content=\"all right well hello everybody um thanks for coming and yeah ready to talk about some cs uh theory does anybody have any questions they want to ask about i suppose i can go first so i last night i uh re-watched the uh three blue one brown video on the the sliding block collision uh problem oh yeah funny video yeah it's a great video so i i uh have a sense of how to connect it to 5.1 but i'm having some difficulties getting everything worked out from the grover move perspective particularly i'm trying to identify some conserved quantities um so i know for instance that as we move as we apply the move over and over if we square the quantities on each of these um like the points on the histograms and we come across them we should get one so to me that seems like a good conserved quantity and maybe it's an analog for energy but i'm having some more difficulty identifying uh the second conserved quantity that we can use to sort of tie everything together i'm trying to do something where maybe we just sum across the magnitudes of the points or maybe multiply by like the number of them somehow some across those but uh nothing seems to work out so i was hoping to get a little bit of assistance on that sure um yeah i actually don't even know if you like really need to like watch that video per se to or follow that at all solve the problem i believe it's also just possible to just you know if i hadn't given you any hint like maybe it would be easier um but i don't know but uh let's just remember what's going on and see what occurs to us so um i'm imagining here i guess like n is little n is four so capital n is like 16. um so yeah let's just remember what's going on in grover so we have well like initially initially all these uh things these bars if you will uh yeah as you said their squares add up to one so there's 16 of them 16 times well i guess okay it's one for root 16 squared that adds up to one so i guess uh they all start out as what i'm trying to say at like one quarter and uh yeah let's just remember what's happening let's say this one this i don't know it's the magic one uh well maybe i can use the power of like copy paste here would that be amazing copy paste oh look at that uh yeah so like let me erase these axes now uh what happens like first you uh negate this thing that's like negative a quarter next i guess the next step is like this reflection kind of across the average let's let's just figure it out just to understand what's going on so we have like 15 things a value a quarter plus one thing whose value is minus a quarter i guess this adds up to 13 quarters same quarter uh oh but then we have to divide the 13 quarters this is going to get gross but well let's just do it 13 quarters is the sum i guess the average is this over 16 so i don't know so this means um 13 over 64. yeah so mu is like 13 over 64. which i suppose is like i mean 16 over 64 is a quarter so this is i don't know maybe i'm making this up perhaps it's like 0.23 or something um so there's like this okay and then we got some new values and do this one more time [Music] paste yeah so i guess then after we do that reflection kind of deal these things will be all like a little bit less than a quarter i'll probably figure it exactly out this one will pop up to somewhere in the neighborhood of three quarters i guess okay so uh and then we might repeat [Music] all right so what what are your what are your thoughts i mean does this uh okay i guess one question is like should we try to like follow the hint or should we just try to think independent thoughts we can go either way okay so i i've done a little bit of both directions so want to try thinking independent thoughts okay uh i've tried to uh formulate how our magic one uh evolves with each grover move how all the non-magic ones evolve with each grover move and then how the average could evolve over each grover move okay good um so it seems like well here we could use the the numbers of the fourths and the sixteenths and whatnot to get exact numerical answers but i think more in general um it seems to me that when we have our magic one and then we like reflect it across the zero line and then we reflect it again across the wherever the updated averages yeah i think it comes out to like the new average is something like the old average minus two times whatever the the magic one is over begin okay it's getting smaller each time or the average is getting smaller the the the magic value is oh i have written down yeah so the average decreases slightly but our magic value increases by around twice the average um hold on let's actually do a little calculation here just to double check um let's even like i mean let's let's do it so okay so these are all at uh i suppose we should write them at 16 64. uh and then if we're reflecting um like a mirror through thirteen sixty-fourths then you're three sixty-fourths away from that mirror so then when you go again these guys will have this new height will be at um i guess 10 64. does that seem right and then this distance truly this was at like negative 16 60 fourths and this height was 13 64. so you're actually 29 64 away i mean correct me if i'm wrong everywhere here uh way so then if you go up another 29 64th you'll be at 42 64. i believe so i think this will be literally 42 64. so uh shall we calculate the new average uh yes let's do that okay so the new mu is we have 15 copies of 10 over 64. we have one copy of um 42 over 64. and if we add these up we get 50 no wait we got 150 192 over 64. and we're also going to be divided by 16. uh does this factorize at all what is 192 divided by 4 it's like 40 eight this is like 48 or six this is three it's also known as three uh so the new average is uh 3 over 16. okay which as you say is like 12 over 64. so indeed this one uh it went down a little that's right um so for example like um you know like one thing that doesn't change the average is i guess reflection through the mean like part of the grover step well it doesn't change the average right but then things do change like for example when you like negate the special one that certainly changes the average it like makes it smaller i see yes so okay good so really yeah i mean everything i guess there's basically two well i was going to say there's like two mystery quantities at all all the time just like the average maybe the special things value but really i guess everything is real there's really only like one mystery parameter that changes over time because i mean like once you know like one fact like for example the amplitude on the special string then it does kind of determine all the other amplitudes right because we all know they're the same height and we have sort of sum of squares equaling one yeah so um yeah in principle i guess that means that like you know the yeah like in class like you know we you know the like okay the the amplitude on the special [Music] one you know it starts out at like one over root n and then it goes to in class we basically said okay it goes to about like three over root n and then it goes to like about five over root n and then it goes to about seven over root n these abouts are like slightly wrong so i guess in principle you could write down like literally kind of an exact formula for like how it makes these uh steps [Music] uh it might not look super beautiful though like it'll involve like some square roots and stuff oh there's something in the chat let me allow me to look at i think it was a comment on the earlier arithmetic we were doing okay thanks let me just get it out anyway oh yeah thanks um yeah so we could do it we could like write down this formula exactly it might be fun um so for example yeah let's okay let's say this uh say this uh special amplitude i don't know i'll call it alpha so if alpha is a special amplitude then like what is the remaining amplitude so i guess we know that like alpha squared plus n minus one like if beta is the other one beta squared is uh one so that means if you think of like alpha as like known quantity than like this is like a beta squared is one minus alpha squared over n minus one so like beta is um i guess square root one minus alpha squared over n minus one and then yeah it's it's starting to look like not super enjoyable like no uh like i guess you'd be like all right well then like the mean is then alpha plus n minus 1 times this beta over n it's not too bad like alpha plus okay if i multiply this by so i get like root n minus 1 root 1 minus alpha squared over and um yeah and then you're like oh okay uh hmm one thing i can say like i one okay yeah one there's a couple things we can say this is like a weird advice but like i'll pop it up just because it comes to mind and actually i'm pretty sure what i'm about to say is like super relevant for this problem but like i'm not just you know blurting out the answer like i'm just like staring at like some weird expressions if you ever see like in your life like this like square root of one minus alpha squared um this looks kind of gross but if you like re-parameterize it can become potentially better what i mean to say is like uh i mean does it remind you of anything at all like the phrase square root of one minus alpha squared reminds me of some sort of trigonometric or hyperbolic thing very good yeah exactly and maybe you wouldn't be so surprised um there should be ply floating around somewhere which means trigonometry isn't that far behind exactly as as uh three blue one brown does say you should look for a circle whenever i see like i mean this is like an automatic life move if i ever see like 1 minus alpha squared i think to myself maybe i should uh write alpha equals either sine theta or cos theta i'm not sure which one let's say just say sine theta because i mean at first you're like that makes things worse like now alpha j seems to be alpha but now it's sine theta but it makes this better because this is now one minus sine theta squared square root and wow that's cosine squared uh it's cosine it's square root of cosine squared theta so it's well i guess it's either oh i guess it's positive uh cosine theta and so you're like oh well that's sort of better like it's a little bit more symmetric go like this was actually what we were calling beta right so like this beta is now cosine theta over root n minus one okay that's beta and like alpha was sine theta so okay it's not so clear that this is like um helpful but maybe it's helpful because i mean i guess i mean this is certainly a new idea that i will need to play with but i think that that's very tempting to like go ahead yeah just draw drawing a circle like so some thing from the video implies that maybe we are tracing some arcs around the circle and we can say something about theta as it grows or as it traces out segments of the circle yeah for example i guess this is like saying that like okay like in general you know at some time like the amplitude on the special uh string we were calling it alpha before maybe we should call it sine theta that's okay if this is theta sine is uh which one i guess it's this blue height and cos theta is well this although the other ones they're all cos theta over root n minus one uh so okay then like what happens like one thing that happens is you do this operation of like of like negating the the coefficient the amplitude on the special one so that's like has like a reasonable geometric picture i mean it just like like if you think of at a moment in time being parameterized by maybe neither alpha nor beta but like parameterizing it by theta then this like grover this part of the reflection is kind of simple it like kind of switches theta to minus data like it flops you down to here now we still have the problem that we've kind of like have stalled a lot from like investigating that other move the one where you like reflect everything across the mean and um so i guess when you do that like if you're sitting at you know uh sine theta is the special amplitude and like all the others all the others are cos theta over root n minus one and okay when you do this like reflection through the mean thing i guess you'll get to a a new theta prime situation where you're like holding on to sine theta prime and like cosine theta prime over root n minus one and it would be pleasant if like theta prime like borrow some nice resemblance or relationship to theta uh it's not so clear i mean how easy is it gonna be so like okay like the mean i guess is sine theta plus you have n minus one copies of this so like root n minus 1 cos theta over n [Music] yeah it's not amazing so then like the sine [Music] of uh sine like your distance of like the special amplitude from the average it's like sine theta minus this thing sine theta plus root n minus 1 cos theta over n like okay i don't know i can put an n here well i guess you got like n minus 1 sine theta plus root n minus 1 cos theta over n [Music] and if that's your distance from the average then like your new value is like your old value plus twice this distance so you go from like sine theta goes to like sine theta plus two times this maybe i'm doing it backwards [Music] if this is the sign theta what do we just do we subtracted sine theta okay let's so maybe probably minus two times this i don't know i'm mixing up my things so i've got like 2 psi i mean you'll get oh so that's n sine theta minus over n minus 2 times the above so we get like how many sine thetas we will get you'll get like n minus 2 and minus 1 sine thetas which is really close to just 2 sine theta but then there's this additional cosine stuff floating around yeah [Music] oh wait a minute oh yeah it's 2 minus and i might have watched something up here uh well okay you can like here i can do the arithmetic but i i'm satisfied with this injection of the sine theta idea yeah to play around with it so i guess you know if other people have questions like definitely let them okay yeah fair enough like that's that's one uh new idea at least that might help out um and yeah we can come back to it but we can also put it on pause for now sure um yeah is there any other question that somebody wants to offer up uh i would be interested in looking at two or three so if anyone has any preference for either of those then two or three yeah those are the other two yeah uh oh uh i mean which one we can just go for three i guess three okay what uh what's going on in three oh we're going to try to prove a slightly weaker version of the schwarzschil llama uh okay what what are your thoughts on three so far well so yeah i was trying to use the hint okay also just a note i think uh i don't think it says that q is prime but i assume it is right for it to be a field so oh it need not be a prime it has to be a prime power no prime power yeah um yeah i guess i was just trying to use a hint i took care of the first part which is first part meaning like this explain yeah because that just i guess falls from what we did in class okay what about this one uh that part okay did that uh i'm trying to remember exactly why uh yeah i mean if you want to go over it we can buy i remember working it out and it made sense okay so let's assume that's fine yeah um yeah so this line stuff is the line stuff yeah um so i one quick question before we actually like jump into it is um for a line to pass through a point does it just mean that like that set contains that point yeah okay yeah actually i mean it's funny i was talking about this in class yesterday i mean uh there's a lot of analogies between okay so i mean we are looking at uh you know vector spaces over a finite field and there's like a lot of analogies between them and like vector spaces over let's say the real numbers which we're all used to but you always have to be like mildly careful i mean these are like um yeah you always have to like think about them they're always inspiring but you also have to be careful so for example like we could have i mean maybe this is probably going to end badly but like we could imagine drawing a picture let's take q to b i'm just doing it in yeah three and then two dimensions two dimensions okay that's good so uh that's like uh n is 2. so when q is 3 it's a prime so it's kind of convenient because then the field fq is you know just the numbers mod 3. that's nice since it's not a prime power so like you can kind of like imagine um um well this is this is i mean the set f three squared it can be like all right this is zero zero uh one zero two zero uh i have to do here zero one zero two one one zoom in like a two one two two and one two um and like um it's like a line so like it's it's weird like you you know you pretend you're like in sixth grade but like everything's like a little bit askew so you're like okay well you know in a line you have like a point and like a slope kind of so i mean a line in general is like you have like a base point and like a vector in the direction of the line and like the real numbers like if this point is called i don't know like x and this vector is called i don't know v vector i guess x is also a vector but and like the line is sort of like all the points x plus like t times v vector where t is like a real number it's like oh this is like your new it's like an infinite line you know this is t equals zero and like i guess this is t equals one and like uh it extends indefinitely and it's like the same story uh you know in finite fields except well you know there's only like three in this case values for t so for example like the line let's say through um i don't know like zero one in the direction uh i don't know um two zero i don't know is it's like all right you start at uh zero one that's in the line and then like you add to zero so that's i guess oh let's let me make this two one so it'll be like more interesting you add like two comma one so you're like all right one two one so it's like this is the next point in the line kind of going in this direction sort of but then like it wraps around so like you're adding like two one again so it's like one two goes here and then that's actually sort of it like kind of it wraps around so i mean like it's sort of a weird line that's like doing this and like coming back here and uh yeah there's only three points on it um so that's a line and yeah so i guess that gives us some stuff to work with for the counting part which is sure stuck on uh yeah how many lines passed through alpha well let's just finish it here i mean okay wait uh how many lines pass through alpha so let's say this one is our alpha so how many lines do you pass through it [Music] like here's another line through it you can take the line through it in direction let's say um one just for kicks i'll call it one minus one although that's actually like one two uh but like one minus one so like we start here and then we go over one and down one that takes us here we go like over one and down one and get back here it's kind of like this is our line comes down here comes through here here and that's like another line this diagram is getting to be a mess um did we get the exact same set of points yeah that's pretty interesting uh the minus one thing i hadn't thought of because that makes i guess it a little more clear what we're looking for like certainly anything that starts on the point will work but then we can also just take any other point and just direct it straight to that point ah yeah so i think one thing that happened here is you see uh in let's go back in our world of analogies like in like i don't know r2 you know we could start this point like uh i don't know zero one that's like zero one you could take the line in direction um i don't know let's say take the line in direction one one again now it like literally looks like sorry in the direction one minus one again so like it literally looks like this this is the line in direction one minus one but i think the analogy of like what we have what happened up before which is like a little bit weird is like you could also say like now me let me look at the line in direction um five minus five and then you might be like oh wait a minute i'm i'm like weirdly so like this was that's like one minus one and then of course like five minus five is like here so you might say like oh surprising like these are actually the same line even though one one and five minus five are different well of course we're not too surprised we're like yeah that's how um i don't know uh parallel lines work or scale or multiples work but i guess that's exactly what happened here too because weirdly um you know same thing like this i guess is a scalar multiple of this like this one is like two times this one mod three uh so all like you know pictures uh aside like you know these two lines i genuinely kind of randomly picked them they're like happen to be parallel lines like because like the direction you're going was like one direction was like a scale or multiple of the other direction um does that make sense yeah so i guess maybe what i was just thinking is that we could use any point and just it would work but i guess uh i guess any for the starting point i'm thinking now yeah you can see at any point for the starting point and i guess well one thing you have to ask yourself is like how many [Music] okay this has like this line this self same line like we drew this like the blue dasher thing and yellow dash head thing but like actually they're the same line because the line is really just defined by like the set of points sure yeah um so uh for example like another similar line or well another line is uh we could go let's say in this direction we get like this point this point and this point and that's like a line and like a parallel copy of that line is if we went like two steps and then like another two steps and then like another two steps would also take us here yeah so it's actually like pretty difficult to get this yeah but um i think it's true well okay i know it's true um that uh two points always determine a line i mean this is like a general phenomenon in r2 with two points and in any dimension like r3 also two points to terminal line and i guess this phenomenon is actually going to also hold um the finite fields yeah so we need one to be the point that we're interested in and then i guess we just need to pick the other so that we won't run into the situation that you just drew where like we consider both of those points on the parallel line yeah that's true actually if we like let's make this picture a little bit less disgusting or like repeat it over here um you try to look at like quote-unquote all the lines i don't know i said quote-unquote we could try to look at all the lines like through this yellow point which this yellow point so one we talked about the first one was like it included this one and this one and this point so like this bizarre thing was a line then we saw this line at like this point this point at this point that was a line what other lines do we have we have like some vertical lines i don't know this point this point at this point that's a vertical line [Music] and uh let's go over here i guess if you go like up and to the right up into the right you get here you go up into the right you get here so i guess like this one is also a line i mean okay i want to draw it ah um that's also a line and [Music] yeah so i think like there's like four there's like exactly i think now we can try to find and see if there's like other lines right so like i don't know like well but i guess there won't be right because like if you start this yellow point if you believe that any two points define a line then like you can pick your favorite other point like i don't know this one but then you're like oh i already have the black line that goes through them so like yeah probably this is like all the lines the way that the black one is drawn it looks like it's crossing the others but like uh it's kind of going off to the top and then coming back around on the right so we basically just covered the whole entire thing yeah to be honest there's like no real natural way to draw like the the lining part of the line like really at the end of the day in these final fields you just like is like a subset of the points and so like you know arguably maybe the best way to draw like the black line is you're just like well it's this point this point and this point and like you don't try to like encircle it or like draw a line through them but still i mean i don't know at the same time you like want to hang on to some of your intuitions from like the regular um geometry of the real numbers um that makes sense i think some of these phenomena will um persistent in uh higher dimensions too so for example like if q is two i can only do the opposite like q is two and n is three so if the numbers mod two in like three dimensions i'm just gonna draw this thing i don't know if you can tell uh like here every line i think has like only two points on it uh and maybe like every pair of points constitutes a quote-unquote line yeah that's interesting that's like a much better way to think about it because i was kind of going algebraic and yeah well you need a mix yeah like you want to like retain like some intuition of geometry but there's only so much geometry in the final field so you also have to like do some yeah like when i when you go to actually prove it you'll need to yeah okay cool um okay let's pause a little bit there and uh we can come back to any of these problems we like but are another question i mean i wouldn't mind looking at two but i want to give everyone a chance to ask questions if i have a question about what sort of uh mathematical machinery or uh just facts we can carry around without proof in these problems on modular polynomial arithmetic like for instance do we know like if does distribution hold in modular polynomial arithmetic like polynomials alpha times the quantity beta plus gamma is equivalent to alpha beta plus alpha gamma oh uh for polynomials yeah like p of x times like q of x plus r of x yeah mod oh right like mod with the coefficients like mod um like some degree d yeah you could even say like well coefficients we always like to typically assume are in a field so i might even say um mod per se but um well we can ask about like this um uh basically i mean you can assume uh things for example like i just say yes this is true it's true that like okay like i didn't like exactly for example define multiplication of polynomials over a field i just kind of took it for granted that um the definition was clear and i mean i understand you're bringing it up because like okay like particularly like this thing on the homework is like kind of confusing um like you could easily ask like well what do you want me to like prove per se here and uh for this particular question like i said you can be very um brief uh maybe it's almost like just you know make sure you understand you can rephrase this make sure you understand why like this can be viewed as a vector space over of dimension l over this um scalar field yeah um as i was sketching out like what i would write if i were to go about and prove it and it's like oh well this proof works if i have this fact it's like okay do i need to go prove that this mechanism works and then i wrote a little sketch for that mechanism like okay eventually we have to get down to a fact where we can just sort of say yes this is right like if i have like i think the base fact that i can always have is that if we are taking just mod and normal field like if we have a number and like eight mod three then you know we can split that number into a multiple of the modular thing plus a remainder and we'll eventually get down to the remainder yeah i guess um uh yeah that's even like overboard for me i used to have the word like you know very briefly in here and i like deleted it but like maybe i should have left it in there i mean i guess what i'm saying is um well one thing is like i'm kind of like take it for granted i want you to like take it for granted that i mean we already discussed it like okay that's a bad color um like um like um you know fp the numbers mod p as a i field write fp is a field and so particularly like all the rules of arithmetic like definitely you take them for granted like addition is like associative and commutative and so is multiplication as distribution i mean this is the nature of a field and even like kind of assuming like all throughout this that like you know it's taken for granted that like um you know we know what it means by like uh like this vector space f p to the n and that this is a vector space like the uh usual laws apply and of course you have any like question about it like feel free to ask but i guess um the thing that i'd like i just want to make sure you think about and is that well uh so in class we you know i sort of defined the this these fields modulo like a prime power in terms of polynomials a little bit informally i mean i said you know um uh for example like i mentioned that like let's do a specific example like f9 is f3 squared you know first i said oh you can think of it as like all the expressions are like a plus bi or like a and b are you know mod three and like i took it for granted that you know there's like the traditional rules for like how to like manipulate these numbers and then a bit later we said like oh you can also say it's basically the same thing but it's like oh it's all polynomials a plus bx where like a and b are numbers mod 3 and like it's like mod this irreducible x squared minus one which basically just means like this is like formally zero so like x squared is formally one like whenever you see an x squared you can replace it by the number one which is exactly you know the deal with i oh sorry minus one this should be mod x squared plus one x squared is minus one whenever you see an i squared you can just formally replace it by minus one and i guess like i just wanted you to think a little bit about like well actually in under these either these two phrasings like it doesn't immediately look like a vector space where you have like let's say column vectors so uh what's up with that um perhaps it's not so actually i i thought of like one or two ways in which uh you know you would answer this acceptably like either you might say like well these objects that we're manipulating like we can like have a mapping of them onto vectors in like a natural way like vectors in like f three squared of like you know you know vectors with two numbers that are mod three it's not much of a secret how you would do this and then you just wanted to like check that like the the usual rules for manipulating these field elements or polynomials are like the same as the ones for vectors and really we're only talking here there's no initial multiplication per say for vectors we're really only talking about scalar multiplication and addition right yeah so i have written out a proof sketch where it's like oh all i have to do is show the scalar multiplication in addition but i'm like okay so i could stop here i could go another level so yeah i think i have a good sense of how much machinery i can carry around i just wanted to ask yeah frankly like you know i just want like one or two sentence saying like yeah scalar multiplication and addition are fine or if you're like a math you know a lover you could be like well um uh what does it mean to be an abstract vector space of our dimension you know it's like a set that um supports you know all these operations of addition it's closed under addition and like scalar multiplication um and so you don't have to literally mention like column vectors per se if you're like really gonna be like hardcore math about it you can just say like yes these objects supports you know a notion of addition and multiple and scalar multiplication but yeah maybe maybe you you got it i've said too much yeah so this i just wanted to make sure that you all were in the right frame of mind for the subsequent questions um because yeah i mean it's a little bit funny like it's asking you to think about like sometimes we think about like f q as just like a set of numbers like we forget a little bit about like how do we get these numbers i mean when it's the numbers um like mod or prime p they're just that's fine but you know this finite field is a bit funny and um you know it's connected with like how you would actually on your literal computer like implement these numbers and literally implement like um multiplication of two field elements uh is there another question yeah i guess i had a question about part b b okay um well i guess i kind of need to say some of what i did for part a for part b that's fine oh yeah so i mean i was trying to just stick them into vectors yeah that's fine the polynomials and then the basis becomes like pretty clear um [Music] but yeah i guess i'm slightly confused with how the modular polynomial arithmetic works because um yeah i guess i have to say what my vector representation was so that's fine yeah um yeah i was just putting like the coefficients into a vector well yeah i was actually putting in the the x's as well okay yeah you can do that but i guess because then the basis would be like uh uh yeah you can form the basis that that's the only way well actually that doesn't quite make sense now that i think about it because well i guess when i say when you see a phrase like a vector space or dimension l over a scalar field f p you know vector space dimension l over a field you know like the simple way to think about that is like okay like the objects here are column vectors of length l and you know l entries and then each of these entries is supposed to be a number from the field yeah so i guess okay i guess like x is just like a symbol so it's not really a number from the field yeah that might make things easier i guess because yeah i had x's in there and it was getting kind of confusing i guess i could still ask one of the questions that i was running into though which is like if i have something of uh okay so r of x here is degree l so if i have something of degree like bigger than l yeah then when i mod it by r of x we know that we decrease the degree by l right uh you even like decrease the degree to something that's less than l oh yeah like so but the degree is what it was minus l or that's not always true um sort of right in the sense of like a mod motto sorry like you know when you're doing um numbers like mod 10 um if you see like 543 that's uh but you want to say like mod 10. it's kind of like saying like okay like all tens count as zero so you could be like oh so i could like subtract 10 from this and get 533 and like these are just treated as equal now um and you could say you know what i'm going to actually subtract like um 50 times 10 so 50 times 10 is like also considered to be zero if i subtract 50 times 10 then i get like 33 and you're like oh i'm gonna also subtract like three times ten and then you'll get down to three wait where did the yeah okay and then you're like okay there's like no more it's not really any more like effective to like subtract multiples of ten like i've kind of got down to the smallest thing i can get down to so like maybe this is where i'll start that's how the degrees work yeah so like more generally let's say we're like uh we're like i don't know if i know like a good think like let's say p of x is uh x cubed plus x plus one and uh we're going like modulo 3. so then i don't know we might have like two polynomials like um two terrible color like two x squared plus one times uh x squared plus um well i could even say like x to the like five plus x plus one and then like mod p of x okay first we multiply this out like normal so we get like two x to the seven plus two x cubed plus two x squared plus x to the five plus x plus one they're like okay now we have to like mod out by this thing x cubed plus x plus one so it's really like actually like you guys are like do it like mentally like all right what what would be like a good multiple of like x cubed plus x plus one to subtract out and you could say oh maybe i should subtract out like subtract you know looking at this like biggest thing which we're trying to get rid of like 2 x to the fourth times x cubed plus x plus one like since you know this is supposed to count to zero that means this whole thing is like also counts as zero and it's like nice if i subtract this out then i'm subtracting two x seven minus two x fifth minus two i'm doing this minus this and i guess that gives me okay the highest power is x to the fifth minus this actually i get like three x to the fifth from here and then plus two x cubed plus two x squared plus x and then one minus minus two is plus three i'm also like all along i'm also going mod 3. like this is like oh cool like that's 0 that's 0. and so i got down to like 2 x cubed plus 2 x squared plus x then i'm still not done because i'm like oh but like you know i've still got x cubed plus x plus one is zero and like it would be like effective to like subtract off another multiple of that so i'm going to subtract off like looking at this top term i'll subtract off two copies of x cubed plus x plus one and i'll get something that's still congruent to the original thing like equivalent to the original thing and i guess i'll get the 2x squared i'll get x minus 2x so like minus x minus 2. if i do this mod 3 i might be making some arithmetic errors here but 2x oops wait a minute no that that went away that's just 2x squared i'll get like 2x squared plus 2x plus 1 or something mod 3. and this is where i stop because i'm like all right i mean i did the coefficients mod 3 all along but like there's no longer like any like effective multiple of my p of x x cubed plus x plus one that i could like subtract from this to like make it smaller meaning a smaller degree because like you know any multiple of x cubed plus x plus one it's going to have like a top term that's at least x cubed then i already got this things degree like below x cubed so um this is where i'll stop and this is where you will like stop and you kind of see that like again with like a modular arithmetic of numbers you know if you have like some a big polynomial like high degree polynomial like mod that's x cubed plus x plus one you're gonna like keep subtracting stuff off until like this thing has degree less than three strictly um okay yeah and so i guess but we don't really know all we know is that that's the degree or the bound on the degree but we don't really know what's going on with the lower order terms not really yeah i mean because that's the issue that i'm running into when i'm trying to put the stuff in a matrix um let's even just look at the problem and maybe we'll do like an example of that problem um let's see where were we somehow i'm naming everything alpha today so fix a particular alpha and consider the function m sub alpha which is like multiplication by alpha verifies the linear transformation and hence representable as a matrix so yeah let's let's try to do an example here the idea there let's go back to um f nine so here like p is three and like l is two and like the irreducible polynomial or the prime polynomial is like x squared plus one this is the thing that we're treating is like formally zero it's kind of like saying x squared is treated as minus one which is some people would prefer to call x i but we can just also call it x so we need to pick like our favorite well a particular value in this field uh alpha to consider multiplication by i don't know let's do alpha to be 2 plus i was going to say 2 plus i say 2 plus x either way [Music] so we want to understand why the operation of like m sub what's called m sub alpha or like this operation or the more expansively like multiply by two plus x operation is actually like a linear transformation in the in the vector space point of view so um you know a generic element of f9 you know a plus b x or a plus b i or these are like mod 3. and how do we multiply this by alpha we call this like beta so in one hand right uh from the previous part we're also sort of in the back of our minds thinking of beta as a vector like a b but we also think of it as like a field element so how do you multiply 2 plus x like alpha beta times beta is 2 plus x times a plus b x so how does it work you're like okay i'll expand this out i'll get like 2a plus 2 b x plus a x plus okay now we get like b x squared and you know x is like i it's it's it's uh it's minus one so we're like okay this is like 2 a minus 1 plus and then we can like a plus 2 b x or if you want to go like mod 3 this is like 2 a plus 2 plus a plus two b x um okay did i make any mistake here okay um so this uh if you think about this as like a vector it's like two a plus two a plus two b and so like the the transformation that took generic element beta to like alpha times beta is kind of like in vector world it like took this vector a b to like this vector 2 a plus 2 a plus 2 b is there actually what's like b isn't it two a minus b i hope so because like there's something that's like really wrong about this so i'm like really hoping there is a bug somewhere because it's plus b x squared and you're approaching oh yeah i don't know why this oh i forgot this b thank god because i was like i don't know where what's going wrong there was something wrong this is not a linear transformation it's like an affine transformation but yeah thank you like i made a mistake here i dropped this b so this is not minus one it's like minus b or you might also call that plus two b so this should have been two b thank god um so yeah like what has happened is like you know the it's kind of like you know the what we sort of saw is that like the like vector representation of like alpha times beta is sort of like we took the vector representation of theta which was this and well it turned into this and like the idea is that like this is some matrix multiply of this i guess it's like this is like let's see if i can figure it out on the spot uh a b i guess it's like two two one two perhaps like this matrix times a b is indeed this so yeah we sort of saw this two two one two when this went this alpha happened to be whatever example we used two plus x so we kind of saw it like this in the end like okay if you want to know if if you're really obsessed with you know this particular field element two plus x you really want to know like hey if i got the vector representation of some element beta and i wish i had the vector representation of this element like two plus x times beta how could i get it well luckily it's like you do like a matrix vector multiply you multiply this matrix against this vector and you get the result so i guess a little bit the question is asking you like you know make sure this like always happens like for every value of alpha in like every field so in some sense like it's kind of um maybe on your computer you would like have some kind of like mapping between like if this is your name for alpha two plus x you'd like have some table where you're like well i'm gonna also remember this matrix because like if i ever wanna like multiply by this i could do it one way i could do it is like i mean i could do it in exactly this way but like it might be more effective to just like do a matrix multiply what was alpha again i'm just trying to see if it lines up with like alpha was like this just like i took like two plus x like a random example two plus f yeah i'm just trying to see if that like the matrix we got lines up with what i was trying yeah i mean there's a question there's something is i don't know going on with these entries and like these entries um but i guess really what this question is asking you is to make sure or to prove that like um maybe this always happens like if you look at you know if we are now like oh like what's the vector representation of i don't know i'll pick another random element like i don't know 1 plus 2x times beta is this like is this like a matrix multiply times the vector representation of beta or maybe it's not like maybe it's not even a linear function like maybe if this is like a b maybe this answer is you know a squared plus one b like something which is not like a linear transformation but it's not it is a linear transformation but uh this is what you have to like reason about yeah that makes sense so what i was trying to do is approach it with like just giving a matrix that captures this but i'm saying it might be or well i guess that's probably what the final answer will be but you're saying we could also like just multiply it out and then show that that came from a matrix yeah i mean i think in this you may or may not find i'm not sure but you may or may not find that it's easier to argue that a matrix exists than to like sort of really say immediately what the matrix is uh i don't know if that's true or not but like i guess uh the way the thing is actually phrased is it's kind of phrased in like a bit of like a like a abstract linear algebra way like um you know like if you remember like all the way back to linear algebra like abstract linear algebra like you know they're always careful to like first define like theoretically the notion of a linear map or a linear transformation and then they're also like oh you can for every linear transformation like you can represent it like concretely as a matrix multiplication and conversely every matrix you know multiplication corresponds to like an abstract linear transformation and like oh this is true sometimes it's good to remember sometimes you're just like you know chill out math like we all are kind of fine with like conflating the notion of like a matrix multiply and a linear transformation i guess i didn't even know that was because yeah i just thought linear transformations were more general so i didn't the reason i was going straight for the matrix is because i thought that that would be how we show that it's representable but not that the fact that it's a linear your transformation would imply that but yeah i oh i guess even the next question is like how to actually compute this matrix so in fact in some sense like yeah you kind of have if you if but if you show c then you're like hey this sort of shows b like if i just explain to you like why this is like a matrix multiply then that i mean a matrix multiply is a linear transformation i guess like if you're into abstract linear algebra like uh you could view this as like a bit of a more like natural progression where like in b you don't even like talk about matrices per se like you could be like well you know abstract linear algebra like a linear transformation is you know just like a map like l that maps you know like vectors to vectors and has the property that like l of x plus y equals l of x plus l of y and i guess like l of like c times x is like c times l of x like that's the definition of like linear transformation and like it might be in fact i'm sure it is easier to verify that the sort of operation l of like multiplying by a field element like two plus x or any field element just satisfies these little um rules for what a linear transformation is and then you could say like oh therefore by like my knowledge of linear algebra i know automatically that there should be a matrix representation for this transformation and then you would like find that matrix in c but if you were like the kind of person that was like not that intro abstract learning algebra you could just be like whatever like i'll just directly solve c and that shows that it's a linear transformation cool yeah that's a cool fact i did not know that you could represent every single linear transformation as a matrix so yeah that's cool it also explains like why the rule for matrix multiplication is what it is so like if you never thought about that too deeply for it's good to think about yeah i mean does that only hold when it's finite or i guess we could have like a uh when it's finite dimension no okay yeah you could have like infinite dimensional vector spaces we won't really in the course and then i guess it's not too nice to define infinite matrices so it's probably best to okay cool thanks yeah um say i guess it's almost uh well time is sort of up but if there's like a very short question i can answer no okay\", metadata={'source': 'Qux1OsgojAY'}),\n",
       " Document(page_content=\"today's lecture is about error correcting codes it's a very important topic in CS theory you could teach a whole course on it and indeed professor Venkat guruswami periodically does teach whole course on it here at CMU we'll only get one lecture on it but it's a nice application of some of the things we talked about last time polynomials and finite fields so the goal of a rector air cracking codes is to cleverly find a system whereby you can take some data the you want either store or transmit and cleverly add some redundancy to it so that even if there are corruptions errors like some of the bits or bytes in your message plus redundancy get corrupted or altered you can still recover what the original message was this topic is study in several different areas including electrical engineering and information theory and theoretical computer science and the first two areas they tend to focus on probabilistic or randomized models for errors and in theoretical computer science they tend to focus on worst-case models for errors and so this being a CS Theory course will focus on the latter at least in this lecture but both are important ok so let's dive in what is an error correcting code it's somewhat of a long definition so an error correcting code ECC is how abbreviated here it's it's a map it's an injective map that explains how to encode the data it'll call this map Inc or encode and it maps strings of length K over some alphabet Sigma 2 strings of length n over the same alphabet where okay there are several things to add here first of all Sigma is the alphabet sort of the symbols for the messages and their encoding okay and will often write a queue for the cardinality of Sigma and Q's often but not always - okay if you're transmitting the bits and bytes that are bits that it'll be - but if you think of your data packaged into bytes or larger packets than Sigma might be larger the set of all possible things you might send Sigma to the K is called the message space and this parameter K is called message length or the sometimes called the dimension for reasons we'll see soon okay so that's the input the encoding mapping is encoding some message of length K the output length and it's called just the length or the block length this will naturally be at least as large as K we're not trying to compress anything here and furthermore we're going to specifically be interested in the range of this encoding map so all the possible things that it may output and we'll call that set of you know n character strings see and this is sometimes called the code itself so sometimes by a bit of an abuse of notation or terminology we don't distinguish too much between the encoding and just the set of outputs of the encoding map these are called the code words okay so any string which is actually inside this set C is called a code word okay so generally and will be bigger than K so the set of all outputs of the encoding function is like a sparse subset of all possible lengths and strings and finally there's some parameters associated to a code and one of which is K over N which is called the rate of the code and this is something that we want to be large okay so this will be some number that's between zero and one and sort of the smaller it is the more redundancy you're adding so ideally K will be very close to and so n will be not much bigger than K so you're not adding too much redundancy so it's efficient and in that case the rate will be close to one okay so the picture here to sort of draw a picture is we start with some message X that the sender wants to send or the person that's storing data for the future once the store and this is a string of length K over this alphabet Sigma and then we imagine that we encode it using this map and we get a code word Y this is a string of length and okay but the reason we don't just you know set n to be K and set the encoding function to be the identity function is that we're anticipating that there'll be some corruptions when we try to send Y to a receiver or when we try to read back Y at a later date if we're storing so what we imagine is that there's some other process that produces up to T errors and that creates a string Z and Z it's also a string of length and so the model we're going to use is that an error involves changing corrupting one symbol to a different symbol okay so at least in this lecture we're not going to talk about the possibility of errors that like insert a symbol or delete a symbol or somehow racist symbol we'll just imagine the case where a symbol gets corrupted change to a different symbol and often times it's a reasonable model that encompasses some of the other models and now what you imagine this is the transmitter transmits well why the encoding of X the message the transmitter was a send the receiver gets Z and now somehow we want to decode ideally this Z and get back X again okay and one thing I you know I one thing I said with that you know you think of the encoding process is taking the original string and like adding some redundancy to it it's we're not actually insisting that the error correcting code be of the form you know first transmit X and then transmit some additional redundant symbols Y might just be any old string that represents X actually often it will be the case that the symbols of X will be a substring of Y in that case it's called the systematic error correcting code but we don't necessarily insist on that and as I said we're also gonna work in this sort of a worst case model of errors so we're gonna have some fixed bounty and we're gonna imagine that like always at most up to T errors could get corrupted okay and an adversary could corrupt them in the worst possible way to try to fool everybody any questions about this okay so we want to think about like what makes a good error correcting code encoding function or a good set of code words for the purposes of being able to do this last step take like a potentially corrupted code word and figure out let's say the Y that it came from which will then allow us to deduce the X that Y came from so one definition we'll need for this is that of having distance so presumably you've seen this before having distance denote a delta between two strings y&z is simply the number of coordinates on which they differ the number of coordinates such that Y I is different from Zi and now I'd like to try to draw a picture of all these things we've talked about so far here so let's imagine that this box here represents Sigma to the end all possible strings over this alphabet Sigma of length n so this is all possible things that a receiver could potentially receive and I'm gonna put these little kind of dots here and these symbolize the codes see okay so these dots each dot here is a code word in C okay so will have Q to the K different dots in this whole space of Q to the N possible strings and we're going to look at now let's say if we have a particular Y here you know according to the scenario we're imagining that like up to when you transmit like up to T symbols get corrupted or changed which sort of makes us think about these objects these are like the Hamming balls in this space of radius T ok so let's T okay so this circle depicts actually all the strings whose having distance from Y which is called the center of the Hamming ball is at most T so having a ball of radius T and you know we might imagine if this was why was the transmitted word and the received word might be any I don't know string Z inside this Hamming ball okay this is a according to our assumptions the thing that the receiver might receive and now when in principle we may ask ourselves could receiver figure out what the intended message was in other words figure out what the originally transmitted code word Y was you see this can happen as long as this Z is not within distance T of a different code word okay so well you should really imagine is drawing a Hamming ball of radius T around every code word here's another code word y-prime this is a Hamming ball of radius T okay and as long as all these balls surrounding each codeword of radius T are disjoint then a decoder can at least in principle solve the problem of given a Z that's you know corruption or having distance T at most from from some codeword figure out what is that code word that it's closest to we'll talk about algorithmic issues later but in principle it can do it and you see what's required for that for all these Hamming balls to be disjoint is that the the centers in other words the code words themselves should be sufficiently far apart and how far apart should they be they should be farther apart than two T okay so what you want to do given a code is look at all of the Hamming distances between pairs of code words look at like having distance between let's say Y and y prime and as long as this is bigger than two T where T is the expected upper bound or the you know the upper bound and over corruption will receive then we're in good shape whatever the received word Z is it uniquely maps to some closest codeword Y and then the receiver can at least in principle determine what Y was okay so that motivates the following definite definition if we're given in a code and we want to understand sort of how many errors T can we potentially tolerate while still being able to decode any corrupted received word then it's governed by this minimum distance between any two code words so we'll make it to finish definition for this this is one of the most important properties of a code the minimum distance of a code which is denoted D it's just the minimum over all distinct y and y Prime in the code and the range in the encoding function of the having distance between y and y prime and the fact that I just tried to convince you of in words is that this process called unique decoding in other words the ability to take a received word we pop to T corruptions and figure out the unique code word that it's closest to it's possible for a given code if and only if the error bound T is that most basically the minimum distance over to okay so at least if we're not concerned at first with you know algorithmic issues like how complicated is it to encode strings to decode them and so forth then it becomes sort of a common toriel question to think about you know how many you know strings can we pack into the set of all n bit strings such that they're all mutually far apart and one idea you might have for this you know common idea in combinatorial situations like this to get a nice set of strings they're all far apart is to simply pick them randomly yep uh I think it's correct well I mean my idea is that it's correct with less than or equal to the floor of the / - what's that uh well let's see if the minimum distance is let's say three so that like all of the code words are having distance three then you can certainly decode if and only have T is at most one that checks out if the minimum distance is two then you know so the picture is you have a code we're here a code word here and there's a code word in the middle like this since one from each of them then yeah I suppose you cannot decode at all unless the number of errors is zero so we better put d -1 let's see does that work d strictly less than now let's put D - do you - 1 / - does that work 3 you should go to 1 and 2 should go to 0 ok I think this is fine good right so as I was saying potentially one way to you know get a lot of code words they're all mutually far apart it's perhaps you could choose this set of code words randomly and that's not a bad idea if I can talk about this towards the end of the lecture the good thing about it is you do get very good rate versus distance trade-off usually all starts to sink distance instead of minimum distance one second and what's bad about it is well it's bad algorithmically it lacks typically efficient encoding and decoding algorithms feel for example if K is like 100 and Q is 2 so there's 2 to the 100 code words okay you could imagine in principle just picking them randomly to be a subset of I don't know all strings of length 500 but how are you going to store them how are you going to like figure out a mapping that map's a message to a codeword and so forth yep yeah you can consider all these things as I said today in this lecture and often in CS theory we mainly consider just errors corruptions where one symbol gets changed to an arbitrary different symbol so we're not going to talk about deletions or insertions or erasers\", metadata={'source': 'H7XFslRXJys'}),\n",
       " Document(page_content=\"okay so a typical basic goal in the theory of error correcting codes is to find good codes or good like encoding functions I have several nice properties they have high rate they have high minimum distance and also there are efficient algorithms for doing the encoding and for doing the decoding either ie you know given Z find the closest Y and therefore the originally intended X message X so let me tell you about a first basic idea in error correcting codes which is very important and that's the idea of using or a called linear error correcting codes and as we'll see many electric air cracking codes have as one nice property that the encoding algorithm is always efficient so they're correcting codes are basically ones where the encoding map is where you think of the strings as vectors and the encoding map is like a linear transformation and in fact essentially all known good error correcting codes are linear error correcting codes so people almost exclusively study in the end linear error cracking codes sorta so linear correcting code as I said first of all it's one where the alphabet set is identified with a finite field okay so a particular it means your alphabet set will only be a prime power but that's fine we know that there's a finite field of size 2 to the L for every L and that's a popular set of values in computer science and it's whereas I said the encoding map which we now think of as mapping we call them vectors of length K over this field of size Q into now vectors of length and of size Q is the linear transformation getting in a particular what that means is this map this encoding map takes as a message X and the encoding is x times some matrix which I will call G let me say that in this lecture there's always like an annoying debate about like whether X should be a row vector or a column vector and try to make the choice that's consistent with like most things in the literature but sometimes I might be a little bit flaky about it and you should just always realize from context whether I mean a row vector or a column vector so in particular here I'll draw the picture here we're thinking of X as a row vector here the picture is this the encoding picture we have some message X of length K that we want to encode okay we're going to multiply it against a K by n matrix okay which is called G okay so this is also K this is n this G is called the generator matrix for the code okay and one thing you can think of is that like the rows of G are some code words in the final code a very small set of the possible code words and to get all the code words you sort of like take all linear combinations of these code words according to X so in much in frighten your mind and you're in your head the case of Q being 0 & 1 so there's no excuse to so the the symbols are bits so X is like a 0 1 vector and when you multiply it against G it like picks out like a subset of the rows of G and adds them up mod 2 ok so this will give you the code word Y of height in okay and so the quote code itself or like the range of all possibilities is the column space sorry the row space of G all the linear combinations of the rows of G and it's a K dimensional subspace Q to the N a vector subspace actually we should insist that this should be full rank okay so that this is strictly speaking true okay so it's a nice linear algebraic situation the set of all code words is just all the vectors in a certain K dimensional subspace of the vector space of F q2 then and that's why sometimes the case just called the dimension of the code because they're almost always in the linear code situation any questions about this okay so one good thing about linear codes which you know they're defined by the generator generator matrix is that for linear codes this encoding is always efficient given G it's efficient okay so anyway who's working with the code is going to implicitly or explicitly have G on their hands and then the process of taking X and figuring out the thing it's encoded by Y it's just a vector matrix multiplied okay so that's great now what about decoding we could ask about this in general the task of given a generator matrix G and a received word Z finding the Y in the row space of G or the range of G which is closest to Z is np-hard but for some nice codes or for some specific generating general matrices generator matrices G this task is in P okay so we're going to try to design error correcting codes with some nice properties which it will in particular make the process of finding a corrupted word going from a corrupted word to like the closest string in the range of G an efficient process okay so there's some notation that we're gonna use a lot in this lecture when you're talking about lis or error correcting codes it's like some funny notation I'm not sure who invented it but it's actually pretty convenient okay so if you see the following sequence of symbols and K sub Q with square brackets or more generally and okay D the square brackets sub Q this refers to a linear code with parameters NK D and Q okay so mapping f K Q to the K I ask you to the end of minimum distance D and funnily enough like the squareness of these brackets is what signifies it's a linear code so if you ever see this kind of notation but with round brackets it means like a general not necessarily linear code with parameters and KD and Q okay so now let's think a little bit about this decoding problem given a received word Z how can we find the closest string or vector in the range of the code to Z in fact one thing we can think about first is how can we just recognize if received word is in the code in other words there are no errors well this is also nice in the linear algebraic setting there's a pretty nice characterization of when a vector is in the range of a linear transformation because you can characterize the range which is a certain K dimensional subspace of the vector space by like the vectors that span it but the vectors that are orthogonal to it okay so you can characterize the subspace by all the vectors orthogonal to it and this these vectors are orthogonal to it will give us a way to check whether a particular received vector is actually in the code okay so we'll need a little linear algebra for this so for a K dimensional subspace code C will define and this is like a linear algebra concept actually see perp to be the set of all vectors W of length n which are quote-unquote perpendicular or make a zero dot product with all vectors Y in the code okay so it can help somewhat think of like you know the geometric picture of vector spaces over the reals where you know if you have let's say a two dimensional subspace of three dimensional space the perpendicular space is like all this the subspace of everything that's orthogonal to that in other words it's like the one-dimensional line perpendicular to the two dimensional subspace next of only B be a little bit careful about these geometric intuitions when you're over finite fields because there's not really a notion of like 90 degrees between two vectors but you can still take this as the definition of W and Y being perpendicular okay so based on maybe your intuition from real vector spaces you may believe that this following exercise is true but C perp is also a subspace of the full vector space F Q to the N of dimension and minus K okay and I guess suppose another fact that like C perp perp is C again okay again thing of like in three dimensions like a two dimensional subspace being C and like the one-dimensional line perpendicular to it being see perp okay actually so now actually since we have like another vector subspace of fqdn we can also think of it as a another code this C perp is a and n minus K sub Q code it's called the dual code to see know illustrate this with a concrete example soon enough and it has an encoding map as well so call any perp that map's a message of like n minus K again to a length and string by X maps to X H okay let's leave this here and this H it's an important matrix for the original code see it's called the parity check matrix which is the name that makes the most sense if Q is 2 so H is n minus K by n matrix ok what's called the parity check matrix for C and the property it has is that the rows of H are orthogonal to every code word ok so this is n minus K height width and ok the property that has if you multiply it against a height and vector call it Z ok then you get the zero vector if and only if Z is in the original code word C ok that's from this definition so this matrix is also very convenient this parity check matrix because it's like a set of checks that you can do on a vector Z to tell whether or not it's in the code if you take you know it's I think of it as e again like or Q is like 2 so these are 0 and ones so take the dot product of each of the rows with Z's these are all like called parity checks because they're adding up certain subsets of the bits of Z mod 2 if they all come out to 0 this happens if and only if Z is in the code you have a question yeah because any I mean if you take any set of vectors with the same rowspan as H you'll have the same property okay so this is also another nice aspect of linear codes once you get your hands on this H then you have an efficient algorithm maybe not for finding the closest code word to a given arbitrary string but at least we're testing whether an arbitrary string actually is in the code like there's zero corruptions okay good okay so these two concepts for linear codes will help us understand some basic facts about minimum distance in codes once I'm through these all finally give you some examples so one fact is that the minimum distance in a code C is also the same thing in a linear code so this is all for linear codes it's also the same as the minimum Hamming weight of a nonzero code word so let me explain a few things about this statement first of all Hamming weight of a string is just the number it's a Hamming distance from the all-0 string or the number of nonzero entries another thing to point out is when you have a linear code the all-0 string will always be a code word but now if you look over all strings other than the all zero string which everyone has the least Hamming weight its Hamming weight is the minimum distance of the code okay and the reason for that is pretty straightforward if you have two distinct code words y and y prime then the distance between them is well it's the same thing as the Hamming weight of their difference this is there over now we think of them as vectors we can subtract them and the difference between any two code words is also a code word by linearity okay so you can see that if you have two distinct code words with a certain having weight Hamming distance between them then you'll get a code word equal to their difference whose Hamming weight is equal to the Hamming distance and conversely if you have a string of a certain Hamming weight then there are plenty of pairs of code words whose difference is that code word okay so this gives you some easier way to think about the minimum distance in a linear code okay and another way to think about the distance of a code a linear code is in terms of the dual code it's also the minimum let me try to phrase this prep early number of columns of H which are linearly dependent it's a non-0 okay in other words you look at the columns age and you try to find the fewest number that you can that are linearly dependent okay so if you're working over the field of size - this just means that the sum of these columns is the all zeros vector and why is that well we know that the distance of the code is the minimum Hamming weight over all code words that are not zero that's what we just talked about and this is the minimum Hamming weight instead of saying Z is in the code we can equally well say that Z satisfies the parity checks that Z H equals zero Z should not be the zero function it's a zero vector well let me yeah I'll put Z on the other side because E is a column ok now you can see what that means I mean if you're looking at you know low Hamming weight strings Z or vector Z that you can put here such then when you multiply them by H you get the all zero string it's really like you're just trying to find you know these three positions if they're nonzero it's kind of like they form this linear combination of the columns okay so to find the least Hamming weight vector Z which would multiply it against H gives all zeros is to find like the you know sparsit's linear combination of the columns that adds up to zero\", metadata={'source': 'JjmGn8ksnGE'}),\n",
       " Document(page_content=\"[Applause] okay any questions about this okay so now I'm going to finally kind of come to some examples based on this so let's think of for now Q being two okay our maybe our favorite situation we're thinking about binary codes where the symbols are just 0 & 1 and let's say we're trying to find a codeword or sorry a code a linear code that has high rates so like lots and lots of strings in it that has some decent minimum distance okay we're gonna think about this characterization of the minimum distance of the code all right so as long as H doesn't have a column which is all zeros then the minimum number of linearly you know the minimum size of a linearly dependent set of columns is bigger than 1 okay if you have a column of all zeros then that single column by itself is linearly dependent okay but as long as you have a column of all zeros this minimum number is going to be at least two what's the simple condition that ensures that this minimum number of columns is at least three what should I mean about a matrix H over the field of size two such that it doesn't have any linearly independent set of columns of size two yeah right mod two or in the field of size two two vectors are linearly dependent if and only if there's a same vector okay so as long as we take any matrix H that doesn't have the all zeros column and doesn't have two columns that are the same then the smallest size of a linear dependent set of columns is three so we'll have a code of minimum distance at least three which is sort of like the first minimum distance that's like good for something because as we saw before when we were fixing this thing about the distance like in this case you'll be able to decode from one corruption as long as we have such a matrix then okay we'll have minimum distance at least three and if we want a code that has lots of code words in it that has a very high rate do we want H to be like tall and thin matrix or a fat and wide matrix short and fat we want to be short and fat right so I mean okay its width if we're gonna have like block length n then its width is gonna be n but if we want K as large as possible then we want this matrix to be as short as possible okay in other words we want you know the matrix to be as wide as possible so trying to compact like a bunch of columns no two are they're the same and we kind of want as many of them as possible if we're trying to get high rate so one thing we could do is just literally put in every possible distinct column except for the all zeros column and this will give us an interesting code so in fact this code when we do this idea is called the hamming code and one way to define it is not by its saying what it's generating matrix is generator matrix is but to say when it's parity check matrixes so it's defined by having a parity check matrix H which is just literally all whose columns are all possible strings of some fixed length I'll call this length R 1 0 1 1 okay all possible columns except for the all zeros column okay so if I fix the height to be R then of course the width is 2 to the R minus 1 and this will be our n okay so the K since this height is n minus K it means the K is 2 to the R minus 1 minus R ok so this is 2 to the are 2 to the R minus R minus 1 and I'll say something about the D in a second code over the field of size 2 and what is this yeah because here I put like oh I should not have this one this should end in a 1 I'm putting all possible binary strings of length R except for the all 0 string sorry about that oh good so as I said before by this characterization since this has no all zeros column and it has no two columns the same the size of the smallest linear dependent set of columns is at least three you can actually easily see that it's exactly three and therefore this distance is three okay great this should say code at the end okay and if you like to think about it this way this is N and this K is basically an minus login it's pretty much n minus log of n plus 1 and this is sort of a nice code in some senses so one nice thing about it is it's rate is great I mean K is very close to n if you're trying to encode a binary string of length K there'll be a number of additional symbols you add it's basically like log K okay which is very small oh the fact that the minimum distance is 3 it's not awesome but it's better than nothing as we saw before this implies that you can at least in principle always correct up to 1 that's a 3-1 over 2 floor I guess corruption or error and actually there's something like a little bit extra cute about the Hamming code in terms of its error correction abilities let's imagine that you indeed have a received word that got that experienced exactly one corruption okay so let's imagine if Y really is a code word well one thing we know is that H times y is the old zero Specter but imagine H is corrupted in one place all right y plus e times P sub I so this is the vector that just has a 1 in the 8th position okay so this really means that like the ice bit was flipped now let's look what happens if we take think of this as our received word y plus e I and we just multiply it by the parity check matrix H this matrix over here Wow hy plus h e I this is 0 because Y is a code word and what happens we multiply this H by E I on the right it just picks out but I call eh and what is the ice column of H it's actually the number I written in base 2 so it's I and base 2 this is actually quite slick if you know that your received word is guaranteed to receive at up to one bit flip either 0 bit flips or 1 bit flip you take a received word Z and multiply it by the parity check matrix if you get the all 0 string and you're like great this was an uncorrupted codeword otherwise you get like a binary string that tells you exactly where the bit flip occurred ok and then you can decode by just flipping that bit back okay so that's cute another cute thing about this code the hamming code is it's what's called the perfect code basically there not too many perfect code families what this means is you know we know here that the minimum distance is 3 and therefore if you put like a Hamming ball of radius 1 around each codeword and they'll all be disjoint for the phrase perfect code refers to the fact that not only they disjoint the exactly partition all of space ok so like every single string is either a code word or is that distance one from a unique code word so in some sense it's uh optimally packing code words into your space if you want this minimum distance to be 3 ok so this is like the optimal rate for a distance 3 code okay so ah that's a cool code the Hamming code it's got amazing rate but really has terrible distance in some times because it only lets you correct one error its minimum distances three so let's now see an example of a code with just the opposite properties like amazing distance and terrible rates and then eventually we'll see some codes where both properties are good so the dual of the Hamming code that we just saw it's sort of by definition the one that has this H as its generator matrix that's this H from the previous board as the generator matrix okay and what that means is to encode a string X you know you multiply it by this matrix called H okay so you see this actually gonna have quite bad rate a lot of redundancy because this is like a length R vector and it's getting multiplied by this matrix that has almost two to the our columns in fact for technical simplicity let's actually do something a little bit dumb let's insert the all zeros column in here okay you wouldn't actually do this in practice it's stupid I mean it means that every code word will start with 0 which is pointless there's no point in having every code words start with zero you may as well just delete that symbol but it'll just make things like a little bit technically nicer if we do this and ultimately not a very big deal ok so with this like tiny little modification we get a code called the Hadamard code okay so except for this like very minor Philip of adding this column of zeros here the dual of the Hamming code is called Hadamard code and what is the encoding map for the Hadamard code well in some sense it's right here but let's think about it in a slightly different way the encoding map takes X and it outputs X dot a for all strings a a right so you take X and then you literally you know dot product it with all possible binary strings of length R which we're calling a here and you output those two to the r dot products okay and that's the hamming code sorry Hadamard code okay so it's definitely as I said gonna have terrible rate because it stretches a string of length R to straight length string of length 2 to the R but it's going to actually have excellent minimum distance let's also just rewrite this in another equivalent way for a string X of length R let's define LX to be a linear transformation by paying arvit strings into a single bit basically by this thought product so LX is going to map a string a 2x 1 a 1 plus da dot plus X our a our ok this is all mod 2 and this is a linear polynomial over f2 okay so this is where we're first gonna connect ourselves with the topic from last time polynomials by linear I mean degree 1 here and it's very funny and stressful because the coefficients are called X and the variables are called a ok so here I know it's terrible but like the X's are the coefficients I mean for a given fixed X L of X is a polynomial in the A's it's a linear polynomial in the A's the axes are the coefficients and the A's are the variables or determinants in determinants I know that's kind of confusing but that's the way it goes and so another way to think of the encoding map for the Hadamard code is just that X maps to like sort of the truth table or the set of all possible evaluations of this degree one polynomial L of X given X L of X is the degree one polynomial whose coefficients are the bits of X and the high tomorrow code encodes X by just listing out all of X's values in the natural order over all possible vectors a it's an N variant or are variate polynomial any questions about this okay great okay so let's see the properties of this code okay so um so how do bard is and to to the our our and now I'll tell you the distance it's two to the R minus 1 sub two code okay so like N equals swear my heart's kind of look like ends two to the R K equals R and the distance is like and over to okay so that's great the minimum distance is like half of all the bits and I'll just say three proofs of this fact but the minimum distance is n over two so the first proof falls well say all the proofs just in words so actually a reason that this is called the Hadamard code is if you just list all the code words not the generator matrix but you literally write down all code words in a row there's a two to the R of them and they're all of length 2 to the R so you get like a list of n vectors of length N and the matrix that you thereby get is exactly the Hadamard matrix that we studied in the you know analysis of boolean functions lecture or rather is be exactly the Hadamard matrix if you use plus or minus 1 for the bits instead of 0 1 okay and that's actually a function of this definition basically the rows and columns are indexed by binary vectors and like the entry is basically the dot product between the row and the column index but with plus or minus one notation and therefore if you think about this matrix with plus or minus ones in it that's like just a list of all the code words to say that the minimum distance is n over two in fact I should add that not only is the minimum distance over at n over two all pairs of code words are a distance exactly n over two so to say that all pairs of code words aren't distance exactly n over two is to say that in this matrix n by n matrix like all strings agreed exactly half the places and different disagree and half the places which in plus or minus one notation is like saying their dot product over the reals is zero because when they're the same you can't plus one and when they're different you count minus one so one way to show that all pairs of code words are at distance n over two is to recall the fact that the Hadamard matrix H sub 2 to the R is unitary or its columns are orthogonal a different way to prove it is with if you recall from last time the Schwarz simple lemma so last time we talked about the Schwarz lemma says that if you have two oh you got padded nonzero polynomial of degree D over f2 then the fraction of inputs that make it output zero is so the fraction make once I make it out non output non zero is at least two to the minus D so in our scenario if you have two distinct code words they generate or they're associated to two distinct linear polynomials and so the difference of these two linear polynomials is another linear polynomial and assuming the two x's or the two strings are different this is a nonzero linear polynomial and so it has degree 1 so Schwartzel implies that for a random input this difference polynomial outputs a nonzero value at least half the time and that's the same is to say that at least half the symbols are non zero and yeah finally you can just think about this short simple proof in like a simple way I mean if this distant difference polynomial looks like X 1 a 1 plus dot dot plus X are AR and the coefficients X 1 through X are not all 0 then a particular there's at least some X I which is not 0 okay now you pick the A's at random which is like picking a random position in the codeword pick all the non a is at random they'll give you some value X I is 1 by virtue of not being 0 okay and then you've a 50/50 chance that AI changes the final answer to 1 or 0 okay so okay maybe we didn't need three proofs in the same fact but this is a justification for why the minimum distance in fact every intra code word distance is exactly and over 2 in fact there's a generalization of this hedemark code you can think about it as an exercise to larger field sizes and it gives you a code or and it's cute to the AR k is still our and the minimum distance is even better it's 1 minus 1 over Q times n okay the rate is still terrible because you're mapping a string of length R to a string that's exponentially longer but the rate approaches 1 as q gets bigger\", metadata={'source': 'Koc3UxeC838'}),\n",
       " Document(page_content=\"oh uh how do our code has amazing minimum distance a terrible rate in fact it's also known to be optimal in a sense that for a binary code if your minimum distance is like and not just n times 1/2 but n times 1/2 plus epsilon then the code can only contain constantly many code words or the constant depends on epsilon ok so that's really like the ultimate terrible rate to only have constantly many code words in your growing block length space okay so okay now we're gonna see a code that has great rate and great distance but it's still gonna have a bad thing and that bad thing is going to be the alphabet size the alphabet size is going to be really huge nevertheless uh these codes which are called the reed-solomon codes I really like the ultimate like great codes in practice so these were developed in the 50s by Irving reading guff Solomon or maybe it's gusts read and Irving Solomon I forget and they're really used a lot in practice so like they're used for storing information on DVDs they're used in like the QR codes those like squares with the weird like blocks that you point your phone at these are all based on reed-solomon codes and these great Solomon codes are basically like a generalization of Hadamard code but to higher degree polynomials so in fact two univariate higher degree polynomials so this was like our variate degree one polynomials reed-solomon codes are going to be about univariate higher degree polynomials okay so let's define reed-solomon codes and they're a family of codes so one nice thing is that you can have any choice of K as a function of n the downside as I said is that they're going to require that the alphabet size is bigger than n which is maybe a little bit weird for example if Q is 256 like maybe sending bytes that means your transmitted words can have it most 256 bytes in them just a little bit odd but okay and finally they're also defined by a subset s of the fields of size and okay so in order for this to be possible this is why Q needs to be at least n the way reed-solomon codes are going to work is this the encoding map which maps vectors of length K the vectors of length n it's gonna work like this you're given a message and this is K symbols long where these symbols are field elements okay and we're going to think of the message as defining a polynomial namely the univariate polynomial whose coefficients are these symbols okay so let me put over here where P of M it's the polynomial in X the univariate polynomial that looks like M 0 plus M 1 X to the 1 plus the thought plus M K minus 1 X to the K minus 1 and the encoding is just going to do the same thing that the Hannah Martin code did namely it's going to output the truth table of this polynomial if you will or at least it's going to output all the evaluations on the set s so it's going to output this polynomial apply to all fuel elements in the set s in some fixed order okay so it's just take your message interpret its symbols as the coefficients of a polynomial and then output that polynomials value on a bunch of points and like usually Q equals N and s is just everything so here's some remarks usually Q is just N and s it's just everything a whole field of size Q okay then really it's just like an maps to the whole truth table if you will of p.m. oh that's one remark another remark is that this is a linear code seems funny because it's I don't know maybe you're like oh it's not a linear polynomial but can somebody say why this is a linear code yep ah that's a good point yeah you can write down the generator matrix you can write down a generator matrix which is just like a matrix multiply that you can do to generate the code I'll remind you of what that is in a second but another way to see it is that to be a linear code the sum of two code words should also be a code word and that's true because like the vector sum of two truth tables is also the truth table of the sum of the polynomials okay so actually if you have two messages m and M prime then PM + PM prime as polynomials is the same as PM + PM prime so the truth tables add when you add two polynomials or add two sets of coefficients but this is a way to see if this columns are generator matrix sorry the generator matrix is a vandermonde matrix which means that the columns of it look like like a ^ 0 a ^ 1 up through 8 ^ K minus 1 for all a and you're set s okay so you know what the message length k we know about the block length and actually so far there's no real relationship between them except that end should be at least okay you know about the alphabet size sadly it's quite large it's typically n what about the minimum distance fact is that the middle of distance are Reed Solomon code is real great so the distance of the resolving code is at least and minus K minus 1 in fact we won't prove this but it's exactly equal to this okay that's really great because I mean you know if K is n over 2 let's say so your mapping vectors of length n over 2 it's a vectors of length n that's excellent rate and then the minimum distance will also be like n over 2 which is like an excellent minimum distance you can correct like n over 4 errors for example the only downside is you know n has to be smaller than the alphabet size so the alphabet size has to be really big so let's see the proof of this well since it's a linear code we know that the distance D is the minimum Hamming weight of a nonzero code word okay well first of all how could you have a non zero code word it means the truth table of your polynomial is all zeros uh so that's going to mean actually that the coefficients are all zeros when a particular this means that [Music] okay so you're looking at the Hamming weight of a nonzero code word that means you're looking at all the places you're counting the number of places where the polynomial P sub M is nonzero so that's n minus the number of places where the polynomial is zero okay so this is n minus the maximum number of zeros of a non zero code word okay but these code words are like the the truth tables of a polynomial of degree K minus one so by this you know important degree mantra we saw last time that says if you have a nonzero polynomial of degree at most K minus one it has at most K minus one zeros this quantity this maximum number of zeros is at most K minus one it's a degree mantra okay that's the end of the proof that the distance is at least n minus K minus one okay so in summary Solomon code is an N K n minus k plus 1 Q code and actually this rate distance trade-off is the optimal possible trade off assuming Q is a at least n okay so if you're willing to have like a really big alphabet size then this is the largest I mean it's the best possible trade-off you can get between rate and minimum distance and that's the consequence of what's called the singleton bound which a very trivial bound that says exactly this for any nkd code not even necessarily linear K has to be at most n minus D plus 1 okay which is equivalent to saying that D is at most n minus k plus 1 okay and the proof of this is super simple but maybe I'll skip it for time but it's literally like a one-line proof that uses the pigeonhole principle ok so as I said uh you know it's a great code if you're willing to large alphabet size for example you can take K to be n over 2 and then your rate is 1/2 and the minimum distance is also like 1/2 times n which is terrific\", metadata={'source': '_nVKsdaNsss'}),\n",
       " Document(page_content=\"okay so uh finally the last thing I want to get to is like sort of this potential best of all possible worlds which is a code that has you know good brain good minimum distance and you know let's say a fixed alphabet size such as to our favorite alphabet size yes let me expand a little bit on what I mean by a good rate and good minimum distance uh what you really want is that's just one fixed code with like one fixed value of n like 300 but you want a family of codes parameterised by n so for a fixed Q let's let curly C be a family of code C n be a sequence of and k d-q codes we can think about letting K depend on N and D depend on okay so for example with the Hamming code it was an N minus log base 2 + +13 besides - if it's hard code it was an log in and over to slice - and for the reed-solomon code it doesn't really fit into this picture because Q is not fixed but I'll write this anyways it's an k + - k plus 1 q but this is not fixed because this has to be at least n so it doesn't really count but one thing we want to look at is called the asymptotic rate all this code family ROC it's just the limit as n goes to infinity of K over N K by K and strictly speaking you should put like the limb in for something if you're a math nerd but just let's call it the limit and also you have the asymptotic relative distance so relative distance just refers to distance over N so like the fraction fractional minimum distance that's written Delta of C which is the limit and goes to infinity this minimum distance over N okay and these are the two quantities that we want to be large so you see that like the asymptotic rate of the Hamming code is amazing it's won by the asymptotic minimum distance is horrible it's zero and for the height of our code it's the reverse the asymptotic rate of the Hadamard code is horrible it's zero its asymptotic minimum distance is amazing it's a half okay so like a absolute constant is amazing and for the resultant code again it doesn't really count because it has a growing alphabet size but you can actually make this asymptotic rate K over N whatever you want some are zero and what you'll get is that the asymptotic minimum wealth of distance is 1 minus R 0 okay so you can make these any two constants that add up to 1 like for example 1/2 1/2 and you know the basic first dream in coding theory theory of error correcting codes is what's called asymptotically good code family they gave such a funny like plain name to it a good code family and what this means is just you know a family of codes with you know Q fixed and are the code being at least some positive constants and the Delta also being at least some positive constant okay so just like a code with rate at least point one and then a relative minimum distance at least point one yeah so you can achieve this for any R 0 because the resolvement code you can actually there's no fixed dependence between N and K like given any end you could take whatever K you want so you can take it to be like R 0 times n okay so you know a question that was like open and they're like sixties is do these even exist can you have a binary code you know where the rate is a constant so like when you encode a message of length K you blow up the length by only a constant factor and where the minimum relative minimum distance is a constant so that like you can correct up to some constant fraction of bit corruptions and the answer is yes these things exist so the first proofs of the fact that these codes exists were merely existential and they were not like practical at all so there's something called the gilbert varshanov bound okay and this gilbert varshanov bound says that for all Q and for all you know fixed numbers Delta zero let go up to 1 minus 1 over Q you can think in your head of Q to be a to if you want there exists a code family see with relative minimum distance equal to the still to 0 and rate equal to 1 minus the Q Airy entropy of Delta zero okay so if Q is 2 this is the binary entropy function remember that P log 1 over P plus Q log 1 over Q that we saw in the context of combinatorics of the binomial coefficients and in fact this number in general is the number such that the number of strings let's say number of qre strings in radius Delta 0 and Hamming ball is basically Q to the H Q Delta 0 of lamb times n ok so it's a generalization of the fact that the volume of a Hamming ball of radius Delta n over binary strings is 2 to the entropy of Delta x n okay Gilbert show that these code exists by just choosing them by like a greedy argument just taking coder balls as because you can wash mobs show that a random linear code has this property is but neither of these things is really satisfactory because you want like a code that's actually has efficient encoding and decoding you don't want just merely that these things exist okay so the last thing I'll say and you can see some more details about this in the notes is that actually a officially league encode a bowl and decodable asymptotically good codes exist this was first proved by juicing in 1972 it'll just say it for Q equals two okay so for binary codes it exists Polly end time and countable and decodable asymptotically good code families and they're pretty good rate and just in straight off so for any rate you want R 0 have Delta which is something like 0.1 1 times 1 minus the rate okay so for any constant rate you can get some constant minimum distance and some constant fraction of errors that you can correct from okay just one word what is the idea you actually take the to encode something in this juice in code you take a string X and first you encode it with a reed-solomon code and that gives you a like a long symbol a string of big symbols of size N and then you encode each symbol with some binary codes okay this idea of like encoding to get a bunch of symbols and then encoding each symbol by a binary code it's called code concatenation okay that's a powerful tool in air cracking code theory yeah but this is like a good fact in you know one fact to know about error correcting codes and we'll see some examples of it in the next lecture about D randomization\", metadata={'source': 'QEd5weCZU0w'}),\n",
       " Document(page_content=\"okay hello today's topic is going to be D randomization and a little bit of the theory of pseudo randomness a couple of reasons while we're talking about it right now one is well to be frank it's a nice application of error correcting codes a lot of the tools used in D randomization come from error correcting codes and also not too long ago we were talking about computational models and we're talking about randomized algorithms versus deterministic algorithms versus quantum algorithms and one thing I mentioned back then is there a number of problems like compute of function problems that can be solved seemingly somewhat more efficiently by randomized algorithms than deterministic algorithms for example testing of a numbers crime has a quadratic time randomized algorithm and the best deterministic algorithm we know is order n to the sixth time something like that but we don't know any like really dramatic speed ups where something can be solved in polynomial time randomly but seems to require exponential time deterministically and why should I see a little bit today why that's true there's a kind of good reasons for from complexity theory to think that all randomized algorithms can be D randomized fairly efficiently now actually one thing I want to stress is that in practice there's really absolutely nothing wrong with having a randomized algorithm if you have like a randomized algorithm that solves your problem well that's great I mean no problem with it we have okay when you you know use a pseudo-random number generator in C or Python or whatever it's doing some ad hoc thing but didn't practice it like works fine in practice there's no real trouble with getting random bits but um still it's quite interesting to philosophize about the question you know is randomness really necessary for efficient computation or not and a lot of the questions that come up when thinking about the question of D randomizing algorithms are of quite interesting theoretical interest in complexity theory so that's another reason to talk about do u randomization okay so let's talk a little bit about the theory of D randomization and pseudo randomness and one thing I'll start out with is like a little schematic picture for what a randomized algorithm is so for a randomized algorithm I want you think about like this we have some algorithm a and let's assume it's solving just a decision problem for simplicity which means that it's just outputting yes or no give it an input or you know 1 or 0 and for randomized socre than one way you often think about it is ok it's get some input X and this X is somehow like you know the actual input for your problem you're trying to solve and sometimes you just think of a as a randomized algorithm that goes along it flips some coins to make some decisions as it's working but let's think of a slightly different setup where we think of the algorithm is also accepting as random string R okay so it doesn't sort of flip coins in advance it just imagine as it gets like a bunch of random bits and it can use those random bits in addition to the actual input to make its decision okay so these are equivalent models it'll be a little bit better to think about it like this okay and these are sometimes colloquially called coin flips okay so randomized all groan gets the real employed and gets them like random bits that it can use if it wants and it tries to solve the problem in the output yes or no giving the correct answer and the standard definition in algorithmic complexity theory is that will say this algorithm a make it a little curvy a you know solves the problem and then in complexity theory they have this a complexity class called BPP which means decision problem solvable in polynomial time using randomness if okay two things hold one a should be an efficient algorithm okay which means polynomial time and furthermore each of the property that it gets the correct answer with high probability on every input okay so what I mean here is for all you know real input strings X the probability over R that a when given X and random bits are gives the correct answer it's at least some number noticeably bigger than 1/2 like 3/4 okay so in this model of you know randomized algorithm solving the problem it's not like there's a probability distribution over X or anything the algorithm does the right thing for every X it just you know gives the correct answer with high probability let's not think too special about 3/4 if you repeat your algorithm you know 3040 times you can make this probability of the majority answer being wrong extremely small and it only increases the running time by a constant factor okay so like you know a good example to keep in mind is the problems testing of a number is a prime if you've ever seen like the milliradian algorithm you know X represents the actual number to test whether or not it's prime the algorithm does some random steps and it has the property that it gives you the correct answer about the prime ality or composite miss of X with high probability yep yeah you know 40% of people will write 3/4 here and 40% will write 2/3 here and 20% well I don't know write something else here maybe 1 minus Epsilon but for every fixed number for every fixed constant between 1/2 and 1 it doesn't change the definition of this bpp class it's a good point ok so this is our randomized algorithm and since we're talking about deep randomization you know what we're going to talk about is the possibility of taking an algorithm like this that you know requires random bits for its function and either getting rid of completely or minimizing the number of random bits the algorithm uses hopefully without increasing the running time very much ok so let's actually assume that this is a little bit funny but let's assume that the number of bits the random algorithm uses the length of R is n okay we'll call n the number of random bits used you can also think of X as having length n so one way you could inefficiently be randomized this algorithm if you're really determined to be a hundred percent deterministic it's just take this out randomized algorithm that solves the problem and try every possible random string for it of which there are two to the N so this is definitely going to be quite inefficient but what you'll know is that if you try all possible 2 to the N strings either you know 3/4 times 2 to the end of them will make the algorithm output yes or 3/4 times 2 to the N of them will make the algorithm output no and whichever one you know is the the majority answer is the correct answer but you don't know like in what order you'll see the the the answer is yes or no so this algorithm really will take you at least you know 2 to the N time times the time it takes to simulate a so that's all very good but I mean at least it shows that in principle you can dear an demise the algorithm if you're willing to spend a lot of time so a typical idea in the theory of D randomization in pseudo randomness is to just try to run the algorithm not with truly random bits with pseudo random bits and in practice is what you do you put in an out or some like pseudo random bits maybe you like I have some pseudo-random function and you give it a seed that's like I don't the clock time or something to do with how the user type on the keyboard and then it generates a stream of not truly random its but pseudo random bits and you just plug them in for R and hope that this guarantee is still more or less true so we want to have something you know similar but actually with provable guarantees behind it and actually we are going to use this model or idea it's kind of essential that we are not going to totally eliminate typically we don't want to totally eliminate the use of random bits what we're gonna hope for is that we'll have what's called a pseudo-random generator and we'll take a short string of truly random bits and expand it to a long string of pretty good random bit okay and the idea is that hopefully this will allow us to greatly minimize the number of truly random bits to get this kind of guarantee which is already good start if you want to get rid of random bits and then maybe that number of Chuli random bits will be so small that you won't even mind in numerating overall exponentially many seeds okay so let me write some things down to make this more concrete and in particular I'm going to give you the sort of complexity theory definition of a pseudo-random generator generator definition what's made in like 1982 by Andy Yau and separately by Macaulay and Blum so there's the definition and it's got some components that won't be so clear but I'll explain them as I go along so let's see script C be a class of boolean functions f mapping n bits to one bit and we sometimes call these the tests for the pseudo-random generator now a mapping gg4 generator that takes a string of length L and that's going to be the quote-unquote seed to a string of length and this is going to be like the pseudo-random bits that we try to plug in to an algorithm is called an epsilon pseudo-random generator and logically but somehow psychologically disturbingly the abbreviation for pseudo-random generator is PRG foresee of seed length L if the following holds true for all functions F in the class these all quote tests in the class let me just write down this expression the probability when you choose a random seed called s of length L and you expand it with the pseudo-random generator into a length end string and you plug that into F and you look at the probability that F outputs one and you compare this to the probability that F would output one on a truly random string are of length n well this should be close numbers and in particular the definition is satisfied if they're close to within epsilon okay so the idea here well okay I'll write a few more things in a second but the idea here is that see thing cover is like a class of some kind of statistical tests F and this class may be of more or less sophistication the sort of taking a random string and try to like guess if it's random or not or say if it's random or not 0 or 1 and G it's like doing a good job as a generator with epsilon small if this property holds you know no function F in the class can really tell the difference probabilistically between whether it's getting a truly random string of length N or it's getting a pseudo random string drawn by taking a truly random short seed of length L and plugging it into the generator G so this is the idea a couple more things one is sometimes this whole definition is abbreviated with the phrase G epsilon fools C check out alike it's very uh punchy phrase and one should also add that typically maybe you want to put this in the definition or not but typically you also want G to be efficiently computable officially and also deterministically computable because the whole point is you know it's supposed to be a pseudo-random generator that you're you know trying to plug into your algorithm and see we want it itself to not take up too much time so the idea hopefully in this picture right is that if you're going to try to G randomize a with a pseudo-random generator G you'll take this seed of length L and plug it into you know G box and it'll output your R for you this is the pseudo-random generator any questions about this so indeed one thing one now asks is though what classes of functions or tests might be we interested in trying to fool and once we decide on that do there exist pseudo-random generators that fool them with you know small epsilon and small L uh for example suppose we're being very we're trying to be very ambitious and D randomized all polynomial time algorithms so we can wait look at this picture and say like let's imagine a is any algorithm okay let's say it's running time is at most n to the ten so if an algorithm a runs in time at most n to the ten and you're evaluating it on inputs of like n then it can be converted into a circuit of size about n to the 10 maybe n to the 10 log N or something which does the same thing as a and so then in this definition the class of functions you might want to fool or all those computed by a circuit of size n to the 10 so let me write this down as well here's like a some example settings for a pseudo-random generator you might want to have so let's say see it's the class of all functions mapping n bit to 1 MIT computable by circuit school in circuits of size utmost n to the 10 I can just pick this 10 kind of arbitrarily of course every spunctional-- you to buy a circuit of exponential size but you know the class of functions computable by polynomial size circuits is the main interest in complexity theory and in particular if you think of ok so here it's a bit of a side note here you might think of a as taking two inputs X the true input and are the random string and I'd actually like to write this as a sub X of R ok just same thing but kind of thinking temporarily if X is fixed and R is the only input so a sub X of R you know basically is in this class see if a runs in time I know somewhat less than n to the tenth I don't know and to the 10 over log N or something so actually finishes add this for all X okay so for every input string X if you think of fixing input the true input to a and thinking of the random bits is like sort of the only input parameter left for a well if a runs in time something like n to the 10 then it's well known that it can also be computed by a circuit of size something like n to the 10 and now say we have a pseudo-random generator G that you know fools with epsilon being point two for fools this particular class C and say it does it with seed length L which is order log N and shortly I'll talk about whether it's this is a plausible assumption that there's pretty strong pseudo-random generator like this that fools all tests computable by n to the tenth sized circuits and stretches a log in length seed to a pseudo random string of length n but there's a say in a minute there's a reason to believe that such a G exists then this would be really great because then you could do this paradigm given any true input X you could say to yourself okay fixing X I can think about a sub X is only a function of random bits and I'll use this pseudo-random generator G over here and I can stretch a log n like string into a full length n pseudo random string I'll run a on the results and this inequality tells me that well I won't be able to necessarily say quite this but I'll be able to say it up to an additive 0.24 okay the output probability distribution of 0 or 1 this says of the output probability distribution of F now put whether it output 0 one changes by at most point to four when you replace the truly random bits with a pseudo-random bits and so that means if it was outputting 1 with probability 3/4 before then it's outputting one with probably at least 0.5 1 now and similarly if whose outputting 0 with probability 3/4 before it's outputting zero with probably at least 0.5 one with a pseudo-random algorithm and that's great because then you can just try all the seeds okay and if you try all the seeds deterministically pass them through run a then first of all there's only polynomially many seeds 2 to the L is polynomial n n here a runs in polynomial time so you'll overall have like a polynomial time algorithm and you'll know if G has these properties that when you try all the seeds a well either output one at least 51% of the time or allow up with zero at least 51% of the time and therefore just by taking the majority answer you'll get the correct answer so maybe I went a little bit fast but to summarize what I just said you can solve the problem that a is solving if this great su random generator exists you can solve a problem deterministically in polynomial time ok and generally the time would be two to the seed length times the time it takes you to run a any questions about that I didn't address why you might believe that such a G exists but I'll talk about that in a moment but if such a G exists does it make some sense\", metadata={'source': '3lCmIM3lH8Y'}),\n",
       " Document(page_content=\"uh okay and I will not try to convince you that or suggest that such a seed exists such a pseudo random generator exists and it's generally believed that such a pseudo-random generator does exist and in particular if you believe that the sat problem is super hard so you normally believe that P does not equal NP but you believe you know that SATA is really hard to solve like in some sense it takes exponential time to solve then theorems from the 80s and 90s imply that such a G exists which is quite interesting this is a paradigm invented in the 80s called hardness versus randomness and I'm not gonna get into very many details but the idea of this hardness versus randomness paradigm is a way to imagine making a pseudo-random generator G that looks like this that takes a short seed this somehow represents a string here may be of length 10 log in and tries to you know this generator tries to make a long string may be of length n may be the seed is called s may be this random pseudo-random string is called R this hard reversal Radames idea due to Nissan a Wigner sin is a way to try to imagine making such a pseudo-random generator G and the idea is you take a bunch of subsets of the input bits sorry the seed bits not necessarily contiguous but I've just drawn them is like ellipses here you take exactly and subsets of the seed and let me not exactly talk about which subsets they are we have to be a little bit clever about this and what you do is you take a really hard function like set and you apply it think of sad as a function Maps you know strings to bits you apply it to all of these subsets of the seed bit and that's how you get your n bits of output here and somehow the idea which is many kind of believable is if this is a truly random string and Sat it's like a really complicated function that's hard for a poem you'll time algorithm to understand then maybe these bits will look totally random to lowly polynomial time algorithm and this construction is actually efficient because this scene is so short all these sets have at most log n order log n bits so even if you take two you exponential time to computes at it's still only palling real time to compute all of these bits this is a vague idea but let me just summarize what was prude about this idea over the course of like five or ten years in the 90s a culminated a theorem due to in Palo Alto and weakness in the Apollo out some weakness in theorem it says the following suppose you have what you believe is a really hard to compute boolean function mapping strings into bits maybe you might think of H as like the Sat function and it has two properties and these are quite careful properties one you assume it's computable in like exponential time by Turing machines and you want this because you want this process of generating the bits here to be polynomial time but at the same time you you want it at the same time to say that it's actually a very challenging and function and the right thing here is to say that it's not computable in much less than 2 to the order end time and for our technical reasons you need it to not even be computable and this kind of self exponential time by circuits sized by circuits this is for all sufficiently large n in that case if such a hard function exists that's sort of exactly you know you can compute an exponential time but you really need exponential time even if you're a circuit then such a pseudo-random generator G exists and as a consequence you know B P P equals P which basically means every polynomial time randomized algorithm can be D randomized to a polynomial time deterministic algorithm so it doesn't prove that you know this is true but if you find this believable and many people find this believable because many people believe that Sat has this property then you're forced to conclude that D randomization is always possible so it's a strong assumption it's a stronger assumption than P equals NP it's stronger even than this assumption called the strong exponential time hypothesis but still believable any questions about this I mean I didn't prove anything but this is the idea yeah yeah that's a very perceptive question the question was like when I say not computable does that mean worst case not computable the answer is yes it merely means not computable in the worst case by such circuits and that may seem fishy in accordance with this picture because this picture is sort of a hardness for random inputs and Sat for example has the property that maybe most I don't know random instances are maybe unset and therefore if you really just use Sat here then maybe most of these bits will be 0 and then it's if that's true then for a random seed if that's true it's actually easy for a polynomial time algorithm to distinguish this from a truly random string because this will be mostly filled with zeros but this is part of the F part of Impala out so weakness in you know the first theorem so this effect showed required a stronger statement here that like circuits of exponential size couldn't even be slightly correlated with the function age but one thing they did was weaken this assumption to worst case hardness yep yeah it doesn't have to be sad there's only an example it's just any function that has these two properties so it should be in the complexity class called e but it should not be a computable by circuits of size two to the epsilon and for some constant epsilon yeah yes so this thing called the set strong exponential time hypothesis is similar to this but maybe it's not about hardness or circus just hardness for against Turing machines the answer is yes you can prove some weaker statements your randomization statements but they're kind of complicated to state and maybe even interpret but yeah okay so in some sense this actually kind of solves D randomization mostly if you were willing to believe this strong complexity theoretic assumption okay so proving something like this is even harder than proving NP does not equal P so people don't really plan on proving this assumption anytime soon so one thing that people have certain to do in the field of pseudo randomness and D randomization is they said okay well let's try to achieve D randomization for specific polynomial time algorithms that we care about but unconditionally you know I'm not gonna try to do randomized every algorithm i'll try to do you randomize some particular algorithm that I care about I'm gonna try to make it from randomize to deter mystic and it's maybe some an ad hoc method or by using one of these things I talked about on the earlier board some D randomization techniques and we'll try to do it unconditionally and actually before I get into some of these specific techniques I want to mention one unconditional result which is pretty general in complexity theory albeit it does not have kind of the best possible conclusion but it's it's a good result and you can sometimes use it if you care about D randomizing your algorithms so I've pre-written it here this is a result by no nice on from like 92 maybe and it's a pseudo-random generator which is pretty good at fooling not polynomial time algorithms but space bounded algorithms so if you're interested in fooling all you know get developing a good suit random generator which is good against all algorithms that use s of n space rather than some time out Nissan gives you a pseudo-random generator where the seed length L is s times log n and this is less important but the epsilon parameter is really great it's exponentially small in s so even for like them maybe the least allowed s log n space it's already polynomial e small that's wonderful and the seed length is OK the dream seed length is to always you know make this order log n because then you can enumerate over all possible seeds in polynomial time and it's it's real great ok so as an example use of this theorem if s of n is order log n ok then epsilon is really great it's one of our poly n which means that substituting the pseudo-random bits for the truly random bits hardly makes a difference to your algorithm it'll change the output this probabilities by only one of our poly n the seed length is uh sorry seems like this order log squared n in particular that means you can you know D randomized in time exponential in this which is like n to the order log n ok this is often called quasi-polynomial time it's not polynomial time but it's a lot better than the naive algorithm of trying all possible in you know random strings which would take to to the end time so it was a very interesting like generic algorithm that is completely unconditional there's no assumption and it sort of quasi polynomial time D randomizes algorithms that use very little space it's sometimes you can use the ideas of this thing to this generator to actually get polynomial time D randomizations for certain low space algorithms and I won't prove this either but there's actually two known proofs of this theorem at least two non proofs one of them uses a tool called pairwise independence which is exactly what I'll talk about next and the other proof due to Impala got so nice on and we derson uses a tool called expander graphs which I'll talk about I don't know four lectures from now\", metadata={'source': 'YoYFZoJeD7c'}),\n",
       " Document(page_content=\"okay any questions at this point okay so what I want to do in the rest of the lecture is talk about two major tools used for unconditional D randomizations of your favorite algorithm and these two tools are pairwise independence K wise independence and the other one is called epsilon bias generators and there's other generic tools you can try to use I mentioned on the first board which I raced a tool called method of conditional expectations there are other methods you can use to try to do randomized your favorite randomized algorithm but we'll talk about these two okay wise independence and epsilon bias generators okay so KY is independent it's actually eluded it to it slightly in like lecture four when we're talking about Chernoff bounds and the second moment method chebyshev and fourth moment method and such things so we'll talk about it in a bit more detail now so what the way you can think about KY is independence this topic is it's a property that a pseudo-random generator might have in some sense it's like it's like a pseudo-random generator that fools the class of functions expressible by degree k polynomials but i won't quite phrase it like that and often this is enough to D randomize your favorites you know particular randomized algorithm you have at hand okay so I'll see you around generator mapping an elven seed to an n-bit pseudo-random string is let me start out with the simplest useful case which is called pairwise independence all pairwise or two wise independent if for all to distincts output positions I and J in your pseudorandom string and outputs if you focus in on any to output positions and you look you know when the seed is drawn at random as always the output of the pseudo-random generator in position I and the output of the pseudo-random generator in position J well it should be the case that this output is the pair 0 0 with probability 1/4 and it should be 0 1 with probability 1/4 and 1 0 with probability 1/4 and 1 1 with probability 1/4 ok so if you for every pair of indices in the output that you squint that when you pick the pseudo-random string it looks like the uniform distribution on two bits quarter quarter quarter quarter actually this is a very wrong name I had to write it because this is what people call it but it's a bad name it should be called pairwise uniform because pairwise independent merely suggests that these two bits are independent but the actual definition that people mean is not only they're independent but they're also 50-50 bits anyways people will say pairwise independent they really mean this that these two bits the iFit and the J's bed are both independent of each other and uniformly distributed and in general this is just the pairwise independence is a property of like a bunch of random variables I mean you call if you have a bunch of random variables you say they're pairwise independent if any two of them are independent and this is not the same as them all being mutually independent okay and more generally this pseudo-random generator is called ke wise independence and again it's a misnomer it should really be called Kayla's uniform but what can we do if for all K distinct output positions a similar thing happens namely the probability when you draw the seed at random that G of s i1 through G s position I K probably that this equals a fixed vector of bits V 1 through BK is equal to 2 to the minus K ok so all K output positions should look like they're coming from the uniform distribution and furthermore I won't write you know for the definition but there's an analogous definition if G is a generator with range Sigma ax to the N where Sigma is bigger than 2 okay I don't define this but sometimes you might be interested in pseudo-random generators that output n symbols that are not necessarily bits okay so in this case you could make the analogous definition that any K symbols and the output look independent in uniform so uh you know the idea is like often when you're analyzing your random algorithm and you have like n random bits that you're using you by assumption they're independent and you use that fact in your analysis and you prove some conclusion but often you don't really need that all the bits are independent if you squint that exactly how your analysis works maybe it only needs that every pair of bits is independent or maybe it only needs every four tuple of bits is independent and in this case you will be satisfied your your randomized algorithms analysis will still hold if the random bits in use are merely you know two eyes independent or four wise independent and the good thing is it's a known fact which I'll talk about that they're unconditionally exist pseudo-random generators that are two eyes independent or four wise or ten wives independent with really excellent seed length namely order log n seed length so let me just first I'll tell you the theorem then I'll give you an example application and then I'll talk about how the theorem is proved so this theorem is due to kind of due to low in the PI and Bob I and II tie and 85 ok says for all K that's the K NK wise independence and for all prime powers Q so you can see that maybe some algebra is going to come into this there exists a polynomial time deterministically computable K wise independent generator and it's seed length is actually with range yeah even with range Sigma to the end with seed length I'll just think of Sigma is 0 1 4 now L being K over 2 log n basically okay so first of all ignore that plus order one it's not a big deal also mainly think about queue the size of Sigma being too so it's outputting bits and don't even get too excited about this like K over 2 just think if this is like K log n okay I just state this because it's even better it's like K over 2 so for example Q equals 2 the main thing we care about see random generators outputting bits and with K like a constant like 2 or 4 or 10 L is order log n okay an order log n is like the seed length we like because it means you can then fully be randomized your algorithm in polynomial time just by trying all possible seeds so right so if you have a randomized algorithm and you like you're staring at it realize that it only requires the random coin flips it uses to be too wise independent or four wise independent or what have you then you can efficiently do randomized it so let's see an example of a situation in which that occurs and over this example kind of briefly but perhaps you've seen it before or at least you seen the example before so uh here's an example usage it's on the topic of factor 1/2 approximating maxcut I'll say what I mean by this okay so max cot is a well-known algorithms problem I hope you know you're given a graph G vertex set V and E and your task is to partition and the vertex set into two parts I'll call it R and R Bar and what you want to do is maximize the number of cut edges which I'll write as number of E from R to R bar just the number of edges that have one vertex in one part and the other vertex in the other part okay these are called the cut edges okay so class can be hard task and so given such a graph finding the maximum cut is and be hard we don't expect to do it but you might try to come up with an efficient algorithm that does a reasonable job of finding a pretty good cut and there's a very very classic simple super simple algorithm that does a kind of reasonable job of coming up with a big cut and it's the most naive algorithm of all pick a random partition and it's a fact that I guess we'll see you know it's easy to see that a random partition cuts half of the edges in expectation okay so if you just choose your partition totally at random you don't even really look at the graph then in expectation the number of edges you cut is half of all the edges is that good well it's better than nothing it's pretty good for a long time actually this was the best polynomial time algorithm for cutting as many edges as you could and let's even see the proof of this fact well let's you know say without loss of generality the vertex said is just called the numbers 1 through N and it's a randomized algorithm and so let's imagine it's random string as usual is called little R and this is randomly chosen to be an N bit string well now we're in a nice set up we have like a one random bit per vertex so we can use that to define our partition so we'll put vertex I into R if and only if you know the random coin flip RI is 1 and now let's just think about how many edges get cut by this algorithm so the number of cut edges is equal to well the sum over all edges IJ in the graph of let's say the indicator the 1 0 indicator that RI differs from RJ right this exactly says that in your random partition I invert X vertices I and J are on opposite parts of the partition ok so we can take the expectation on both sides and by linearity of expectation which holds for all random situations independent not independent it doesn't matter we get the expectation of this indicator the expectation of an indicator random variable is the probability of the event being indicated this is the probability over our R I does not equal RJ ok which is just the sum over Eddie edges i J of 1/2 R I and RJ at random it's the probability they are different from each other's 1/2 ok and therefore you get well the total number of edges in the graph over 2 which is what I said an expectation you half the edges well now let's take a look at this analysis do we really need that the n random bits the algorithm used were independent no we didn't need anything about them here that was linearity of expectation and then the only thing we did was to assert that this random string r had the property that for any two fixed bits Rin RJ the probability that they're different is 1/2 and unfortunately I've erased it but this is definitely true provided ours the orbits are pairwise independent that's almost exactly what it says it says that well all the four possibilities here are probably a quarter so in particular the probability they're different is 1/2 so you can directly see that this whole analysis holds up exactly the same if the random string R is merely pairwise independent or two wise independent ok and now this theorem assures us that you know we can take K to be 2 and Q to be 2 and we can you know the by the theorem can replace you know random are with the pseudo-random generator G that takes order log n bits and bits okay and the expected value of the number of cut edges when you use a random seed in the generator and this output string is still the number of edges over to which I suppose by the probabilistic method means there exists a seed such that when you use that particular seed the number of edges cut will be at least size of e over 2 and so if you just try all seeds and see how many edges get cut by each of these pseudo random generator outputs at least one of them will give you at least half the edges ok you can try all the seeds in polynomial time that make sense okay so that's an example of like just noticing that your algorithm really only needs pairwise independence let me give her like another sort of brief example well let me leave it at that as an example let me say yeah one thing that we talked about in lecture four is things that like you know often if you have a sum of random variables you're very interested in controlling the probability that they're far far from their expectation and we saw various tools for doing this like chebyshev's inequality and turn-offs inequality and so forth and one comment I made then which you can think about is that if you have a sum of random variables the variance of that sum it's expected square of the sum which if you expanded all out involves expectations of products of pairs of variables and therefore the variance of a sum of random variables is unchanged whether those variables are truly independent or merely pairwise independent which basically means as a rule since the variance is the only thing that goes into let's say chebyshev's inequality it basically means it's a rule if you're ever analyzing your algorithm using chebyshev's inequality typically you'll find that the analysis holds up exactly the same if your random bits are merely pairwise independent you can use this generator in that case or for like fancier things we saw the fourth moment method which sometimes gives better bounds and like that will hold up if your random variables are only four wise independent favorite Chernoff bounds for example typically need that that variables are truly independent but if you're using a weaker thing like Chernov then often pairwise is enough okay so uh let's see a little bit about how we would prove this theorem we'll start seeing connections to algebra and codes so let's think about this particular theorem in the case of Q equals 2 and K equals 2 which is the most basic and interesting case pairwise independent bits so consider the following pseudo-random generator G which Maps L bits to uh n bits where n is 2 to the L minus 1 so that's a great stretch is taking like log n bits to n bits and it works by taking the seed s and mapping it to well the row vector s times this matrix that we saw before that has like all the binary strings except for the all 0 string written as its columns and this is all over the field of size 2 so everything is mod 2 so this when you multiply it out gives you like a length and row vector R ok the maps L bits the 2 to the L minus 1 bits every possible binary string except the all 0 string and the claim is that this is too wise independent or pairwise independent and you know if this claim is true then we've proven the theorem in this case it's great because we've mapped you know log n bits to n bits its pairwise independent this is certainly efficiently computable it's you just write down this matrix and do the multiply so let's think about why it may be pairwise independent so what do we need to do we need to focus on all pairs I of J of output positions here and argue that when okay this matrix is fixed when s is chosen at random these will be two random bits and we just have to argue that you know we see the four possibilities with probability 1/4 each so what are these two random bits well it's just s times like the eye column dot the eyes column and s dot the J column ok and really all we're gonna use about the eyes and J's column here is that they're not the all zeroes column and they're distinct okay in fact we're going to use that that these two columns are linearly independent two linearly independent vectors although it's a very pathetic statement over f2 to say that two columns are linearly independent just means they're distinct and not 0 but I'm gonna leave this as an exercise for you if you do you know S dot some columns this like s3 in this picture if these are linearly independent and this is you know truly uniformly random then these will be these three bits will be uniformly random [Music] okay so this is not a very hard exercise I mean you can think of but in this particular case if you know you're just one column as long as it's the nonzero column if you dot a fixed nonzero column with a random vector you get a random bit now you have to think about like let's say if this is another distinct random vector u dot it with this truly a distinct fixed vector u dotted with this random vector not only will it be random but it'll be independent from this guy it follows from the fact that the difference of these two columns is not the zero all zeros column oh great so in some sense when we're proving this claim what do we really rely on I said that we really relied on all pairs of columns here are linearly independent and if you think back to the last lecture hopefully you remember that uh really the least number of linear dependent columns in a matrix is equal to the minimum distance of the code for which this is the parity check matrix I don't if you remember that fact from last time but really what we've relied on in this claim is that the the dual code okay this was the really the generator matrix basically for the Hadamard code the dual code which is Hamming code has minimum distance well three if you recall I'm pretty glair than two and actually this gives us this idea basically you know this fact that you know if you have linearly independent columns and these output bits or symbols will be uniformly random together with the characterization of like the the largest number of linearly independent sorry the characterization that the minimum distance of the dual code is like the size of the shortest set of linearly independent columns it gives us a general way to construct k wise independent generators okay so sort of a generalization of this exercise is that if you take X a vector of length L over a finite field Q and you map it to x times G a vector of length and this mapping is KY is independent according to this definition if and only if the the dual code GE dual has minimum distance okay greater than K okay and so if you know good codes then in some sense there duels give good you know KY is independent generators so sort of like a corollary and final exercise for you if you use reed-solomon codes you got pseudo-random generators letter KY is independent and have C blanks basically K log base Q of n may be plus order K here okay so really just using this fact and plugging in what you know about reed-solomon codes you basically get this theorem except you have K here instead of K over 2 but if you're like really enthusiastic about getting the best possible parameters then you can use the dual of the BCH codes which are a little bit better and that's what gives you K over 2 okay but this is good enough for almost all practical applications\", metadata={'source': 'X_KlKreI5d8'}),\n",
       " Document(page_content=\"okay so that's all I'm gonna say about Kay wise independence which is a good technique for tea randomizing some algorithms and now I'm going to move on to a different and very useful technique for Dee randomizing explicit outer limits are called epsilon bias generators and actually again as we'll see the theorem that says that you know epsilon bias generators with really short seed lengths exist is really the function of the existence of good codes okay so let me talk about this topic of epsilon bias generators so a generator pseudo-random generator mapping L bits to n bits is called an epsilon bias generator again epsilon is a parameter and you want it to be small if it well I can write it like this epsilon over 2 fools all the class of all linear functions on f22 then alright I'll also write the meaning of this explicitly in other words for all strings w of length N and W should not be the all zero string if you look at the output of the pseudo-random generator on a random seed s and you got it with W the probability that that equals 1 this is all mud too so it's either 1 or 0 it's very close to 1/2 it's between 1/2 minus epsilon over 2 and a half plus epsilon over two okay and just to explain this a bit more imagine you fix a vector of length and W which is not the all zeros vector and you dot it with a truly random string then the resulting bit is 50/50 zero or one that's an easy fact I guess it's like the one column case of this fact basically just pick any one in W there's got to be at least one because W is not the zero vector draw all the random bits other than the one where W has a one they'll contribute something to the dot product but this last place where W has a one gets dotted with yet another random bit so it's equally likely to make one or zero and secretly likely the final answer will be 1 or 0 ok so a pseudo-random generator G is said to be an epsilon bias generator if it almost has this property you know for every W the dot product that the pseudo-random generators output makes with W is almost 50/50 it's 50/50 up to an additive width of Epsilon ok and this is the property that again sometimes your randomized algorithm analysis will only use this fact and in that case if there's a good you know shorts to seed length pseudo-random generator that's epsilon biased you can just plug it in and numerate all the seeds and you've got to do randomization so again I'll give you the the theorem statement about these existing then I'll show you a little application and then I'll talk the last thing I'll do is talk about how you prove the theorem so the main theorem here is a do to know or and nor from 1990 and they showed that it's achievable the epsilon bias efficiently deterministically computable epsilon biased pseudo-random generators exist with seed length L being order log and over epsilon and that's great because I mean it means you can even set epsilon to be one over n to the 100 so that this is really close to 1/2 and then your seed length is order log in and you can enumerate all the seeds in polynomial time and you're happy in fact subsequently people even got excited about like nailing down this constant and there's a nice paper by alone gold Reiko sod and Peralta from 92 where they showed you can even make the constant 2 so they achieved seed length L equals 2 times log n over epsilon maybe plus order 1 and in particular you know if you exponentiate this it means that when you're doing your D randomization the number of seeds you need to try is like N squared over epsilon squared and that's even really pretty good the reason you care what the Clemson is like ok your enumerate over all the seeds one's like and yeah so this means that this enumeration only takes quadratic time yeah that's true but in fact actually notice if you allow yourself seed length and you can achieve the epsilon equals zero so I think it's consistent great and people are really happy about this and then actually quite recently there have been improvements including this latest almost perfect improvement due to Tosh ma using well some coding Theory techniques for a few years ago you got it down to basically and over epsilon squared seeds okay there's a plus little of one on the epsilon and it's basically optimal because it's known that n over epsilon squared is a lower bound okay and so if you think of absolute as a constant that's really great because this is linearly many seeds okay so at the end I'll say just a couple of words about how where these Epsilon Pi generators come from it's again from err correcting codes but let me mention an application and this application is maybe not a hundred percent is compelling is the last one because what I'm going to show you is taking a randomized algorithm that's quite fast and uses n random bits and I'll turn it into a randomized algorithm which is about as fast and uses log in random bits order log in random bits but if you're actually enumerated over all them it would make the algorithm kind of more slow than you wanted or at least it shows you can dramatically reduce the number of random bits you need from n to order log n so if you you know think of random bits as a precious resource then you've done an exponentially great job you know there are examples where you can actually use this to great effect to you know if you randomize your algorithm in a great amount of time but well I want to pick an example that doesn't take too long to explain so this application is - one of the very first pieces of randomness in efficient computing due to FRA volts from the 70s and it's the task of verifying matrix multiplication so you probably know that if you're multiplying two n by n matrices you can do it the trivial logarithm it takes cubic time order n cube time the standard algorithm but though these famous algorithms like strassens algorithm that runs in time like n to the 2.8 or something and there's like you know 40 years of sophisticated work that got this running time down and like the world record now it's like some end to the 2.3 Dhokla thought time algorithm for multiplying 2 n by n matrices these algorithms are extremely sophisticated so what I want you to imagine is that like your friend tells you oh I coded up that super sophisticated algorithm and I've got an end to the 2.3 time algorithm for multiplying matrices you're like wow that's great but like I'm very suspicious that you correctly implemented the algorithm because it's very complicated and this might motivate you to solve the matrix multiplication verification problem we are given not just a and B major season you want multiply them but you're given a and B and a purported C and you want to check that a times B equals C like C is the output of your friends suspicious algorithm so in the matrix multiplication verification problem the input is three matrices a B and C and for simplicity let's just assume that all this is happening mod 2 that's actually not really a big deal and that's the input and then the the question is to decide hey is it true that a times B equals C now okay in principle first of all you can solve this problem yourself just by multiplying a and B yourself and checking to see if the answer is C so you can do that by the standard algorithm in and cube time well it's sort of defeating the purpose if you know your friends are going supposed to be really fast you could also yourself try to implement this like super efficient but super complicated algorithm but in fact you can solve this verification problem in quadratic time which is cool because it's you know faster than any matrix multiplication time algorithm we know so it's faster than just actually multiplying a and B and checking if it equals C only catches it uses randomness so this is what fry walls came up with in the 70s and here's the idea for the randomized algorithm for this it's very obvious idea I mean in retrospect you choose Y a random vector of length n uniformly at random and then you check if a be y equals C Y this is a length n vector this is a length n vector now if you're not smart about this this might take you a while but maybe cubic time but if you are smart about it you can do it in quadratic time because this is a n by n matrix times a length n vector so that you just do the naive algorithm and it's order N squared time and this one of course you don't do it by first multiplying a and B and then x and y you first do B times y and this is order N squared time and it gives you a vector and now you do a times that vector so it's another matrix vector multiply so the whole thing is quadratic time okay so you can do this whole check in quadratic time and notice actually it's really linear time in the size of the input so this is actually super efficient in the size of the input and the analysis I'm going to show you in a moment is that this is an effective algorithm randomized algorithm in the sense that well if a times B really equals C this check will always pass and if it a times B doesn't equal C this check will actually notice with probability 1/2 which is good because then you can repeat the check with 20 different wise and boost that false positive rate of 1/2 down to one in a million ok so let's see the analysis of this algorithm sort of the randomized correctness well first of all of course if a B really does equal C then the probability over Y that you get equality here is of course 100% so that's great i Milligan what happens if a B does not equal C ok and maybe a B and C a B and C differ only in like one entry what's still not going to be a problem so let D be the matrix a B minus C mod 2 and by definition this is not the zero matrix because a B does not equal C so since it's not the zero matrix it has at least one nonzero row ok so maybe this is d and maybe this particular row little D this is not the all zeros row great so when you multiply ok so now let's consider let's think in our minds about D times y which you can see is pretty relevant for analyzing this well I'm pretty good at one of the entries of the vector D time capital D times y is little D dot y ok and as we've discussed if d is a non zero little D is a non zero vector and you dot it with a uniformly random vector and the probability that you get one or not zero is exactly 1/2 ok and what that means is when little DIY is not zero this event implies that the matrix capital D times y is not to all zeros vector by definition means that a be y minus CY is not all zeros vector which means that well ABY well not equal CY okay and the check will fail you see this or whatever a B differs from C this test will detect that difference with probability 1/2 and as I said if you don't you're not satisfied with that just repeat the test like with 20 different Y's and you can boost your success probability to a very great value okay so this algorithm this randomized algorithm uses order N squared time and it uses well N or order n if you repeat it a few times but and random bits but now as I said imagine you think of random bits is a very precious resource well this fact right here is really all the whole analysis depended on and so if Y is the output [Applause] you know like epsilon equals point one biased pseudo-random generator and this uh probability no matter what D is this probability might not be a half but you know it'll still be at least 1/2 minus 1/2 times point one forty five percent okay that's still good it means you know with this epsilon biased Y your probability of detecting a B being different from C is at least 45 percent so maybe you repeat it 21 times and you still are very happy ok in a particular if you use this particular AE GHP thing actually it's not even gonna help you that much if you use the Tosh MA thing so just use the a GHP pseudo-random generator epsilon is the universal constant point one then you can still solve the problem in order and squared time no I should not only mention that order N squared is the number of seeds but you can compute the generator in order N squared time and you only need log n random bits really it's a bit expensive if you actually want to do the test itself takes order N squared times so you repeat it for all the seeds that time kind of builds up what so we'll just leave it as doing it you know one time or twenty one times and you know get order log in random bits in still quadratic time any questions about this okay cool so in the last five minutes I'll just show how the problem of getting an epsilon biased suit around regenerator is basically the same problem as getting a good binary code and then I'll again say while these good binary codes exists and there's a few more details about how to get them in the notes but we're not time for going over it but it's like a combination of reed-solomon codes and how to mark codes concatenated together solves the problem so let's see just the connection to coding theory and be done so let's say let's say G is an epsilon biased generator and what I'm going to do is show you how from it you can get like a code with some interesting properties so I want you to do given this epsilon biased generator define matrix m which is okay the parameters here a little confusing it's like an N by 2 to the L matrix where L is the seed length for G and n is the output length through G as usual and I want you to make this matrix M like this okay it's the height and matrix and I want you to put in the first column G applied to the seed that's all zeros in the second column I want have a G applied to the seed that's all is you know zero is except for a 1 and I want you to do this for all the strings so the last column is G applied to the string of L ones okay so all the pseudo-random outputs of G are plunked down as columns that's the matrix M and ah good and now imagine multiplying this matrix on the left by a nonzero vector W sorry I'm gonna switch to maybe this board actually let me switch to just over here Oh so my definition of the fact that you know G is Epsilon biased what do we know we know that like if W is not the all zeros vector if u dot it with all the columns then about half the time you'll get a zero and about half the time you'll get a 1 or about means up to Epsilon all right so literally by the definition we know that for all W and F 2 to the N which are not the all zeros vector if you choose a random column which is like choosing a seed uniformly at random we know okay that the probability that WGS equals 1 is between 1/2 minus epsilon over 2 and 1/2 plus epsilon over 2 which if you think about it means that sorry I sort of said W times M this vector matrix multiply on the right has between well 1/2 minus epsilon over 2 and 1/2 plus epsilon over 2 fraction of ones in it which actually means that like it's Hamming weight is at least well it's a fractional Hamming weight let's just say a fraction of ones in WM is at least 1/2 minus epsilon over 2 and also at most 1/2 plus epsilon over 2 but if you just forget about this at most 1/2 plus epsilon over 2 here this is literally exactly saying that M is the generator matrix of a linear code whose minimum relative distance is 1/2 minus epsilon over 2 a binary code minimum fractional distance at least 1/2 minus epsilon over 2 and at most 1/2 plus epsilon over 2 and it's also not hard to check that like having a good trade-off between L and M is equivalent to having a good trade-off in your code between the message length and the block lengths so having a good rate so it's like you just need to design a code a binary code with good rate and excellent minimum distance 1/2 minus epsilon over 2 plus this other funny property like no two code words should be farther apart than 1/2 plus epsilon over 2 fractionally but actually 99% of the ways it turns out that you design a linear code if it has minimum distance 1/2 minus epsilon over 2 it'll also have maximum distance at most 1/2 plus epsilon over 2 so indeed you take like a really decent binary code with good rate good rate and excellent minimum distance and it's basically automatically equivalent to an epsilon by a pseudo-random generator and that's exactly how you get this these theorems about this existence okay so for all the details you can take a look at the notes but I'll just end there\", metadata={'source': 'B4rQdlvi6XU'}),\n",
       " Document(page_content=\"all right let's do it let's talk about cs theory i love it um how's it going y'all do you have any questions you can start vonska if you want i don't oh um i guess okay uh i was mostly stuck on two b and three i was just looking at the piazza post that was for tv i was saying to consider using the analysis of boolean functions and i don't see the relevance because building functions just go from uh embed binary strings just one bit so i'm not really sure how we could use those here um [Music] all right yeah um let's see any thoughts on this problem well we can just try an example um so okay this is about you have the um hadamard code so with uh some block length n so let's let's do an example what's what's a good choice of n for an example that we can do no that's probably pretty good n equals four i guess that's like r equals two so uh do you wanna remind me of like what's what's up with this code like what's going on with it or anyone can jump in yeah so we have like all of the our bit binary strings as their columns yeah so that's a 0 0 0 1 and 0 1 1. i guess this is like the generator matrix yeah okay so great so we can just write it all out now so like the messages i guess the messages could go here so i can't write today messages so one message is zero zero and then if i multiply it against this matrix i'll get out i suppose i'll get out zero zero zero zero another matrix message is zero one so if i multiply that i'll get like zero one zero 1 [Music] multiply 1 0 then that gets encoded as 0 0 1 1 and if i send the message 1 1 then it gets encoded by zero and i have to do some calculation and add the two rows 0 one one zero okay great so well okay it's a bit of a small example that's fine we can work with this so there's four code words and what's the idea of what we're doing here trying to show that for any received word which will be of length four there are not too many code words that are kind of close to that string hmm okay let's see if that seems true uh so we could take a code word that's like well it's gonna be a pretty small example but we can just do our best zero zero zero zero i guess like a distance one while distance zero we have one code word oh sorry this distance zero and that's this one well it's kind of nice at distance one we don't have any more additional code words but there's still one and then i guess at distance two we start to get a lot of code words they're in fact they're all at distance i mean these ones are at distance two two two that's like all of them um [Music] well i guess you know okay so uh half n is kind of the boundary here so in our case where n is four half n is two so luckily this like one minus epsilon is you know a little bit less than one so this kind of saying that okay i guess i'm hamming sorry how to mark code um it might be the case that there's like lots of code words from a distance um n over two from a given string but maybe it's like a little bit less than that there aren't so many so we arguably saw an illustration of that phenomenon here um we could do n equals eight um should we do it i guess why not um so that is eight like r is three so let's see we're gonna write down all the strings of length three here see if i can do it in a nice order one zero i guess i can do it like this zero one and this goes one one zero zero one one and that's like one one one one one okay and then the message is gonna go here now we're gonna multiply we'll get out some length eight thing so we can just try it again let's see how it goes zero zero zero asset message and then that gives us the code word zero zero zero zero zero zero zero zero it's not lined up too well [Music] there's zero zero one i guess that gives us the bottom row zero one zero one zero one zero one you know it's actually like a bit slicker i think no that's fine uh zero one zero that gives us the middle row it's like zero zero one one zero zero one one we could do this one it's just the sum of the bottom two rows it's like zero one one zero zero one one zero all right i can do this zero zero zero wait i missed one oh i missed zero one one whoops did that mess things up wait no it's really messed up what happened here that's what i meant to write uh okay zero zero zero zero one one one one zero one we add these top two rows okay let me just see if i can well i better do it like this zero one zero one one zero one zero i'm almost done hang in there i got out of the top two rows like zero zero one one one one zero zero and adding all the rows okay like zero one one zero one zero zero one phew okay hopefully i did it right um yeah so now we can take a look do you have a favorite code word or a received word in mind i guess you can just choose one random let's do it let's do a random string of length eight no takers somebody give me give me a string of like eight oh there we go in the chat great zero one one one zero zero zero zero great so let's see what's the distance this one's here is three distance here is oh it's gonna be hard one okay i can't do this yeah well let's it's worked out poorly um yeah this is a one two three okay guess what we're ideally gonna find that like perhaps there are like many strings at distance four or more [Music] maybe not so many like a little bit less than four it's like three actually this is like a lot of work to write this down uh do you know like a more efficient way to like write down this table how do i know like what number should go here oh you're taking the first and last row and looking at the one two three the sixth column and adding those together yeah so what's a good way to number like okay in row i'm gonna pick a neutral name here in row a column b uh how can we figure out what goes here actually what's a good way to name the rows or like what are the rows kind of associated with what's a good name named rose i mean there's like eight eight rows we call them one through eight or you have a better suggestion i guess like you should name them based on the the binary number that they the binary strings they correspond to yeah why not so this will be an element the row indices will be like one zero one cubed in this case rs3 and how should we name the columns um well they're also yeah they're also elements of zero one to the right okay true okay uh three okay so now if we think about like the row a index a and the column name b as a string um what's in entry like you know a b of this table it's the dot product right uh yep i guess it's mod 2 a dot b you know mod or you know in f 2 3 if you want to sound a little bit fancy uh okay and now maybe we get like a formula so like i mean the thing we're interested in is like in comes z a received word um what length is z uh yeah it's eight how should we it's got eight i guess we were yeah we think of it as like rows uh sorry like a row vector how should we name its entries just one through eight or is there a better suggestion no suggestion any misunderstanding but isn't it just like zero into the eight zero one to the eight yeah because it's eight foot where are you talking about each like index or well uh i mean like um okay actually i put it this way let's say you want to write a computer program this is cs theory after all to find out like all the um distances you know i started doing this here it was like kind of tedious like this this was my z like the distance okay the distance of this from this is like one [Music] two three i don't know if it's coincidence it keeps being three um you know like uh i don't know figure out all the distances like like maybe i want to like write a function that's like you know all the distances and it takes in like a z and somehow it's like you know i want to like output a list of like distance of z to each code word you just like print it so okay how should we oh should we write this code and we can assume this is like a string of length two to the in this case three well we could say r you want to like i guess like xor d with each code word and then like okay i think i'm over complicated but then you like add up like the number of ones you have in that resulting string um okay i don't think that's really what you're doing uh all right so i got my pen already like what what should be the first line here i mean it's pseudocode um since we've accessed so we have access to the do we have access to this table i guess we could just make it yeah you should i guess you should just make it okay um and then you want to like i guess like okay so i guess one way you could do this you could like okay i don't think this is exactly what you mean but you could like go through each row in the table and um x or each word would see each row and the table uh okay so um great so uh how how shall i enumerate this well it's always a possible adaptis where a and b are length three binary strings sorry i was spaced out for something i think what you said was right but as i said i just spaced out let's do it again sorry it's all the possible a dot b's yes well we have to do a little bit carefully so the rows are indexed by what binary strings like three yeah what were we calling them a i suppose yeah okay so now uh what next um [Music] yes so we like that product with sorry i lost my train of thought um one second sure anybody else can jump into by the way if they have some good ideas so my intuition is that right now we're trying to be xor is that correct like i'm sure every what's every computer xor what's up well dexter dot product whatever you want to call it yeah uh not of every row of the eighth row which is the row we're currently oh right right right z with the eighth row so how can we do that oh [Music] yeah yeah which vectors i mean let's just do by hand you know we don't mean no need to get vector yeah go ahead i mean so so so we're not exclusively assuming we can just do x uh where we're doing the dot product we're like explicitly computing the dot product like for a like i xor with like zi and then the sum of all of those like isn't that what this is uh well a is standing for the index of the row so what are the entries in the the oh i see i see if the index is a right so it's a times a dot be right like whatever the column is um yes the columns are indexed by b's right so if you but maybe maybe my intuition is wrong here but like at least from what it seems like is that the the i j row or i j entry in this matrix can be computed by taking the dot product of a with b is that correct um that's sort of what we said here yeah yeah so a b entry of the matrix is a dot b mod 2. right so if you like sort of so essentially if you do a dot like if you if you do a dot b dot z or eight so b will give you yeah yeah sorry go ahead uh so if you do a dot b x sort with c that should give you your guess if i do this little done sorry i'm just gonna i guess if i do this then like the compiler will complain that b is not yet defined all right uh i guess you'd like sum over all the b's right thumb over all the b if you wanna like sum over all the b's b 0 1 to the r of which we sum over a dot b x a dot b okay go so this will be the entry and then xord with z with what oh wait not z maybe c you're like the b index of c ah very good you want the beast entry of z that's kind of what i was getting at yeah z b so yeah what's going on down here sorry go ahead oh i just feel like they should be able to write this in like a vectorized form using like some matrix vector i'm not sure that was what i was trying to figure out um probably but it's a good thing to um oh right excellent it's um i guess one thing i could say is like maybe it's also like a tip like uh you know in math they're very like um they like to index like vectors and things with the numbers one through n um but you should really willing to be more like um uh like a cooler programming language where you know you can index things by any set it's like you know it's like not a list or an array it's like an associative array or a dictionary or whatever i mean like like if you have like the adjacent this is a tangent but if you have the adjacency matrix of a graph um you know you can be like all right i'm going to like number my vertices 1 through n so that like you know i can talk about the n rows and n columns but you can also be like no whatever like i'm gonna like index my rows and columns by the names of the vertices and so i'll talk about you know like uh you know the entry and in a spot u comma v where like u and v are just vertices so it's like a nice touch and um it's looking up before i mean it's true here that z has length eight but it's also good to like name its entries or in its indices by well like the b's if you will like the the the strings of length it was three in our old case um so yeah so i mean this is a good uh star and so like here we're going through so now we can say like all right uh we're gonna we're gonna go through all the we can like say for b h b zero one to the r so yeah we have on one hand you can do like a dot b that's like the entry in this you know matrix here it's like the b entry of the eighth code word and yeah we're going to be like comparing it with like the beef entry of z and well yeah i mean i guess now we want to like count one if they're um [Music] different and uh zero if they're the same so you know i guess we'd be like if i can do this here one second i guess you'd be like um you know distance equals zero and then you know you could do like if this differs from this then like distance plus plus okay and then you know like print uh okay the distance of z from the eighth code word is well this distance variable okay cool and this way you know we could compute all the distances of the string z from the different code words does that make sense i mean we haven't really gotten okay so i mean we have to think about like the um the problem in the question in the problem which is i guess about like hoping that it's rarely the case that this variable gets too large i mean it's kind of a little bit like we're like okay if distance is bigger than half minus epsilon and then you know um well actually we're actually we wanted to rarely be too small what are we trying to show we want that there are like very few code words with small hamming distance maybe i was saying it wrong before we want it so that like it's rare that the distance is quite small so if the distance is less than this and i might print and we're hoping that like this very rarely prints uh i guess it prints it at most like one over epsilon squared times or something um okay good so let's see so sorry i had a um here is it like reasonable intuition to think about this in like the geometric picture where you have like code words and then you have like some epsilon so you say some like ball around the cold war each each code word and they were saying that essentially um there is a limited number of like potentially other code words within any epsilon ball of like any any point in that space like i mean this is high dimensional probably like a high dimensional hypercube but like at least as an intuitive picture um my my intuition is like saying something about like distances in like a high dimensional hypercube for instance and talking about minimum kind of distances is is that like a reasonable picture to have in my head uh yeah like if these green points are supposed to represent the like code words somehow in space and you're trying to show that for every not necessarily code word like z if you take this hamming ball of radius a half minus epsilon times n there are very few you know green dots inside there and we kind of know that like if you just like make this radius instead of this is like you know 0.49 and if you instead make it like .49 n you make it 0.5 and then it's possible that you could have like some z that has like tons and tons of code words inside this uh ball of radius point five n but it seems that for this code if you make it point four nine and then you'll have like almost very few code words so if it's 0.5 n wouldn't that like be when you have all the four words in them technically because like at least based on like what we're trying to prove it's one over epsilon squared so then epsilon is zero so whoa that's just an upper bound it's you're trying to show that like the number of code words in any ball of radius you know uh f minus epsilon n i can't remember maybe there's an over 2 here i can't remember uh is at must one over epsilon squared but i didn't ask you to show this like at least one reps on squared so i mean um yeah maybe arguably you could plug in zero but you'll just get that this is at most infinity which is true yeah um okay so now maybe we can like go back to the suggestion a little bit about maybe vectorizing this a little bit um actually i mean um [Music] let me ask you a question what kind of object is z i mean this is a function part of me i was kind of thinking as a function from um [Music] through the binary strings to one bit yeah very good exactly that's great so you can also think of z as a boolean function mapping r bit strings into one bit and uh good then i guess we could think of our code as like a a set of specific boolean functions where they all share this common property that they have like exactly one like exactly half of them mapped to zero and half of them up to one that's right uh for example like you know you know you're comparing like a given z like this it's kind of like the truth table of a three-bit boolean function turned on its side and yeah you know you're also comparing it with like this i mean that's one of the code words you're comparing it with so you know we could call this okay this is like a particular code word i'm not sure which one maybe this is code word you know i call this c sub zero one one it's like this particular code word this is also like a uh you could also think of it as a boolean function like you know like one code word uh c and we're computing the zero one product of those functions yes that's true or computing how the dot product does function well yeah the dot product over the reals like because it's not much too right like you're you're computing like the sort of i know you're saying like you know how similar or different these functions are yeah yeah what is this i mean okay so this is a particular string c zero one one it's a and now we're thinking of it as a function like what's the i mean what's the algorithm for computing this function first of all what name should i give to the argument here a oh no sorry e take i mean it doesn't really matter but let's go for b um yeah yeah we're doing for this particular one like what's what's the algorithm for computing i mean what is the beef entry of this uh truth table zero and one dot b that's like mod two yeah true true true um [Music] [Music] does this remind you of anything well let's uh let's say for now because perhaps we can uh say for now i mean yeah we have this particular boolean function we have some other mystery boolean function z and we're quite interested in how similar close or dissimilar whatever z's truth table is uh with well c zero one ones and and also the other like c zero zero zeros and uh you know c one one zeros and etc and i guess we want to show that for any function z there are very few c sub whatevers with very similar truth tables um let me pause it there for a minute i mean we spent a little bit of time on this problem we can come back to it but perhaps well maybe we can spend a a little time on another problem does uh someone want to ask about another problem or we can we can keep talking about this problem i had a quick question on one c one c okay yeah i don't think it should be too difficult but i haven't figured it out yet so um aha so these are about concatenated codes and one c is about showing that if they're both linear codes then that concatenation is also linear okay yeah so i guess i think i understand like the whole concatenation setup but at least the way i'm trying to do it is building some new generator matrix in terms of the others but i don't really know how to do that basically because there's a part of the setup that i don't know how to use like a matrix to represent the mapping basically okay um yeah let's see i mean one way you can try to show a code is linear is by you know sort of explicitly saying what a generator matrix for it is that would do the job sure let's try an example um let's try an example with some small numbers so i guess we have to pick a lot of things like um um n [Music] k and d [Music] and q and oh boy a little q oh so many letters wow intense okay uh i guess the ds doesn't matter so much for the point of point of view of this problem uh i guess q has to be q to the little k so i don't know should we choose some numbers here what should we choose it's just q to be 2. that's one of my top prime numbers so q is two and k how about k let's use a nice small number but like let's not make it two let's try three because i don't want to make everything two because then i'll get confused so that means q will be like eight and then and i guess i must be bigger than okay let's make it four and then what else do we need okay [Music] i guess it can be anything so i don't know let's make this k do you have a suggestion um well whatever let's just make this like five and this like seven let's see how that goes uh okay so let's see so we have like some outer code and it's going to be a linear code and it's going to map field elements of size it's going to make five elements of size eight into 7. so i guess it's going to have a generator matrix that's the easiest way to kind of think about it so it'll have like a matrix of height 5. it's kind of like g outer it'll be like width eight and um yeah i mean i guess these will be elements of field of size eight that's like a bit it's kind of annoying because like we don't have like some popular way to name uh the field elements of size eight although maybe you remember how do you like to name the elements of field of size eight or do you have a different favorite field that's like a oh in terms of the the polynomials of the smaller field i guess just over two yeah okay i guess so yeah i suppose that means we need like some irreducible degree uh eight so i guess yeah there would be like polynomials of degree three um over you know with binary coefficients like you know typical entry here i suppose will be like you know 1 plus x plus x cubed or something i don't know uh okay so there's like many entries here um okay good so then like you know if you okay actually let me just call them like a b c the what what's the eighth letter of the alphabet the h like oh that's confusing because okay anyway uh h so this will be like some let's say these are the field elements and like f eight uh just because i don't wanna write these polynomials so this will be like some you know a nice thing with like some field elements in it and then you know to encode you'll like take some list of field elements here of length five maybe it's like a c f f a and you'll multiply it against this and you'll have to remember like what is um how to do field arithmetic but you'll get out some like code word of like oh i keep writing a at the beginning c f c c g e a c okay uh good so then uh we're gonna kind of like re-encode each of these right like this is how it works yeah this is where i was stuck for like because in the actual construction right we need to like map each of those things to an expansion that we can use the the inner code on yes so what kind of this the inner code here well let's draw a picture for the inner code so that's why the q is two and n is four and k is three so like the inner code like g inner i guess it's all like on a three by four matrix and it's like prepared to like operate with bits right this is gonna have some zeros and ones in it okay it's only with four um and it like takes some like you know message here with like three bits and you multiply it and you'll get out some code word with like four bits uh yeah so we gotta we gotta paste these two things together um [Music] so yeah tell me your thoughts i mean yeah this is like kind of just where i'm stuck right because we need to somehow map each of those things to their i guess in this case binary representation and then apply the g inner matrix to each of them individually and then somehow this whole thing has to be just a single matrix ah yeah so i don't know how to get the expansions of those elements using like a matrix multiply i guess um i had clarification maybe correction here is it necessary to even like like come up with this matrix like as long as you show linearity that's sufficient right like as long as you show that like the two over like like basically addition of like the um addition works it's closer to addition than scale or multiplication but that's sufficient is that does that like hold or do you actually have to show a matrix uh yes i guess you don't have to show a matrix i mean really um a linear code well uh actually this is sort of poor notation actually i don't see what this footnote says but well i guess okay so the code is like identifying the code with the encoding map i guess that's fine um i mean i wrote this so i guess i'm criticizing myself here uh yeah but um it's true that like a linear code okay so yeah when you have a code like you can think of two ways like sometimes people think of it as just like the set of all code words and um sometimes it's also identified with the map that like sort of takes a message and says like what code word is associated to and they're kind of the same thing right like as you know if um you know there's 20 different messages and 20 different code words there's always going to exist some mapping that takes the 20 messages to the 20 code words and some i guess sometimes you care about that mapping and sometimes you don't care about it so much and then you could say that a linear code you could define like a linear code in a couple of different ways you could on one hand just say like forget about the encoding maps and everything like a linear code is one where the code words um form a linear subspace in the vector space sense yeah i was just under vector addition and we kind of know that that yeah in one second we kind of know that um if the code words have this property that they form a subspace then there is a way to like express them as like the range of some matrix multiply i mean i guess these are kind of like linear algebra facts that i sort of maybe took for granted when i was talking about it um but it is they are true and therefore you're right that like you could just uh argue basically that like all the code words in the concatenated code form a subspace and therefore well then you're like done it's like a linear code yeah like i was just thinking like a homomorphism between like vector spaces is another like is another linear operator to some extent like so like humvee w essentially if you have like b is your ink outer and w is your antenna then like kind of kind of composition of linear things is also linear just by properties of abstract properties of the algebra really yes now that said you have to make sure and this is getting on the original question too that things like um kind of type check so like here this is a map um this is a map you know like probably like a well we called it uh anc outer like a linear algebra would call it like phi or something but um i don't know we'll say it's like a i don't know l1 it's like a linear map or l outer it's mapping um let's say uh vectors of length five over this field f eight um oops into vectors of length eight kind of confusingly over this field of length eight and this here we have like a map the encoding map maybe it's l2 it's like a vector space map it's up or mapping like there's of length three uh over the field size two two vectors of length four or the field of size two um [Music] and what you said is sort of true about like i don't like maybe compositions of like linear maps or being linear maps but how do exactly this relate to the original question like how exactly do things like type check here like none of these four objects is the same so it's not quite like we're maybe just composing one linear map with another one so like what is the kind of thing that lets you kind of glue these two objects together i mean these two maps together or like what is the like object where you're like well this object in the upstairs world of outer is somehow the same thing as like an object in this downstairs world so that each element in there is um encoded using inner right so yeah so like each um element here this is the field element this is in f8 it's sort of you know this is also a size eight you know like it's eligible to be operated on um by l2 now there's a bit funny because f8 is not the same thing as f23 um but i use like a chronic product though or something with to like to get like a multi-linear map for each potential um like where each g outers like values ends up being some chronic product of the g in hers dude because it's multi-linear essentially it i mean not multi-linear it's it's like ends up being like a tensor kind of at least like that yeah yeah true so i'm like what are we eventually shooting for right this is supposedly going to be a code uh with message length you know the product of the case and blocking like the product of the ends like the symbols is going to be the small symbol size which is 2. the product of the case is 15 product of the ends is 28. so we're eventually kind of trying to like if we're going to like you know actually do this and make a generator matrix which kind of as you mentioned you don't you might be able to argue you don't have to like explicitly say the generator matrix but anyway uh this is going to be 15 by 28 matrix of bits and um wait did i get that right something is not checking out three four yeah 15 by 28 it's okay so maybe this is fine wait did i multiply wrong oh i wrote eight here but i wrote seven here that's what's confusing me i need to choose one let's choose seven let's not have too many things that are eight um so this should have like seven things in it um yeah so you're absolutely right there's something um maybe it's like not a coincidence that like this matrix over here it's like 15 by 28 and you know the row or the number of columns is like seven times four and the number of rows is like five times three um so indeed like it may be that if you're familiar with this kronecker product um which you need not be for this problem but if you're familiar with it you might you might think to yourself oh probably this this matrix is just like maybe the chronicler product of this matrix and this matrix but that also doesn't type check right because well when you chronicle product together two matrices you like you're multiplying elements and like you can't really multiply up an element from f2 by an element of f8 um [Music] so it's a little bit like well we a little bit had this like issue that this like f8 it's like here is not the same as this f2 to the three albeit there of size they're both of size eight um but uh well i mean maybe we can still do it i mean just see if i can imagine like let's try the simplest let me in order to try to do it like let's try to make like the simplest possible tiny thing so let me let's try again let's say we have like g outer is like um two by three the g outer and again i don't know why i'm gonna do the small like call the elements of this field for now like these letters a through h so i don't know maybe it's like d d a and like g inner like i don't know i'll make it like i don't know let's make it also two by three but now it's going to be like zero one zero zero okay one one zero okay so let me ask you know ask you like okay uh please encode for me um uh we're supposed to encode for like a so this is like little k is two big k is two so we're supposed to be encoding four bit messages let's think about how do we encode like this message message one zero one one how do we do it see yeah wait a minute i think this is impossible like i'm getting botched it up yeah uh what is the the message should just be of like length two right um are you right let me reflect upon what's going on here uh well should it since big k is is two right a k is two yeah so it should be like an a and a b or something like that right well i mean if we if the the question is to be believed i'm just staring at the question here oh you're starting with the other representation i see i mean we're supposed to like i chose both the ends in both the k like both ends to be three and both the k is to be two uh so then if this question is to be believed then this is four and this is six oh sorry this is nine so we should get like a code that maps four bit strings to nine bit strings yeah i was still thinking of it in terms of the uh well yeah without getting into into the details the the a and the b representation but i guess we are going to have to it might be easier to do with it like this so yeah so how do i mean how do we encode this one zero zero one zero one one oh assume you use the inner encoding first in the other encoding right but it's not clear so um i mean it's a good question because like i guess it makes us like really make sure we understand like what is the definition yeah uh so what is the what is the procedure here i mean don't i'm not like asking you to like directly like write down the um like we're struggling before with what is the what is the generator matrix i'm not saying like oh let's not immediately write that down but just let's do it any old way like explain how we're supposed to take this like string of length four and turn it into a string of length nine well from my setup the first thing i did was i turned this into like the a and the b representation uh okay so i assume that there's a more clever way to do it for this part thinking about it this way well let's just do anyway so what's what's the first step well so you can view uh an element in like little n times big n as bracket and the big end or like yeah i know what you're saying yeah but you're saying here like um uh you can view a list of like for me step one is to take the original message and turn it into the domain of uh what outer takes in okay so outer takes in like a pair of um say field element i'll call the bits bits and i'll call these like things from the field of size eight i'll call them field elements so yeah outer takes in a pair of field elements so where do we get this pair of field elements um i mean i guess when i was doing part a i just kind of said that we could view them as equivalent but now that i need to actually give a matrix for it it's not as clear you don't have to give a matrix for it but like what i mean view what is equivalent like the representation of the input in this form and how we're going to feed it into g outer okay well i mean we have four our message is like four bits so [Music] yeah indeed how do we get this fed into like g outer yeah i actually just don't know if someone has anybody else want to chip in any ideas oh intuitively at least like i don't i don't actually know how to do this but like we know that capital q is equal to q to the k right like little q to the k so it's two to the eight oh sorry two to the three so technically uh if we were able to get like three three like bits then those three bits would would correspond to like one uh one element in like f eight or something yeah so i mean this wants this wants uh this code like wants two field elements which is like so so my assumption is sorry yeah go ahead i was thinking like like my my like i don't know if this is like correct i don't actually know how to do this but like i think one zero one maps to like one field element and zero one one maps to another like just because like you need three bits somehow but i'm not sure like how you use how you like convert like basically um we need to like take the message and create two field elements from it and then encode those two field elements in g outer encode those two field elements in some way through yeah that's true so like from the message we need to get two uh field elements and each field is like from size eight so that's representable by three bits in some sense we need to get like six bits to feed into this um code and okay so in general this is k times k little k times big k this message width so i mean i guess i'd like actually suggest like grouping it into little k groups of size capital k or capital k groups of size little k one of those or the other it's a little confusing here because they're both two um but i guess something either way that like suggests like you gave it like this right so now we have um a couple of two-bit strings is there a natural thing we can do with two-bit strings yeah we can encode them with g inner so like in this case um we know that okay you can code this with gener you like multiply on the left by 1 0 and that will give you 0 0 1 and then 1 1 you'll like multiply on the left with g inner and you'll get out um 1 1 1. that'll kind of give us like this would give us like 0 zero one and then one one one okay so now we have our six bits so what was your next idea there's a reason we want to get like six bits right oh sorry i was muted um oh sorry yeah i was i was saying um yeah so the three i was just saying like there was you have two chunks of three bits each bit each chunk of three bits is like corresponds to some field element on f8 yeah exactly this is potentially uh yeah this is like somehow a field element and this is also some field element and in fact you know if you remember the last homework there's like some um this question you know remark is anyway if you're my glass homework there's even like kind of a natural way to think of like field element of size eight as like a three bit vector like a vector of length three over f two i think this was on the last homework at least if memory serves um yeah so in some sense uh these are field elements i don't know what names they are maybe this is like d and f or something you can say um but yeah and maybe perhaps you're saying all this on mute like now that you have the df yeah i guess there's like a natural thing to do with this right plug it into this guy d f you know then you'll have to remember how field arithmetic works but um uh that will give you out some three field elements e um f i don't know and um yeah if you recall i guess the product of the ends was nine so in the end this is supposed to be somehow equivalent to nine bits but i guess there's a natural way and that's happening this is like three chunks of things that all have a three-bit representation so i guess this is how the encoding goes at some level and yeah now just like i guess i suppose solve the problem of like oh why is this a linear code you have two choices i mean you can try to think about like why this whole description that we just worked out can somehow be shortcuted or compressed or expressed as like some giant it's not too giant um matrix multiplication that puts like one zero one one here and has some four by nine matrix here and gets out the nine bits whatever c e f corresponds to hypothetically here so just why there exists this matrix or as you were saying josh i mean you can also it's also valid to just like argue that the space of all or the set of all possible final nine-bit strings is a linear subspace which i suppose amounts to it being um closed under f2 edition basically um right so i guess i guess like at that point if you're doing the latter one then it's then the mean like it seems like the main non-linearity comes from essentially mapping these um chunks of these k chunks into field elements or or like there's like there's i mean i guess if you have a one-to-one mapping one-to-one correspondence between k chunks and the field elements like chunks of size um big k so it's like yeah chunks all sides like that three bits here and two elements then you can see that there's some equivalent starvation between them and if there is then then like they're basically the same thing so yeah i guess like this key intermediate point we got to here i mean i don't know if this could help was like this df if you will so we kind of got like from four bits to like six bits and then from six bits to nine bits so like the four to six was linear because of supports on the new property let's see the four to six was linear okay because basically the four you took these two bits and also these two bits and you multiplied them against this matrix so i guess like if you put this matrix here and then this matrix here again that's doing it right yes oh uh is it no wait a minute no uh maybe you have to do it this way there should be a way maybe you put it like this no uh maybe put it like this yeah um something like this right yeah yeah yeah perhaps like this like um zero zero one what was it one one zero zero zero this is like where this is like where chronic er it seemed like there was something to be chronic her product yes chronicler e yeah one might ask i mean i suppose okay so i post this matrix if you're since evidently you're familiar with them i guess okay if this is the correct thing to do to get from four to six bits then this matrix i guess is like zero zero one one one zero chronicler product exactly one zero zero one right right right yeah um i mean it's at least the right shape it'll give you out like some six bit string and then hopefully there's some six bit to nine bit string mapping and i guess again here like these six bits like it was crucial to think of them as like two blobs of size three and [Music] so yeah a little bit like the question is like yeah now can you get to this thing from i guess this uh sorry i don't know if you can even see what i'm scrolling like the object we have here is like this guy it's a two by three matrix of field elements so i guess it feels kind of like a two by nine matrix of bits in some sense and maybe here somehow we're looking for a six by nine matrix of bits but um so so i assume you can just like um if you if you have like these chunks can you not just like rename them like because there's a one-to-one correspondence between like three bit chunks and um field elements and you could just like i mean from the last homework we can represent every field element as a matrix essentially so you could just like say that like each one gets to be a matrix and then matrix multiplication is like like linear like essentially we know that this last step is linear because there's a one-to-one correspondence between um between like the bits and the um fill elements like would that not like be reasonable mathematically because you're saying like there's just like renaming office essentially the same thing yeah i think well you have to be a little bit careful like this is our outer code that we were supposed to try to like you know we're gonna take this thing which is like two chunks of three bits and we kind of in our mind are trying to think of this as like two field elements and we want to kind of map this to here and like multiply these against each other and that's going to give out three field elements which okay we're hoping or we can think of it as like nine bits i suppose the thing is that like you need like the okay to get this this thing here it's somehow supposed to come from like this thing thought of as a field element times a plus this thing thought of as a field element times okay let me not call this a um g and one needs to i guess somehow argue that there's like maybe like a matrix of bits that you can put here such that like when you multiply by these bits but you're like thinking them as a field element and you like multiply it against this matrix that'll give you out some bits and let if you're thinking of those as a field element like you want not to give the right answer but maybe this question yeah maybe this also could remind you of something like i mean we want we want like them somehow we're trying to argue that like the product of two field elements can be represented by some kind of matrix multiplication yeah that's the last one work yeah but that was the last time mark so i think like i mean similarly on the last homework i don't know if you were i think you were there when we were talking about it like there's a bit of a similar question there where like um think you were asked to show that sort of like the action of like multiplying by a field element is a linear map and again like you could solve the problem like one of two different ways you could be like oh i'm a very abstract algebra person to show that it's a linear map i simply have to show that like satisfies the axis of linearity like you know this map applied to the sum of two field elements is equivalent to the map applied to this one plus the map applied to this one or you could be like more concrete and be like okay in order to show that like this map is linear i'll actually try to argue that there exists like um like a matrix that represents this action so that like multiplying field elements is like a matrix multiply and um and either of those are fine and i think you have to use at least either of those for this problem too well so that was my last i was just making sure that for this problem it is possible to construct the matrix because for the one in the last homework it was like i i was going down that rabbit hole trying to make the matrix but not to be like almost i mean at least i couldn't figure out how to do it that way i had to do it a different way um so but it seems like based on the stuff we've been talking about for the last 15 minutes that it is possible to actually make a matrix in this case that will work um although if you want like an absolutely easiest way yeah i guess you want to like argue that like okay if this is we're calling it a so you want to argue that like there should be something that represents like multiplying in uh f 8 by a such that you know if you put in like three bits here and that's supposed to stand for some field element like i know c and you do this matrix multiplication like uh 0 0 1 comes out and that this is supposed to stand for like you know the encoding of like c times a and um [Music] i mean it is true it's kind of implicit in the last one it's true there exists such a matrix of bits and you need not can you not assume like i mean so so maybe maybe my intuition is strong my intuition is like any dalva field can be like like like this like fa can be represented using a communicative matrix kind of um can you you can express any like element in like uh in like a field like this a finite field using a matrix and then all the operations on the matrix kind of just do the same thing as like you would do in the polynomial is that is that incorrect oh no this is true and it was kind of the point of that problem on the last homework i mean can we not just like use that directly or remember something um yeah you may uh yeah if you like want to cite the previous homework that's that's fine um i guess like yeah the way i mean you know that there's a way to i mean if you know like a little bit of linear algebra there's a way to know that this like matrix sort of exists without actually saying what it is basically by arguing that like it's a bit abstract but like that the action of multiplying by a is a linear operation sounds funny but operation on um well e on f2 cubed well in the sense that like you know if you have like this operation is a times something and like um if you have like alpha plus beta this is indeed like alpha times a plus beta times a and if you like a times i don't know c times alpha this is like c times alpha times a so i mean this is what was going on in the last problem as i said it's a bit abstract like it is a matrix sorry this operation of multiplying by a specific field element like a or b or g or whatever is a linear operation and therefore it must be representable by a matrix um i guess like on the last homework you could have sort of been more explicit and like sort of explaining exactly how you even identify this matrix i guess for this problem like it's not so important you just need to remind the ta is there in your solution that this exists but i guess like yeah you do need this property um [Music] i guess you need the fact that like it's not like you need that like okay when you finally come down to like representing you know representing you know f8 elements by you know three bit vectors i guess you do need that like the multiplication corresponds to um a matrix multiplier so that like you know like the the representation of like um x times well it's about i don't know x times a as a three bit vector is like equal to the representation of x which is like a three bit vector times i should have put them horizontally dot dot dot times some matrix m that depends on a just like a three by three matrix um see i guess you'll need that to like argue well in this problem um okay i guess we're a little bit over time uh any other very quick questions before we wrap it up uh what do you think this coding will be up because there are actually a couple parts that i'd like to rewatch especially regarding um 2b earlier they're like really in there and oh when the recording for this will be up do you say yeah yeah oh i don't know it takes like forever to like zoom to like encode it and then i gotta upload it to youtube so i can't say much for you beyond like sometime this evening okay sure but uh some of these people are online where we're there so you can you can ask them as well okay thanks for coming everybody i'll see you tomorrow\", metadata={'source': 'exIlIMINIys'}),\n",
       " Document(page_content=\"okay welcome to lecture 13 there's gonna be a first in three lectures or three or four lectures about spectral graph theory and I put here a nice resource for the topic is Dan steelman's newest book spectral an algebraic graph theory so a spectral graph theory is a really nice topic to study in the CS theory toolkit class and one reason for it is that its topic us equally beloved by complexity theorists and algorithmic theorists the two main camps in the area spectral graph theory intersects with both of these camps interests so it's a it's a nice central topic another nice thing about it as we'll see is it connects to a lot of interesting things not immediately related to spectral theory like expander graphs and sparse has got problem axes cut problem even the unique game's problem and other such things okay so in some sense spectral graph theory is about the study of graphs and it's gonna be only about for us the study of undirected graphs so we're gonna always assume we have an undirected graph G vertex set V and edge that e other extensions to undirected graphs but we won't talk about them in these lectures we will make some assumptions about this graph that we work with first of all we'll assume it's a finite graph there's also extensions of spectral graph theory in photographs but we'll stick with finite graphs we're going to actually allow multiple parallel edges and we're gonna also a lot of self loops so these are two annoyances that you know sometimes people like to disallow I think it's actually nice in spectral graph theory to allow both of these things we are going to disallow something we're going to disallow vertices of degree 0 ok isolated vertices as they're called I mean these are generally pretty pointless in a graph house a bunch of extra vertices not connected anything lying around and so or a deed we're gonna just not allow them it'll make things convenient ok and I should mention that when I'm mentioning degree of course let me just draw a quick example of a graph with five vertices the degree of vertex is just the number of edges that it touches somebody here's a example breath and maybe I have three parallel edges here and maybe also a self loop on this vertex so self loops generally count for one on a vertex so this particular vertex here with all the action going on has degree 1 2 3 4 ok now I can actually be dealing in these lectures with not necessarily regular graphs and I want to mention a key point which is this if this is the first or a second or third time you've been studying spectral graph theory it's really a very useful helpful suggestion simplifying suggestion to assume that the graph is regular this makes things a lot less complicated so let me say if you've never seen this before maybe assume that the graph is regular ok and meaning all the vertices have the same degree and this will definitely simplify your life so I'm gonna carry out everything in the case that it's not necessarily regular but as I said if it's the first few times you study the spectral graph theory just stick with the regular case in your head ok so yes I said you can generalize this to infographs or directed graphs or reversible Markov chains but okay we'll just with undirected graphs for the remaining area of these lectures okay so we're imagining we have a graph on our hands but more than this the main object we're gonna be studying is actually labelings of the graph vertices by real numbers so as I say the key or a key thing that we'll be studying is labelings of vertices v by real numbers and you may think of this and we will as actually functions mapping the vertex set V into the real numbers ok so this will be a central object of study for us given an undirected graph okay and these kind of objects functions or labelings arise naturally so I mean some examples might include maybe if the graph is modeling some physical object maybe F is the temperature of the vertex or maybe the voltage if it's some kind of electrical network or maybe if you're embedding these graphs into real space and perhaps F might be a coordinate in an embedding of this graph into r2 or r3 r Rd something like that F might be the zero one indicator of a subset of vertices this will be a very common example for us if we're studying a graph we'll have a subset s of vertices a hand and we'll let F be the function that assigns one to the vertices in this and zero to the vertices outside us and so forth so there are many examples of these kinds of functions mapping vertices to real numbers and not only will we think of them as vertices well also at the same time think of these functions as vectors in particular let's think of them as column vectors okay so our vectors will look like this and in spectral graph theory well we'll be associating vertices and matrices and things sorry vectors and matrices and things to graphs the coordinates or entries will always be indexed by the set of vertices so it's convenient if the vertices are called 1 through n but this is not necessary in general will be you know indexing let's say call them entries by vertices and then we can imagine F value on each of the vertices plugged into the column vector okay so given a function f you just arrange all of its values on the N vertices into a column vector and that's how we think of the function as a vector now as I said this spectral graph there is kind of a linear algebra type topic and we have vectors involved now and so of course it's good to point out that well we can do linear operations on these functions or vectors so we can add two functions point wise if we have two functions on the vertices F and G we can form the sum of them F plus G this is the point wise or vertex wise some of these functions and we can also multiply them by scalars okay so for any real number see you can look at C times F okay and therefore you know because we have these basic operations of scalar multiplication and addition we can think of the collection of all functions mapping the vertex set of a fixed graph into real numbers as a vector space a real vector space and of course that makes sense because well we also said we could think of these functions as vectors so naturally their vector space and it'll also be important to know what is the dimension of this vector space and of course the dimension of this vector space is n well in particular it's the number of vertices which we typically do know by n okay so that's how you give it a fixed graph G on n vertices you think about functions on the vertices or labeling to the vertices by real numbers and how you think of these as vectors of length and index by the the doctors in the graph okay and now what I'd like to tell you is what I view as the absolute key to spectral graph theory okay so I'll even write it in kind of big capital letters here key to spectral graph theory and what this is is actually a way of taking a function on the vertices and condensing it down to a key single real number non-negative real number and it's sort of the average a squared difference of F's values along edges okay so I'll write that in more math symbols so we're gonna look at the average or expectation obey a random edge in the graph so I'm gonna write this like this u tilde V and this means that what's going on here the probabilistic experiment is that we're choosing a uniformly random undirected edge UV in the graph so it's a little inconvenient here that yi stands for expectation and the edge set and I'll come back to this in a second but let me go on with the quantity here so picking a random edge UV and then we're gonna do given a function f on the vertices is look at f of U and we're gonna look at F of V together difference and which like what you have the variance of a random variable you can look at their difference we look at a non-negative quantity but it's annoying to look at absolute values so you instead as in variance square the quantity okay so this is going to be a key quantity for us the average over all edges UV of the squared difference of F value at U and s value at V okay actually for reasons of making subscripts formulas a little nicer we also put a factor of half in front and you'll actually see later why that's natural but don't worry about it too much for now if you decide not to put this factor it's fine but let's put the factor in and we need some notation for this quantity and we also need a name for this quantity and this is a really challenging topic no one has like a super great name for this quantity so I'll tell you my names and we'll settle on one of these possibilities as we go on let's start with notation so for the purposes of these lectures I'm gonna write this Kikki quantity as curly or a calligraphic e of F okay I think historically and mathematically uh the e here this curly easy is standing for energy because it's some kind of energy in the graph I think maybe if it really is the energy if the f voltages or something but I don't know anything about physics or electronics oh never mind about that right so as I said it's a difficult question what to call this very important quantity some names include the Dirac wave form with a graph or a name that not many people use although I like it is the local variance of the graph because in a sense as I said it looks very much like the variance expression for variance and in fact if you have the complete graph with self loops then this really would be the variance of the random variable f of u when you as a random vertex but this is some kind of like local version because you don't look at all pairs of F values you only look at once F values along edges one might also call this and we'll see why later the analytic boundary size of f that will make sense if I'll make more sense if we think of f is the zero one indicator of a subset of vertices maybe another common name is like the laplacian quadratic form this is some kind of quadratic form in the function values so with all these names maybe let's just call it the quadratic form for the graph perhaps that's the best thing to call it anyway it's a very important quantity and we'll spend a fair bit of time studying it just one more little comment here about this topic of choosing a uniformly random edge I've actually written u comma V here in round brackets as if the vertices are ordered now in an undirected graph we normally think of the edges as being unordered I mean I mean and it's reverse are always considered in there but what's actually very typically nice in spectral graph theory is to stick with undirected graphs for sure but to sort of in your mind think of each undirected edge as a pair of opposing directed edges so one going from u to B and we're going from V to u and it's really good to think about even though this experiment when you're choosing a random edge UV in the graph really think of it as like half the time you're choosing the directed edge that goes from u to V and half the time you're choosing the directed edge that goes from V to u ok it's not really important for this thing because in any case what we do with this quantity is symmetric between U and V we plug them into F and take the difference in square but this is something good to keep in mind ok so this is a quantity that it's gonna be great to study so let's learn some basic facts about it let me erase all this business about its name which is a little bit annoying we'll just call it the quadratic form for the graph ok so this is some kind of functional associated with the graph it takes in a function of the vertices and spits out a number so let's learn some facts about this quadratic form and these are facts that are actually going to remind you of facts about variance in probability theory okay so one basic fact is this quadratic form value is always non-negative and of course that's obvious because it's the average of some squares of things so square is always non-negative okay another thing to think about is how does this quantity scale if we do a scalar multiplication so if we take some function f and we multiply it by 3 multiply all the function values by 3 then you can kind of see from the expression that the quadratic form value multiplied by 9 in general the quadratic formula of C times f is C squared times the quadratic or value for F product property shared by variance in probability theory and one more example here or one more fact the quadratic form value for a function plus a constant okay so if you just take the same constant value 10 and you add it to F values on every vertex that actually does not change the quadratic form value at all because you know it just gets subtract in here and or so a plus 10 here and plus 10 here and these cancel out when you subtract okay so just like variance it's invariant to translation okay and um perhaps the key intuition to keep in mind for this quantity is that it's small in clothes this quadratic form value is small if F values are somehow smooth in that they don't vary much along edges its values they don't vary much watching quotes along edges okay so f might overall have somewhat widely differing values but if you know they don't change too much as you walk along edges this will be the situation where the quadratic form value is not that large okay good so let's do some concrete examples and perhaps the most important concrete example is one I alluded to before where we think of a as the indicator for a subset of vertices let's check out this key example so let's assume we have some subset of vertices s and we're gonna let F be I'll write it like this the indicator function for this subset of vertices okay so that just simply means that F on a vertex U is 1 if u is in the set and 0 if u is not in the set okay now let's compute the quadratic form for this function okay so in this case this quadratic form so what's the formula okay we have this perhaps annoying factor of 1/2 at the beginning okay times the expected value or the average value over a random edge UV of absolutely you minus F value of V squared so let's plug in F value at U is the indicator of s on you - the indicator function of s on V squared ok and what is this quantity well these two numbers here are both 1 or 0 so if they're the same value both one or both 0 then their difference is 0 the square is 0 so we don't count anything in hand if there are different values 1 is 1 and 1 is 0 well then the difference is plus or minus 1 but anyway the square is 1 so in the expectation you count 1 so really this is counting 1/2 times the expected value of a random edge of the indicator random variable about you and veer on opposite sides of the partition s and s compliment okay so this is sometimes written as the indicator for the amount that UV crosses the cut defined by s and it's complement okay so it's the probability you pick a random edge that it straddles being inside and outside s we have the expectation of an indicator this just means a probability that the event being indicated occurs this is 1/2 times the fraction of edges that are on I'll write it like this this thing means the boundary of s okay so this is all the edges then have one vertex inside s and one vertex outside of s okay we're looking at the fraction of all the edges which have this property that they're on the boundary times one half and just to draw a little picture here let's say our graph somehow looks like this this is our graph G and our subset of vertices s these vertices okay so G has some edges that are not on the boundary that are either inside or outside but then G has some edges that are on the boundary these yellow ones here are the ones that on the boundary so we're looking at a fraction of all edges that are on the boundary and now you might think man it's annoying that we have this annoying factor of 1/2 outside but actually the factor of 1/2 is quite nice and natural because again you should think of the undirected edges in the graph as being really a pair of directed edges one that goes from u to V and one that goes from V to u so half of the fraction of undirected edges on the boundary is kind of like you're counting just the directed edges going from inside to outside ok so that's actually nice again if you think of all undirected edges is two directed edges and we're looking at edges that go from inside to outside so another way to write this quantity is it's the probability if I choose a random and UV but I'm think of it as kind of directed but the directed edge from u to V is like stepping out of s if you will in other words that you as innocent V is outside s but the probability that if you take a random directed edge that you're stepping from being inside us to outside s and really this is something that well just generally this notion of the boundary sighs the edge boundary size of a set s in a graph is something that we care about a lot I mean algorithmically we care a lot about finding small cuts and graphs partitioning graphs and just sets of vertices so there's not a lot of edges crossing the partition finding communities etc in in social network graphs so this boundary size of a set of vertices s is very important and it's a special case of this quadratic form value when the functions you look at are restricted to zero one indicators of vertex sets\", metadata={'source': 'gwxuipf-9IQ'}),\n",
       " Document(page_content=\"okay great so let's continue so we have been talking about the quadratic form for a graph and it pretty good one thing this involves is choosing a random edge and looking at function value at the endpoints of the edge so we've been talking using a uniformly random edge we should probably also talking about choosing a random vertex - so should we just think about choosing vertices also uniformly at random no so this is actually one place where the fact that we're potentially working within a regular graph makes a difference if you're just working with a regular graph then when you're thinking about choosing a random vertex you should just choose a uniformly random vertex but if you don't have a regular graph then it's important to choose a special probability distribution on vertices which is what we'll talk about now okay so how to choose a random vertex let's think about this okay so I'm actually going to define the way to choose a random vertex in a slightly funny way I'm gonna define it as follows so here's a process for choosing a random vertex in the right way start by choosing a uniformly random edge well let's call it choose a uniformly random edge UV okay and then just forget about V just output you now actually as we'll see later you could also say forget about you and just output V would be the exact same thing okay so this is a funny way to draw a vertex at random and if you think about it for a moment it's sort of biased towards choosing vertices with very high degree because they touch the most edges let's call this distribution pi so pi is going to represent a probability distribution on vertices and if we think about it let's say for this graph here with five vertices and five edges this is an irregular graph notice and what are the probabilities with which each of the vertices is chosen under this apply for this graph well if you notice we'll choose a random one of the five edges it doesn't really matter too much which one and then we choose well say the first vertex in that edge so remember when you draw a uniformly random edge we think of it as being directed so if we choose a random edge process we choose this edge at random there's a 50-50 chance that we're doing the version that goes from the center vertex to the outside vertex and a 50/50 chance we're doing the directed edge that goes from the outside vertex to the center vertex ok what that means is under pie-pie of the center vertex is 1/2 and all the other vertices have the remaining probability and so each of them by symmetry has let's call it PI of a leaf is 1/10 okay so you can see this is definitely not the uniform distribution of vertices the central vertex actually has half the probability mass by virtue of the fact that it is a very high degree okay so pi is a probability distribution that is biased towards the vertices of high degree and in fact it's very easy to see this is almost immediate to compute that it's probability mass PI of a vertex U is directly proportional proportional to the degree okay so PI u is proportional to the degree and therefore we can exactly say what it is why u is in fact equal exactly to the degree of the vertex u divided by not the number of edges but twice the number of edges okay and again that's because this is a bit messy let me rewrite it really when we think of a drawing are uniformly random edge we think of drawing uniformly random directed edge and there are two times cardinality of directed edges okay and then the probability that its initial vertex the one we select here is U is precisely the degree of U okay so as I mentioned the way to think about this is that this gives a weight to each vertex which is somehow a measure of importance for the vertex and another key fact that we can see from let's say this formula here is that if it's a regular graph if G is regular than pi is just the uniform distribution okay so if G is regular then pi is just the uniform distribution on V okay that's one I mean the reason why it's nice the first few times you learn about this stuff to just imagine that the graph is regular and then the natural probability distribution have on vertices is just the uniform distribution okay there's one more fact that we need to kind of always keep in mind about this or to remember about this nice probability distribution PI on the vertices and that's the following suppose we do the following kind of experiment first we draw a random vertex u from this probability distribution PI which I've just finished defining and now having drawn you suppose we let V be a uniformly random neighbor of U okay and I'll actually typically write this as V tilde U and now we think of this as a way of choosing a pair of vertices U and V first drawing you from the special distribution PI and then taking a uniformly random neighbor from it the fact is that this process is equivalent to our old friend drawing a uniformly random edge UV you know it takes a little bit of thought to see why that's true maybe one way to uh it's one way to understand it is to look at this formula here so first you is chosen in this little experiment and the probability that you is chosen is degree of U times two divided by two times the number of edges and then conditioned on you what's a conditioned on getting to some vertex u we choose a uniformly random neighbor of U okay how many neighbors does it have it has degree of u many neighbors okay and so each you know each of them v1 v2 v3 here's chosen with equal probability and that equal probability is one over the degree of U so the probability that we'll choose U and V together is this quantity here the degree of U over 2 times the number of undirected edges in the graph times all right over here probably of choosing U and V together is this quantity times one over the degree of U okay and you see then these two things cancel out and we get the probability of choosing a particular directed edge u comma V together is in fact this quantity 1 over 2 times the norm undirected edges or 1 over the number of directed edges ok so we indeed see that the the combination of these two steps produces a uniformly random directed edge okay and so um this is showing another way in which this special probability distribution Payan vertices is sort of compatible with the uniform distribution on edges now if you think about it what this means is if we draw you from PI and we let V be a uniformly random neighbor then u comma V is a uniformly random directed edge and therefore if we were to do this and then output V well then V is can be thought of as being drawn by first choosing a directed edge and then outputting the terminus of that directed edge and as I mentioned before that's really the same thing as the probability distribution that defined PI and therefore the reason for that is in the definition of PI you know the probability of drawing the directed edge UV is the same as the probability of drawing the directed edge vu they're both equally likely so either endpoint of an undirected edge is equally likely to be the terminus or the initial point okay and that's why this probability distribution up here had we chosen V instead it would have been the same probability distribution PI well going back here we see that effectively doing this probability distribution to get gets it gives us a uniformly random edge and then if we output its terminus we get V so all of which is to say this is a key property V output by this this little procedure in this box is also distributed according to PI it's kind of messy I'll rewrite that a little bit okay so this actually leads us to thinking about what's called the standard random walk on our fixed under a gray directed graph G the sanno random walk on a fixed undirected graph G is simply the one where when you start at a vertex you choose one of its neighbors uniformly at random and you go to that neighbor and then you choose one of that vertices neighbors uniformly random and you go to that neighbor okay you keep going walking throughout the graph vertex vertex and always choosing a uniformly random neighbor of your current location as the place you go to next and one thing that we've just discovered is that if your initial point your initial vertex is not fixed but it's random and your initial random vertex is not let's say uniformly random but is drawn according to a special distribution pi then you have a very nice property if the initial vertex is drawn according to PI and you take one step the overall distribution of your second vertex is also pi that's the V here and therefore if you were to take another random step the overall distribution in the whole random experiment of your third vertex is also pi and if you do yet another step the overall distribution of your fourth vertex is also pi so let me summarize this a little bit as a corollary of all these discussions fix some number T a natural number sort of the number of steps you're gonna make in a random walk so in your mind think of T is like 17 or something and now draw an initial vertex u according to our special probability distribution pi which again for a regular graph is just the uniform distribution but in general it's proportional to the degree and now imagine you do the standard random walk as I described starting from you and going 40 steps O'Casey started a vertex you you stab randomly to around an Avery stab stab keep doing that and you step t times so I'll write it like this T and you look at the final vertex V that you end up at then in this overall experiment V's distribution is also PI and that's the reason that the special distribution PI is called the invariant distribution for the random walk on your graph or sometimes the stationary distribution I slightly prefer invariant though okay the invariant distribution for the standard random walk on the graph and it's so named because if you choose an initial vertex that's according to this distribution then for all subsequent steps of your walk the distribution of your position is invariant it's always PI you draw from PI you take a random walk of 17 steps you look at where you end up it's always the location where you'll end up as always has probability proportional to the probability of being any one place is proportional to the degree of that place okay so that's a beautiful property of this special distribution on vertices the invariant distribution but now let's look at another case maybe the slightly less beautiful case where you have some random ver sorry you have some vertex u 0 which is not random I didn't choose it from PI I chose it from some worst case distribution maybe my adversary chose u 0 and now I do a standard random walk from this vertex okay so maybe I chose some obscure vertex that has degree 1 to be my use 0 but then I start doing my random walk ok so let's say we now do a random walk starting from U of T steps I forgot to mention this a while ago but you can see now the reason why we disallowed vertices of degree zero because if you have a vertex of degree zero and you imagine starting there then there's no way to do a random walk from it and no longer make sense and another reason if you recall the well-known another reason but if you recall the definition of the stationary distribution it gives zero probability to isolated vertices because they have degree zero so it's kind of unnatural to consider them so that's why we don't consider them they're pretty cool you can't do a random walk starting from them okay so let's go back to the situation where we have an arbitrary initial vertex u zero we imagine doing a T step random walk from it where we choose a random neighbor at each step now we can ask what about the distribution of V here so we don't necessarily know it's the stationary distribution PI but you may imagine that if you walk for a really long time perhaps you will it end up start to end up or collect up with the vertices that have the higher degrees or in the case of a regular graph where all the vertices have the same degree you might imagine that after a long walk you're almost equally likely to be anywhere okay so the question we want to ask ourselves is you know as this parameter T the walk length goes to infinity does the distribution of V 10 in some sense to the invariant distribution pi now there's a couple of edge cases or kind of freak cases where this doesn't happen and you might imagine how it might not happen one way it could not happen is if G is a disconnected graph so let me imagine a picture of this let's say you have a graph here G and it happens to be disconnected so it has one connected component here another connected component here and let's imagine that nevertheless the graph is regular maybe every vertex has degree 3 but there's two parts to it and there's no edges between these two parts so then this invariant definition a distribution PI would be the uniform distribution but you can see of course if you start you zero in some you know arbitrary vertex here let's make this you zero and you start doing a random walk you'll always be on in this case the left part of the graph and therefore no matter how long you walk for there's no chance that your final position is close and distribution to the invariant distribution pi which is the uniform distribution you don't have a chance for B to be uniform on all the graph it'll actually probably end up being uniform on just this part of the graph in which you started but in general it has no chance to be uniform on the whole graph notice on the other hand though that if you zero the initial point is itself chosen from pi then it has a 50% chance if these are equal sized parts of being and starting on the left side a 50% chance doing the right side and so it is true actually in that case that your final vertex is equally likely to be any of the verses but yes in general if you have a disconnected graph this hope suggested by the question has a negative answer there's one more like annoying a case where this help fails to happen and that's if G is bipartite and in this case the hope sort of fails to happen for again a bit of a silly reason you know some kind of odd-even issue with T so if you have a like a bipartite graph G it has some vertices here on the left hand side some vertices here on the right hand side and a bipartite graph has the property that all the edges go back and forth between the two parts they'll go from the left vertex to all right vertex and again you can imagine the graph is regular if you want so that the invariant distribution is the uniform distribution on all vertices but now you can see let's say you start at some fixed vertex u 0 and maybe it's on the left well you see that if T is odd then your final vertex will always be on the right whereas if T is even your final vertex will always be on the left so it is true if T is a super large odd number that your final vertex will probably pretty close to uniformly random among the vertices on the right but that's not uniformly random on all vertices which is the stationary distribution pie and similarly if you know T is an enormous even number the final vertex will have a distribution which is probably gonna be close to uniformly distributed on the left vertices but that's not close to being uniformly distributed on all vertices so these are too annoying cases in which this thing you might imagine that doing a long random walk eventually leads you to the invariant distribution to cases where this fails but what's very nice is these are the only two things that can go wrong so let me finally say otherwise the answer to this question is yes as long as you have a connected graph and it's not bipartite then no matter where you start a random walk if you walk for a really long time and will start to get quantitative about this later but if you want for a really long time then in the limit the distribution of where your vertex is after a large T number of steps is exactly the invariant distribution okay so that's another nice property of the invariant distribution it's sort of the limiting distribution as long as your graph is connected and not bipartite okay but you know in the CS theory we're not just interested in the limit as T goes to infinity we always want to know quantitative bounds okay and so like the second question we might ask here is when the answer is yes you have a connected non bipartite graph how fast how many steps do you need how big should t be if you want to get epsilon close in some sense to the limiting invariant distribution pie and what's interesting and this is why we're gonna start to get into spectral graph theory and linear algebra and eigen values what's interesting is that these spectral eigenvalue considerations exactly in to enter into the picture when answering this question so the answer to this question really has to do with spectral or eigenvalue considerations and let me give you a taste of why that is so let's think of an example let's imagine you have a graph G and imagine that it's almost bipartite graph so it has a big blob of vertices oh sorry imagine that it's almost a disconnected graph I should have said disconnected so it has a big blob of vertices on the left and a big blob of vertices on the right and there's plenty of edges inside each of these blobs and but there's only very few edges that go between the blobs just a couple of edges that go between the blobs and imagine again that this graph happens to be regular so the limiting distribution is the uniform distribution just to keep things simple now this is a situation imagine the two blobs are of equal number of vertices as well this is a situation where if you call this set of vertices s the left blob one part of a partition or one part of a cut then these few edges that go from s to as a compliment well this is really a tiny cut as it said okay there's very few edges here on the boundary of s and intuitively that's gonna block the random walk from getting to the limiting uniform distribution on all vertices in a short amount of time you see if you started at some vertex on the U zero on the left-hand side and you walk around for a while you know you only have rare chances to get over to the other side s bar only when you happen to get to one of these you know in this picture or two vertices that has a chance of crossing to the other side can you get to the other side and maybe if the degree is 10 or something you might only have a 10% chance of even crossing to the other side even when you hit these vertices okay so somehow the the lack of edges on the boundary of s is going to be a barrier to this random walk mixing or getting to its limiting distribution quickly and of course if there are no edges on the boundary this is the disconnected case where I'll never get the invariant distribution and in the previous part of the lecture we talked about the fraction of edges on the boundary of a set s and we saw that it was exactly equal to this quadratic form script e applied to the function f which is the indicator of the subset s and it's a general phenomenon that if you have some set s where this quantity is quote-unquote small it all again be vague now and we'll make things more precise later then that's going to be a barrier to having fast mixing time or fast convergence to the limiting distribution and conversely what we're gonna eventually see by studying ultimately eigenvalues of the graph that fast convergence the property that the random walk from any starting point quickly gets through the stationary invariant distribution it's going to be equivalent to this quantity the quadratic form of F never being small okay this thing being not small for all F so I put all these things in quotation marks because I'm still being a bit vague about what does it mean small or large but as we'll see this is exactly going to characterize when you have facts mixed in a de graph the property that this quadratic form of F is never small\", metadata={'source': 'M8TGCBaebtg'}),\n",
       " Document(page_content=\"okay let's continue we were discussing our fact that if you are interested in how fast a random walk seen a random walk mixes on the graph then it seems you may need to study this important quantity the quadratic form for the graph and we had some quantities than quotes with you know said well we won't understand if this is small or large and what this means is we're gonna have to get into some actual calculational detail with this quantity and work out some some math so let's enter into some understanding of this quantity and we'll see that linear algebra really starts to enter the picture here okay so as I was saying before let's say we are studying function of the vertices F that labels each vertex by real number and let's say that we choose a random vertex U according to this special invariant or stationary distribution PI then putting these things together the value of F at U the label of U is a real random variable and it will be interesting for us to study this just from a pure probability point of view as a random variable so what do we do typically when we have a random variable we typically first calculate its mean and it's variance so let's do that let's say the mean of this random variable which we'll write as the expectation over U drawn from PI of F of U or sometimes for shorthand will just write expectation of F this is quite a brief shorthand but that means the average value of f not according the uniform distribution in general but according to this stationary distribution PI that's the mean expectation of F and let's see a typical example our favorite typical example is always when F is the indicator of a subset of vertices okay so if s happens to be a subset of vertices and F is the indicator function for that set of vertices then what is expectation of F what is the mean of this random variable well it's the expectation of an indicator so it's a the probability of the event being indicated so it equals the probability over you drawn from the stationary distribution PI thank you is in s okay and this is a very natural quantity it's something like the the weight of the set s or you'll often hear the term the volume the set s it's a bit fancy but it's actually a nice term the volume or even the density of s let's stick with these two terms the weight or the volume of s and in the case where the graph is regular and therefore pi is the uniform distribution and this is simply the fraction of vertices in the graph which are in s the fractional size of s which is a very natural concept more generally though it's proportional to the number the sum of the degrees of the vertices in s or the the number of edges the number of edge endpoints that are in s okay so this is the right way to measure the size of a set s in a possibly irregular graph it's through this volume of S which is the expectation of the indicator of the set great so that's the mean of this random variable and now let's think about the variance of this random variable and I should mention that this variance of the random variable will end up being some kind of global property of the graph G so sorry of the the function f so let's look at the variance it's the variance of this random variable f of u when u is drawn from stationary distribution PI okay let's also write um mu for this mean okay so then we know that the variance is equal to the expected value of how much the random variable f of u differs from its mean squared again when u is drawn from the invariant distribution PI so that's one formula for the variance you know we know another formula for the variance is it's the expected value of the random variable squared minus the square of the expected value so that's expected value again you drawn from PI of F of U squared minus the expected value of f squared actually there's one more totally standard formula for the variance of a random variable and we'll see it in this context just now it's actually half of the expected squared difference between two independent copies of the random variable well let me write that and it'll become a bit more clear I claim this variance is also equal to the following third thing 1/2 expectation when u is drawn from PI and V is drawn from PI independently of F of U minus F of V squared okay so we take two independent random vertices we look at the value on these two vertices take the difference square it and also multiply by half okay so the fact that this is equal to the variance is actually just a general property of you know variance I'll even show you the proof here really quickly the proof is involves just expanding up this square so as a proof this is uh well let me just do the quantity without the half this quantity here is the expectation of F of u squared minus two times F of U F of V plus F of V squared okay and this is final in area of expectation expected value of F of U squared plus expected value of F of V squared minus two times expected value of F of U F of V now the important thing to remember here is that U and V are drawn independently but they have the same distribution so first of all this expected value of F of U squared plus expected value of F of V squared this is the same number twice because U and V have the same distribution this is just two times expected value of F of U squared and because U and V are independent the expectation of a product is equal to the product of the expectations so this is minus two times expected value of F of U times expected value of F of V which is two times the expected value of F of U squared because again U and V have the same distribution okay so this was all done without hanging onto the factor of 1/2 here but we put the factor of 1/2 here and these twos go away and we have expected value of F of U squared minus expected value of F of U squared which is this alternate formula for variance okay so those are just a quick justification of a standard formula for variance but we bring it up because well it looks very similar to our formula for the quadratic form of the graph G applied to V the only difference there between the quadratic form and this formula for variance it is right here how we choose U and V if we just look at the variance of F which as I said we can think of some kind of global variance of the function f or the global variance of the the labelings of the vertices that involves just choosing independent random vertices U and V whereas the quadratic form involved choosing a random edge UV so it's more like the local variants or the variants of the function along edges so in fact let me compare these two things let me rewrite here that this is the variance of F the sort of global variance and we can compare that to the quadratic form for the graph at F the exact same formula except that we chose a uniformly random edge UV so as I said I really like to think of the variance of F of U it's just the global variance of the function and the quadratic form as somehow the local variance of the function of the function along edges um for example just as a look ahead if the local variance of F is always a large ish fraction of the global variance of F this is exactly when we call the underlying graph G an expander graph okay but that's just a bit of a something to get you interested for future lectures okay so that's great and it helps us think about the quadratic form of the graph in terms of a more regular concept of variance now we're really going to get into a little bit more of linear algebra because it'll help us again compute these quantities so now I'm gonna make a definition which is a little bit psychologically disturbing the first time you make it because I'm gonna make a definition for a non-standard inner product or a non-standard dot product which is something you're allowed to have in linear algebra but people are not normally used to it and again the reason we have to make this is because we're dealing with irregular graphs if you only want to worry about regular graphs and you really don't have to get into this like funny business that I'm about to tell you what's in life no the graphs are regular I feel that it's good to talk about this okay so let me make a new definition and this definition is going to be an inner product in the sense of vector spaces so let F and G be two functions on the vertices of a graph or two vectors indexed by the vertices of a graph I'm going to define the PI inner product of F and G so I'll write it as you know angle brackets like a normal you know product but I'm gonna put a PI down here to be the expected value of f of U times G of u when u is drawn from the invariant distribution okay and the intuition you should have for this quantity this is sort of funny PI inner product of F and G it's just the correlation of F and G how similar they are but again it's sort of weighted by the importance or degree of each vertex so again if pi is the uniform distribution because let's say the graph is regular then this expectation is just a usual old average of f of u times G of U and so it's very very closely related to the standard dot product of F of G F and G when thought of as vectors it's the same as the standard dot product just divided by 1 over N we'll use an average instead of a sum so again let's think of F and G as vectors now by just stacking F and G's values into column vectors okay and now I have this funny inner product on them and if as I said PI is the uniform distribution then we're really just averaging the sort of pairwise products of F and G's entries so we're really just doing the normal dot product between F and G and then dividing by n and that's not a very big deal so this is the let me just say is the normal dot product or the usual inner product for n dimensional vectors scaled by one over n if G is regular okay now if G is not a regular graph then this is not the regular old dog right there some kind of like weighted dot product again where you weight the different coordinates which means you weight the different vertices in proportion to their degree and this is something that can you know trip you up the first few times you see it so it's the first few times you see it just focus on the regular case what is the fourth fifth sixth or hundredth time you've seen it then you know try to get along with this funny inner product okay now uh you may reno remember from your linear algebra class the definition of a general vector space inner product but i want to tell you that this is a valid general inner space vector space inner product which basically means it satisfies all the usual inner product properties you expect so i'll remind you what those properties are so let me say that this is a is a valid vector space inner product in the sense that it satisfies the following properties first of all it's symmetric the inner product of F and G is the same as the inner product of G NF that's clear you know we're only gonna worry about real numbers here not complex numbers here so know any of that funny conjugating the second vector business that we saw in quantum it's linear in the following sense if you take let's say a scalar product of F plus some translation G and you inner product that with H and the normal linear thing happens this is C times the inner product of F and H plus the inner product of G and H all these things you can just check from immediately from the definition using linearity of expectation actually and the last product which is a crucial product for inner product is that if you look at the inner product of a function with itself well let's plug in the definition for this so this is the expectation when u is drawn from the invariant distribution PI of F of U times F of U or F of U squared well the property is that this is non-negative that's an important property the inner product of any vector or function with itself is non-negative and furthermore oops furthermore you have equality it's actually equal to 0 if and only if well the only way this can equal 0 the average of non-negative numbers if is if all these non-negative numbers are 0 ok and remember strictly speaking PI as a probability distribution puts at least some probability on at least every vertex because we are assuming the no vertices of degree 0 ok so this this quantity being 0 can really only happen if f is 0 and all the vertices so if and only if F is the 0 vector or the 0 function ok so these properties all together mean this angle brackets PI thing oh I should have mentioned I should really put PI's underneath all of these angle brackets I'm gonna eventually stop doing that because it's annoying but I'm always meaning this special inner product here what we've seen is that it satisfies all the usual properties I've been inner product so you should just go ahead and use it like it's an inner product and not stressed too much about the fact that has this like slightly funny definition it's just a different way to dinner product I should finally mention is a little bit of a notation this quantity will be somewhat important for us the inner product of F with itself and I'm going to stick with like kind of a linear algebra notation we're gonna write this as sort of the squared length of F ok and somehow this this 2 in the base here is coming from the 2 in the exponent here ok so let's just play around with this notation a little bit more and then we'll love not enough linear algebra for a while and we can go back to thinking about the quadratic form and cuts and graphs and convergence of random walks to the stationary distribution ok so let me just end here with one remark let's consider our our favorite situation for functions the indicator function of generic subset of the vertices okay then we can use sort of probability notation as well yes we won't use this notation too much but this is sort of probability notation the one norm of F if you think of f of u when u is chosen from the stationary distribution as a random variable then this notation is sometimes the notation used for the first absolute moment of the random variable anyway let's just say that it's literally equal to the expectation of U drawn from the stationary distribution the absolute value of f of U okay and well F is a zero one random variable so the absolute values are pointless so this is the same as expectation of F of U without the absolute value okay so this is just the mean of F as a random variable you sometimes wrote like this but of course in this case we furthermore know that f is a zero one indicator random variable so it's expectation is just equal to the product probability of the event being indicated just the probability when u is drawn from the stationary distribution that you as in s okay this is a quantity we talked about before this is what we called the volume of s okay so that's sort of the 1 norm of F but actually here's another funny point you see the expected value of f of U this quantity here well F of U is either 0 or 1 which means that f of U squared is the same as f of U because 0 squared is 0 and 1 squared is 0 so all these equivalent quantities are also also equal to the expected value of f of U squared which is as we saw up here what we're writing is the squared to norm of F or the inner product of F with itself okay so for our favorite kind of function the indicator of a set of vertices you know both this squared to norm or the inner product of F with itself or the one norm of F all these things are like all equal to each other and they're all equal to a very important quantity the sort of volume or fractional vertex size of the set of vertices of s being indicated\", metadata={'source': '6P8nVAolKog'}),\n",
       " Document(page_content=\"okay so in the last segment of this lecture I'd like to get back to this discussion we were looking at here when we were talking about the variance of a function versus its sort of local variance for the quadratic form and one thing that we said was that the given a graph G whether or not the standard random walk from an arbitrary starting point mixes quickly in other words it quickly gets to the stationary distribution it is determined by whether or not this quadratic form of the graph f is never small now we have to get into quantitative things now because we need to understand how small could it be and for the question of how small could it be actually there's a bit of a trivial answer the smallest it can be is 0 so I'll talk about that for a moment we're also going to talk later about how large it can be and there we'll need to take scaling considerations into account but always you should be comparing this sort of local variance the quadratic form of F with the sort of global variance the regular variance of F when you just choose a uniformly random sorry when you choose a vertex u from the stationary distribution and look at the variance of the random variable f of U okay so let's get at this question first of how small can the quadratic form be so we'll ask how small can this be and you know I said there's a rather trivial answer to this question it's a non-negative quantity and it can be 0 so of course the answer is 0 but a question is when is it 0 so one obvious case when it's 0 is if the function itself is 0 if f is constantly 0 then the quadratic form is also 0 but let's ask ourselves is there a non-trivial F that makes the quadratic form 0 you know can there be some non trivial such F I think it depends a little bit on what you mean by non-trivial but the answer is potentially yes so uh here's another function which makes the quadratic form zero the constantly one function okay so here is answer yes at least if you count this is non-trivial if F is constantly one then the quadratic form of F is zero and well let me just remind you why that is the quadratic form of F I'll just repeat the definition here it's 1/2 times expectation over a random edge UV of f of u minus F of V squared okay and so you see if F is always equal to one then this difference is always zero and therefore the expected square of the difference is also zero and indeed if F takes on any particular value constantly if F labels every single vertex by the same value then the quadratic form value this a of F scripty of F will be zero okay as we saw before one of the properties of this quadratic form the script EMF is that if you add a scalar to every value of f it doesn't change the value of the quadratic form so if I could even once we knew that the quadratic form is zero when f is constantly zero then we learned that it's a zero for all F that are constant okay so props you might think all right well these are all the ways in which the quadratic form of F could be zero well that's not necessarily the case either so let's think about it carefully when this quantity might be zero so let's imagine drawing our graph by my graph G and let's say we have some vertex here and let's put another vertex here and let's say F has some value at this vertex here 1 okay it could be anything but maybe by adding a constant F we can make it equal 1 alright that's fine now if f has an edge here let's call this a u1 call this you two well you see that if the quadratic form of F is going to equal zero then you V must also get the label at one under F okay because there's a chance when we draw a random edge UV that we choose this specific edge between u1 and u2 and then if the overall expected square of the difference is going to be zero well f it better have the same value here as it has here and now we can kind of continue with this reasoning right if there's some other vertex out here u3 and it's connected to u 1 then F must also give it the value 1 if the quadratic form of F is gonna be zero because there's some chance that this will be the edge chosen in the random experiment choosing a random edge and so I better have the same values on the two endpoints of this edge okay and similarly every edge here must have its endpoints labeled by the same value so we can kind of conclude that s value must be 1 everywhere so it looks like I'm telling you that actually it is the case that as soon as you assume that like one value of the function f let's say u 1 is 1 then all the values of the function f must also be 1 if the quadratic form is gonna have value 0 but not so because F might have more oh sorry G might have more than one connected component so G might have another connected component this is possible and maybe F gives this vertex and this connected component the value 5 and this is not going to be inconsistent with the quadratic form having value 0 it is true that all the other vertices in this connected component must also be labeled 5 by F because all the edges here have a chance of showing up in this experiment and therefore f has to give the same values to the endpoints well they could all have different values the connected components could have different values okay so if G has 3 connected components like this you know then each connected component could have a different labeling according to F maybe this is minus 3.2 its minus 3.2 on all vertices and such an F that's one constantly here and five constantly here and minus 3.2 constantly here we'll have the property that you know when you pick any edge the end point difference will be zero it's so the quadratic form is zero so what have we sort of concluded by this discussion I'll write it as a proposition so given a graph G the quadratic form of a function f is zero if and only if f is constant on each connected component of G and in fact there is an addendum to this statement which is going to get us yet further into linear algebra which is that actually the number of connected components of G is equal to the number of linearly independent functions F that have the quadratic form equal to zero okay so we know all the functions that have quadratic form equal to zero or the ones that are constant on each connected component and we also know that we can think of functions as vectors in RN where n is the number of vertices and so there's a notion of vectors or functions being linearly dependent or independent and it turns out that if you look at the number of or the maximum number of linearly independent FS you can get that have quadratic form equal to 0 this is equal to the number of connected components of G in a particular you know if the connected components I'll just write components for brevity are let's say s 1 through s well one thing you can note is that the functions indicator of s1 indicator of s2 dot indicator of SL are indeed linearly independent okay might not be too used to thinking about functions you know vector space rather than vectors but you can see like okay let's say we have a vector representing a function and the coordinates corresponding to the set s 1 or these up here so the indicator of that set looks like this vector and maybe the second waves are 2 connected components and s 2 is the second connected component and it's coordinates are the ones down here so the indicator that would look like this vector you see that indeed these are linearly independent vectors meaning there's no way to take a linear combination of them that gives the all zeroes vector unless you take the all zeros linear combination okay and that's just because I mean the disjoint supports so in general I mean if you're more than two connected components all the ones will be in disjoint places and there's no way to take a linear combination of zero one vectors or the ones are all indistinct locations to get the all zeros vector unless you take the all zeros linear combination and in general the set of all F's I should say that all F's such that this quadratic form of F equals 0 is equal to like all the things that look like some I goes from 1 to L this is sort of sloppy notation but see I times the indicator of s I okay so really the set of all functions that have quadratic form 0 is the set of all functions which are constant on each connected component or it's all linear combinations of these indicators of the connected components and this is actually really um there's two the nice things to say about this one this is really the first condition we'll see between kind of a combinatorial graph theoretic property namely number of connected components and a linear algebra constant you know you know the number of linearly independent vectors or functions that have a certain quadratic form okay so really this is what spectral graph here is all about like finding relationships on one hand between nice combinatorial properties of graphs that are about their geometry and things that we care about and linear algebraic properties of the quadratic form and the adjacency matrix and things like that I'd also like to say that this is a very basic proposition that the quadratic form of sorry that the number of linearly independent F that of quadratic form 0 call it K is equal to the number of connected components K of G there's a sort of robusta fication of this fact which is very interesting and we'll look at it in some sense later which says that if G has K mostly disconnected components or if G's vertices can be partitioned into K clusters such that there are very few edges going between the clusters this holds if and only if you can find K linearly independent functions where the quadratic form P of F is small so again I have to put all these things in quotes because there's precise quantitative aspects to it which I'm not going into yet but it's a very interesting and non-trivial robusta fication of this concept the number of connecting components is the number of linearly dependent functions with quadratic form exactly zero and this idea is at the heart of say some very the best known approximation algorithms for the sparse Escott problem in optimization algorithms theory ok so that's what I want to say about the topic of minimizing the quadratic form of F over all possible F the minimum possible value is 0 and we can say some interesting things about functions that make it 0 now let's talk about the other side of the spectrum which is the question of that's right like this we're about maximizing script F script PFF maximizing the quadratic form over all functions app so we're fixing your graph and now asking well how big can this quadratic form U of F be now there's a obvious problem with asking this question as it is you need some kind of scaling considerations because you take some function f have some quadratic form value a of f is 5 and then you multiply F by 2 then the quadratic form value goes up by 2 squared which is 4 so we go up to 20 you can just multiply F by a large constant and make the quadratic form value as large as you want ok so just recall this sort of scaling fact that the quadratic form of C F is equal to C squared times the quadratic form applied to F ok so what this means is you know the right way to ask this question is to fix some scaling of F and then ask what's the among such fixed scaling fixed versions of S which one has the largest 5 of the quadratic form and really as I alluded to earlier if we go back here the most natural scaling is to not just look at all apps but to look at all FS whose global variance is equal to some fixed value and our favorite value for variances is 1 ok so the right way to ask this question is to look over all [Music] given a graph look over all functions f whose global variance is 1 and now let's ask how large their local variance or quadratic form value can be ok so the right way to ask this question is to look at what is the maximum possible value the quadratic form of a function f subject to the variance of F was the variance of the random variable f of u when u is drawn from the stationary distribution is equal to 1 ok and one quick thing to say is that we could have equivalent ly written less than or equal to 1 here because you know if I let you have a variance that's less than 1 that doesn't really help you if you're trying to maximize the quadrat form or particularly like if you found some quadratic form value and your variance was a quarter then what you should do is like just double your function and that makes the variance one which is still allowed and that will also make four times the quadratic form value okay so you should always if you haven't done it yet scale your function by a constant to make its variance as large as allowed namely one and this will only improve your quadratic form value okay so this is the question that we're interested in now maximizing the quadratic form among all functions whose variance is that most one now before we talk about that I would like to say that sometimes we do another similar looking thing which is maybe maximize the quadratic form Y of F subject to the expected square of F being 1 remember this we wrote like this the inner product of F with itself or this is the expected value of F of random u squared equals 1 okay and this may look like a slightly different question but as I'll argue to you in a moment it's actually the exact same question first let me make the opposite obvious observation that again for this second question I mean forget about the first question for a second but for the second question we can also only write less than or equal to 1 here because again trying to maximize a of F you may as well you know scale up your function as much as you can so if you have a expected F square that's at most 1 just scale it up until its expectation of F square equals 1 okay so these two things are equivalent these two things are equivalent and what I want to tell you now is actually these two problems are also equivalent ok and let me quickly justify that for you of course we know that the variance of F is equal to the expected value of F squared minus the expected value of F squared so all with respect to actually a random vertex u from the stationary distribution or just inverting this the expected value of F squared it's equal to the variance of F plus the mean of F squared okay so the expected value of f squared as always at least as big as the variance of F okay so in particular that means if you can achieve some quadratic form value with an F whose variance is 1 then you can achieve the same quadratic form value with the same F and that F will have expected F squared at most 1 so any valid solution to this maximization problem is always a valid solution to this maximization problem in order for this maximization problems value is always less than or equal to the one where you're maximizing over expected value of F of squared is that most one but the converse is also true and let's see why let's say you have some function f and it's expected value of f squared is at most one and it achieves some quadratic form value Y of F well I like to say is you can also achieve that same quadratic form value with a function f whose variance is at most one and that's not immediately obvious because the variance in general is wait I think I've done this the wrong way around yeah perhaps I've described this the wrong way around but well I mean - let me let me try again let's say you've achieved some quadratic form value Y of F with a function whose variance is let's say even exactly 1 and you're wondering if you can also achieve the same quadratic form value with a function whose expected square is also 1 and you might be worried because the expected square of F is always bigger than the variance so now you might think oh maybe the expected square of F is bigger than 1 so it's not allowed as a solution here but remember that if you add or subtract a constant to a function then it doesn't change the quadratic form Y of F ok the quadratic form is invariant to translation and the variance is also invariant to translations so if you add or subtract a constant to F it doesn't change its variance so if you have some function f acheiving let's say variance one alright most one and achieving some quadratic form value you can add or subtract a constant to it and it won't change anything still a variance one or at most one in the same quadratic form but you can potentially make the expected square of F smaller and indeed what you should just do is translate it by subtracting its mean the first typical step and standardization to get it down to a function whose mean is zero we get down to a function whose mean is zero doesn't change the variance or at the quadratic form but this term here will become zero and you will have made the expected value of f squared equal to the variance of F and so if the variance of F was bounded by one and so is the expected value of f squared now so it's legal for the second optimization problem and you've achieved the same quadratic form value okay so sorry forgetting that a little bit backwards but I hope I've convinced you now that these two quadratic programs if you will these two tasks of maximizing the quadratic form EMF with respect to either a variance bound or a second moment bound or actually the same task okay so sometimes we like to think of it one way sometimes the other way okay so now let's finally actually think about it let's say we fix the expected square of the function to be at most 1 or equal to 1 and we want to find a function f whose quadratic form is as large as possible okay so how can we do this well if you recall again the the formula for the quadratic form which is here now trying to make this large so we fix some scaling thing which fixes something with the global distribution of s values and we're trying to make this quadratic form as large as we can so intuitively we want to sign a real number to each vertex so that the endpoint differences along edges au is as large as possible okay if you think of a function as mapping a vertex into a point on the real line we kind of want to embed the vertices into the real line so that edge is and points are as far apart as possible let me just write that intuition down again so the intuition for maximizing the quadratic form is we want to embed the vertices into the real line this will be F so that edge endpoints are as far apart as possible and if you think about this there's one kind of graph where you can be extremely successful at this task so let me ask this as a little question for what kind of G will you be most successful and embedding the graph into the line such the edges gets sort of stretched as much as possible G well you'd be most successful and I hope by the time I finish writing this you'll have thought of the answer the answer is seemingly bipartite graphs because for a bipartite graph there's a natural thing you can do which is to let's say give all the vertices on the left part of the vertex set the value 1 and all the vertices on the right side of the vertex set with say minus 1 okay so again on the topic of you know maximizing quadratic form of f subject to the scaling condition that let's say the expected value of f squared is at most 1 if G is a bipartite graph and it's vertex set is got this bipartite by partition v1 v2 then let's consider this F let F be the indicator of vertex set 1 minus the indicator of vertex set 2 or in other words F of vertex u is plus 1 if u is in the first vertex part and minus 1 if u is in the second vertex part ok so let's first check that this function f satisfies the you know scaling condition and that's easy because the expected value of f of U squared is well it's 1 because f squared is always 1 its values are plus or minus 1 so that's great it's a expected value of f squared is equal to one that's nice now what about the quadratic form of this F well if you think about it for a moment you'll see that the quadratic form value for this F is two why is that well remember the quadratic form is 1/2 expectation over a random edge UV f of U minus f of v squared and if you have a bipartite graph and when you draw a random and at UV u is always on one side B is always on the other side so one of these will be plus or minus 1 the other one will be the opposite minus plus or minus plus one and the difference will be plus or minus two the square of the difference will be four so the whole expectation will be four and then we multiply by a half we get two okay so that's a modest calculation and so we see that subject to this scaling condition if the graph is bipartite we can achieve a quadratic form value of two and may be reasonable to think intuitively that this might be the best thing you can do for any graph and that's the case so in some sense this will be the the last thing I want to mention today here's a proposition for any graph bipartite or not quadratic form applied to any function f it's always at most two times the square to norm of F okay which recall is two times expected value of F squared okay so I'll prove this to you and that's great because you know under our scaling condition that we make that this isn't one right most one it shows that no matter the graph bipartite or not the largest quadratic form value you can have is two so let's prove this straightforward proof with one little trick in it so this quantity as we know is a half expectation over a random and F of U minus F of V squared okay and we're just going to expand it out so we get 1/2 x and we'll use the linearity of expectation to at the same time so we get expected value of or just you so really let me write it a bit more slowly this will be the first quantity we get when we square things out and using the interval expectation the expected value over a randomly chosen edge UV of just F of U squared now remember if you draw random edge UV and then just remember only U which is all you need to do to compute this F of U squared the distribution of you alone is precisely PI the invariant distribution and that was exactly how we defined the invariant distribution in fact ok so what I want to say here is that we may as well just write here u drawn from PI because we don't need to know V in order to compute the thing inside the expectation okay similarly let me do the the part you get when you look at F of V squared it'll be very similar you'll get 1/2 times the expectation again over a random edge UV of F of V squared and again this quantity the middle only depends on V so what we're really doing here is choosing a random edge UV and then forgetting about you as we argue before this is again just a fancy way to choose V from the stationary distribution or the invariant distribution PI so we can again say this is just V drawn from PI ok so we have two copies of a half expected value of f squared that's nice ok and we have the cross term so let's handle the cross term here we get minus we have this factor of 1/2 here but we're also gonna get a 2 from the cross term so those will cancel out and so we'll get it - expected value over a random edge UV of F of U F of V ok and actually unlike in the case we did this a calculation a little bit before when we're talking about the global variance there U and V were independent vertices drawn from PI they weren't drawn according to an edge unlike in that case where we had independence between U and V we can't just say oh the expectation of a product is equal to the product of the expectation because F of U and F of V are definitely not independent random variables because U and V are drawn as a random edge now here's a general life tip that I mentioned perhaps before whenever you have an expectation of a product of random variables and they're not independent and you don't know what to do use Koshi Schwartz and that will at least break them up into something that only depends on the first random variable and something that only depends on the second random variable and hopefully that will be good enough so let's do that Cauchy Schwartz actually tells us that the absolute value of this expectation can be bounded by sort of the two norm of the random variable f of U at times the two norm of the random variable F of V okay and therefore we're subtracting this quantity but we can instead to get an upper bound we can add this upper bound from Koshi Schwarz so we're gonna use Cauchy Schwartz here and let's first tidy up the first term of course these two terms here are identical even though they use different letters U and V so here we have expected value of F squared 1/2 plus 1/2 is 1 by Cauchy Schwartz the second term can be bounded by plus the square root of expected value of F of U squared times the expected value of F of V squared okay remember again here are the expectations really over a random edge U and V but as we argued before you know this only depends on u this only depends on V so we can just forget about V for the first expectation u in the second expectation and then these random variables of U and V are both distributed according to PI so this thing inside the square root is again expected value of F squared and this thing inside the square root is again expected value of f squared about the square root of this multiplied it against itself so we just get finally another copy of expected value of f squared okay and so finally this equals two times expected value of f squared and perhaps I can fit it all on one screen there you have it we have an upper bound for any F on the quadratic form script D of F then the bound B is two times the two norm squared of F okay great so that concludes that proof let me just end with a few comments on it first just write here first as an exercise for you I'll let you show that equality of e F equal Inge two times the square root to norm of F is possible well we know it's possible if G is bipartite saw that with function f that puts one on one part of the vertices and minus one on the other part of vertices well this is an if and only if okay so the only way you can get this quadratic form to equal twice the two norm or the only way you can get it to equal to two under our scaling conditions is if F is sorry if G is bipartite so that's not a very hard exercise and lastly now that we talked briefly about a robusta fication of this fact again we have a fact about sort of the coma tutorial nature of G my part tightness relating towards these more algebraic or linear algebraic quantities like the quadratic form and the to norm and so forth so the robusta fication of this fact is also very interesting it says that equality is almost possible if and only if G is close to being a bipartite graph again as always there's quantitative aspects you should put those in quotes but if you have a graph G then it's possible a mod to get a function f with variance or to norm at most one whose quadratic form is close to two like two minus Epsilon if and only if G is in some sense almost bipartite or if it has an almost perfect maximum cut okay and so this connection between quadratic form and two norms and maxcut algorithms is also very interesting and in fact to really exploit this connection and understand the relationships here and understand the quadratic form what you really need to look at is another classic topic in linear algebra namely eigenvalues and eigenvectors so that'll be the topic in the next lecture like in Victor's and eigenvalues of graphs\", metadata={'source': 'iobNzf18i-4'}),\n",
       " Document(page_content=\"all right everybody uh oh boy I love spectral graph theory so much this is gonna be great I will recap some things I talked about in the last lecture so in spectral graph theory we're gonna be talking about undirected graphs G and that'll be like fixed at the beginning for all time the graph are considering self loops okay parallel edges okay and we'll really be focusing on is functions function is labeling the vertices of the graph by real numbers and if there are n vertices then the set of all such functions mapping the vertices to real numbers forms an N dimensional vector space you can just stack all the function values into a column vector and that's your vector space there and I should also mention we had a whole lecture on analysis of boolean functions and analysis a boolean function is really just this whole theory restricted to the special case where G is the hyper cube graph you know this one right here so then of course we're also thinking about functions mapping you know boolean strings to real numbers it's just the special case of all this theory where this is our graph G so now we'll study it in general I should also add that if this is the first you know a few times you've seen this stuff definitely just imagine in your head that the graph is a regular graph because that makes a lot of things simpler one thing in particular makes simpler is this I mean this is the weirdest thing that it takes a little getting used to so we're gonna be talking about this vector space of functions mapping of vertices of the graph to real numbers and we're gonna have an inner product in this vector space but it won't be the usual inner product it'll be a different inner product which is allowed by the laws of linear algebra and we'll write it like this with angle brackets subscript PI I'm gonna stop running the PI everywhere though because this will be the only inter product we use so the inner product between two functions is sort of the correlation between them but when you draw the vertex you from a certain probability distribution a very special probability distribution on the vertices which I'll denote as PI and this special probability distribution will just be the uniform distribution in the case of a regular graph which is why it's easiest to think about regular graph but in case you have an irregular graph then you have a different probability distribution to care about so this probability distribution PI which is called the invariant distribution or the stationary distribution there's two ways to define it one way to define it is just by sort of giving little algorithm that draws from PI and that little algorithm is draw a uniformly random edge UV but then output let's say the first vertex U it's actually the same as if you decide to output V because if you draw a uniformly random edge from u to V you're equally likely to pick the one going from V to u so as I mentioned in the last lecture we're only gonna be thinking about undirected graphs but sometimes we'll really think about that undirected edge is like a directed edge from u to V and the directed edge from V to u ok so this probability distribution is more likely to pick vertices with higher degree and indeed it's not hard to check that you can just write down a formula for this special distribution the probability mass that put on a vertex u under pi is proportional to the degree it's exactly the degree divided by while the number of directed edges to x and the number of undirected edges okay so this is just like a it's like a similar to like a you know the usual dot product between two vectors you line up all the F of U values and the G of U value is U multiplied together and add them up but you add them up with some weights where the weights are proportional to the degree okay so this will make things go more smoothly alright so why is it called the invariant distribution or the stationary distribution it's to do with what's written on this is half of the board so one thing we're really going to ultimately care about and we'll talk about someone in this lecture is the following let's say you do the standard random walk that's what this stands for on the graph that means you're out some vertex and each time step you choose a random neighbor of its and walk to that neighbor now we'll eventually be concerned about it's like what happens you know if you're concerned about where you'll end up for a moment while if you start from an arbitrary or worst-case starting point you zero but if you start from the probability distribution pie then you'll always stay in that distribution so what do I mean by this it takes me a while when I started learning to this to get my head around it when you're doing the you know random walk in a graph your first instinct is that okay at each step you're remembering like what vertex you're on but you should also think more generally of doing a random walk in thinking of each shading at each time step what is the probability of distribution of a probability distribution of vertices you might be on so once you fix a starting vertex you zero and you walk for ten steps at each time step and particularly the tenth step you have a probability of being at each vertex and you should really imagine how this probability distribution on the vertices evolves over time and you can even imagine not starting at some worst-case vertex U but U 0 but picking your starting vertex from a probability distribution on vertices as well and a magical thing happens if you pick it with this probably according to this probability distribution pie then of course your youth your sorry your 0th vertex is in this probability distribution pie but also as we saw in the last lecture now the first vertex will also be in probability distribution pie and the second vertex will also be in probability distribution pie and so forth no matter how many steps you take and that's why it's called the invariant distribution once you're in this probability distribution you will continue in that probability distribution forever when you do a standard right and walk okay and again this is just the uniform distribution on all vertices in the case of a regular graph otherwise it's proportional to the degrees and you know a major thing that we're eventually going to study we talked about before is well if you start from a worst-case vertex how many steps you need to take before you're sort of close to being in this distribution great so ah now once this inner product is chosen and we have a vector space we can start doing some linear algebra which is where the spectral graph theory kind of gets its name and really much like with analysis of boolean functions the special case where the graph is the hypercube graph in some sense what we really really care about is functions from the vertices to 0 and 1 ok we care about boolean valued functions we care about subsets of vertices these are the thing that usually interest us we extended the case of real valued functions because I mean it's mathematically natural and it helps us study the boolean valued case ultimately that we care about but we're always going to be coming back to this case in where you're looking at zero one functions or indicators of subsets of vertices okay let's what I've written here if F is the indicator of a subset s of vertices well then many these linear algebra things that we'll be mentioning have a nice meaning so for example last time we introduced this notation the squared norm of a vector or function f in this setting we use the words vector and function interchangeably this is the inner product of F with itself and by definition this is the expected value of or the average value of f of U squared the average squared value of the labels as always under this probability distribution when F is zero one valued though finally f of U squared is the same as f of u 0 + 1 squared 2 themselves are themselves so now you have the expectation of just F which is expectation of an indicator it's the probability of the event being indicated so in the end it's this quantity it's the probability that a random vertex is in the set S which in the regular case when PI is the uniform distribution is just the fraction of vertices that are in S and sometimes it's called the volume of s it's a fractional volume so this is the right way to measure the size of vertex sets ok any questions so far okay continuing this recap great so with all that set up we came to study what is maybe the most important thing object in all the spectral graph theory which is this quadratic form which is a thing which Maps functions or vectors or labeling of vertices to numbers and it still native doneita denoted by this or at least I denote it by this curly E and it's basically the average squared distance of the function value across edges okay so this notation expectation you sym V means pick a random uniformly random edge so you pick a random edge you look at the squared difference of s functions values across that edge okay x factor hat okay somehow measuring how much f is changing like locally along edges and it's very instructive compare it to compare it to the variance of F which measures just how much F varies in this quadratic sense globally so this is where you just pick two completely random independent vertices U and V not from an edge but just two independent ones and measure the squared distance between s values here okay so it's always sort of good to study this quantity but you need a scaling condition because you know this quantity if you just SS values it goes up by factor four so if you want to for example talk about maximizing this quantity you need a natural scaling condition and scaling so that the variance is what say one is the right thing to do so we ended the last lecture talking about the following two basic questions which functions F minimize this quadratic form and which functions maximize it and the minimization question is a bit easier but it already starts to connect make this connection between you know graph theory and combinatorics on one hand and linear algebra on the other hand which is the real raison d'être of spectral graph theory so it's not hard to see that the minimum possible value of this is 0 of course that's could be just the constantly zero function but actually the set of all functions f that make this have its minimum value 0 is a bit non-trivial it's all the functions which are constant on g's connected components and you can have a different constant value and the different connected components you see if that happens well then across any edge f4f will always differ by zero and in particular you see that if G has L connected components then you end up getting L linearly independent functions or vectors as I said functions and vectors are synonymous to us here linearly independent functions which make the quadratic form equal to zero okay and as I mentioned last time there are some like robust if occasions of this fact where if you have you know a linearly independent functions that like almost make this quadratic form zero then maybe you can cut your graph into L pieces that are almost disconnected okay finally on the other hand we talked about maximizing this and as I mentioned you know it doesn't make sense to just ask about maximizing it you have to put in some scaling condition and you can equivalently as I mentioned last time either put in the condition that the the two norm squared of the function and the average squared value is equal to one or at most one or that the variance is equal to or at most one you'll get the same result in each case and as we saw by a little arithmetic including kosher Schwarz the maximum value of this quadratic form assuming the scaling factors one is two and two is possible if and only if the graph is bipartite so that's the connection between linear algebra stuff and the common torques of the graph and you can see you as we saw last time if you have a bipartite graph if you label with FF labels all of one part of the vertices by one and all the other part of the mercy's by minus one then this quantity will indeed be two and the variance will be well the expected square will be one okay that's it for the recap any questions okay so in today's lecture we're gonna get into actually even some more terminology and notation all towards kind of getting at this idea of studying random walks in the graph G the convergence rate and how you know small sets of or sets of vertices in the graph that have few edges coming out of them can be like bottlenecks for fast convergence of the the random walk to the invariant distribution and how all this stuff connects to this quadratic form okay so let me remind you a couple of details about how we proved this little fact and it started just by really expanding the definition of the quadratic form early II and I will omit the arithmetic but it went like this it's curly e of F by some arithmetic equals well this two norm squared of F minus this quantity an expected value over a random edge UV of F of u times F of E okay and then we actually applied Cauchy Schwartz to this quantity to say that it's at most even the negative and it is also at most the same quantity and that's how the bound two times the two norm squared arose as an upper bound but actually what I want to focus on now is this quantity it's actually pretty similar to well it's you can kind of see it's kind of similar to this quantity you can derive one from the other relatively easily but we'll just take a look at it for a moment so this boxed quantity here start I'll just write it in a slightly different way since we know picking a random uniformly random edge can be done in in two steps you can pick first of vertex according to the probability distribution pi and then you can pick here I mean pick V to be a random uniformly random neighbor of you okay and then you look at F of U times F of V so I didn't do too much there but one thing I can do now is pull this F of U out here so I got the expected value and you just drawn from PI F of U times the expected value when V is strong from you know well as a random a review of F of V just write this because I'm quite interested in this quantity so we're gonna think about this quantity a little bit so you know imagine you is fixed and now what we're looking at is the average value of F unused neighbors okay so that's a nice quantity for every vertex U you can look at the average value of F unused neighbors and you can think of this itself as a function on the vertices the function that map's you to this average you can even sort of in your head may imagine some kind of physical diffusion process where at each time step of vertexes label is replaced by the average its neighbors and somehow it's a little bit what happens in the standard random walk so unless I will be actually kind of thinking about this function that map's you to this average and I gotta give that a name to this function and I'm gonna give a weird name to this function but I mean I hope you'll come to like it soon enough I'm gonna call this function KF so this function KF map's vertices to real numbers okay so it's one of these vectors or functions as usual and this is this definition KF the function KF applied to a vertex U is this quantity the average value of F around used neighbors so why do I write it as a KF well you see this this process of going from the original function f to this new function KF it's like an operation on functions it's like an operation that replaces one function by a different function and it's even actually a linear operator in the lira bra sense right so we have a like a mapping from vectors to vectors and it's actually a linear mapping in the sense that well if you apply it to like the sum of two vectors or the sum of two functions you can see it's equal to the sum like KF and kg okay that's just because if you replace f here by F plus G then by linearity of expectation you get the average of F plus the average of G so this K if you will is like a linear operation that takes vectors to vectors so it's like the classic object that gets studied in linear algebra in fact you can represent it by a matrix which we will do so let me first say its name so it has again several possible names but K is I would say either the Markov operator or you might call it the transition operator for the standard random walk or you might call it the normalized adjacency operator let's see in a second for G you can also call it use the word matrix instead of operator as well ah so good so as I said it's a linear operator that transforms functions into functions or vectors into vectors so in in plastic linear algebra style you can represent it its action as a matrix and let's think about that and that's just a bit of a spoiler I mean in the most simple case where you have a regular graph a D regular graph this matrix for K is just gonna be one over D times the adjacency matrix of the graph but let's think a little bit about how it works so we're looking for this matrix okay which when we multiply it against like a function or a vector remember this thing is indexed by the vertices of V uh we want to get out a new vector or function KF okay we have a formula for the youth entry of this vector KF it's like the average of F's values when you average just over the neighbors of U so this youth entry and the result kind of comes from the youth row here so what we need to put here basically we need to put in each entry here well just a nonzero value when the Associated column is a neighbor of U and in particular we should put one over the degree of U there because you know when you're averaging over the neighbors of U you take each of them with probability one over the degree of U so I hope you caught all that what I'm saying here is that the entries here okay UV are equal to well one over the degree of U if UV is a an edge otherwise zero or the way you could say this is this is the probability that you would step to V you know in the standard random walk starting at you this is bad notation but you're saying if you're doing a standard random walk and you're at you then this entry here is the probability that you go to V in one step okay so as I mentioned in the special case where it's a d regular graph so all these degrees are D this is nothing more than one over D times the adjacency matrix so in general I mean K is the adjacency matrix for G just normalize so that the row sums are 1 okay which makes it a quote-unquote stochastic matrix well maybe there's disagreement over whether or sarcastic matrix is supposed to have its rows something to 1 or column something to 1 but anyway it's a non-negative matrix where all the roads sum to 1 and as I mentioned G is d regular then we have that ok it's just 1 over T times a ok and some that'll be more convenient to work with K because that has a probabilistic interpretation than this matrix a which you're more used to working with maybe I should also say that when it's a d regular then K is a symmetric matrix meaning it equals its transpose but this does not hold true necessarily well at all if it's a non regular graph as we'll see though K does have some nice property which is analogous to being a symmetric matrix so all is well but it only actually be a symmetric matrix in the regular case okay any questions good so uh let's just play around with this matrix K a little bit more we'll see how it connects to like the random walk and stuff so uh one basic fact it's not hard to show it's not if you have two functions it's quite natural to look at this the inner product of F with K G okay and it's natural because it has this interpretation it's the expected value over a random edge UV of F of U times G of me this is a pretty natural formula over here and in some sense you can almost think about this is like a definition of K I mean K is that thing such that this is true uh it need not always be true let's see if we can come up with a counter example let's make an irregular graph it's very hard I guess like this graph is irregular okay so if this is vertex one two three and the matrix a the adjacency matrix would be like one is connected to two and two is connected to one and two is connected to three and also to one yeah wait a minute am i doing this well it's very challenging but I think I got it is this right okay so that's a okay and so K is uh this might be a bad example though so K will look like this 1/2 oh no it's an OK example 1/2 1/2 1 zero zero zero zero zero zero sorry oh if you can read this yeah it felt symmetric as I was writing it down but it's not symmetric right if you reflect this along the diagonal it doesn't it's not the same this is a good example to keep in mind though because our paths graph it's not so unnatural right but it's not regular so you have to worry about these things even for such small cases as path graphs great ah so I'll just double check this for Marilla for you with a little proof in a moment but let me make a little observation I mentioned this before but you know this means like pick a random edge but also bear in mind that it's ordered like you know it's sort of like a picking a random directed edge from u to V and then you look at F of U times G of E but actually again the probability that you pick the edge from u to V is the same as the probability you pick the edge from V to u so this number is actually the same if you reverse the roles of V in U which means that have I written this yeah this is also equal to the expected value over I mean V comma U being a random edge of F of V G of U which now if you just by commuting multiplication and move this over here all which is to say this is also the same thing as K F comma G ok so this is I said that in a funny way but this is the fact that I'm trying to get across that these two things are also the same and if you're an extreme linear algebra expert then you know the name for what this means about K it's not necessarily symmetric if it is symmetric and this were the usual inner product it would be true yes self-adjoint is the terminology an operator which has this property like when you move it you can move it from one side of the inner product to the other it's called self adjoint and I'm only dealing with like a real vector space here so there's no funny business about conjugating complex conjugation and this is like a little magic wand you can say sometimes to you know justify this I mean it's it's a true fact and it's sort of like the replacement for this the statement K is symmetric in the non regular case and it's like somehow this inner product is exactly cooked up to take care of everything so yeah this is a fact that you can use you can move the K between the two sides great so it'll share many of the same properties of symmetric matrices as we'll see like it has real eigenvalues and so forth okay actually let me just uh prove this fact see if I can cram it all into the same board so why is this true I guess uh yeah let's just try to write it yeah let me do it more clearly [Applause] so let's just expand the definition so we don't get confused here so I'm studying the inner product of F and the function K G so by definition of the inner product this is the expected value of the function at F at U times the function K G at you and now we can remember the definition of K G F of U then I finish of this is the average value of G around the neighbors of U okay now we can do like a similar trick we can pull this expectation to the outside and got this expectation over first choose a random vertex U then choose a random neighbor of U and then compute f of U times G of V okay and but as we've seen a couple of times this whole experiment together is just the same as choosing a random directed or yeah sure directed edge okay and why should we be interested in this particular quantity well I'll just give the favorite example that we always care about the case when F and I guess also in this case G are functions with range zero and one so they're indicators of sets of vertices so now imagine you have some graph G and now you have two sets of vertices s and T actually they need not be disjoint but I'll draw them as disjoint and this quantity here is going to be related to like the number of edges that go between s and T okay so if f is the indicator of a set s and G is the indicator of a set T then this quantity here inner product of F and K G it's the expected value for a random edge UV of well using this formula indicator of S applied to u times the indicator of T applied to V okay this whole thing is 0 or 1 it's the indicator for the event that U is in s and V is in T we have an expectation of an indicator so this is just the same it's the probability that a random edge has the property that its first endpoint is in s and the second endpoint is in T this is the fraction of edges that go between s and T ends of some quantity that you may care about in particular T equals s it's the fraction of edges which are internal to S or a third way to put it is it's the probability that if you take a random walk in the graph of length one starting from the invariant distribution that you'll go from s to T in fact I say that because like now I want to think about [Music] taking longer random walks in the graph so let me make a little observation that again is related to the meaning of this matrix or operator K so let's let Rho be any probability distribution on vertices and intuitively you might think of it as like a probability distribution for the first vertex in a random walk it's like are starting probability distribution and maybe we want to see how it evolves as we take steps and true to its name we're going to think of this as a row vector so instead of a column vector will stack it's n probabilities in a row and as I said you know this is V one position this is a V n position as I said suppose we do this first draw a vertex u from this probability distribution row and then do one random step from u to V okay so in other words let V be a random neighbor of U so altogether this generates a probability distribution on V probability distribution of vertices you'll get if you start according to Rho and then do one step and I claim call an exercise but I hope you can sort of just verify it in your heads as the next minute passes that you can figure out what it is I should have added these words well if Rho Prime is the distribution end of V then we have this sort of formula you get Rho prime by taking this Rho vector Rho and multiplying it against K because you know like the V sentry of Rho prime over here like what's the probability of ending at V it's like you consider all starting positions U which is I don't maybe here and then it gets multiplied against the you throw here and like I kind of tells you all the probabilities you'll go to each of the subsequent vertices given that you're at position you and Rho u okay so case really tells you how the probability distribution transforms on vertices when you do one step of a random walk so one corollary we have of this is that for our favorite probability distribution on vertices PI we have pi times K equals pi this is expressing that pi is the invariant distribution but of course if we start in this probability distribution row started random walk here and let's say we do two steps then this really corresponds to multiplying on the right by K squared right I mean one step if you start a row and do one step resulting distribution is given by multiplying on the right by K like a zero Prime if you multiply on the right by K again you'll get the distribution of vertices after two steps and you know that's the same as multiplying on the original distribution on the right by K squared okay so uh let me seem kind of believable let me say some more in symbols like I sign I sort of only explained to you how K squared acts on probability vectors well in general K squared which you know it's the same as K composed with K if you think of K is an operation how does it operate that operates as K squared applied to a function f and then evaluate it at vertex U it's the average of F values around like the sort of two neighbors of U meaning the W's that you get after doing a two step random walk from you again it seems hopefully it seems believable but just to get used to all of this notation I'll do a proof okay so give an F just so we hope to many symbols let's let G denote hey F and so therefore K squared f is like K applied to K F which is K G okay so K squared F yeah just kg lied to you and now let's sort of remember the definition of the operator K it just averages G's values over the neighbors of U so this is the average over V a neighbor of U of G of V my definition of K but now what is a G its KF so okay this is the expectation over V a neighbor of U of K F applied to you okay to be continued on the next pane okay and then we'll remember inside the expectation the definition of K again sorry that's the expectation over just looking good this should say V good so this is the expectation over W a neighbor of V of F of W okay now it's I mean we're basically done so put these expectations together we're first choosing a random neighbor B of U and then we're choosing a random never W of V and we're looking at the average of s values on W okay so I hope you'll therefore agree that like for all natural numbers T how does Kate the T operate k to the T applied to F evaluated at U is the expected value of x value on some W where W is formed from you by taking a T step walk okay so Katie's operation has this property I mean way to think about it is like you forty times you like replace each vertex by the average of its neighbors you do this a bunch of times sort of the F values like diffuse throughout the graph and this even holds for T equals zero right okay here I'm raising a matrix to the zeroth power so I get the identity matrix I I have f at you it's just F of U and indeed this is the average of s values over all use zero step random walk starting at you okay so this K will really be the thing we'll need to study when we're thinking about like how fast was a random walk in this graph mix how close how long does it take for a random walk that close to the invariant distribution and I'll indeed tell us about sparse cuts in the graph\", metadata={'source': 'OLD2lult4o8'}),\n",
       " Document(page_content=\"okay let's go back to AA now with this okay in our pockets let's go back to the most important topic of study in spectrum graph theory which is this quadratic form so we'll get back to this guy EMF there's a half average over edges the squared difference of s values okay and one of the first things I wrote on this board today was after like a tiny bit of calculation and you see that this is the same as the to norm of F squared or FC inner product with itself minus expected value on a random edge UV of f of u FMV it's actually some how we got started on this whole definition of K business okay so as we've now finally seen this is the same thing as f you know product F minus F inner product KF should equate these two things in your head this F goes to this after F goes to that F okay so now uh well inner products are linear so we can write this in this inner product of F with F minus KF and I'll now write f as the matrix the identity matrix times F that doesn't do anything and I know I can say this is the inner product of F with I minus K it's the difference of two matrices times F and having done this this allows me to introduce well I don't say the last piece of notation but one more major player and spectral graph theory this matrix I mean it's just a small tweak on K you just subtract K from the identity matrix but it has a special honor and glorified name L it's basically the laplacian of the graph actually this differs a little bit from some people's definitions may be that we call this the normalized laplacian so perhaps I'll call it that too what if you ever heard this discussion of like graph laplacian is like some key player and spectral graph theory well now you've you've met the object at hand so okay this L the identity matrix minus K is say the normalized laplace matrix or operator for g plus an operator and as always you should I mean unless you're a real sophisticate you should always go back and remember what's going on when G is a d regular graph the most basic case so the most basic cases for D regular G well we know that in that case L is the identity matrix minus K is just one of our D times any and let me just write this like this it's 1 over D times D times the identity minus a ok and this is object is the thing that is sometimes called the like unnormalized laplacian or some people just call it the laplacian ok so this is the matrix you get you take the matrix the adjacency matrix a and you subtract it from the times the identity matrix you put these on the diagonals and you put like minus ones where they're edges and this is all well and good it's like a fine thing to do and a great thing to study for regular graphs but then like if you read like text books about like irregular graphs that kind of hamon hall a little bit about laplacian in that case and sometimes you see some crazy formulas with like a capital D to the minus 1/2 matrix floating about everywhere and it gets a little messy so that's why I like doing it the way I've done it here and so for this class these will be our definitions okay so just to be clear you know this is a matrix or an operator so it takes vectors to vectors or functions to functions so we could say how does it operate if you have a function f and you apply the laplacian l to it you get a new function LF and what is LF svali well by definition LF at vertex u is well the identity applied to F which is just F at you so fu- the KF thing which is the expected value or average value on the neighbors of U of F of E okay so applying L to a function f is like you keep F but at each vertex you subtract the average of the neighbors any questions you should have a question you should be like why what what is why did you make this operator and I'll tell you the reason what is the meaning of this operator the meaning of this operator is it's that thing L such that inner product of F L F is this quadratic form okay so the point is we just love this quantity so much it has a lot of great meanings as we'll see that we're like hey this is a very convenient formula for it F inner product with L F where L is I don't know this thing so like this thing as is does not have like such an amazing meaning but it's nevertheless elevated to like a high and lofty position because of this formula so because we really care to study this we end up studying like this matrix L a lot yeah I don't really know who applause was really either other than a French mathematician or this also connects to some other aspects of physical reality involving like I don't know electrical networks and things but I don't know anything about physics great yeah so to me like this L is like that's the thing that gives you this formula e yeah you'll also hear this like this terminology is also the infinitesimally of the like continuous time random walk so you'll eventually see these formulas that look like e to the minus TL this is like the the continuous-time analogue of the standard random walk anyway let's leave that and think about this connection it's more a particular you know supposed to be a class about computer science so I'll sort of mention some kind of connection to a very famous and important problem in computer science and algorithms problem the sparse this cut problem home of finding the well sparse is caught in a graph so let's take a look at that let's again take our favorite scenario where we have a subset of vertices s and F is the indicator function for s ok so as we know well the inner product of F and LF this quadratic form of F which when you plug it into that formula it's the probability when you choose a random edge UV that U is in the set s and V is not in the sentence ok so this is sort of the right because you'll kick up a nonzero contribution here when u is in and V is out or vice versa okay and then this factor of 1/2 takes care of this directionality so this is a nice meaning it's like the fraction of edges that are going from inside us to outside us so the volume of the directed edges on the boundary of s and it's very nice to compare this to somehow the size of s which she liked to express this way as we saw earlier in this lecture the tuner of squaring up a for the inner product of F with itself expected value F of U squared which is what's the probability if you take a random you know that U is in s okay sometimes we call this the volume of s and what's going to be nice is actually to look at the ratio of these two quantities because if you look at the ratio of these two quantities then on the right you get a conditional probability it maybe I'll be more clear if I wrote V naught in s and u NS up here you see in this case the ratio in a product of F L F over in a product of F F it's the probability when you choose a uniformly random edge u to V that V is not in s conditioned on you being in s okay so it's like the probability if you pick a random you from the set s I mean with probability proportional to the degree and then you do one step that you get out of I mean this is terrible notation but that you get out of s okay there's a number between zero and one and you might think of it as like the like escaping probability for the set s so like imagine you're doing a random walk and I tell you at certain time that you're in the set s then this quantity represents the probability that you'll get out of s in the next step okay you can imagine that if s is a set where this ratio is small it's so piece of the graph or if you're doing the random walk you'll have a tendency to get stuck in this piece now if I tell you you're inside the set of vertices a so you're doing a random walk then the one step will be unlikely to get you outside s so I said s where this is quantity is small is like a bottleneck for random walks in this graph and it's also like a sparse cut doesn't sometimes call it's like a piece of the graph which is sparse in the sense that like there are very few edges coming out of its relative to its own size okay this quantity is sometimes called the conductance of F or sorry of s and sometimes donated capital denoted capital Phi of s okay and for example the the famous sparse is cut problem in algorithms theory it's just the task I give you a graph G find the set s that minimizes this quantity well it's not quite that because you could take s to be all the vertices and then the escape probability for all the vertices is zero so generally you want to look at you know if you have a set of vertices s you should sort of in the denominator put either the size of s or the size of compliment of s whichever is smaller that's like the fair way to compare or sometimes Sparsit Scott is just defined to be fine the set of vertices s a volume at most half that has the smallest conductance and there's a very important problem in algorithms especially for divide-and-conquer-type a Lim's on graphs you can imagine if you had an algorithm is great at finding a sparse is cut or a sparse cut in graphs it could be very useful for divide-and-conquer-type all girls right I mean you find a sparse cut sort of means a very high level you know there's very few edges coming out of s relative to its size maybe you can recursively solve some problem whatever you're interested in on s and on s complement and then maybe it's possible to somehow patch the two solutions together in a global solution to a global solution in a not too difficult way if there are not many edges connecting s in its complements that's the idea I mean one idea why this sparse is cut problem is important in algorithms and what's kind of shocking and distressing is well it's np-hard to find the sparse this cut but it's much worse than this in the sense that we may be happy with an algorithm that returns a pretty sparse cuts on a given graph maybe you have to find the sparse is cut so you might be interested in an algorithm that always found efficiently a set s whose conductance with within factor two of minimum or factor three of minimum or factor ten of minimum but for all these problems I just said there we don't know if they're in P and we don't know if they're np-hard which is very distressing situation to be in because we're supposed to this great theory of complexity theory and algorithms and to not know if something that's NP or NP hard is very bad so it's like famously noun that you can find a small set of smallest conducting self to a factor of square root log n in polynomial time or up to a square root of the value itself we'll see that next lecture hopefully but on the other hand like it's only known that to find a set that it minimizes the conductance up to a factor of one point zero zero zero zero zero zero one is np-hard or maybe it's not even np-hard it's like hard assuming NP is not in some exponential time or something yeah anyway so we know very little about the complexity of solving this problem yeah oh yeah we did this at some point but I guess you can see it sort of from this formula here if F is the indicator of a set s so s values are either 1 or 0 you pick a random edge if the two endpoints are both in the set or both out besides a set you'll get 0 here if one's in the set and once outside the set then the difference squared will be 1 and you'll count 1/2 but so it's sort of half the probability that a random edge has one vertex inside on one vertex outside or you can say that oh I want the first it's exactly the probability that the first vertex is inside the second vertex is outside great so that's a bit of algorithmic motivation and the Serio here is that we have this quantity that we sort of care about a lot and if F is the indicator of a set of vertices a zero one function and it's exactly equal to this ratio and this ratio gets studied a lot in linear algebra as you may know it's related to the eigenvalues of L and in fact there's a polynomial time algorithm for finding like the F that minimizes or maximizes this ratio trouble is it might not find a zero one valued function so you can't use that polynomial time algorithm to find the best smallest conductance s although again next time when we talk I hopefully I'll mention chiggers inequality it'll tell you some way to kind of overcome this but ah let's get into that I can value business now even if it's not necessarily immediately gonna help us solve this as far as this cut problem okay so let's get into questions about like minimizing or maximizing this thing which is related to minimizing and maximizing this quantity subject to you know some scaling condition about the FS it's gonna be a little bit easier to talk about it at first if we talk about maximizing this numerator even though in sparse is cut you're more interested in minimizing it somehow just be more convenient and well I basically want to do over the next or must the rest of this lecture is just talk about well the connections between this and eigen values so let's go back to this question about maximizing as curly e the quadratic form of F which we now know has this formula related to L F inner product L F which is this quantity half expectation you draw it UV an edge a few minus a B squared as your menu by the way like the this is I mean in the especially in the regular case this is like you know you have like the row version of f times L times the column version of emphasis the way the matrix vector wait to picture it at least this would be if you have the standard inner product okay so let's say we Richardson maximizing this and as you know we need some kind of scaling condition to make sense of this so subject to the two norm squared of F or F Gaynor product F or the expected value of F squared being one okay so a little bit of math talk here in subsets where I have a quantity we're trying to maximize which is a function of n real unknowns the N values of f is this quantity it's some quadratic quantity so it's a continuous function of these n unknowns and subject to this condition that these N quantities squared when averaged equals one this means that they all lie on an ellipsoid but anyway it's a compact set so for some math talk we're maximizing a continuous function on a compact sets and therefore by calculus a Maximizer exists so in this problem Maximizer exists and let's give it a name Phi okay so this is a function to norm squared one that maximizes this quantity and here's a claim I would like to make and we'll probably prove this claim this function Phi has a very funny property if you hit it with L and form L Phi you got a new vector which is parallel to Phi okay and this is alpha equals lambda times v for some L but for some lambda okay so well for people in the linear on who remember their linear L well this means that Phi is an eigen vector or sometimes called eigen function in this case because vector and function are synonymous for us I can a vector of the matrix L so prove this claim in a second but let's do some facts about it good so ah once we know this then we can say something about this number lambda this all-important quadratic form applied to this Maximizer Phi well it's by inter product elf I but this is just Phi in a product Lambda Phi and the inner product is linear so this is lambda times Phi Phi and that's lambda because the normalization condition is that this Norman umber is one okay so this Maximizer Phi has the property that when you hit with L do you get scalar multiple of Phi that scalar multiple is the actual quadratic form value for this Phi and it's the maximum one and we know actually buy some theorem we proved last time and I mentioned the beginning this lecture that this number is actually between zero and two and I want to make one more observation about it uh I mean fully do the proof but it's a easy fact to verify that this maximize will have another nice property which is that its average value across the vertices will be zero or another way of putting that is that the inner product of Phi with the constantly one function is zero I think it's the inner product of Phi with the constantly one function is this by definition or if your thing in geometric terms Phi as a vector is perpendicular to the constant one function at least with respect to this funny inner products and uh let me just say the proof in words if you don't fully get it then it's okay it's something like we talked about last time it's just that let's say you have some Maximizer five for this little program here and I mention its expectation is not zero some other value mu then imagine replacing Phi by Phi minus that expectation Phi minus a constant the constant being its expectation well I wonder if that does not change the the quadratic form applied to Phi because if you just subtract the constant from a function everywhere then it just cancels out here so it does not change this quadratic form but it's not hard to convince yourself if you take a function and subtractive it's mean from itself it makes the to norm go down I mean in fact if you strike the mean from itself it makes the two norm or the expected square become equal to the variance which is always less so subtracting a mean from the function doesn't change this but sort of like only improves this instead of in the sense of helping you in the sense of making the two norm squared smaller and then if it becomes small in one you can multiply it by a scalar multiple to make its expected squared go back up the one and make this bigger so I said that in a funny roundabout way but hopefully you more or less buy it but I just want to remember this easy to do fact that the maximize relative property that it's uh it makes a zero in our product with the all ones vector okay oh let me prove this claim I think we have time okay so why with the Maximizer five for this program have the property that alpha is parallel to Phi this proof I'll show you is basically like Lagrange multiples although I went on the garage multipliers although I won't really say that except just now for the claim okay so assume for the sake of contradiction that Phi and L Phi are not parallel oops okay not parallel okay so the picture and your head is like here's Phi here is so maybe L Phi okay and they're not parallel so you can drop a perpendicular here making alright and go and I'm gonna call sy the unit vector in this perpendicular direction okay so sigh is unit vector in this picture it's perpendicular direction okay it's that's not nonsensical by virtue of the fact that we're assuming they're not parallel okay so it's a well-defined direction okay now let's epsilon be a small nonzero number to be named later and we're going to consider F being Phi the putative maximizer plus a little bit of this orthogonal direction sy and we're going to get a contradiction to the optimality of Phi eventually so one thing that we can tell is that the squared length of this new F I'm considering it's basically one in fact it's exactly 1 plus epsilon squared by Pythagoras Farion theorem because Phi and Phi are orthogonal and now let's look at what you would get if you tried F up here well okay F and L F in inner product by definition it's five plus Epsilon sigh inner product with L Phi plus epsilon L side okay so let's break this up we get Phi and L Phi and then we get to epsilon inner product of Phi and L sigh so you actually get like Phi and ELPS I and then here we get like L Phi and PSI but these are actually the same because l is self-adjoint because k is self-adjoint which means you can move k back and forth across inner products and it doesn't change things and that's also true of the identity matrix I you can move it back and forth across in approximately changing things and so it's true of I minus K which is L good and then okay there's one more term which is epsilon psi inner products epsilon L sy I will just write that this is plus Big O of epsilon squared so I'll think of all these vectors Phi sy as fixed but epsilon is really going to zero so I don't know sy inner product L psi is some number but anyway it's multiplied by an epsilon squared okay so now if we look at this ratio F L F over in a product of F F well I just divided this number by the squared norm of F which is 1 plus Epsilon so basically just divided it by 1 so I basically didn't do anything I mean I did something but also only up to order epsilon squared so even this divided by 1 plus epsilon square this divided by 1 plus epsilon square it's only changing this by order epsilon squared yeah yeah yeah it's really I mean if you expand this out it's really you get one copy of Phi and epsilon L sy well I move the epsilon out here and you also get like one copy of sy and epsilon L Phi so let me move the epsilon is out but this this like long story I said in words I was claiming that this thing is equal to this thing which is also this thing okay this is just because you can switch the two elements in inner product this thing is subtle but it follows from this fact that I verified before that the matrix K and therefore all sorts of the matrix l is self-adjoint which basically means you can do this move so it's the if everything was at the regular case then L would be a symmetric matrix and then this would be obvious if you were to town yeah so I did that move a little bit fast okay so I guess what I'm saying is I I'd look at this ratio because if you have a general F whose two norm squared is not necessarily one then basically you know what you want to look at is the ratio of this to this okay because you can always scale F by a constant and make it achieve this quantity okay so if I replace F by F divided by square root of this then the F that I've made here would be achieving this much in the quadratic form but now the point is this was supposedly the maximum possible value you could achieve and the other point is this number is not zero by the picture oh I should have okay so this let me use that self a jointness again to say that this is L Phi and PSI so now it's like inner product between L Phi which is this thing and sigh which is this thing which by definition is not zero so this is a number which is not zero and so you can see like what regardless of whether this is positive or negative if you choose the sign of epsilon appropriately and make epsilon real small you can make this number get bigger than this number and this order epsilon squared will not screw that up if you make epsilon small enough so if you make up something really tiny then you can really neglect this but by choosing the sign of epsilon right you can make this whole number get bigger than the supposed of optimum so that's the end of that proof okay so ah great so I'm almost ready to put this all together into a statement on the key theorem called the spectral theorem which will finally get us to the eigenvalue eigenvector story which we can then study in the last spectral graph three lecture oh good so now I mean implicitly we found the Maximizer with some function or vector Phi it was parallel to L Phi and had this mean zero thing so now let's just resolve the maximization problem but disallow Phi as a solution and what I really mean is disallow all the vectors perpendicular vectors parallel to Phi so resolve the problem but maximize over the same conditions and also the condition of being orthogonal to Phi so now I want to say consider a similar thing a similar problem maximize F you know product LF subject to the same condition before F inner product F should be one but also F should be orthogonal to Phi the solution we just found let's sort of get it like a new solution well this is still the same quadratic function and this is still a compact set now it's like an ellipsoid intersected with like a hyperplane but okay it's still a compact set so it has some other solution so this you can repeat the whole same argument but I just did and you'll get some other like some new maximizing function five prime and you can do the same argument and it'll have the property that L Phi prime is again parallel to Phi prime for some lambda Prime and this lambda prime will again be like the maximal value achieved here so it would have to be at most lambda because lambda was like the real real Maximizer and this is like the new Maximizer when you disallow stuff parallel to Phi and it's also gonna have that its expectation is 0 same argument in other words its orthogonal to all ones vector as before and now you can do it again you can say oh I want to solve the same maximization problem subject to this two norm condition but I want to do it over the subspace of all things orthogonal to my first solution and orthogonal to my second solution kind of trying to find like a third like new solution and you'll get like a new lambda and it'll have all these properties and you can keep repeating doing this finding like the best Maximizer in the remaining subspace and they'll all be orthogonal to the all ones function slash vector so when you kind of get down to the end like the all ones direction will be the last direction left and actually if you think about it if you apply L to the all ones function remember L is the operator that takes a function and replaces the value of the function at a vertex u by its same value minus the average of its neighbors so if you do that to the all one's function everything gets replaced by one minus the average of its neighbors which is also one which is zero so L applied to all one's vector is the all zeros vector and this is sort of like the last Maximizer you're left with so sort of the last optimal value will just be 0 so I'll finally summarize everything that you got out of the arguments with the theorem just kind of like the base theorem in this area sometimes I think called the spectral theorem or maybe the spectral theorem is a theorem that's related to this but basically this is the main theorem so that's this give it an undirected G there exists orthonormal functions or vectors if you will these are your Phi's and these are going to be eigen vectors or eigenfunctions for L and we'll call them Phi 0 Phi 1 up to Phi and minus 1 this Phi n minus 1 is actually the Phi the like the first one so the indexing is a little bit backwards this is like the Maximizer is the second Maximizer this is gonna be the last Maximizer so in fact this Phi 0 is always going to be the constantly one function and they're orthonormal so this is great because these are n vectors and an n-dimensional space so they're like a they're like an orthonormal basis for the space you can express every function as a linear combination of these orthonormal functions and these are like the very special great super star orthonormal basis functions for the graph G because they're I Gonzaga vectors of the laplacian and also of K ah and real numbers associated real numbers lambda 0 lambda 1 up to lambda and minus 1 this loss first one is 0 this last one is at most 2 this is the the real optimizer optimizer just one second I should actually write this there in ascending order here that's the order okay such that well this last statement is just that they're Augen vectors which that L apply to Phii is land AI Phii okay so L has very nice action on Phi I just scales it okay so basically we've shown that in this nice setting of an undirected graph you have this like most enjoyable linear algebra situation like an orthonormal basis of eigen vectors and all the eigenvalues are non-negative okay so the next time we will see the glorious synthesis of eigenvalues and sparse this cut and other such things\", metadata={'source': 'eu_KruoOpLc'}),\n",
       " Document(page_content=\"so today we're gonna have our third lecture on spectral graph theory actually there'll be one more lecture kind of related to spectral graph theory when we talk about expander graphs in fact as a connection between spectral graph theory and D randomization so two topics that we've we'll have seen before on today's lecture I want to talk a little bit more about wolfenberg get to really talk about eigenvalues and eigenvectors hence the name spectral graph theory and i'll make some connections to this as far as this cut problem we talked about last time and conductance and then we'll talk about this question about how fast do random walks on the graph G get to the invariant distribution so I'll recap once again some of the things we some of the set up we have so we're always going to be sort of in the back of my lines having a fixed undirected graph G that we're studying and there's a very special probability distribution on the vertices of G namely the invariant distribution or stationary distribution which we denote by PI it's proportional to the PI of U is proportional to the degree of U and this is the uniform distribution and the notable special case where G is a director or a regular graph and it's also the distribution where if you have a random walk in a graph and it's attending to any distribution on vertices at all then it's this invariant distribution and for almost all graphs except for some annoying cases when the graph is disconnected or bipartite you have the property that if you start at any vertex and walk for a really long time then the distribution on vertices will tend eventually to this stationary distribution we also have this linear algebra setup so we have we think about we're always thinking about functions mapping vertices to real numbers which we can also identify with vectors of height and if there are n vertices and we have this special inner product for two functions which is kind of like the correlation between the two functions F and G or the average value of f of u times F of G sorry F of U and G of U but taken with you being drawn from the stationary distribution our favorite setting actually for functions once we truly care about our functions that have range 0 1 or indicator functions of subsets of vertices and for example if F is the indicator of a subset of vertices s then if we look at the inner product of F with itself which on one hand we sometimes following you know geometry convention is denote as the squared norm of F go ahead if we plug in the definition we see it's the expected value of F squared under this distribution pi values are 0 and 1 so the square is just the same as the on squared version so this turns into the probability that a random vertex drawn from the stationary distribution pi is in the set s so in the most normal case of a regular graph when PI is the uniform distribution this is just the fraction of vertices that are in S or generally it's proportional to the number of edges touching the vertices in s and it's sometimes called the volume of s and in the previous lecture motivated by this thing what we're going to study today the random walk on the graph we introduced this operator K a transition operator you can also think of it as a matrix so it transforms one function to another function and if you apply K to a function f it gives you a new function KF where the value of K F at a vertex U is like the average of F's values in used neighborhood so it's kind of like a little diffusion operator and we check that it had some nice property if again if the graph is a regular graph then k is nothing more than the adjacency matrix scaled by 1 over d the degree and in that case k is a symmetric matrix in general it's not a symmetric matrix but it does have this property that's greatly enjoyable it's related to symmetry called self-adjoint Ness says that the inner product of F and K G is the same as the inner product of K F and G and in either case you can easily check that they're both given by this more interpretable quantity it's expected value of F of U times G of V when u v is a random edge so you can sort of think of this as also sort of the meaning of the K operator as I said we were motivated to introduce the K operator because of thinking about random walk on graphs and remember when you think of this random walk on gras you should okay when you actually do it you're like tracking the location as time goes on of a particular vertex you're hopping from vertex to vertex but in a more abstract sense so you should really do is keep track of the probability distribution on where the vertex in the random walk will be as time goes on so even if you start at a deterministic location then even after step one you have a probability distribution on where you'll be in time one and a probability distribution on where you'll be in time two and so forth and we really want to keep track of these probability distributions the invariant distribution is has the special property that if you start your random walk and a vertex drawn from the invariant distribution then at each subsequent time step the location of your random walk will also be in the invariant distribution but it's more usual to start your random walk from let's say like a deterministic fixed vertex or some worst-case distribution row on vertices but as we saw some before it connects to this transition matrix K if you say okay what I'm going to do is I'm going to start my Verta of my random walks from a vertex drawn from this arbitrary probability distribution row and then I do two step T steps of the standard random walk or at each step I go to a random neighbor of where I am then the distribution of where I am at time T is given by Rho thought of as a row vector times the teeth power of the case k matrix finally with K in hand we introduced this one more matrix L it's sort of like a trivial twist on K it's just K subtracted from the identity matrix but the real reason we introduced it the normalized laplacian matrix or operator is because it has this property you know when you do the inner product of F with LF you get this key quantity that we love so much the quadratic form of F so the local variance of the function f across edges okay so the last thing we did was we proved a theorem that told us that the matrix L is a very enjoyable matrix from a linear algebra point of view there's these like freakish annoying you know matrices and linear algebra that have bad things like they don't have an orthonormal basis of eigenvectors that's gross but L doesn't this property L does have an orthonormal basis of eigenvectors and another like beautiful property in general Ligon values of a matrix can be complex numbers even if the matrix has real entries but this L has real eigenvalues which is also very pleasant and our theorem was this you know this this laplacian matrix L has an orthonormal basis of eigenvectors so there's some you know basis for all n dimensional vectors Phi 0 Phi 1 up to Phi n minus 1 and Phi 0 is a particularly simple vector it's the vector of all ones and it has they have associated eigen values lambda 0 lambda 1 up to lambda M minus 1 things are ordered by the magnitude of these eigen values and we verified that this lowest eigen value corresponding to the all ones vector is zero this is even another nice property all the eigen values of L are non-negative and we saw before the largest eigen value is at most 2 and it equals 2 if the graph is bipartite just to remind you from linear algebra or does it mean eigen value it means it's a function or a vector where when you multiply by L you just get a parallel copy of the vector you get a scalar version of Phi I scaled by the eigenvector lambda so I will tell you what is the merit of getting into all this eigenvalue and eigenvector stuff in just one moment but i want to make one more remark once we have this we can say something very concrete about the eigenvectors and eigenvalues of k does anybody know what one can say about that um anybody know what the eigenvectors for k are yeah there's the same they're also 5 0 4 through Phi n minus 1 and the eyes I gained victory is won - the I thought and value of lambda so this just comes from the fact that like K it's like triple e defined in terms of L and vice versa so this is K is identity - L and well let's just verify what's going on here you see since K is I minus L we see that well if you happen to consider what happens if I hear phi i with k well it's i minus L Phi I and this is gonna be easy this is the ID this is i Phi I which is just Phi I minus L Phi I and Phi is an eigenvector for L its eigen value is lambda I Y I so we get 1 minus lambda I by I so we're like oh great tada like K also is the Phi I also has the property that if you multiply it by K you just get a scaled copy of Phi I approves that Phi is an eigen vector and it's eigen value is 1 minus lambda so K also has a great you know it's equally delicious matrix it has an orthonormal basis of eigenvectors the same Phi's and talking values are just a 1 - all these things so they're between minus 1 and 1 which is also kind of pleasant I mean so that's very nice and all right so therefore ok has the same eigenvectors and I might also say eigenfunctions because we're identifying functions and vectors in this world and its eigenvalues which I'll call Kappa I r1 minus lambda I and we have the ordering that like a minus 1 is less than or equal to Kappa and minus less than or equal to kappa 1 less than or equal to Kappa 0 which is always 1 yep yeah this is the idea that we'll get to towards the end of the lecture that we're gonna want to show that unless he looks like some weirdo situation like a bipartite graph or a disconnected graph if you start at any vertex in a in your graph and do a center random walk you kind of the distribution where you are quickly gets to the stationary distribution and you know you can see from this flourish bullet point that's very much related to like what happens if you start with a vector and you multiply it by a high power of K and maybe you can already see I'll get into this soon I mean this is a reason why it's so great to have these eigen vectors and eigen values for Allan for K because as I'll mention shortly what does it mean that you have a basis it means that if you have any function at all such as Rho you can express it as a linear combination of these Phi's and then if you're interested like hey what happens when I multiply this row by K it's very easy if you know the eigen if you know the expression of your vector in terms of these Phi's because multiplication by K effectively just replaces Phi I by Kappa I times Phi I so if you do this over and over again you'll easily see how these coefficients in this linear combination change and that will help us understand how quickly this row kt turns into pi or something very close to pi so in fact let's introduce some notation for this this is not standard notation but I like it so given any vector or function okay so it's living in this n dimensional vector space we can and we have an orthonormal basis we can uniquely express it as a linear combination of the Phi's so the notation we need is just notation for the the coefficients so we'll just write this okay so we're gonna write f as like something times Phi 0 plus something times Phi 1 where these are you know real scalars and I like to write F hat 0 and F hat 1 up to F hat n minus 1 okay so these F hat i's are scalars one reason as i mentioned last class if you just tell this entire story but for the fixed graph G which is the hypercube graph then you exactly recover analysis of boolean functions and this F hat notation will make sense although you'll choose to index your files not by the number 0 through 2 to the N minus 1 but by these subsets of coordinates 1 through n ok so that's just a notation you know once we know that F every F can be expressed as a linear combination of the Phi is just giving names for the coefficients so let's do that thing that I was exactly talking about let's take this equation I'll call it star and let's ask ourselves the question you know suppose you know had some F and somehow I figured out these coefficients the F had eyes great and now I was interested in doing the same thing for LF you know I kind of wanted to understand what is the expansion of L F or K up I mean it's basically the same kinds of calculations well if we multiply this that I call it star star by L on both sides a vector equation there multiply on both sides this matrix L well we get the sort of expansion for lf-cc okay so we can everything is linear so we'll get this is a scalar F hat 0 will get L times pi 0 here what by definition L times Phi 0 is lambda 0 times Phi 0 we happen to know that's 0 actually but let's not necessarily remember that just yet and similarly think will happen for all the other you know I've had I if I I terms so all guess that this is lambda 0 times F hat 0 times Phi 0 plus lambda 1 times F hat 1 and Phi 1 plus stop I thought was lambda n minus 1 and minus 1 times this so once we I mean on one hand moving to this is like a little bit unnerving you had some nice I don't know vector F and now I'm making you think of it in this different basis but once you get over that like psychological hump you're very happy because like now if I'm interested in applying L or K or something to my function I can immediately in easily deduce what its expansion is in terms of this these basis vectors as I mentioned you know we also have this property that lambda 0 is 0 so like this term drops out if we want okay so you could say I mean if you want to write this in some funny symbols we could deduce that L F hat I is lambda I times F hi um another nice thing is this let's say we also had I should also mention you know now if you imagine like applying L to this again well this will go up to like lambda I squared and if you apply L tetons you'll just multiply all these coefficients by lambda I to the T that's gonna play a role when we start studying random walks we also know that other formula though say we have an another function G and we're like okay I'll write it as a linear combination of these Phi's well one thing we're always interested in when we have two functions F and G is their inner product and let's see what happens imagine we take this inner product of F and G and we plug in F 6 panshin and we plug in G's expansion so we have the inner product of two sums of N Things it's linear so you can like expend like the inner product is linear you can expand that all outs and you'll get like sum over I and J of F hat i G hat J times the inner product of Phi I and Phi J but now a good thing happens right at the Phi's are orthonormal which means precisely that this is one if I equals J and it's zero if I does not equal J so all the some ends where I differs from J dropout and when I equals J this just becomes one so we got this nice formula the inner product between F and G is sort of like the thought product of their coefficients in this basis this is really like you know the dot product formula for inner products in the standard inner product world it's also like this uh plantar all or parts of all formula we saw in the special case of analysis of boolean functions okay so it's another way in if you manage to know your functions F and G in this basis and you have an easy formula to figure out their inner product so let me write some corollaries of this fact that we will use some time put them on this board so we can save them and saw all these corollaries again in the analysis of boolean functions lecture but specialized to the hypercube case here's some corollaries one if you look at a squared length i EF seen a product with itself then we get that this is the sum over all the of the squares of these coefficients nice quarrel corollary is I think we're sometimes just interested in is the expected value of F or the average value of a function f well this is by definition the inner product of F with the constantly one function and the constantly one function is also no.50 so really this is already written in sort of its own expansion we have a coefficient of one on this guy and a coefficient of zero and all the other Phi's so by this formula this is just f hat zero okay so when you write F in this form you know one nice thing that happens this is really the constantly one function by the way and this is also nice this is the expected value of that okay so this is a constant function that's just constantly equal to F semion so this first piece the expansion is like the mean of F and like these are kind of the fluctuations of F from its mean it's a corollary of this sort of hence we get that the the variance of F of U is well the variance is the expected square that's this minus the square of the expectation which is the square of this so we're just taking all these terms and taking away the F hat zero squared term so this is the sum over all I strictly bigger than zero in other words not equal to zero f5 I Square and finally in the last nice formula which is going to be good to compare to this one this quadratic form that we care about okay we remember that's inter product of F and LF well here's F here's L F and now we just need to multiply their coefficients together point wise and sum them up so this we can see it's the sum over all the I's you can omit the zeroth one if you like of lambda I times F hat I swear so I know it's like a lot of formulas but they're gonna have some payoff just now in particular we're gonna get some payoff by comparing these two quantities\", metadata={'source': 'AZb2CFmFWVA'}),\n",
       " Document(page_content=\"so yeah it's time to relate these to convert Oriole properties of the graph G so let's get back to studying this thing that was called conductance which is kind of the conductance of the set was sort of the escape probability for that set in a random walk and we were particularly interested in the sparsit's cut problem which was roughly speaking to like find the set of minimum conductance this is sort of like the set that's sort of the worst bottleneck for the random walk in a graph and we observed that like the conductance of a set I'll remind you of the definitions in a bit but it was sort of like it was the ratio of this to this when F is the indicator of the sets or it's very close to that to some small twists and so we're interested in the minimizer of that overall functions F which are indicators of subsets that's a specific kind of function well we also considered this problem where we didn't insist that the functions f were zero one value you could just minimize overall F and let's take a look at that question suppose we are interested in minimizing oh this quadratic form of f subject to the condition that the variance of F is 1 so this is closely related to this conductance business when I claim by comparing these two formulas you can easily figure out what this minimum is can somebody say that's right basically which is the smallest that lambda that's maybe no longer up there its lambda yeah maybe I'll I'll add this fact that was on a previous port this is how we named our Atlantis but yeah it's the smallest lambda other than the lambda zero which is always zero which is by our convention lambda 1 so why is that well okay in this program like I'm allowing you to choose any F you want which basically means you can choose you can think of choosing s values or you can think of choosing FS coefficients in this basis and I let you choose these coefficients to be anything you want so basically these coefficients can just be like any old real numbers you want and that's equivalent to what you're minimizing over so you have this constraint that the variance is 1 if you use this formula that's saying okay the sum of the squares of F hat I excluding the zeroth F hat 0 is fixed to be 1 and what are you trying to do you're trying to minimize this weighted combination of the squares I've had I I guess it's more clear if I make the observation that I can also put greater than 0 here because lambda 0 is 0 okay so if you're the person trying to design a good F here or one thing you see is it doesn't even matter what you put for lambda 0 because it's not involved in the constraint then it's not involved in the thing you're trying to optimize that's also clear for another reason right I'm 2 0 sorry I've had 0 is the expectation of your function it's the average value of the F we know that if you take a function f and add a certain constant to it it neither changes the variance nor the quadratic form so monkeying around with the expectation doesn't make a difference so anyway with that in mind you're like well I may as well just play with the one through F hat n minus one I fixed the sum of squares b1 I'm trying to make this as small as possible well I should just put all my F hat I squared it on whichever is the smallest lambda which is lambda one so this is lambda one and it's achieved by five one it might possibly be achieved by a Phi two if lambda one happens to equal lambda two but that's the situation and I also want to remind you that this minimization is the same as this minimization minimize the ratio over all F okay because these quantities both have the property that if you multiply them by a constant they go up by the square of that constant okay if you want to be picky you should probably minimize over FS where the variance is not zero so maybe variance of F is not zero or F is not a constant function great so oh this is kind of good if you happen to be interested in this program of minimizing this subject to this or minimizing this ratio it's the answer is very easy answer lambda one done well what we're really interested in as I'll remind you in this sort of sparsit's cut problem is this minimization problem but restricted to functions f take on the value 0 1 and that actually turns this into an np-hard problem which is a shame but we'll see it's not so bad the value of the minimum among zero one functions is somewhat closely related to lambda 1 which is an easy to compute thing so let me remind you from last time if you have a subset s of vertices its conductance it's this 5s it's the probability if you take a random edge that V is outside s given that you as NS we can think of it as you pick a random vertex in s proportional to its degree and take a random step and see if that takes you out of s okay so this is like the sort of escape probability for s and we saw maybe in more than one lecture this is exactly given by this ratio well okay almost given by this ratio the numerator is to get this is the quadratic form the denominator we just had volume of s so we saw last time this quantity is exactly equal to this let me also mention that this volume is the expected value of the indicator of s and the Sportsters cut problem is to determine the minimum conductance set or and let's say it's conductance all right 5g or the minimum over all sets s of this conductance now I was written here this is slightly bogus it doesn't quite make sense because as I mentioned last time it's trivially zero because you can take s to be everything or you can also make it like undefined by taking us to be the empty set or something so you should the thing you should always really do as I mentioned last time if you have a set you know this is GE and you have some set of vertices ass oh the thing you really should put in the denominator here is either the volume of s or the volume of compliment of s whichever is smaller because somehow it's at the same edge boundary s NS compliment so let's it's maybe more natural to normalize by the smaller of the two so the way you normally see this written is oh it's among all s whose volume is not zero and also at most 1/2 and otherwise replace it by its complement but like there's another instead of doing this you can do another slight trick that gives you a slightly different definition which is going to be more convenient so instead what you can look at is the minimization of this quantity which this is the fraction of edges on the boundary of the set s these edges here and the denominator you can put the volume of s times the volume of its complement and you can basically minimize over all s here well it should not be the empty set or everything and one thing is these two quantities only differ by a factor of two at most two you know one of these is always smaller than the other one because you're always inserting here a number that's between zero and one of the denominators so it's only making the fraction go up well in the other hand if you're always picking the s whose volume is less than or equal to 1/2 then the volume of the complement is between 1/2 and one so the number you're dividing by is between 1/2 and one so it's only changing the ratio by a factor of two if you didn't quite catch that like just take my word for it that these two problems are basically the same and as I also mentioned last time we have no good algorithms at all for sparse is cut even finding this approximating this value up to a factor of a thousand we don't know any efficient algorithm to do it so like right now we're not interested in quibbling over factors of two ok so the reason I finally say all of this it's a little bit of like a boring arithmetic aside is if we look at this denominator now it's mu times 1 minus mu where mu is this thing the mean of the indicator or the volume of s and that's you know mu minus mu squared which is actually I claim this is the variance of the indicator of s because the variance of the indicator is expected square of this function which is the same as the expected edition of this function because at zero one valued which is mu minus the square of the expectation ok so I scribbled a lot here and maybe even catch it all but what I'm finally trying to say is this problem sparsa Scott finding the set of vertices with the smallest escape probability or the smallest conductance or like the worst bottleneck seemingly for random walks is so this thing this equality it's like now if I put the variance down here what I'm saying is it's exactly equal to or maybe up to a factor of two this program but when you're only minimizing over zero one functions okay so let me write that more clearly so I'm saying is this combinatorial quantity by G the size of the sparsit's cut or the skate probability of the least conducting set it's basically up to a factor of two given by restricting this minimization problem to zero one valued functions we're doing a minimization problem and saying oh I'm now ruling out some of your allowed possibilities you can only choose a certain kind of F so that means this minimization problems value is going to be smaller than the minimization problem where you restricted to only choose zero one functions so at long last what I'm trying to say is if you're interested in this the sparse is cut value a lower bound for it up to a factor of two is this very easy to compute quantity lambda one so that's great if you have a graph GE you can easily compute using linear algebra its eigenvalues and eigenvectors of L and so you can easily cope this number lambda one and this gives you a nice lower bound for the conductance of the graph if this lambda one turns out to be like a big number remember these numbers are between 0 & 2 if it turns out to be like a big number like point one then you're like oh great Phi G is at least point oh five so like if I for every set in the graph the probability is stepping out of that set in a random walk is like at least 0.05 so after constantly many steps you feel like you have a good chance of getting out of it whereas let's say you computed and lambda 1 turns out to be point zero zero zero zero zero zero there 1 and maybe you're not sure what to think you might worry that 5g is small but actually this is so far only just a lower bound on it but it turns out there's a nice result called chiggers inequality and it kind of gives the reverse inequality but up to some mildly disappointing square root so it says that the conductance of the graph is at most a constant I think the best constant is like root 2 or something but just remember that it's some universal constant times square root of lambda 1 so in some sense that actually qualitatively is nice it tells you that if lambda 1 is really small then Phi G is also really small now this small can get square root is so if lambda 1 is 0.0001 then maybe Phi G is like 0.01 or whatever the square root of that number but at least it tells you qualitatively this eigenvalue lambda 1 the sort of second smallest eigenvalue of l is like a reasonably good proxy for the minimum conductance in the graph now I also said before there's no efficient algorithm for like coming up with a factor 1000 approximation to this number which is a little bit in tension with the fact that I'm also telling you that lambda 1 is sort of qualitatively a good approximation for this number but it's just because like these numbers could be very very small so like the ratio between this and this could be is like large as 1 over the square root of lambda 1 itself but qualitatively this is alright and another nice aspect of it is this fact has kind of got a very simple constructive proof what do I mean by that basically you can effectively and efficiently turn Phi 1 the real valued function that achieves lambda 1 into a zero one function that achieves the Associated conductance let me just write this and be given any non-constant f achieving a good ratio of this quantity let's say at most lambda for example you could take f to be Phi 1 and lambda to be lambda 1 but actually just given any real valued function f that achieves a pretty good ratio there exists some s of this form you can either take all the vertices where f of U is at least let's say theta or all the vertices where f of U is most theta this is for some theta with conductance at most basically the square root of this lambda it's what I'm saying is let's say you find some real valued function that has a pretty good value for this ratio and you're like man I really wish this was a zero one function because then I'd have a set that had small conductance well you can basically try all thresholds theta and you say all the places all the vertices where f value is bigger than theta I'll put them in the set all the places where it's less than theta I'll put them out of the set and there'll be some value of theta for which that's a great strategy in the sense that you'll get a set whose conductance is no bounded by the root of lambda\", metadata={'source': 'e50MmIM345E'}),\n",
       " Document(page_content=\"okay so uh I really want to prove cheaters inequality but I will not have time so therefore it'll either be on homework or it'll be maybe some kind of like bonus video but it's not too hard to prove it's kind of entertaining thing in spectral graph theory okay so what I want to get to for the remainder of the lecture is this discussion of mixing time in the graph for the random walk in the graph and one thing that we've sort of seen now is that lambda 1 this smallest nonzero eigen value of L is like a good proxy for this minimum conductance in the graph the size of the minimum the conductance of the minimum conductance set so it sort of suggests that if lambda 1 is quote-unquote large like maybe like at least point one or something that there are no bad bottlenecks for the random walk and therefore you might hope that a random walk will as I say mix fast which means have the property that after a small number of steps the distribution of where you are is close to the stationary distribution so the idea of slope is that lambda 1 being unquote large should imply no fast mixing of random walks and this is basically true but there's like one slight annoyance it's basically to do with bipartite graphs so like a bipartite graph has this annoying even odd property right if you start on the left side of the vertices and you walk for an even number of steps you'll always be on the left vertices and odd number you always been the right so it may well be that the invariant distribution is uniform on all the vertices but you'll never mix there because you'll always for any fixed T you'll either be like always on the left always on the right that's like a very stupid annoying and it's basically the only things blocking this from being true so we'll see like a couple of like hacks to get around it but before we can quite say these hacks now is the time where it's more convenient to start studying Kay rather than L it's a trivial change because like L is I minus K and K is AI minus L but it's good to make this trivial change so let me now recall have this K the transition matrix has the same eigenvectors constantly one function Phi 0 through Phi n minus 1 and has eigenvalues a 1 which is Kappa I guess this is cap zero created equal to Kappa one greater equal to etcetera Kappa and minus one and because the largest possible value for L is I cannot only for L is to this largest possible value is as small as possible value is minus one and it's equality this is a good thing to remember if G is bipartite so indeed this annoying case arises this bipartite case arises when you have this last eigenvalue being minus 1 and in general negative eigenvalues are like super annoying so there's a hack to get around them but okay so this intuition or how we can express the same intuition or hope in terms of the Kappas with this relationship so like say that large mant like at least epsilon well then the new version of the hope would be if Kappa 1 is less than or equal to 1 minus Epsilon this would imply fast mixing ok so the hope is that like you know the largest eigen value of K is Kappa 0 it's always 1 the hope is that like ok if this then the second largest eigenvalue is like kind of far from 1 there's a big gap that should be equivalent to fact it's fixing yeah because of this bipartite stupidity it's not quite true but what is true is that the real condition or a correct condition is like all the eigenvalues in absolute value or all right most one minus Epsilon other than zero where Kappa 0 is always 1 so this one is actually true and like the point is like this being bounded away from one should lead to fast mixing but like the annoyances like if this is minus 1 your bipartite and you won't have fast mixing ok so you really need this guy to also be bounded away from minus 1 [Applause] so let me state up here on that I hope we'll get time to prove that makes this precise okay so you have some graph G as always and say you have this condition that when you look at the transition matrix which again in the regular case is just the jason c matrix divided by the degree D and you check out all its eigenvalues and they're all bounded away from one by like a noticeable amount Epsilon other than the trivial one which is always one then for any worst case starting distribution row on B if you do this first you draw the initial vertex from B and then you do a T step random walk so standard random walk and you make T big enough so and T is at least some constant even like 2 is fine but some constant times log n over epsilon then Rho T by which I mean the distribution of U T is and this looks all precise but I'm going to end with an imprecise statement very close to PI the stationary distribution and I will eventually make this very close to precise but what a thing is that as long as this epsilon is large think of it as that like at least 0.1 or 0.01 then this will be like Big O of log n so it says that you know there are n vertices in the graph you only have to take exponentially fewer steps in a random walk like order log n steps and you'll basically get to the stationary distribution or if it's a regular graph you'll basically get to the uniform distribution it's kind of interesting because let's say it's a graph of regularity three and has n vertices I'm trying to think while I'm talking about what the diameter of this graph could be the diameter being the distance between the two farthest apart points it the diameter can't be too small in a three regular graph or a four regular graph but this random walk makes this quite fast you just need order log n steps and you're kind of in the uniform distribution at that point we just make a really brief aside about the negative eigen value is okay or annoying maybe I'll just say this in words and you can read some exact details in the notes uh there's a hack to get around this like annoyance about bipartite graphs and negative eigen values of K and it's called the lazy random walk hack and what is the lazy random walk it's just a slight twist on the standard random walk I mean the lazy random walk and each time step you flip a coin with probably a half you take a random step like normal like along an edge and what's probably half you don't do anything you just stand still okay that's called lazy random walk and let's make some observations about it one it only affects like the like the running time of your your random walk by a factor of two an expectation okay so you know what the random walk the Santa random walk does in time T is kind of comparable to what a lazy random walk does in time to T because basically and half the rep watt steps and lazy around and walk are wasted otherwise it looks like you're just doing the standard random walk so that's good but what's also good is it like breaks this stupid even odd problem in bipartite graphs now because you don't necessarily take a step in each like an actual step in each time step if you have a bipartite graph it's fine there's no problem like half the time you'll stand still half the time you'll take a walk so there's no more like odd-even thing and now there's at least intuitively very much the possibility that like even for bipartite graphs with a worst case starting point or distribution you know relatively quickly the random walk will get to the stationary distribution the invariant distribution and indeed that's gonna be true so that's great another wonderful property of their lazy random walk is it's very easy to relate it to the standard one because I mean as a little exercise for you the transition matrix for the lazy random walk is like half times I plus a half times the original transition matrix seems pretty believable in given the the definition of it and in particular just in the same way we easily deduce the eigenvectors and values of K from L because it was just shifting by like I you can similarly do the same thing here so like like sort of Kappa I for like the lazy version is like 1/2 plus 1/2 Kappa I and that's kind of cool because the the Kappas are all between minus 1 and 1 but if you do this transformation 1/2 plus 1/2 times 2 it then they stay in there relative order but they just get like shifted like this and now they're all between 0 and 1 so that's cool you no longer have any negative eigenvalues which annoyed you it also doesn't change much the gap of the largest one from one if like Kappa one was 1 minus Epsilon and sort of capital AZ 1 will be 1 minus half Epsilon so like epsilon 1/2 Epsilon no big deal and then like once all your eigenvalues are positive you don't have to worry about these absolute values anymore and like it just matters what Capital One is or just matters what lambda 1 is so this is like an elegant trick for fixing some annoyances but anyway we're actually not gonna even use this trick we're just gonna go ahead and prove this theorem with the absolute values in here okay so I think we have time to prove it any questions though right now yeah there aren't any really unless you're like extremely I mean some people probably get like really extremely excited about like nailing the constant factors like and this here and elsewhere and in which case then okay you've maybe be unwilling to pay a factor of two and sometimes like you don't even need to like literally do it like you can just in many cases you can just like analyze the standard random walk conveniently by you can actually do the standard random walk but like analyze the lazy random walk and say well they're gonna be the same up to a factor of two but yeah if if for example if you have a bipartite graph then you better do this you can also I mean there's no particular reason to make this a half either like if you make you know there's probability of standing still like 1% you'll still break this even odd problem with the the bipartite graph yep part of me yeah yeah well that's exactly the there is weirdness with this super graph two connected graphs but it's actually not weirdness it's like correctness in the sense that like as we saw before like the disconnected graphs is really the case that's like that's like the ultimate bottleneck when you have two connected components like it's the ultimate sparsa Scott when you have this set s and there's literally zero at just going between it so like actually this is the sort of correctly capturing that situation as we saw even in the first lecture like I think or maybe the second one Kappa one equals one and the gap is zero if and only if the graph is disconnected so let's sort of then it's sort of exactly right like it's telling you with Capital One equals so this gap epsilon is zero you're like you're never gonna mix and like it makes perfect sense because it's disconnected so in fact this is like this kind of stuff is like a robusta fication of that fact like it's kind of telling you if Kappa Wan does not equal one you're connected that's true but like if it kind of does not equal one in a strong way like you're kind of far from one then you're kind of strongly connected like every there's no like even like very sparse partition and that's more related to cheekers inequality I guess but you know that's also connected to the mixing of random walks if your graph is a cycle which is to regular great question if you have a cycle graph which is too regular so here you can clearly see the random walk does not mix you don't get like awesome mixing time right because if you take a red and walk on the cycle it's too regular so in particular you know the uniform distribution is the stationary distribution what if you take sort you know from like one particular point on the cycle and start doing a random walk like you definitely not going to get to the uniform distribution until you've at least gone like n steps cuz like you don't even have a chance to get to here let alone have one over n probability mass here until you've taken n steps so in some sense that proves to you that the epsilon for this graph must be really small and indeed it is you can actually the cycle graph is like nice enough you can like calculate an explicit formula for these eigen values and this will be like I'll now embarrass myself by not remembering if it's 1 over N or 1 over N squared but I think it's proportional to 1 over and 1 minus 1 over N squared is that right somebody in the know ok let's say it's right yeah that's right because in fact that people who are in the now know that if you do a random walk on a cycle like N or in the path graph which is similar you have a chance to reach everywhere after n steps but you don't actually really mix until you take N squared steps yeah so that's a good example of a graph where you don't have a nice gap I should mention on the other hand if you have a graph of constant degree or usually a regular graph and this gap is large like at least a universal constant that's literally the definition of an expander graph so we'll have a lecture about expander graphs and I'll get into it more but like this is the definition of an expander graph like a graph where this epsilon is not little o of 1 okay so I want to prove this theorem and strangely we need to take like a little digression into one of my favorite topics but we won't degress far into it it's the topic of what's a good way to measure how the +1 probability distribution is from another one we can have a whole lecture on this but I don't think we will I'll just do a real brief version here so in this theorem you know we're gonna be analyzing for a given like Rho I row I let's say is the distribution of the random walk after I steps we're gonna want to know things about like what does it mean Rho I is close to PI and only just the right row here for any generic probability distribution Rho what does it mean for road to be close to PI there are many different ways you could quantify it I'm gonna pick one that's like not the most standard but it's going to be convenient you could say that raw and PI are close if Rho probability mass on you / PI's probability mass on you is close to 1 for all you I mean that's a reasonable way to say that Rho is similar to PI and that's still going to be the way we choose given row I'm gonna write this quantity as f of you you might write it as f sub Rho of you but I'm just gonna write f of U here okay and this F of U is a function mapping the vertices to the real numbers it's a non-negative function it's actually for like the math lovers you can think of it as like the density or relative density probability distribution with respect to this one and my closeness measure now I'm going to quantify what does it mean for rugby close to pi I'll say that the somehow the distance between rho and pi is somehow the average amount by which f deviates from the ideal value 1 well as as usual with these things like i don't care if it's positive or negative so i'm gonna square it because that's nice and i'm gonna take the expectation or average here with respect to pi so this is a bit funny but this is gonna be my definition of the distance you can see that it's going to be 0 if and only if f of U is always 1 which is if and only if Rho equals PI so that's nice this distance has a name it's called the chi-squared divergence or a chi squared distance between rho and pi but we'll just take it if you're a sophisticate you might already have a favorite way of measuring the distance between two probability distributions perhaps you like the total variation distance in that case you should prove as an exercise that this is at most this while the total variation distance isn't most the square root of this so particularly if this is small then so too is the total variation distance but if you don't know at elevation distances that's fine you can also relate this to the KL divergence and other measures of closeness okay but this is going to be our definition of closeness between Rho and PI and I want to make one more little observation this observation is that it's not related to this measure per se but for any probability distribution if you look at this F which is like this ratio thing its expectation is always won this expectation is as always with respect to pie and what is the proof of that I'll write it over here you just write out the formula and it proves itself which I'll do in a second but this is actually another reason why it's good fall F like a density if you know a probability density function right it is like a non-negative function that integrates to 1 that's what's we're saying here but you don't have to say those fancy words if you don't like but let me justify this fact the point is that the expectation of F of U just by definition well let me write the definition of expectation it's the sum over all u of Pi of U times this thing we're taking the expectation of well by definition that's row of U over PI of you hey you know these cancel out and we get sum over U of Rho of you but rows of probability distribution so this is one and another thing I want to point out is that this expectation of F is f hat 0 we're going to use that in our calculations that was always true and a corollary of this fact is now if we stare at this formula of expectation of the squared difference between f of U and one but I just showed you one is always the mean of this F so have the expected squared deviation of F from its mean so this is the variance of F that's pretty reasonable right this measure of distance is how much this quantity is varying from one if you will and that's kind of cool because we had a like a I in vector eigenvalue coefficient formula for this right this was the sum over all I except for zero of F hat I squared okay so any questions yep uh yeah you can generalize this to continuous probability distributions a row in pi one thing is uh you always have to take care that you know Rho never puts probability mass on a point that PI doesn't put probability mass on but one thing we checked a million years ago is our PI's always put nonzero probability mass on every vertex by virtue of having no isolated vertices okay so the idea for proving this theorem let me give myself a little bit space now that we did this calculation idea for proving this theorem is just to track this quantity the distance of the probability distribution that the vertex is in like Rho 0 Rho 1 Rho 2 up to Rho T using this formula this formula here and we'll just see that as you take steps this quantity goes down rapidly okay so proof idea well another aspect of the proof idea is instead of tracking Rho 0 Rho 1 through Rho T per se remember Rho I for me is the probability distribution of where you are after I steps the random walk assuming you started out in a distribution which I should have called Rho 0 and this should have said res 0 you should have gotten out of a for making that mistake earlier okay so instead of tracking these will instead track these like ratio functions these density functions and we want to argue that ft becomes very similar to what function I seen this is not I want to give the answer but it is the answer yes we wanted it to become very close to the constantly one function right because F being a constantly one function is saying that the associated rho is equal to pi so we want to say no matter what f 0 is like pretty soon ft will become the constantly one function by the way like I just know it's here whatever row you have the expectation will always be 1 okay so you want the function in that it's always going to mean 1 and you want like the variance to go down down down down down okay so say thanks for doing the proof now say we're given any arbitrary worst-case row 0 and therefore f 0 we can write its expansion in the eigen vectors and eigen value eigen vector basis for K and it always starts out with 1 I guess I no longer have it up there well kind of I mean the zeroth eigen vector is always Phi 0 and the Associated coefficient F hat 0 is equal to the mean of the function that's a simple formula but we know it's always going to be 1 so f 0 it will always look like 1 and then all the other stuff f hat 1 Phi 1 plus F hat 2 Phi 2 plus stuff I thought plus F hat n minus 1 Phi and minus 1 ok so whatever that is starting distribution row 0 is we look at the associated density where you basically divide the function by PI we look at its expansion and the eigen value basis it's going to start with 1 plus and then some linear combination of these other files okay so that's F zero which we don't have much control over but now we have F one so what is F 1 well any guesses how to express it in terms of F zero yeah you may know if you quit said it but you may guess correctly that F 1 is equal to K apply to F 0 this is the transition operator and I leave that to you as an exercise or you can read the surprisingly long chain of like seven in equal seven equality's which proves this fact I mean we saw last lecture that if you have a probability distribution like Row one and you think of it as a row vector it's equal to row zero times K on the right and like it almost pattern matches although these FS are actually row divided by pi and like we're multiplying on the left but anyway I assure you like a trivial calculation will justify this yeah yeah it's funny um it's because you have this divided by pie in there uh I'm wondering if I should just like sprint write the full proof it's kind of a bit long but yeah it's funny like I spent like a while try it on your piece of paper like I could not find like a extremely short proof of this it like took I don't know as I said like seven equality's or something it kind of uses in a weird way the the self-adjoint ness of K anyway at some exercise in the last ten minutes while I'm talking see what the shortest proof you can write for this but its own at least if you're used to this stuff which maybe you aren't but if you're used to this stuff this feels very believable like K is the operator that like it's like do one step in the random walk and ultimately it's trivial because you can just mechanically write down everything and you'll deduce this by this time I could have just written the proof but anyway great okay we'll see how much time I have maybe already thing anyway ah well the good news is let's say you believe this which please do because it's true then you're loving life because now you can just easily write a formula for f1 and you can easily write a formula for F 2 and so forth it's kind of we saw this before right we deduced from this that f1 is K times f0 I just hit each of these things by K well the constantly one function when hit by case stays the constantly one function because constantly one is an eigenvector for k with eigenvalue 1 so we keep this one and then the isotherm here it gets multiplied by Kappa I great and remember these Kappas are between minus 1 and 1 so in magnitude you shrunk all the other coefficients and of course you will therefore concluded by induction that ft is 1 plus Capital One to the T F hat 1 5 1 plus Kappa to the power of T times F hat 2 by 2 plus the dot Kappa n minus 1 to the TF hat n minus 1 5 and minus 1 and now a great thing has happened right like if these numbers let's say all these Kappas were strictly between minus 1 and 1 then you raise them the power of T there like exponentially fast going to 0 although now you think exactly about these issues you're like well if one of them is not one but it's point nine nine nine nine nine nine nine nine nine nine nine nine nine then you know that's the power of T doesn't go down to zero so fast indeed sort of the rate at which they all go down to zero is controlled by how far each of them is from being away from minus one and one and why is that relevant well remember our measure of how close you are to the stationary distribution this distance is given by the sum of the squares of the coefficients other than the first coefficient whoops okay and so therefore we can conclude that this notion of distance between the final well the final distribution roti and the limiting distribution pie the variance of K well of ft it is the sum is I goes from 1 to n minus 1 of Kappa I to the 2t F Phi I squared and now let's just upper bound this by its maximum these are all non-negative so that's valid this is upper bounded by the maximum of kappa i ^ 2t times the sum of the squares of these guys which is the sum of the squares in the original f so times the original distance okay in a particular of this quantity under our assumption that in the absolute value kappa i is where's our assumption here all these kappa pies are bounded away from 1 minus 1 by epsilon well this is at most 1 minus epsilon to the 2t or using our favorite approximation this is that most exponential in minus 2t Epsilon okay so as long as epsilon is big by making T simply big you know this factor goes down exponentially against the sort of original distance any questions about this there is sort of one remaining question I mean okay I didn't exactly pin this down and we didn't exactly pin this down either like what is this original distance how big could it be actually be arbitrarily big but let's now pass to the well let's found it so yet another exercise for you which is simple it uses convexity is if you ask yourself what is the worst possible starting distribution it's an exercise to show the worst possible starting your distribution will be some distribution of the form 100% probability distribution on a certain vertex it's pretty believable right like I'm not saying which vertex but I'm saying the worst starting distribution will be of the form for some particular vertex u0 put all your probability there that seem pretty believable that this is like the least mixed starting distribution right it's easy to prove using convexity so the worst in terms of making this quantity big Rho zero is of the form well Rho zero of u 0 equals 1 for some u0 vertex which means that the Associated F 0 is has 1 over pi u 0 if u equals u 0 and 0 else and therefore for this guy the sort of initial distance which is the variance under PI of this F 0 except most the expectation of f 0 squared this is under PI which is well this is a very simple function to compute the expect square of with probability pies of you zero you get one over pi of you zero squared and otherwise you get nothing so this is one over PI you zero okay so what it's saying is this quantity the worst thing can can be is this reciprocal so in fact I guess the the largest this could be is if you zero is like a degree one vertex in which case this would be like 1 over 2 m so I guess the largest this could be is 2m okay and so you see like T would need to be like log M over epsilon times a constant in order for this to become truly small or let's just reduce even to the the simplest case if G is let's say G is regular then this one over PI 0 by V 0 is this is gonna be like an over D ok it's at most n I guess it's not much different from saying it's almost 2 m okay and therefore if T is greater than on a lon and over epsilon then this distance of Rho T from PI by plugging that in this T into here multiplying it by this it's gonna be at most n to the minus 2 times and which is 1 over n ok so maybe this is like a notion of very close so in summary let's say if you have a regular graph and you start the random walk from any fixed vertex and after just logarithmic and an over epsilon many steps and by the way the diameter of the graph you know you know constant regular graph is always at least log n because if you're regularity is three you can reach up most three to two three of the T vertices and T steps well even at most two to the T vertices and T steps so to reach everything you need to take at least log n steps and this is saying that you're just you've been as long as this epsilon is at least a constant then so the graph is an expander order log n steps are enough to get you super close this distance being like 1 over n to the uniform distribution okay cool so that's as I said it's kind of it for spectral graph theory but we'll have a next lecture about expanders after spring break\", metadata={'source': 'hsi9j4MMYB4'}),\n",
       " Document(page_content=\"okay in this addendum to lecture 15 we're gonna talk about the proof of chiggers inequality it was originally proved actually by alone in 1986 based on earlier similar results by Dodds ook and alone Millman it's named after cheater due to a 1970 result of cheater where he proved I guess the analogous result but in the setting of manifolds rather than good old finite graphs so let me recap what I've already told you about chiggers inequality briefly so cheekers inequality is concerned with this quantity Phi sub G which is the minimum conductance of a graph G it's the least conductance Phi s of any subset s of vertices in the graph well as I mentioned in the lecture you have to restrict to sets s whose volume is not zero and at most a half okay remember that the conductance of a set s it sort of represents the escape probability for that set s in our standard random walk on the graph so if I said has small conductance it's a sort of a bottleneck for the random walk it's a set of vertices where it's hard to escape from okay so taking a little look here in more detail this is can also be this minimum conductance can also be written like this and when you put it in this form it's almost the same as this ratio as we saw so it's you know up to a factor of 1/2 so the the conductance involves you know the probability that step in the graph escapes s condition of being in s divided by this denominator which is like the the oops the volume of the set s okay and if you're assuming that the volume of the set s is between 0 and 1/2 then its complement has volume between 1/2 and 1 so if you were to throw in a factor of volume of compliment of s in the denominator it would only change things by a factor of 1/2 which is not going to concern us very much and what's nice when you have this volume of s times the volume of s complement the denominator that's equal to I guess mu times 1 minus mu where mu is the volume of s or you might say another way to write that is mu minus mu squared and when you see it in the form as I mentioned before there's also the variance of the indicator function of the set s okay so this conductance up to a factor of 2 or 1/2 is the minimum over all sets of vertices s and now once you have the s couple in the denominator you just have to rule out s being everything or nothing it's the minimum over all sets s other than the trivial sets everything and nothing of the ratio of the quadratic form of the indicator of s just up here to the variance of the indicator of s ok and that's interesting it's interesting to compare that to as we saw this quantity here which is the same minimization program the minimization except it's the minimization instead of over indicators of subsets you minimize over all functions all real valued functions F on the set of vertices again excluding the trivial case of constant functions which would make this denominator 0 and let me just tidy this up a little bit well Stice about that is this program here the minimum of the quadratic form of f to the variance of F over all non constant functions f as we saw is a very simple quantity related to Augen vectors and eigen values namely it's the second smallest eigen value of the laplacian of the graph known as lambda 1 the laplacian of a graph always trivially has 0 as its smallest eigen value lambda 1 is sort of the second smallest eigenvalue and we saw that in the previous lecture based on some sort of little calculation or some fourth set of formulas for the quadratic form of f in the variance of f in terms of the eigen values of the [Music] graph laplacian together with the coefficients and F's expansion in the eigenvector basis of the blossom okay so we saw given any F we had on one hand this formula here for the quadratic form of F it's a weighted combination of F squared coefficients and where this F squared coefficient is weighted by lambda I the I 5 in value and we also saw that the variance of a function f is simply the sum of the squared coefficients of F so if in both cases we actually exclude the zeroth coefficient the one that is the coefficient sitting next to the 0th Augen value vector of the laplacian which is always the constant function 1 and that's because the 0th eigen value is always 0 okay so long story short if you could minimize over any F which basically means you can make these squared coefficients whatever you want well if you're trying to minimize this expression you sort of move all the squared coefficients mass onto the smallest possible eigen value coefficient which by our ordering is lambda 1 okay so this is just a recap and it says that for this program here where you're minimizing over all real valued functions F the minimum is an easy to compute value it's lambda 1 it's the second smallest eigen value of L and the F that achieves this minimum is what we called Phi 1 it's the associated eigen vector so this program is easy to solve exactly and hopefully and this is what cheaters inequality tells us it will serve as a good proxy for this program where we're only trying to minimize over zero one functions F okay and it turns out that you know computing this minimum exactly is np-hard this is the sparse us cut problem for a given graph but cheekers inequalities sort of tells us that this minimization problem which is easy is a good proxy so let's take a look at cheaters' inequality precisely so here's chiggers inequality written out precisely as i've written it in the past it says we have a two-sided relationship between this sparse is cut value phi g the minimum conductance in the graph and this easy to compute like in value lambda one now one side of the inequality is called the trivial side or the easy side and it's this inequality here it basically just says that if you look at this minimization program here where you minimize over all zero one valued functions well you're minimizing over a smaller set than when you're minimizing over all real valued functions so the answer lambda one when you minimize over all real valued functions is bound to be at most the minimizer when you only minimize over zero one functions okay there's this factor of a half because of this factor a half messing around in the definition of conductance so that's the easy side of kicker's inequality and really cheaters and equality refers to this other side of the inequality this is the hard side or the mean side which actually says that lambda 1 is in some sense not too bad a proxy for the minimum conductance it says that the minimum conductance Phi G is that most some constant times square root of lambda 1 ok so qualitatively this is kind of nice this side of the inequality says that if the conductance is small then the lambda one eigenvalue must also be small and this side in equality says that conversely if lambda 1 is small then the conductance that must be also small now this side of the inequality is not as quantitatively a sharper comparison it's you know best possible actually up to the value of the constant but you know there's a square root involved but still qualitatively it says this fact that if lambda 1 is small if and only if the minimum conductance is small now in the there's you know various slight different proofs of chiggers inequality and depending on how careful you are you get a different constant factor here it's not too important to stress that about this constant I think in the best proof you can make this constant square root of 2 and I think in the proof I'm gonna give you the constant will be 4 but I didn't even exactly check that too carefully the point is just assume universal and in fact small a constant okay finally as I mentioned in the lecture cheaters and equality is sort of a little bit better than this if you think about the upper bound what is it telling you it tells you that if there's some function f that makes this minimization program small that achieves this a small ratio lambda one then there actually has to be a set with also I said a vertices s with small conductance okay where smaller gets changed up to the square root and cheaters at equalities proof is actually very constructive it's very well constructive as I say so as I mentioned we can phrase it as follows let's say you have just any real valued function f on the vertices and again it should not be constant should not have very non zero and let's imagine that it achieves a small value for this minimization program up here in other words let's imagine that it's quadratic form value this G of F is quite small compared to its variance and quite small means up to a Latin factor okay so in this hypothesis you can take f to be Phi 1 the eigen function or eigenvector associated with lambda 1 in which case you can take a lambda to be lambda 1 but you know you don't start with the absolute minimizer f any F that achieves like a small value you'll be able to turn into a set of vertices whose conductance achieves is you know qualitatively about a small okay so what the the stronger version of the constructor version says is you give me any F that makes this program small whose quadratic form is small compared to its variance and the theorem will give back to you a set of vertices s with small conductance so it's volume well between be between 0 and 1/2 is desired and its conductance will be at most some constant factor times square root of this should not say lambda 1 it should just say land it whatever this lambda is okay so whatever amount lambda your minimizer or your approximate minimizer f achieves you'll get a comparably small to square root conductance set and you know to get this statement up here you just take f to be minimizer which achieves the minimum value lambda 1 okay and normally that it's not just the set s exist there's actually a pretty straightforward way to derive it from f in fact the set s will either always be what I would call a super level set or a sub level set of F so what does that mean it means there will always be some value real value T such that now if you take this real valued function f and take all the vertices where F has value at least T and try that as your s that could potentially be like a set s of reasonably small conductance and the theorem in fact will show that they'll either be some there will always be some value of T such that either this idea taking all the vertices where s value is at least T or the opposite idea taking all the vertices where s value is at most T one of these two and for one value of T you'll get a you know instead of the desired small conductance okay so practically speaking you can just try all values of T which they're really only n plus one because f if their vertices you know f only takes on n values so there's only a sort of n plus one relevant you know intervals where T can be okay so this gives you a very nice in fact a constructive way of taking a function f achieving a small Raleigh quotient as this quotient between the quadratic form the variance is sometimes called and converting it to a set with small variance okay so now that I've recap the cheaters inequality let's go on to the proof and I should mention well let me say two things here one um this said I'm gonna use some shorthand for it I'm gonna write it as f greater than or equal to T okay I'm gonna use analogous you know notation like F less than or equal to T or F less than T its funny but this is going to denote a set of vertices give it a function f it's the set of all vertices u where f of u is at least T I'm also gonna not explicitly prove this in fact statement it'll just be sort of obvious from the proof that we're gonna construct this set given like the function f by taking either a super we'll set or a sublevel set okay so I'll leave that to you to verify as the proof proceeds okay so let me first before diving into the proof talk about the idea of the proof and the idea of the proof is kind of compared to like a kind of calculus situation if you will so I want to sort of imagine plotting F least F values so the vertical axis is gonna be the reals and the horizontal axis is gonna be V the vertices in the graph so this doesn't exactly make sense it's only an idea actually more or less makes sense if the graph G in question is a path graph okay in which case you know the vertices just look like a long path and that sort of looks like the real line sort of like this and let's imagine that we sort of plot F so values ah this okay I guess in this picture s values are all non-negative they don't have to be but anyway let's imagine f values look like this and now remember how does cheaters and equality work it's sort of you imagine that the input to the theorem is a function f which has a good ratio between as good ratio between its quadratic form and it's variance okay so let me write dagger for this inequality and you should think of the input the chiggers theorem as this function f which satisfies the dagger inequality in this box for like a good small value of lambda and you know the desired output is this set s we're trying to convert f to s such that s is conductance is comparably small good so imagine this is our sort of quote-unquote input function f what we're trying to convert it to this set s and this assumption this dagger assumption about F is some kind of you shouldn't even as like a Lipchitz condition on F or a smoothness condition on F remember this quadratic form of F is sort of like the average squared distance or difference between s values along a random edge okay so in this picture you know imagine it's particularly if you think of G is being the path graph it's kind of the average squared difference between F values and two neighboring vertices okay and so therefore you know to say that it's small as our dagger hypothesis does at least compared to the global variance is to say that X values doesn't change too much locally so it's sort of Lipschitz it's it's it moves smoothly it doesn't have wild spikes and now if you look at what our strategies are going to do or so strategy supposedly for finding a set s is to we're gonna pick some threshold T and take a super level set or sub level set so perhaps we'll take a super level set we'll take all those vertices where F's value is bigger than T and really this is a set of vertices so it's like maybe this set of vertices and also this set of vertices and we're trying to show that this set is also sort of has comparably small quadratic form and remember what is the the quadratic form of the indicator of a set s it's like the fractional boundary size is the fraction of edges that go from inside us to outside us and in this picture it's sort of just like the you know on this picture a set s is like a collection of intervals and so the boundary size is sort of just like the number of endpoints you know 1 2 3 4 okay so we're trying to show that this super level set or some level set idea for some value of the threshold T gives you a set that you know it's not like zillions of disjoint chunks that have like a lot of edge boundary size it's just you know one chunk that has like a small boundary size and perhaps you can imagine that comparably if you know F you know had were not smooth we're not kind of Lipschitz if FS uh you know a rolly quotient here or it's lambda we're very large things that means F is like an extremely Wiggly function and in that case you might imagine that this exactly the strategy of taking a level set will not work out so well then you would get like some you know huge chunk of disjoint SS and there'd be a very you know big boundary size to them okay so this is sort of the idea of the proof if you will and now we're gonna try to actually execute this proof of converting a function f with small lambda into a set s with small conductance okay before we really get into it uh I have to make sort of a small without loss of generality starting condition as we're going along with converting this function f into this set s there'll be a couple of stages where we'll say oh I wish I've had this like mildly convenient property like he's non-negative or some other such property and it's gonna be without loss of generality essentially that s f has these convenient properties so in fact let me uh make a definition for what these convenient properties for the proof are gonna be they're not really motivated yet but you know you'll see them as they come in so let me say that well let's say a function G on the vertices is quote-unquote convenient just for the purposes of this proof if two things hold 1g should be non-negative and two this one's a little bit funnier but we're gonna look at the support of G okay so the support of G is the set of vertices where G has a nonzero value and we actually want this to be kind of a lot of the vertices okay we don't want G to be like okay G's non-negative we don't want it to be like everywhere non-negative we actually want to have like a lot of zero values this will be convenient and specifically we'll request that the volume of this set in the regular case is the fraction of vertices where G has a nonzero value it's not that large almost a half okay so these are the two convenient properties of a G well of you know the input function f that we're going to try to convert to a set and the claim is that we can assume that argument function f is convenient you know losing a factor of what's 1/2 or too I suppose and our assumption dagger about F okay so what we're gonna do is you know given our input function f that satisfied dagger we're gonna play replace f with a closely related function which is a convenient and B house the same inequality except maybe up to a factor of two or something okay in this factor of two well you know enter into our final constant factor in our conclusion but I was being a bit chill about that anyway okay so how do we do this how do we make a function f convenient it's actually like a basically a simple trick of the type that's often used and again like calculus arguments and it'll go in two steps so here's the proof I suppose step one first translate F ie add a global constant s values so it's median sort of the value of F that it takes on half the time it's below the median and half the time it's above the median so its median is zero okay so precisely what I mean by this is the volume of the set of vertices where F value is positive should be at most 1/2 and also the volume of vertices where s value is negative should also be at most 1/2 okay so basically a range for 0 to be the value such that f is half the time less than zero or maybe less than half the time and half the time more than zero okay you can always achieve that by adding a global constant to F just to change its median around and why can you do this you can do this absolutely without loss of generality without changing our assumption about f that dagger inequality at all and that's simply because as we've seen several times the quadratic form of a function f and the variance of a function f our you know translation invariant if you add a global constant to a function f it doesn't change either of these two quantities okay so we can take a F we can make its median zero and it doesn't change anything at all about our assumption here this yellow rectangle assumption about F okay that's a good start and the other trick for this convenience is to split F into its positive part and its negative part and one of these two guys will be the convenient version of F so just what do I mean by this let's say again that I draw a picture of F's values somehow this is V this is r this is f so the so-called positive part of f is the function which it's zero whenever F is negative and otherwise it just equals F okay so this thing I'm trying to draw in yellow here this yellow is f positive okay so just to repeat this is a standard move f+ is you know you can write it as the max of f and zero and similarly we're gonna define f negative it's a slightly funnier I'll try to do it in like this green negative it's the same as f whenever F is negative sorry that's not right what you do is you you look at the function which is the same as f when f is negative and zero otherwise and then you take the negative of this so this is actually the screen is actually negative of F negative so really F negative is like you flip this up this this is this is like the real F negative okay not super rate but it's the maximum of negative F and zero okay so let me let me take this part out it's the it's the negative or the reflection of this green function this is a classic way to break up a function f and the point is that f is f plus minus F negative okay it's the difference of these two functions these two functions are both non-negative functions which is great because as I said one of these is are gonna be our convenient version of F so they're both non negative and F is the difference of them and what I want to argue to you is that one of these either F plus RF minus is be a good replacement for f it's gonna be convenient and it's gonna basically have the same inequality assumption as f up to maybe a factor of two on the parameter lambda okay so one thing about that is actually easy the fact that both of these are convenient so f plus or minus are both convenient and that's easy to see on one hand they're both non-negative that's great that's one of the properties of convenience and on the other hand you know this condition right here that the volume of points where F is positive and the volume points where F of negative are both almost 1/2 implies the other aspect of convenience can you see so let's take F Plus for example we know from this assumption that this region where F is strictly positive is has volume at most 1/2 or it takes up at most half of the vertices okay and this region where F is strictly positive is exactly the support of f plus okay that's the part where it's nonzero okay so this this condition here where is it this condition here exactly implies that f plus is convenient and similarly this condition exactly implies that F minus is convenient okay so either of these F plus RF minus is convenient and now let's just try to show that they both more or less satisfy the inequality that we were assuming about F so let's see that so recall that the quadratic form of F is 1/2 times the expected value of a random edge UV of f of u minus F of V squared so the local value averaged over a random edge of the squared difference between us values at the endpoints and what I want to claim is that claim is that the quadratic form applied to F plus he's at most this and actually I also wanna claim that the quadratic form of F minus is also at most this okay that's a good start because remember our starting assumption about F is that it has a small quadratic form compared to its variance so I'm gonna show that F + + f - have even smaller quadratic form and then I'm gonna show that one of them has a variance which is not much smaller than F so the thing about the quadratic form is easier let me just try to say it in words what's going on here you pick a random edge like maybe this edge maybe from here to here again pretending that the graph is the path graph and then in the quadratic form for F you look at the squared difference between F's values at these two points now let's let's consider the quadratic form applied to F + well it's gonna have the exact same expression but you know with pluses here and you know one of a few things can happen in this expression depending on what's going on with s values at U and V when you choose a random edge U and V if they're both positive like in this picture then you'll still get F of U minus F of V squared in your contribution to the quadratic form of F plus so nothing changes but other things could happen it could be that for a particular edge UV s values were both negative in that case this squared difference would be contributing positively to the quadratic form of F but in F Plus you just be getting zero minus zero squared so you'd be getting less in the quadratic form for F plus okay and similarly if you know UV is a random edge where F had one value positive one value negative okay it's contributing the squared difference of those values to quadratic form of f of F but in F Plus you know one of those values gets changed to zero so the squared difference is not even less on this edge okay so it's contributing less to the quadratic form of f plus and the same argument holds for F - so I hope this sort of justifies in words to you the claim the quadratic forms of f buffs and F - are only smaller than F okay I want to now make a similar but slightly harder claim for the variance let me claim that the variance of F is at most two times the variance of F plus plus the variance of F - okay I hope I got this claim right and the constant factor right I was just working this out before recording the video and I hope I didn't botch it the point is let's suppose I prove this claim then what does that mean I can bring this factor of 2 over here and then I'll get that variance of F Plus and variance of F - add up to at least half the variance of F and so one of them has to be at least 1/4 of the variance of F okay I suppose I can divide both sides by another factor of 2 and I'll get that the average of this variance in this variance is at least 1/4 variance of F one of them has to be at least the average and so what I conclude that either F plus or minus has variances at least 1/4 of the variance of F ok and so I'll pick that one to be my new convenient replacement for F it has at least quarter of the variance and it's a quadratic form is smaller okay which means that it satisfies this inequality here up to a factor of 4 left I'd say F hand side went down the right hand side went down by a factor of at most 4 so you know the lambda value changed by a factor event most 4 okay so I just need to justify this little claim and again I'm gonna kind of sketch that let me put all the factors back where they were [Music] so again similarly if you remember the variance of F has a very similar expression to the quadratic form of F but with it's the sort of the global variance so this is in fact equal to 1/2 which we don't need to worry about the expected value over just independent random U and V of the same thing F of U minus F of V squared okay sort of the the global average of f of U minus f of e squared when these are not necessarily coming from random edge okay we have a similar expression I mean the analogous expression for variance of F plus and various of F minus and I'll just talk this through in words we can make a similar argument to what we did before let's say we take a random U and V maybe this is somehow you this is somehow V and maybe F's value at U is up here F of U and this is f of E okay so for each pair U and V they contribute some quantity so I'm not negative amount to the variance of F and you say let's say they're both positive then in the analogous expression for F plus we'll be getting the same contribution because F Plus will have the same value at U and V if they're both positive and so they'll make the same contribution to F Plus on the other hand if they're both negative then you know it makes zero contribution to the variance of F Plus because F Plus would have values zero U and V but we make the same contribution to F minus okay so if we were to just add the variance of F plus and the variance of F minus this would be at least the variance of F if like U and V were always magically had this property that F for the same sign on them hey because for every pair UV it either contribute to F Plus as variance or F minus as variance the same amount that it did to experience so there's one last possibility right that forgiving U and V like maybe one of them is makes it F positive and one of them makes F negative so how much is this contributing to the variance of F it's contributing this height squared that's the contribution to the variance of F like a decent amount to at least the variance of F plus or the variance of F minus you know as contributing this amount squared to the variance of F Plus and this amount squared to the variance of F minus and at least one of these contributions is half the contribute to the overall variance so sort of the worst case is if you know this height were H and this height were like negative H and then the or the s values were plus or minus H and then the contribution to the variance of F would be 2h squared that would go into the variance of F and you'd only get contribution of H squared to the variance of F plus and minus okay so when you sum up the contributions the variance of F plus and minus you get 2h squared but FS variance gets for H squared and this accounts for the difference of the factor of two okay so I don't want to dwell on this too much more because it's really just without loss of generality warm-up for the real proof but finally take my word on it before it if you didn't follow all that but for either F Plus or F minus we have the same assumption and therefore we can assume that F has these two key properties convenience okay remember these two properties are that we have a non-negative function and the volume of the support isn't must-have okay great so now it's been a lot of warmup we'll finally get to what I you know really consider the proof of kickers inequality and it goes back to this idea I mentioned at the beginning of the proof uh and we're gonna need a lemma that crucial lemma this in fact has a fancy fancy name in calculus it's sometimes called the co area formula in analysis but it's pretty simple in the context of graphs let's assume you have a convenient function G actually we don't even need convenient we just need it to be non-negative okay so we have some function on the vertices that's non-negative that's gonna be convenient but I will say it's not negative then okay this is the content the integral from zero to infinity of the probability of a random edge UV but this UV I'll say crosses gee greater than T DT let me write the rest of the lemon I'll explain this scary-lookin quantity is equal to the expected value over a random edge of G of U minus G of V in absolute value okay that's not super clear but I don't want to make sure I didn't go off the side of the screen let me rewrite this a little bit so I could value absolute value G of U minus G of V okay so the right-hand side is not too complicated it's the it's sort of like the local variance of G but when we use this one norm instead of squared to norm like we do in the definition of the quadratic form okay it's expected absolute value the difference okay what's going on with this scary left-hand side okay first of all what does it mean so we're picking a random edge UV and we're going out the probability it crosses this thing now remember this thing G granted and T is a set of vertices right it's the super level set T it's all those vertices where G has a value bigger than T so that's some set s maybe this is your whole graph this is uh you know G's values kind of go up like this and this is the super level set G greater than T to say that UV crosses it it's just to say that this edge UV has one vertex inside and one vertex outside okay you could go either way so G's value is less than T on sorry it's greater than T on you or V and it's less than or equal to T on the other one okay so really it was probably here and then we're like integrating it over all T there's a bit of a math trick you can kind of mentally think of this you know an integral is kind of like you know summation or an average you can kind of think of this is like the average crossing probability when you average over T not quite but that's a good mental picture so sort of like the average crossing picture probability when you pick a random T not really but sort of and it's saying that's equal to this kind of local variance of G which makes sense you know the the more G varies the faster it shoots around you could imagine that you have like a higher probability that a random edge will have one value on one side of T and you'll have another value uh she's only the other point will be on the other side of T okay and also to just try to make this look a little bit less scary remember a homework exercise or a well known fact is that if you have a non-negative random variable Y and we have this a simple fact you can compute its expectation as the integral from 0 to infinity of the probability that Y is at least T DT okay that was a homework exercise as the formulae may recall and this left hand side is you know it's doing something very similar to this fact okay so a bit of a math trick okay so this is gonna be the key lemma and so let's prove it it's quite straightforward prove okay well let's just look at this expression I'll just rewrite it it's the integral from 0 to infinity the probability of our random edge saying in words that T is between G of X oops G of U and G of V DT and in fact let's do a classic trick of replacing a probability by the expectation of an indicator so I'm going to do that using erasing I'll change this to the expected value and then I'll take the expected value of this thing the indicator random variable for the event that T is between GU and GB okay so it's a funny thing we're like integrating over T and so at this point t is like a fixed real number and now we're choosing a random U and V and saying oh let's look at the event that GU + GV straddled this number T well as always in these tricks and in this proof that the expected value of y is the integral of this whatever or whenever in life you have like a double sum you should exchange them so an interval is like a sum an expectation is like a song you should always exchange them when you see them I don't even need to check my notes to see that that was the next move it had to be so let's exchange them this is equal to the expected value of our random edge UV of this integral integral from 0 to infinity of the indicator T is between G U and G V DT ok now let's think about this so we have sort of now at this point in the picture we have U and V fixed and G and u G of U and G of V are some numbers so let's imagine this is the real line maybe we've picked U and V and G of U is this number and G of E is this number and now we're integrating this zero one indicator dot T this is the T axis if you will the T is between GU and GB so we're integrating this function this step function this is this indicator function here this is 1 so we're integrating this function the indicator that the parameter T is between GU and GV ok so the integral is just the area of this right it's about height one and its value is the absolute this rates width is the absolute value of the difference between G U and G be great so this quantity is equal to the expected value of our random edge UV this rectangles area is as I said it's just GU minus G V in absolute value okay and that's exactly what we claimed it would be up here okay so that's the end of that proof okay so this little trick this co area lemma sort of lets you relate like the probability that a random edge will be on the boundary of this super level set G of T to some kind of notion of the variance of G as a function okay so the last step is we're actually almost kind of done except that you know this is the variance quote-unquote variance but where you use absolute values and we wish that it were the variance when used squares because you know that's the actual variance of G okay so we have two steps ago one step is to you know both these two quantities are sort of almost what we care about one first step is actually going to be relating this quantity here to exactly the conductance of super level sets of G and the second step is gonna be relate this to like an actual variance with squares of G okay so let's try to do that so let's start with a corollary of this Co area formula it's going to be that well this right hand side that we just saw this sort of thing that's almost like the variance of G is at least maybe two times the conductance of G times the expectation of G okay remember choosing non-negative functions this expectation is non-negative quantity okay and this is the part where we're gonna relate okay on the right hand side we have here we have the left hand side in our corollary so we're gonna be relating this scary integral thing to the conductance okay so um what's the proof okay so what we know is this quantity here is exactly equal to something by the lemma it's equal to an integral from 0 to infinity DT of some quantity and what was that quantity well let's look back at it it's this quantity and actually this is precisely the sort of edge boundary size of the super level set G greater than T remember G greater than T is a subset of vertices essentially twice the edge boundary size because in our definition of edge boundary or our definition of like the quadratic form of an indicator we only counted like sort of the directed edges going out and this probability were we're counting UV as crossing if it crosses either out or in okay so what I'm trying to say is if you you know put the definitions in appropriate appropriately this thing we're integrating is literally two times the quadratic form of this G greater than T okay remember this G greater than T is like the well here I mean the the the the indicator function for those points where G is at least T are greater than T okay so now we use we relate this to conductance so okay well we want to say as we know that for every set of vertices in the graph they're edge boundary size is at least the conductance of the set times sort of the volume of the set okay that's the definition of the minimum conductance is Phi G and if I do have some value it means that every set in the graph a volume between zero and a half has edge boundary size the least Phi Z times the sets volume okay so what I want to say is this is at least this quantity here the edge boundary size or twice around resize of the set G greater than T is at least I'll keep the factor 2 Phi G times the volume of G greater than T DT okay and this is literally by the definition of Phi G it's like the the number which sorry it's yeah it's the least possible ratio of this edge boundary size to this volume size okay actually there's a slight catch here right in the definition of Phi G you're only allowed to make this assertion when the set itself has volume at most 1/2 right the definition of the conductance you only look at sets of vertices whose volume is at most half and say that they have kind of conductance at most we're sorry at least 5 G but luckily that's great this is where convenience comes in because the set of points where G is greater than T its volume is certainly at least the volume of sorry it's at most the volume of G greater than zero okay it's uh G's value is bigger than T it's bigger than zero and we know by convenience that this volume is at most 1/2 this is convenience okay so here we're assuming G is convenient okay and so that's just so that we can make this deduction here great so let me erase that okay good so now let's just write this definition of volume slightly differently this to integral from zero to infinity actually I can bring the fight Jie out as well to 5g it's just a constant the volume is the probability for random you that it's in this super level set G greater than T just really like saying that G of U is greater than T and now actually if you remember that basic probability inequality I was I was reminding you of this thing is exactly the expectation of G of U the expectation of a non-negative random variable which G is by convenience is exactly this integral of the probability over T of the probability that the random variable is bigger than T so G of U is the random variable here where u is drawn from the distribution PI as always okay and that actually also completes this proof see what we got was this quantity is at least two times Phi of G times the expectation of G great and so let's just rearrange this okay so a further conclusion is the following assuming G is not constantly zero we deduce I'll just put Phi G on one side and divide by expectation of G okay I can divide by expectation of G it's nonzero because G itself is not constantly zero I got the five G is at most half I suppose times a ratio in the numerator I have expectation over a random edge UV absolute value of G of U minus G of V okay and denominator we just have expectation over U of G of U okay and you see uh this is looking quite good if you recall way back to our original goal our original goal was to get a set s which is gonna be a super level set the has conductance which is somehow comparable to lambda and what is lambda it's the ratio of this quadratic form of F to the variance of F and now way down here where we are we've shown that 5g is that must some other ratio that looks suspiciously like the quadratic form of G divided by the variance of G but not quite somehow it's like there's some squares missing there were just some squares in there would look extremely good and so a bit of a twist ending here we're gonna apply all this work we've done to conclude this not with G being F but with G being F squared okay so like plot twist apply this with G being F squared okay so an actually there's a few things to check to make sure that this is legal first of all okay we're assuming that G is not constantly zero that's equivalent to assuming F squared is not constantly zero which is assuming that F is not constantly zero which is part of our assumption we're assuming this F originally had nonzero variance so it's not constantly zero that's fine next all along you're imagining that og is supposed to be the convenient function so now we have to check that F squared is can convenient now originally I showed you it was without loss of generality to assume that F is convenient but now I've done a little bit and switch and I want to say that oh I want F squared to be convenient but actually that's fine I mean we just have to look at the definition of convenience to see that it's fine s squared is actually always non-negative so that was one part of the definition of convenience whereas convene here at so that squares always non-negative that's super easy and here we just need to show for the other definition of convenience we have to show that F Squared's kind of often zero the volume of the place where f squared is strictly bigger than zero is at most a half but the places the vertices where f squared is strictly bigger than zero are exactly the same as the places where F is strictly bigger than zero once we make F convenient as we did at the beginnings we made F convene at the beginning so it's not negative so it's either zero or it's strictly greater than 0 and the by convenience the places where it was strictly greater than zero it was that most half of all the volume and now we're looking at the places where F squared is strictly greater than zero it's the same as the places where F is strictly greater than zero so therefore they said also has volume at most 1/2 okay so great I've pulled this little switch and I showed you that you could assume F is convenient now I'm telling you O therefore actually F squared is convenient as well so all these reasonings that we've done about G we can apply with G being F squared great so we get that the minimum conductance of G up to a factor of 1/2 it's actually even working in our favor but again I'm ignoring these constant factors it's given by this ratio and we can replace G with F sorry s squared ok so let me just rewrite that Phi of G isn't one of us to have expectation you drawn from V of all right like this F squared of U it's a bit funny to write it like that but I'll write it like that minus F squared of V in absolute value over expected value over U of F ok I'll write the normal way here at F of U squared ok and now it's looking much better the numerator is almost like the variance except this years are like sort of in the wrong place we want FPU - f sorry it's almost like the quadratic form except we want like f u minus FV all squared not absolute value of difference of squares but now it's just like simple math tricks okay so let's do the simple math tricks the numerator here is expected value very random edge UV absolute value of f squared u minus f squared V okay let me write the normal way actually f u squared myself of V squared [Music] okay now I'm gonna use that a squared minus B squared is a minus B times a plus B that's a fact we all know put absolute values on both sides so we turn everything here into absolute values okay and I'll apply this here because we have the difference in absolute value of two squared things so this is equal to expected value over a random edge UV of absolute value of f of u minus F of V times absolute value of f of u plus f of E okay great and uh whenever you have a an expectation of a product of two things and you don't know what to do do cauchy-schwarz so this what we're gonna do koshy Schwartz so if we Koshi Schwartz we got this is at most square root expected value over a random edge UV of okay first term gives us sort of exactly what we're gonna want square of an absolute value we can drop the absolute values we get f of U minus F of V squared and that's gonna be glorious on the other side we have expectation over a random edge UV F of U plus F of V squared that's a little funky but we're gonna show that it's you know not too big it's not too big compared to the overall variance or squared value of F okay so this quantity is a dream because it's exactly by definition two times the quadratic form F okay and finally finally finally we can use our assumption about F dagger which told us that the quadratic form of F is at most lambda times the variance of F so hooray we can finally use our only assumption in the proof this is that most two times lambda times the variance of F okay and so that's good we've got the variance on the right hand side here it's been Square rooted [Music] in fact we can even say that this is at most two times lambda times the expected value of F squared the variance is almost is always at most the expectation of the square I think that might lose something but we arranged for F to have median zero which is similar to mean zero anyway so it doesn't really lose much anyway it's true over here were just gonna do something trivial here's another very trivial fact a plus B squared is at most two times a squared plus B squared okay this is uh you kind of wish it were a squared plus B squared that's true it's not permanent up to a factor of two I mean the suppose the difference of the two sides is this is equivalent to a squared minus 2a B plus B squared which is a minus B squared I think I got that right okay so that proves this little fact okay so we'll plug that in here again with a being f of U and B being F of E ok and we got that this thing is almost two times expectation UV F of U squared plus F of V squared okay and that's kind of cool because now linear of expectation tells us remember here u AV was a random edge but you know isolation U has the distribution of pi and V also has the distribution of PI as we know so this is equal to exactly equal to four times the expectation over a random U of F of U squared okay which is actually the same thing I mean we got expect the value of F of squared in both places here okay so finally or pretty much done here putting everything together we conclude that this is that most square root of two times lambda times expected value of f squared times square root of four times expected value of F squared okay great so this is I don't know 2 root 2 times root lambda so this is where the root lambda comes in times uh just expected value of f squared okay so this was our upper bound on the numerator and take a look tada our denominator is also expected value of f squared okay so this whole star which is half the ratio of numerator and Nader is you know therefore star isn't most half the ratio of the numerator denominator it's 2 root 2 root lambda expected value of f squared over expected value of f squared this denominator is the denominator this all cancels out and we indeed get some constant I suppose root 2 times root lambda but our final constant is not root 2 because of that without loss of generality convenience factor at the beginning well it just turns it into some other constant and we're done right so we we just bounded finally the conductance the minimum conductance 5g by some constant times root lambda and let me finally just say I'll let you take one last look at the statement of the theorem here right this was our final statement bounding 5g by a constant times root lambda we said give me any function f that's non constant that has this bound on its quadratic form as a function of its variance then we'll pick out this set s with small conductance and in fact we managed to do that if you recall how our set s eventually arose well you have to look back through the proof but you know the key inequality came from here this is some sort of average over T we showed this average over T was effectively at most something and therefore there exists a value of T where it's values that lost something okay so maybe I'll leave that last deduction of the proof to you but the form of the proof actually shows that this constructive method of finding a small conductance set s given a function f with small rally quotient it follows directly from the proof methodology\", metadata={'source': 'R9fbkHEgiBw'}),\n",
       " Document(page_content=\"Oh, woah! Who are you? I'm Julie from 4 months in the future. Actually? Are you here to tell me what's \\ngoing to happen? Yeah. Yeah I mean... because of the, the butterfly effect I can only really give you, um, some loose details, but we'll go through the basics, yeah. OK. Cool! So do you want the good news? or the b-b-bad news? Oh. Um. Good, good news...? Oh, great choice. Yeah. \\nCool cool cool. Um... so. Things have never been better for climate change Well, that's great! Dolphins in the Venice canals. Really? I know. I know. Well, its see, I saw it on Facebook, I don't know if it's a real thing. My aunt posted it, but it seems pretty legit. Oh. Ok well that's incredible. You know especially given the Australian Wildfires. The wha... Oh yeah. 'Cause I mean I think those are gonna be the defining feature of 2020. Yeah, you'd think... Oh no? Not even a little bit. Really?! Wait, because they're...they're a pretty big deal. Yeah, your definition of a pretty big deal is gonna change. \\nFor sure. Wow. Ok, so what is the \\nbad news then? You are gonna want to pull all your investments. What? Yeah. Just get everything out of the stock market. Blurgh. (Whispers) Ohhh, it's a recession. You know what? \\nPut a little money in Zoom. Is that a conferencing app? Yes. Trust me. Ok... While we're being proactive here, actually, you could just do a Costco run real quick. It's gonna save you a lot of hassle. Costco? Do you have any hobbies? You know just something to keep you busy? Um.... I... No. Not really. (Gasps) You should get a dog! I want a dog! You know I want a dog I just... they're a lot of work. You gotta walk and go outside with them every day. Right, the walks are gonna be clutch. Right, but, I mean, I have to leave them because I have so much travel \\ncoming up. (laughing) (more laughing) Oh, no, you don't. Well, yeah. I travel for work. Oh you won't be working! But how do I afford stuff? Do you take vitamins or juice? No! Ok! It might be a good time to just, you know, get that body in tiptop shape. Get get get strong. Get those lungs a-pumpin'. You know. Cardio. A little aerobics. What the hell is gonna happen?! Look, I know. It all sounds scary, but... it's really not that bad. You know. For you! Right. I mean for some people it's blurgh It's not. But for YOU it's really not bad. Um, I would, you know. Would you rather be a busy \\nshopping mall or at home on the couch \\nwatching Netflix? Uh... couch? Exactly. You're gonna be just fine. Ok... What do you know about the world \\nof big cats?\", metadata={'source': 'Ms7capx4Cb8'}),\n",
       " Document(page_content=\"hi welcome back this is lecture 16 of the CS theory toolkit course this lecture is about expander' graphs so expander' graphs what are they well in fact we mentioned them a little bit in the last lecture these are essentially graphs that are highly connected yet which have not too many edges so they have lots of applications in computer science theory they're almost ubiquitous this resource I've mentioned here is a wonderful survey about them tells you most everything you need to know should also mention that when I was a graduate student I found expander' graphs really mysterious and I didn't feel like I understood them very well so hopefully I'll get the chance to do you mystify them for you a little bit ok so there is not one precise universal definition of expander' graphs it's a little bit more of a concept so we can make precise definitions but let's start out a little bit more at a high level so roughly speaking water expander graphs so an expander is generally means a graph which have three properties first the graph should be highly connected okay so it shouldn't be a disconnected graph and it shouldn't you know have some vertices that are you know just dangling off with only one connection to the rest of the graph every set of vertices should be well connected to all the other vertices in some sense we'll be more precise shortly the second property of an expander graph is that it's sparse meaning not too many edges so you can sort of get the ultimate super connected graph if you take the complete graph which has on the order of N squared vertices in an N vertex graph but when people talk about expander graphs they usually insists that the graphs be sparse most ideally they should have only a linear number of edges Big O of n edges okay so this is a bit of in tension with the idea that all the vertices are well connected to each other it's in tension with having not too many edges ok so these are two key properties of expanders but there's one more property typically when people talk about expanders they want to use them in applications and we'll talk about applications today to err correcting codes and D randomization and almost always in these applications what you really want is an explicit graph so why do I say explicit well that's in contrast with a random graph so as I'll mention surely if you just choose a random graph with n vertices and you know Big O of n edges with high probability it will be highly connected so that seems to be like a good expander graph but what people are talking about expander graphs they don't like these randomized constructions they usually want an explicit deterministically created graph okay so those are the three high-level components of expander graphs and we can be more precise and we will but let's talk about them a little bit more in turn I'm actually going to start with property two here sparse because it's the easiest one to talk about so what does sparse mean well I should mention that of winning the context of expander graphs will be talking about N vertex graphs and as always in computer science theory usually you don't just want you know one graph or an expander graph on a thousand vertices generally you want to talk about families of graphs drops on larger and larger number of vertices you want to say you have an expander graph family if you have you know a sequence of graphs on a larger and larger number of vertices n which is somehow sparse and each graph is somehow sparse and highly connected so what I mean by that in more detail is the following we're going to focus our attention only on regular graphs for simplicity that's typical in the study of expander graphs as we saw on the spectral graph theory lectures everything is 50% more pleasant and simple if you focus on regular graphs so we will so we'll be talking about D regular graphs and the sparsity will refer to the fact that will typically insist that d is an absolute constant which I can write as order 1 and this is with respect to n going to infinity ok so you should really think of D is like a universal constant like 3 + or 10 or b-but you fix and then you're gonna consider larger and larger graphs each of which is you know D regular so particular if you have a D regular and vertex graph it means the number of edges is well I guess it's uh D times n over two we write it like this D over 2 times n ok so what means this you have an N vertex graph the number of edges is a fixed constant D over 2 times n so as I said before you know we think of these as a graph with a linear number of edges okay so that's sparse um take that off let me talk a little bit now about highly connected ok so what do I mean by highly connected well again there's a few ways you could quantify this one way we could quantify it in fact is using this notion of minimum conductance from the previous lectures so we looked at their Phi G this was the minimum conductance this was the minimum over all sets of vertices s whose size is nonzero and we look at up to half the number of vertices in our set s of the conductance of s and what was the conductance of s it was the fraction of edges like UV with you in s and V outside s divided by the fractional size of s the fraction of vertices in s ok so this was some measure of in the regular case if you were taking it gonna take a random vertex from the set s and take one random step from it what is the probability that step would take you outside us ok so it's a the conductance you know could also be called the expansion of the set s you know the probability you get outside the set s when you take one random step from one random vertex in s okay so it's a number between zero and one and you know the has good expansion if this number is big and this phi g the minimum conductance of the graph looks at the minimum of this expansion over all sets of vertices s of size between 0 and half the vertices so here's a picture this is the graph G and this is a grass set of vertices s okay if you're interested in sense of size bigger than n over 2 then you should really look at the compliment sets and we're sort of looking at the fraction of edges that like do this that are on the boundary of s divided by the number of vertices that are in s themselves okay we want this to be large and one way to say that the graph is highly connected or a good expander is just to say that the condition might be Phi of G is at least 0.01 okay or you can you know fix some absolute constant epsilon that's bigger than 0 like 0.01 or 0.1 and it's just up this minimum conductance be at least epsilon that would be an epsilon expander well no and what I want to say is it's best to not necessarily be too picky at this stage at the exact definition of expansion I should have added by the way that the idea here is you fix an absolute constant again like 0.01 and then you want larger and larger families of graphs that all have this conductance being at least this absolute constant independent event okay let me go back to what I was saying you know it's not best to be not too picky about the exact definition so here we kind of looked at the number of edges on the boundary of s divided by the number of vertices in s let's call edge expansion but it's also reasonable to look at the number of boundary vertices for s 4s let me make that oops / you know the number of vertices in s so a boundary vertex is just a vertex that has at least one edge into s okay it's not quite the same concept as boundary edges because like a vertex on the boundary of s could have more than one edge going into s but it's pretty related because we're focusing on D regular graphs where D is constant so the number of edges touching each vertex is almost a constant so it can change these ratios by at most D so you know hoping that this concept this quantity the fraction of vertices let me just make it white you know it's at least some absolute constant Epsilon it's another reasonable way to define expansion one more thing that's a being a bit flexible you see it's sometimes harder to have good expansion the bigger the set S is you know if you have the expansion of like one the edge expansion of one vertex it's always 1 which is as good as possible because the probability of escaping from that vertex is 1/2 percent in a random walk unless you have self loops which we're gonna ignore also here but the larger the set is maybe the harder it is to expand so maybe sometimes you're a little bit flexible on insisting that this size of the set go all the way up to 1/2 n maybe you only want sometimes you only want expansion of sets ass with I don't know s at most 0.01 and again where this is some fixed constant okay so these are a notions of highly connected will make some of them even more precise later so now so far you know I've said that we want graphs or arbitrarily large graphs large large and graphs that are sparse you know every vertex has a constant number of neighbors it's D regular and which are highly connected and it's not too hard to show that random graphs have this property [Music] it's actually just a simple application of the probabilistic method especially if you're not trying to get too picky about the parameters so this was an idea due to Pinsker in 73 you can show something like the following fix any D that's at least three and then there will exist some you know let's say epsilon 0 which depends on D and well let's just call it epsilon and it's strictly positive so if D is 3 maybe it's point zero zero zero one D is 10 maybe it's 0.001 let's like get not too excited about the parameters such that for all and greater than or equal to some constant and 0 which also depends on D a random and vertex D regular graph is like an epsilon expander so let's just say has this minimum conductance or minimum expansion at least epsilon so that that is a theorem that's not too hard to prove it's sort of a probabilistic method and it's mildly easier to prove when you're talking about bipartite expanders which I'll talk about shortly but anyway it's still not hard to prove you know for every set s that you're worried about expansion you also consider every small set T which might unfortunately have the property that it captures all of SS edges but you show that a random D regular graph that's very rare and you can it's so rare that you can even Union bound over all pairs of sets s and T ok so this simple theorem shows that arbitrarily large expander' graphs exist even for degree D as small as 3 by the way you cannot make the degree much smaller than 3 well any smaller than 3 if you take a random to regular graph like what's a to regular graph well it's just a collection of cycles so if you take a random to regular graph first of all it probably won't even be connected and even if like by a miracle it's connected it's like one giant Hamiltonian cycle this has very bad expansion because if you take say half of the vertices for your I set s then you know they have a linear number of vertices but only two edges on the boundary so their conductance is proportional to one over n okay but three as soon as you have a three regular graph you have the opportunity to be an expander and in fact you are if you choose the graph with high probability which leads me to the final desert Arata mm-hm which is explicit so having these sparse highly interconnected graphs is great for applications but often you don't want just a random graph you don't want just the existence of a highly connected sparse graph you really want you know you want to say like here's my n you deterministically give me a graph on n vertices that's a great expander so yeah let me say here explicit means that the graphs let's say adjacency matrix or adjacency list G on n vertices should be constructible in deterministic polynomial and end time okay so yeah it's not the just to randomize existence proof you can really you know get your hands on an invert tax expander graph and deterministic Polly time and that's quite useful for applications in fact believe it or not there's even stronger notion of explicitness which will be crucial for some applications such as applications in D randomization let me say what that is so to say that some graph families are strongly explicit basically means that in deterministic polynomial time you can construct a graph on two to the N vertices which is a good expander now you have to think a little bit of what that means how can you in polynomial time create a or describe a graph on two to the N vertices well you describe it implicitly you could describe it by an algorithm that can compute let's say the adjacency list situation in polynomial time so what this really means is given n you can have an algorithm that takes us input the name of a vertex u and a number between 1 and D representing like the index into a neighbor and outputs you want your deterministic algorithm the neighbor of you in the graph in poly log in time and it should be deterministic okay so graph family is strongly explicit if you have some algorithm that it's sort of like the algorithm is I'm thinking of a graph on exponentially many vertices well let's write this as a capital n think of capital n is extremely large the algorithm can take in the name of a vertex which requires only log of capital n bits to write down Plus this you know number between 1 and 3 of D is 3 that's not a big deal and it tells you the name of the eigth of the vertex u which is also a log capital n bit string it does it all in poly log poly of log in time okay so in some sense it means you have access to an exponentially large graph even though you're a polynomial time algorithm okay so what are we gonna do in this lecture can make of this a little bit more explicit and we're gonna first actually talk about some applications of expander' graphs without yet knowing that explicit good expander' graphs exist and then after that I'll show you some things about the art and science of constructing explicit expander' graphs so before I get into the applications I want to mention like one completely concrete kind of an expander graph that will be useful for the applications and this kind of expander graph will be a bipartite twist on what I've talked about so far so let me give you the picture here this is sort of a bit of a bipartite expansion twist so in the bipartite expanders I'll talk about you have a bipartite graph with one vertex set you and the other vertex set V ok you can think of them as both having n vertices and to be a good expander is similar but we're only going to talk about left expansion expansion from left to right and regularity from left to right so we're going to be looking at graphs that are left regular so each vertex on the Left has degree D and to be a good expander will be to say that for every vertices s here on the left side as long as you're at most I don't know a constant fraction of you the size of you when you look at the neighborhood so I this diagram is a little confusing but if you look at this which is the I'll call the neighborhood of s that's all the vertices that have on the right which have a neighbor back in s to be a goodbye party expander is to say that the cardinality of the neighborhood of s there's at least some you know epsilon fraction of the cardinality of s okay so if you have a set s in principle the neighborhood set could be of size D times cardinality of s if like every single vertex in s went to D completely disjoint neighbors but won't be that great we'll just hope that the neighborhood set of every set s is at least a constant fraction of the size of s for all sets s of size some constant fraction up to a constant fraction of the vertex size of view oh boy actually I've said this room what I really want is oh no I said it right no problem never mind great so this is like a slight twist on expansion with a similar concept and I bring it up it's a little knowing that to bring it up but you know it'd be easier just to focus on the first you know notion of expansions for regular generic graphs and conductance but this will be the version that's like convenient to explain the applications so let me even focus on one explicit set of parameters so that we can have it on hand when we talk about applications and here's the picture that you should have so actually I think a person called basa LIGO showed that my part expander bipartite expanders of the type about to mention exist if you pick them randomly so a in the parameter setting there we have n vertices here and n vertices here and we're looking expansion we have d regular graph we're looking at expansion from left to right and we can fix any D to be at least 64 and pass along I showed that for any D that's at least 64 the size of the neighborhood set of s is at least 0.8 times D times the size of s for all sets s of cardinality at most point 0 2 over D times and ok so let me just a comment on this the it says you can have D any fixed constant that's at least 64 great and this amount of expansion is quite strong as I said before we have a set s on the left it expands to the right the maximum possible number of vertices in the neighborhood set is d times cardinality of s so this expansion is great it's 80 percent of that and it even works for all sets s of size up to some you know constant small constant point two divided by D times n and this divided by D is also necessary because if you're insisting that the expansion be like a constant times D times the size you know there are only n vertices on the right-hand side here so up to constants you can't do better than this ok so as I said I'll so long ago back in the 70s actually show that with these specific parameters random bipartite graph or just for every vertex on the left here you pick d random neighbors for it on the right has these properties in fact for application reasons I'm even going to mention that you can have a slightly smaller size set of vertices V on the right this is you this is V for the application I want to show I want the size of the vertex set on the right to be a little bit smaller than the size of the vertex set on the left so I'm going to pick it to be size 3/4 N and notice that that makes my life even harder if I want to expand here because the left hand the right hand side is now smaller there's like less space for the right hand side vertex vertices to expand into okay so it's even it's even cooler if I could make the right hand side 3/4 N and great I think if I did the calculations on my paper right earlier today Wasel ago show that a random graph with all these properties well a random gravity regular graph has this property with high probability when the vertex sides are sizes RN and 3/4 in and if I got those numbers a little bit wrong you know don't quote this lecture for exact numbers so this is the kind of bipartite expander graph that I'm going to show you can be used to get to cool applications one application in coding theory and one application in D randomization and for both of these applications you know basa Legos result that a random graph has these properties is not good enough particularly for the D randomization application like by definition if your task is D randomization you don't want to do anything randomized and also for efficiency and practicality in coding theory you also don't want like random codes you want explicit codes so that's sort of motivating you know the desire for explicit bipartite x pantographs like this and one might ask okay well do we know how to explicitly get these kinds of bipartite expanders that I tell you are so useful for applications and roughly the story is as follows roughly thanks to you know technology developed over the last a couple decades for I would say for almost all expander variants you know bipartite not bipartite edge expansion vertex expansion we know explicit constructions that almost match what randomized constructions get almost match parameters of randomized constructions okay so I put almost in quotes there and you know you kind of have to check the literature if you want really you know strong clothes matching of your favorite particular expander variant but you know generally we have very good explicit expanders and I'll talk a little bit about how we get them the last part of this lecture well let me just mention that for this explicit case where I told you I'm going to show you some applications okay we don't have explicit expanders that achieve exactly these parameters but it comes close so we have what I will tell you is we do have explicit constructions where let's say D equals 64 and instead of this 3/4 maybe you have 0.9999 times n and instead of this point O 2 maybe you have 0.0001 but that's it should notice that I did not change this point 8 we can really get the number of point 8 there you can really get any fixed constant that's less than 1 and from one of the applications it'll actually be important in that point eight is bigger than 0.75 but okay these two other constants you know they're not so important they make some of the parameters worse but it'll be fine for us okay so we just just to say we do have explicit constructions of these kind of bipartite expander graphs which are going to be good for these two applications I'll show you in coding theory and D randomization\", metadata={'source': '4mVRXBqOJIo'}),\n",
       " Document(page_content=\"okay so let's move to application number one of having these nice bipartite explicit expander graphs so let me just remind you again the expander graph the bipartite expander graph that we're gonna assume we have for the first application and this first application will be in the theory of error correcting codes so once again we're going to imagine we have we can deterministically construct in polynomial time an expander graph with two sets of vertices and we a bipartite expander graph on the left hand side we'll have some n vertices and on the right hand side will have slightly fewer vertices I'll call this set of vertices R and here we had 3/4 n vertices and let's also fix the degree to be the minimum amount that was allowed which if you recall was 64 so we'll assume that every vertex on the left has degree 64 and the vertices on the right let me draw them here we don't necessarily know anything about their degrees okay so we're only assuming this is a left regular bipartite graph and what is the condition on the what is the expansion condition on this graph that we know the condition is follows let's assume we have any subset of vertices over here s and s can be any set of vertices as long as it's not too big it also cannot be the empty set so let's assume that the cardinality of s is not 0 and it's at most well some small fraction let's say 0.2 divided by the degree which is fixed to be 64 times n ok so for any subset of vertices like that what we're going to do is we're going to look at all the vertices that they're connected to on the right-hand side we'll call that the neighborhood of s and of s all the vertices on the right-hand side connected to vertex s from the left hand side and the expansion condition is that this neighborhood set is quite large always so its cardinality while the most it could possibly be is cardinality of s times 60 for the degree times 64 and what we're gonna the the condition of the expansion condition we assume is that it's a good chunk of this it's at least 80% of this 0.8 times the degrees 64 times cardinality of s okay so uh just to remind you it's now known through the art of deterministic construction of expander graphs that we can deterministic lis efficiently construct bipartite graphs that look like this maybe this point o2 is a bit smaller and maybe this 3/4 is a bit smaller but something like this these parameters that can definitely be achieved at least with a random graph okay and now let's talk about application number one so application number one of these kinds of graphs are a two good error correcting codes so let me remind you briefly about error correcting codes so we're gonna be constructing err cracking codes which a map some they're gonna be binary code so they'll be using zeros and ones that we've mapped some string of length K to a string of length the bigger string of length N and what we want is for it to be good one thing we want is that n is not too much bigger than K so ideally for quote good codes you want and to be just a constant factor bigger than K so not too much redundancy and the other desert autumn that we want for a quote unquote good code is that it has a very good minimum distance so any two code words should be far at least as far apart as some absolute constant times n so constant factor relative distance which means you can always correct the number of errors that's up to half the minimum distance so this will mean if you have a good code that you can correct a constant fraction of errors in the N bit received word and what we'll see is essentially as soon as you have these expander graphs these parameters you immediately have a good error correcting code which is quite hard to find in general furthermore will actually see not just is it give you a good iraq tracking code of course it's an explicit one efficiently constructible and will in fact have extremely efficient decoding algorithms so that it's another beautiful feature of these expander based codes okay so before I get into saying exactly why these bipartite expanders give you good codes let me just prove to you one small fact about such bipartite graphs I'll call this claim one so we have two claims in this little story and it's the following suppose you have any set of vertices s which has this you know condition that it's not the empty set and it's at most you know 0.02 over 64 and here I refer s should be a subset of the left vertices L then what I want to say is it has at least one vertex V in its neighborhood set which is a so-called unique neighbor ok so what do I mean by that what I mean by that is just that it V which is a vertex in R has at most one neighbor back in s ok so let me draw you a little picture maybe here's s maybe here's some vertex V and perhaps this is the unique neighbor for s or a unique neighbor for s again what does that mean well V has some edges back to L and the property of being a unique neighbor we know has at least one edge to a vertex in s because V is a neighbor of the set s maybe being a unique aber means it has exactly one ok so let's prove this little claim and the way we'll prove it is well let's assume for the sake of contradiction that every V in the neighborhood of s has at least two neighbors back in s okay so it has no unique neighbor what we're gonna do is now um count the number of edges touching s so on one hand since we have just decided that every vertex V in the neighborhood of s has at least two neighbors in s that means sort of each of these vertices V in neighborhood of s you know contributes or donates or you know it's associated to two edges two distinct edges which touch s so by virtue of this assumption we conclude whoa by virtue of this assumption we conclude that there are at least well so many edges touching s and what is this so many well I just told you there's at least two for every vertex in the neighborhood of s but by assumption over here we know that this is at least two times 0.8 times the degree sixty-four times the cardinality of s and the key point here is that two times 0.8 is bigger than one okay nothing more special than that so this is let's say strictly greater than 64 times the cardinality of s but this is indeed a contradiction why is it a contradiction well I mean this bipartite graph has its left regular with degree 64 on the left so the maximum number of edges that could be touching s is sixty-four times cardinality of s so that's a contradiction and it completes the proof of this little claim okay great so let's put that claim in the bag we'll need it for our analysis and now I'm ready to tell you how to get an ear correcting a good air correcting code out of this bipartite expander graph and the idea is really quite simple I'm going to define the code it's a binary code of block length n by specifying that it's parity check matrix H it's basically the adjacency matrix of the graph well not quite the adjacency matrix I don't literally mean view the graph as a full graph in the adjacency matrix I kind of need the bipartite adjacency matrix you'll see what I mean in a second so what I'm saying here is this parity check matrix for the code H it's going to be in F 2 to the R cross L it's a little bit backwards but R cross L so this parity check matrix H will look like this it'll be indexed by the rows sorry it'll be indexed by the right-hand vertex set R and the columns will be indexed by the left-hand vertex set L remember this is 3/4 n vertices and this is n vertices ok here's the picture back up here so the rows will be indexed by the right-hand vertex side the columns will be indexed by the left-hand vertex side and right so we'll just put it'll be a 0 1 matrix and we'll put a 1 in an entry if there's an edge from that vertex on the left of that vertex on the right so I'm pretty clear if you think about it for every column because it's regular in the left for every column we'll have you know 64 ones in here in each column ok and what does it mean for this to be the parity check matrix well remind you what's in coding theory terms and that'll also sort of give you like a picture involving the graph so what does it mean for a matrix to be a parity check matrix it means that if we look at any received word Z ok something call this Z this is in F 2 to the N Z is a code word if and only if H Z equals zero okay that's the definition of the parity check matrix H Z is the zero vector of 3/4 and height okay so zero so this is your one vector and these H's rows are zero one vectors as well and each of them you kind of dot product against Z and to be a code word all these dot products should be 0 now let me think explain to you with the picture if you think about that what does it mean let me try to get rid of this orange stuff here what it means is you see given a received word Z in F 2 to the N you can think of it as being you know its indices is being labeled sorry these indices is being associated with the vertices of L and therefore you can think of the code words bits as like labeling the vertices of L so Z 1 Z 2 up to Z n give it a code word these are like 0 1 entries labeling the vertices and now a each row of the parity check matrix each parity check is associated with the vertex on the right-hand side and we don't know anything about it this degree remember but I mean it has edges to some vertices back in L and it's a parity check in the sense that the constraint is that the parity or XOR of these bits should be 0 that's exactly what's going on with this matrix scenario so to say that for every row let's say that Hz is 0 is to say that for every row of H if u dot it with Z mod 2 you get just the the number 0 well mod - that's exactly saying that for every vertex on the right here if you look at its neighbors back in s and just add up mod 2 the labels that Z gives to them those should add up to zero okay so your a codeword Z if when you you know put your bits onto the vertices of L they satisfy all the sort of parity checks or X or checks implied by the right-hand side vertices great so that's the definition of the code and it's a linear code it's a linear subspace so what are the two things we always want to know about a code we want to know about its rate and we want to know about its minimum distance so it's rate is easy to work out or it's a dimension is really equivalent ly so the dimension or the quote unquote K for the code well if you recall for linear codes it's just going to be n minus the number of parity checks three quarters n okay that's kind of easy to see because each parity check is like a linear constraint on Z and so every time you insist on a parity check it kind of knocks out one dimension and then we have three quarters and parity checks so the Z's that satisfy all these parity checks will form a subspace of dimension 1/4 n okay and this is the thing that's usually called K and this is great this is wonderful because it means the rate which is K over N is 1/4 and that's wonderful that's a nice big number it means n it's just 4 K so if you want to encode a message of K bits you only use 4 times K bits to encode it so that's great just a blow-up of size 4 okay so it's got a good rate that's one of the criteria we need for a so called good code and the other thing we need is a good minimum distance okay which means all the code words are far apart in Hamming distance and one thing you may recall about a linear code is that there's a nice characterization of its minimum distance it's also the minimum Hamming weight of any nonzero code word so in a linear code the all-0 string is always a code word it always passes all the GRT checks of course so what you want to do is you want to look at the nonzero code word that has the lowest Hamming weight and it's having weight is the minimum distance of the code okay it's at that distance from the zero code word but also it's not hard to see that like every pair of code words has to be at least this Hamming weight apart so my claim is that the minimum distance of this code is actually greater than this quantity 0.02 over 64 and this upper bound and the allowable set size s that you can apply the expansion guarantee to and if I prove this claim that's also super great because this is indeed a constant fraction of n it means you can correct up that half this number of errors and your aircraft encodes you can correct 0.01 over 64 and many errors and that's a constant fraction of n so that's that's that's great that's what we're looking for okay so to prove this claim it's gonna be easy I'm gonna use the other claim claim one as I called it I suppose this is claim too but anyway okay so as I said the minimum distance in this linear code is the the minimum weight of a nonzero code word so let me just lazily do this by contradiction again so let's assume for the sake of contradiction that Z is an N bit vector which is not the zero vector and it's a code word and it's a Hamming weight is at most this point o2 over 64 and ok and now of course you may have guessed this we can think of Z is the indicator as the indicator this is your one indicator of a subset s of the left vertex set remember when we have received word Z we can always sort of a label the vertices of the left set by the bits of Z and you know all the bits labeled one we think of as the subset s okay and now you may be not be surprised as to what we do next we apply claim one and claim one tells us therefore that this set ass okay it's not the empty set because Z is not the zero code word but it does have size at most this key bound claim one tells us that this set s has a unique neighbor one of these unique neighbors in our call it V not really sure we need to call it anything because let me just show you that what happens in the picture now if we go back up here let's imagine this orange square here was V this unique neighbor oh it can't be because this this neighbor has a two versus two neighbors back in s so let me erase this one okay so now um yeah now this V could be a unique neighbor oops yeah well it is in the neighborhood of s according to this picture let me extend the neighborhood of s here hypothetically okay so this vertex V is a neighbor of mmm gosh I guess it's not let me extend s as well great I think I finally drawn the diagram nicely so here's s and this vertex V here is a unique neighbor in the sense that it's a neighbor of s but it's has no other edges that go into s and now let's just think about the parity check associated to this vertex V well this parity check is just you know X ORS together all the labels of Z that si gives the vertices which are neighbors of V so it you know this one and this one and this one and this one but remember I mean the set s is all those positions or all those vertices where Z has the coordinate one so it means that you know the label here is zero and then this one is zero and this one is zero and although the edges that go to something on desks get zero but you know the single unique edge that goes back to s gets one so the XOR is one which means the parity check did not pass so that proves it's not a codeword that's the contradiction or if you want to think about it this way I'm not sure if this will help that much but if this is the row associated to V and these sort of thoughts let me say these yellow columns are the ones where Z has a one which is equivalent to the set s then what we just showed here was that you know there's a unique one here will be zero and all these other s rows and there could be zeros or ones elsewhere but it doesn't matter because when your dot product this row is Z you just pick up these products or these bits so you get exactly one yeah I suppose the picture was clear okay so it has a unique taper in are called V and then by our picture argument it implies that the parity check associated to V does not pass contradicting Z being a code word okay so ah great that's it ah so this shows that once you have this expander graph this by part expander graph you automatically get a good code for it and you can deterministically efficiently construct it given n so that's great and it's always nice to have a linear code that you can officially construct because then encoding is very easy well you have to actually work out the generator matrix for that I wrote a little Piazza post about how you go from the parity check matrix to the generator matrix but basically it's you take out a few columns and then take the transpose and it's very nice to have you know then your code a generator matrix because then encoding is easy you just it's just a matrix vector multiply and what about decoding so actually that turns out to be another beautiful aspect of these expander based codes all the things I'm saying by the way I forgot to mention it but they originate in while papers by Tanner and also Spielman and sips are in Spielman from 96 and one thing they proved which I prove to you here maybe it'll be an in homework but it's quite easy is that this code has pollen real time poly n time decoding from up to you know half the minimum distance so from less than D over two errors where I've written capital D for the minimum distance oh you're right little D but we use up four degree in graph so yeah this algorithm or this code has an efficient decoding algorithm it takes any received word which has fewer than minimum distance over two errors and finds the nearest codeword just a couple of comments on this first of all this very simple algorithm crucially is as I mentioned this before but it really uses that point eight is greater than three-quarters okay so this point eight which is sort of the amount of expansion that we got in our neighbor set sizes that's quite good expansion and we rely on the fact that it's bigger than 3/4 to get this fact about decoding another great fact is that if you're actually very careful you can take this poly and time and make it linear time okay so that requires a more work than this argument that is easy and perhaps will be on homework that one's not as easy but that's a beauty of it that this is uh not only is it a good code but has the super fast linear time decoding algorithm which is also wonderful and let me tell you one more wonderful thing I'll just tell you I'm not the super fast algorithm but a polynomial time decoding algorithm it's so easy I will write it for you here in two lines here's a decoding algorithm that's polynomial time here it is you're given Z received word and let's assume it differs from a codeword in at most D over two places and we want to find that code word so here's the algorithm it's a while loop while H times Z does not have Hamming weight zero flip any bit of Z that causes this Hamming weight to decrease okay so you know if you get down to the case where H times es Hamming weight 0 it means it's the zero vector which means it's a code word and if you're not out of code were then just go through all the bits of N and try flipping them and see if that decreases you know the number of failing parity checks if it makes more parity checks accept and if flipping a bit causes a parity check but more parity checks to accept then it causes us to break so it makes the overall number of successful parity checks go up then just do that flip that's it that's the whole algorithm and the analysis that it in polynomial time gets to a code word within distance D over 2 is not that hard it's like a page long maybe even not gonna be homework who could say this is an example of a simple belief propagation style algorithm and yeah the fact that it works again relies on the expansion properties of the bipartite graph\", metadata={'source': '7Kh4lLmEmck'}),\n",
       " Document(page_content=\"okay so now let me tell you about a second application of these bipartite expander graphs that we've been talking about and this one will really use heavily the fact that we have explicit deterministic and even fully explicit constructions of expander graphs because the topic is D randomization okay so this is application to do you randomization so we talked about D randomization some already in this course and I'm gonna show you now is how it's almost like a little magic trick you can do if you're not interested in fully D randomizing an algorithm but simply getting better probabilistic guarantees about your algorithm without using more random bits than you need to and as I said it's a bit like a magic trick you'll see that like we get something for nothing almost from these expander graphs so to set this up let's set the stage for a simple randomized algorithms the randomization problem so let's say that a is a randomized algorithm with one-sided error so like an R P style algorithm error for a decision problem P and normally I don't like to use the letter P for a decision problem because it reminds one of complexity class polynomial time let me stick with it P and one reason I chose letter P is an excellent example is the decision problem prime a letís imagine you're this randomized algorithm a is trying to decide if a given number is prime or not and you know the Miller Rabin algorithm the most standard prime malady child stink algorithm he's a randomized algorithm with one sided error has the property that well before we get to the property let's imagine this viewpoint of a randomized algorithm one way a viewpoint of randomized algorithm a is you know it gets in the true input X and you know what outputs yes or no and it flips some coins it's doing that and it gives the correct answer with high probability but it's also nice to think of this viewpoint where a is a deterministic algorithm that gets access to random bits and that's how we'll think about it again so let's imagine that is a deterministic algorithm that gets access to our a random and bit string okay so we kind of pull the random bits out of the algorithm and now to say that a you know solves the decision problem P with one-sided error and I'm assume it has error of 1% for this example we'll assume that has these properties so if X is a yes answer if X is in the language think of Prime's again then when you choose the random string are uniformly at random the probability that you get the yes answer out of a when it's FedEx in R is 1 okay so if X is in the language it'll always say yes and if X is not in the language and the probability over R that you got the wrong answer yes is that most as I said 1% or 0.01 okay this is kind of like how the Miller Raven algorithm behaves if you give it a prime it will always say prime and if you give it a composite number it might possibly rarely accidentally say prime even though it's composite okay so what we want to imagine for this sort of little a dear itemization magic trick is that we want to reduce this error without using any more random bits or without let's say conserving random bits okay so we're matching that like random bits like randomness is like super precious and so we really want to use very few random bits but we want to decrease the error so let's also say that a is a algorithm that takes time T so one thing you can do to reduce error is the thing that you always do to reduce error just repeat the algorithm a bunch of times and in this case say yes if and only if all the answers were yes okay so that's the standard thing to do you could repeat let's say D times the algorithm independently with fresh random independent bits each each case and what would happen so you would use D times T time and you would use D times n random bits ok we're assuming that each run requires n random bits and what is the error probability of this repeated algorithm well it's one percent to the power of D ok that's the classic standard error amplification or error reduction thing if you pick D to be you know 100 that's pretty good you take a hundred times more time and your error probability goes way down but you have to use a hundred times as many random bits and you know in this little example I want to really focus heavily on conserving random bits so I'll consider that quite expensive so here's the second strategy that uses fully explicit expander graphs and what we're gonna do is take you know one of these bipartite expanders of the type we've been talking about this bipartite span derp but on 2n plus 2n vertices ok so I'll explain what I mean by that just now but what I'm saying is well actually these bipartite graphs we talked about before had like you know and on one side and 3/4 and on the other side just ignore that 3/4 thing let's assume they have the same number of vertices on the left and right but let's assume that same number of vertices is too to the end now this is like I'll put an exclamation mark here because this is a bold move what we're relying on here is this strongly explicit property ok so let me remind you what does that mean we're assuming these bipartite expanders are strongly explicit so we don't literally like write down the the adjacency matrix of this bipartite graph vertex set Alain armed with two to the N vertices here and 2 to the N vertices here but the other I'm sort of mentally knows about it and what does it mean that it mentally knows about it this is a strongly explicit thing it means the argument polynomial-time can understand the adjacency relationship of this graph so if you tell if you're the algorithm and you think of the name of a vertex over here okay this is named by n bit strings and you also I mean we'll stick with the the D regular graph or a 64 regular graph if you if you the algorithm thinks of a number between 1 and D or 1 in 64 specifically then it can figure out in polynomial and end time like the names of these D neighbors ok these are also n bit strings and the point is that you can work out the neighborhood or a neighbor relationship in poly n time okay so this randomness reduction or so this error reduction algorithm that conserves randomness is going to sort of take this graph onto to the n plus 2 n vertices and remember let's once again what is the property of this the expansion property of this graph it's that the neighborhood set of any subset s ok so if s is a subset of L and the size of s is at most you know 0.02 over D or over 64 times N and the number of neighbors of s is at least 0.8 times D times cardinality of s ok so what is the actual algorithm here for trying to decrease the error and algorithm a so step one pick a vertex L on the left hand side uniformly at random okay and that's picking a random and bit string so this definitely costs you and random bits no doubt about that but now just consider its neighbors let r1 through our D be ELLs neighbors okay so these are right hand side vertices which are also named by n bit strings and by the strongly explicit aspect of these bipartite graphs where you can compute this neighborhood relationship in poly log the number of vertices time or poly little n time you know this calculation of computing the these vertices takes poly n time which is nice and now we have D and bit random string so we just run a randomized algorithm a on X with random strings are went through our D these are n bit strings and we do the usual thing right I mean if they all say yes then we overall output yes otherwise if one of them says no we can output now okay let me say yes if all answers are yes okay and let's do the analysis of this well several things are very good so this is a poly end time algorithm in fact well it involves a T as well so let's just put that in a box since we're talking about here the the time for this algorithm is well poly n to compute these D different strings plus we ran the algorithm D time so poly n plus D times T okay so compare that with the naive or standard repetition method okay we added poly n time but that's nothing too special and of course if the string X is in the language then all of these runs no matter what random coins you use because we have one sided error all these runs will keep you back to answer yes so your overall answer yes so that's fine so the only thing we have to worry about is suppose the string is X is not in the language what is the probability that will incorrectly overall say yes okay so let's think about this for a moment if the string is not in the language then our assumption here is that the probability over our random input for a random coins for a that you incorrectly say yes is at most one percent okay and so you know we're going to be feeding in for our random strings you know vertices from the set are these are and named befriended strings and what I want to do is think of the set B of oops the set B of sort of bad strings for the algorithm hey I'll call set B and what I mean here is let B oops let's do this in white let B subset of R be the at most one percent bad strings random strings for a on X the ones that caused it to incorrectly say yes even though X is not in the language and now in order to analyze you know the probability will use any of those or will use all of those strings let's let s the subset of L be the set of bad choices for little L okay so remember this is little L here and what does it mean for the initial choice little L to be bad what it means is that the its it means that the neighborhood of this vertex L is a subset of B right because what do things go really wrong it's when you pick a vertex like this L all of whose neighbors are in the bad set be okay that seems quite rare because then in that case all your random strings are that you use will fall in the bad set B remember those are the strings that cause a - incorrectly say yes even though X is a no string great well it seems there shouldn't be too many such strings and indeed that's the case so here's the claim we claim that the cardinality of this set s is at most this magic bound point O - over D times 2 to the N and why is that well otherwise if s is bigger than that well the first thing I'll do is let me just do something kind of simple let me just pare it down to a set of exactly that size choose any subset s prime of cardinality sort of exactly 0.02 over D times 2 to the N ok maybe round that to an integer but let's not worry about that okay so just pair it down to some subset that has exactly point O 2 over D to the 2 to the N strings and now the point is like this S prime is just small enough that we're allowed to apply the expansion hypothesis to it so this s prime is an expanding set and therefore you know by the expansion property the size of the neighborhood set of s prime is at least 0.8 times D times the size of s prime ok which is 0.8 times D times 0.02 over D times 2 to the N and just the point here is that the DS cancel out and this is 0.16 times 2 to the N and that's point 1 6 is cooked up to be well greater than 1% it's actually greater than 1% times 2 to the N by quite a bit okay but now let's think about this you know every vertex in s L has its neighborhood in B and in particular every vertex in s Prime has its neighborhood in B so we know that for all L and s Prime the neighborhood of L that single vertex is a subset of B ok so that just means the neighborhood of s prime is a subset of B but we know that the cardinality of B is at most one percent times 2 to the N right because what does B it's just the set of bad strings for a ok and by assumption the set of bad strings for a is that most 1% of all strings okay and now we have our contradiction because well of course this one hand we saw the neighborhood of s prime head size bigger than 1% x to the N but it's a subset of B and B has size only 1% times e to the N ok so that's a contradiction and that proves our claim great ok so now what do we finally conclude from this well it's that this set s of bad initial choices little L is quite small it's almost a point out to over D fraction of all initial L so the failure probably are the the wrongest probability of our algorithm is bounded by 0.02 over d the probability that the initial little L is in s so what we finally conclude here is that for this new algorithm the D randomized algorithm has error probability at most 0.02 over d and this is great so I mean if you what because in particular it only uses n random bits okay because the only random bits we needed were to choose this initial L okay so let's compare it what we have is okay we have polynomial n n which is fine to assume plus D times T time so that's roughly the same as like the naive algorithm that just repeated D time and we use only n random bits so this is why I think it's like a magic trick like we didn't use any more random bits than just a single run of the algorithm needs just kind of amazing you know compared with the the naive algorithm which is uses D times n random bits and okay we don't get the error probability down to 1% to the power of D we got it bound to more like a constant over D rather than exponentially small indie okay we got it down to 0.02 over D okay so that's um you know if D is 64 then we got it down to 0.01 over 32 so we you know took a factor of 32 off the error probability while only increasing the running time by a factor of whatever 64 okay so in general you get like a linear error decay as a trade-off with the amount of time you use but kind of amazingly you use no more random bits so that's kind of amazing let me end here by this is kind of a somewhat simple way to use expander grass doo-doo randomization there's a bit of a more standard way that gives you can give you this exponential error decay so let me just put this as a remark if you use a similar idea but you take like a very large expander graph not necessarily bipartite but think of a just a regular usual graph on again 2 to the N vertices so you need fully explicit expander graphs and so each vertex name is like an n bit string you can take like such a graph with say regularity 8 so every vertex has eight neighbors and then what you can do is you can generate a bunch of n bit random strings by picking one initial random and bit string like we did and bits and then doing a random walk and as you do each step of the random walk you only have to choose a neighbor so you only need to pay three bits of randomness if the regularity is eight to get to a new and bit string and you get to a new end bit string and so forth and you can show actually that this achieves sort of the exponential error decay so let me write this using this idea of like a length T random walk on a you know non bipartite let's say eight regular just pick that because it's a nice power of two bigger than three 2 to the N vertex expander graph which of course you need a fully regulated mana graph to get that you'll use again you as you take this random walk you collect of T and bit random strings and then you run your randomized algorithm a using each of them and say yes if and only if all of them say yes you use n plus three T random bits let me just write as n plus order T random bits those pluses and T's look similar so n plus order T random bits and plus three T random bits and you can show that the error actually goes down exponentially it's like most 2 to the minus theta of T okay so it doesn't go down just like 1% to the power of T but goes down like some constant to the power of T and I won't prove this well it's not too hard the analysis is extremely similar to the analysis we did in the last lecture about how random walks in an expander graph or a graph that has its minimum conductance large or its second smallest laplacian eigenvalue large makes very quickly so the analysis it's a very similar if you want to prove this fact and only that this is about you know the way I've described it is about reducing error in one-sided air but this can also be proved for two-sided air algorithms you can equally well take them and at the cost of n plus order T random bits and T repetitions of the random algorithm reduce even a two-sided error algorithm down to two-sided arrow that's exponentially small in T\", metadata={'source': 'bHzi3Gcgheg'}),\n",
       " Document(page_content=\"all right let's get started i have a new i always feel like i'm saying this but like a new zoom setup today for some reason so hopefully everything will go okay uh y'all can see my screen and hear me i assume um all right well uh welcome back after a break uh hopping how are things going and do you have any questions about uh cs theory it's all good got it all figured out i guess i could start um i was just wondering about 7.3 maybe you could start with that 7.3 this one is about max cuts and some eigenvalues and things okay what's your what are your what are you thinking uh i i guess um in this one for part b i was somewhat stuck on like how to shoot so so i guess for part a i was able to get it using the relay quotient kind of formulation but i wasn't sure how to like how to use that in um part b i wasn't sure exactly like i was trying to get like the rayleigh quotient in forms in a form where we can talk about like the the um the quadratic form of h or something but i just like wasn't sure how to do that so yeah maybe we could just discuss that sure why you tell me about how you did part a okay um in part a kind of we ended uh up using the um the the we we reformulated the problem in terms of like the relay quotient where we were talking about like the max uh so so it was like basically the reverse of like chiger's inequality almost um where we used um instead of using the min we use the max there um so let me like write something so that we can all see yeah what should i write um again i guess the thing that we were looking at was the um max of uh i guess we were talking about like f and f uh the in the inner product of f and lf all right and that like when f is um the norm of f is one two norm of f is one okay yeah yeah okay so yeah what can you tell me about this expression um i think in this one the sorry um so so in this one we can kind of use like the same kind of thing with like chiggers inequality where you know like basically that if you have like if you constrain f to be a um a characteristic function so it's like one s star then it then it'll be um worse than if you have um worse than if you use the the this form here in this form is equal to the quadratic form in some sense off um f okay because okay let me see so uh well this particular f doesn't have two norm one right right i'm just saying that like um we we want to be able to um say that opt-off g is like it's basically opt of g need gives us a characteristic function like one s or like indicator of s but um but like we want to bound that by this this um related quotient kind of thing essentially so like okay so uh let's see what's going on here we have max cut we're trying to partition it fraction of edges between us okay so let's say okay so let's say you're saying uh you're saying like let s which is a subset of v achieve the optimum yeah yeah okay so we could define oh yeah let's put a star just so we don't forget it's it's like generic so we could define like could indeed like define let's say f star to be like the indicator of this s star yes exactly right okay so now what so um basically you can take the quadratic form off that um f star there and claim and that's like defined to be e i'm not exactly sure but it's like either two times the opt of g or like half i i i didn't like i wasn't sure about the constants there well it's not defined to be that oh sorry it's not different but we know that this would be like true because this is just the this is the quadratic form this quadratic form of f star right yeah and wait how can we relate this to the number of edges uh oh yeah this is our this is the number of cut edges over the number of edges right the number of cut edges over number of edges is it divided by two i think right because this is the moment i just come out sorry okay so i was working on this the option uh we faced basically that value right there is equals the quadratic form which is like one half the expectation of um [Music] uh like f of x minus f of v squared yes so this is yeah indeed this is always equal to the this quadratic form put too many curls on the f i'll start there yeah my pen is like a little bit different today so it's like less uh nice so it should be like this thing right this is the quadratic form i just have to put star everywhere here yeah the square sorry which is your handwriting yeah which is uh sorry what are we going to say oh yeah which is then the number it's the fraction of edges which step out from s to from v to not v uh yeah it's like uv that are going from like in s to this would be like not in us oh yeah s sorry uh uh star so uh whoa right so how does this relate to the fraction of edges that are caught by s so the fraction of edges are are the fraction edges that go from s star to not s star and from not i start to s star it's twice that yes so like fraction of edges you know between s star and its complement so this is like definitely true up to possibly up to a factor of two right um let's just figure it out let's see like the whole graph is this graph um and let's say this is like s star um so the fraction of edge is cut is one and uh what is this expression you pick a random edge you look at this difference uh i guess if you pick a random edge then like in this specific case like f u minus f v will always be plus or minus one so square so this whole expectation will be one this will be giving you a half so i think like this is probably a half oh yeah yeah the fraction of like cut edges right uh okay so i guess that means in this case this quantity will be like a half times off yeah that's right okay all right and then kind of following some of the intuition that we that that like cheaters inequality like has in some sense we can kind of like say that um well this will be no no better than sorry this will be like did this this um quadratic form will be upper bounded by like the max that f can take on like the max if you could like if you don't restrict f to b um if you don't restrict f to be the specific indicator of s star for all indicator function then like this will always be upper bounded by some like max um f uh inner product with like ls does anything yeah this is at most this that's true because f star has two norm at most one you might worry that it's like not so sharp though because like f star you know it's two norm is like uh squared it's like expected value over v of like f of v squared which is like this is like the volume the fraction of vertices in s in this case um so it's like you know potentially less than one in fact it probably will be less than one in a typical case you might worry a little bit that like you're kind of losing just a little bit here do you know what i mean i see i see so because i mean essentially like that like top form is essentially will give us the right-hand side of the equation right because that that's precisely well oh yeah this is indeed yeah i mean by basically your algebra this is literally lambda max or lambda oops yeah n minus one of l yeah exactly like so that's like high note sorry yeah so we want okay so what are we trying to show we uh trying to show that this lambda n minus one get my yellow pen here oh you can't even see yellow let's try to do red i'm having a hard time with my pen these days uh this is uh supposed to be okay the optimum is supposed to be at most two times lambda n n minus one two times its maximum eigenvalue and um [Music] yeah let's see so this is indeed maximum eigenvalue ah okay so it's fine you're saying optimum is at most two times is that going the right way oh no it's not going the right way i think it's the other way yeah well we got the factor of two it's like kind of wrong like this observation we've seen so far is like okay this up this thing when you choose f star to be like the indicator zero one indicator of the optimum that this quadratic form is a half times ops and like definitely this is at most this uh which equals the maximum eigenvalue but this is giving us like we put everything together here that gives us like uh op is at most two times this lambda of l which is weaker yeah but we're trying to get this should be like it should go to like a half that's true maybe that was the mistake that made it like be not solvable for me um so um yeah this new pen system i'm having it's so messy okay i'll try to i'll try to survive and stop talking about it um yeah okay so we somehow need to improve this argument we're gonna do like part a right so essentially we need to um i mean can't do anything i mean this is definitely true you basically need to involve like a factor of like a quarter or something in like the in this to actually get the boundary we want yeah well maybe we should just like examine what's going on and like this specific graph it's a great graph um [Music] let's just check what we know about this so um [Music] let's see okay so the opt in this graph so let let's call this uh okay g is this graph it's a nice graph so opt of g is uh one because there's a cut s that partition or cuts 100 of the edges um yeah and what do we know about lambda uh uh the eigenvalues of l here let me just put max actually do we know what this is here look at p1 because it's bypassing part of me uh i i just i was just thinking like this is like bipartite right so it would be two so like i mean so it should be one like that i i mean i forget if it's two or one i forget whether like the lambdas or the capitals are like the scaled options but yeah yeah uh it's two yeah the maximum eigenvalue is two and so in this case actually i guess it sort of like exactly checks out in the sense that um you know in the sense that uh this the second largest eigenvalue or the largest eigenvalue here is two and then like times a half gives you one and that is indeed the op so this is like an example where this inequality that we're trying to prove is exactly um optimal like tight right actually let's try to remember why is it two i mean indeed it's true for all bipartite graphs but um that means there should be uh so i mean that means okay so what do we know like this means that you know um there should exist like an f such that this quadratic form is equal to two times the two norm of f squared this is like an f that maps the vertices into the reals so uh yeah for this graph do you know the f that does that it's just one zero uh let's see and zero um i think that works so this thing uh is the quadratic form oops which is you know as we saw half the expectation over a random edge of f u minus fv squared okay which is a half wait is it half yeah and then this one is the expected value of f squared and what's that in this case is it half wait wait a half square f squared so f is like one zero so it should be one right uh i don't think so this is is that what volume of um yeah that's true since this is an indicator it's like it's a half yeah yeah it's a half right right sorry it's like yeah it's expectation over you drawn from pi this is a regular graph so it's just the uniform version of f let's just write it down u squared so like okay it's like half the time you get you know this one this vertex here you get oops yeah one squared plus a half times zero squared which is right that's right um let's see so okay so this turned out to be whoops this turned out to be a half right and then this left-hand side also turned out to be a half oh but it didn't quite work right because like you know half is not two times a half yeah so we have to skip yeah so maybe we need a different function right does anybody have a suggestion maybe we could have one and minus one yeah i think that's what we did in class let's try that as an experiment so let's see so okay if we did one and minus one let's just change this one to plus one oops this one minus one okay so let's work out this thing that's expected value over a random vertex f of u squared okay with our new f this is always plus or minus 1. so it's square is always one so expectation is one so like this bubble would be one okay what about this thing it's the quadratic form thing okay so it's a half expected value over our random oops edge f u minus f v squared okay so i guess for any edge there's only one edge like one of these will be plus one and one of these will be minus one you subtract them you get plus or minus two and then you like square it and you get four so it this thing in the the brackets is always four so this expectation is four and then we times it by a half and so therefore this is to check out make sense yeah so i think for this one where it's like plus or minus one this is two and then it's good because like here's the factor of two and then it's one so at least here's um okay here's here's a case where the everything is sort of exactly tight um [Music] so maybe suggest that like if this was g was your in like entire graph then it would be good to study this particular function so perhaps that gives a suggestion of like maybe what to think about for part a is is there something where like volume of the cut set is at most half or the volume of like the set of the mac for the max cut i mean this is true in the sense that like if you have some g and you know s is some set of vertices okay then the in the maximum cut problem like you count the fraction of all edges that are you know going between s and like negation or not negation but like complement of s and uh it's like complement of s out here and you know the you know the volume of s plus the volume of like s complement is always one so one of these two numbers has to be at most a half it's that's true the smaller one is at most half um consider functions instead of um like just taking on values one and zero taking on values of minus one and one then um yeah you could do that so what what do you mean in like uh like your suggestion in the general case like something where instead of like the indicator of s it's like um the two what is it like two times the indicator of s minus one or something where it's like i see what you mean yeah yeah uh okay let's say if we have say s we can even have f star if we have in mind it's like the optimal cut is some subset of the vertices i don't know let's just like uh i don't know let's just define whoops uh i don't know g to be uh as you say like the plus or minus one indicator so plus one if u is let's say ms and minus one if u is not an s right because the example kind of indicated that would give us like the yeah at least for that one edge graph so i mean um there's a couple i mean i guess we there's a lot of quantities floating around like g l g and like opt and um [Music] well i guess these are the a couple of things we have to consider well there's also like you know lambda n minus one [Music] l so should we try like calculating one of these things and see what happens sure uh what should we calculate yeah i guess if you do the quadratic form thing then it's like very similar to the to the thing when we assumed the indicator of s or like because it's it's very close to the same thing it's just like the transformation of that and then you could just like use properties of the quadratic form to solve it or something yeah okay well let's calculate that so that's this thing is this quadratic forms expectation pick a random edge gu minus gv squared um okay so uh i can't make it scroll down anymore that's weird uh okay so yeah what is this quantity equal to um okay i was thinking that this would be so i i wasn't i wasn't like sure about this per specific one like i don't know how to do this maybe but like i was like this is like the quadratic form of um two times the indicator minus one so i can just use like properties of the quadratic form okay yeah we can do that so this is indeed as you said two times the indicator of s star minus one this is sort of like a function it's like a constant okay yeah we can do this remember we like have this yeah local variance name for this so we do have some properties so for example yeah what is this equal to four times the um because four four times the quadratic form of the indicator of s yeah very good because we know that this uh this is like translating by a constant that doesn't change it and then this like multiplies it by the square of that constant that's true yeah okay so then this equals we did this before this was like four times we did we have times yeah was it like half times odd yeah that's right yeah so now it's like in the right form almost that we like need in some sense if we can balance this by um the that rayleigh thing then we're good uh yeah so like this thing we just decided is two ops and um yeah we're probably good i mean uh [Music] yeah it's like this is always true yeah and that didn't matter it didn't matter on what f was there so well it matters a little like f has to oh right right yeah that's true um yeah but like the eigen the eigenvalue the max eigenvalue of l kind of yeah yeah so so i mean but we can i think you can still operate around it by this now right perhaps what is what is this thing two norm of g um should be less than one is that true um wait um no yeah written down here's g right so i guess it's two times sorry it would be like four times the volume of this four times the volume of s [Music] the expectation of x squared which is always one oh yeah that's right yeah yeah okay let's see let's just let's assume like so we don't like right away let's assume you can finish 7a yeah yeah yeah um okay so uh what else so seven i mean some point three b maybe yeah that's that that's i guess yeah i'm not sure if like this like thing helps solve that but like my intuition there was that essentially we can think of now l as like l plus diag h but i wasn't sure how to then like take that diago h and like write it in for in terms of um the f in the relay quotient thingy uh okay um perhaps let's pause here for a moment because we have to spend some time on this question so let's see i mean let's uh we can come back to it and maybe we'll come right back to it i don't know but like um are there other questions that people have maybe about this problem or maybe about a different problem [Music] um i was looking at one c it was stuck if we wanted to look at that one sure one c so here we're asked okay g is this hypercube graph okay it's my favorite graph pretty much uh right so it looks like this i can draw uh okay reach that s to find this function phi s to be oh this expression which maybe we even remember from the analysis of boolean functions lecture show that each one is an eigenfunction for g's laplacian l and determine all the eigenvalues of l okay what so let's see what are your thoughts on this problem yeah so i haven't gotten to i mean it reminds me a lot of the analysis of moving function stuff like you said but uh i don't know i don't know if there's not that much to work with this problem but i can't get that far like for the first two parts um it helped to like think of the uh matrix vector multiply as a linear combination of the the columns and that helped a lot for the first two parts and so i was trying to do that on this part but um yeah so if you do that you get that like l times phi sub s is like the sum of the the parity function the phi sub s of u times the youth column of l hmm um well let's just write all the definition so uh what is the definition of l f of l s um if f is any function from v into r well so i get do we want it generally here this specific l uh yeah uh generally oh uh it's like i minus k yes it's uh i i'm sorry i minus uh k of f okay that's true so that's i f minus k f which is f minus k f okay so what's the definition of kf anyone else can of course pop up with an answer it's it's the transition matrix or like it gives you the probability that you're or it gives you the value of after the next step it's like one step for a random walk um sort of what is the i mean in this in the nature of like type checking what kind of object is ks what's its type so it's a function that maps yes very good vertical i'm trying to remember like the high level interpretation there's so many definitions flying around but i think it's like a lot yeah true yeah so kf is a function and what's like taking the vertex and tells you stuff about the neighbors right yeah does anybody want to say exactly okay uh i think it's oh is it just like the expected value of the a neighbor yes well it's yeah it's the expected value when v is like a neighbor of u f v so average uh you know like if you have f it's like a labeling of the vertices by numbers and then like kf it's also a labeling of vertices by numbers and like the labeling for a specific vertex is like the average label around the neighbors um okay so that's nice so let's do it 4k fi s how can we figure okay so now we have to like think a little bit what is k5s out of vertex u well i could ask but i guess i could just be like a robot and write down like what we know it's like the expected value over v a neighbor of u of phi s v um let's do an example so let's see how can we do an example uh where's my pen um okay so let's do let's do an okay yeah let's do an example let's say like n is three or no i think it's called d is three so like v is like all the binary strings of length three so let's say um i don't know like let's say s is a subset of let's say one comma two it's like a subset of one two three um okay so phi s on a string u if i recall it's just like u 1 u 2. so far so good um so let's do an example let's say like u is i don't know plus minus minus then phi s of u is i'm running plus and minus instead of plus 1 and minus 1. well it's this times this so i guess it's minus one but what is k phi s not u maybe i'll let you answer this should just flip it right because all the neighbors have exactly one bit the difference uh so you think it's plus one oh i guess i'm thinking about the general case now but the uh okay wait let me think for one more second yeah who are the neighbors of you i guess i can just write that down the old names i mean it's how many neighbors did you have oops my uh what i said would have been correct but i forgot about this constraint so ah i see what you're saying i think i know what you mean yeah yeah because i didn't take s to be everything which is probably like the most illustrative case but happened to not so okay how many neighbors does you have it'll always have three okay yeah and what are the three neighbors in this case uh minus minus minus plus plus minus plus minus plus yeah plus minus plus very good and then what is uh phi s's value on these neighbors so it's value here is plus one yep plus here uh wait is s sorry i wasn't paying attention when you defined s is that the first and second numbers or like are like what are we indexing by oh yeah i meant like this is one okay so and this is three yeah yeah so then the next one will be plus and the one after that will be minus right okay and if we average these up we get i guess third one third yep thirds okay in this very specific case this function value is a third yes should we even draw like a picture somehow like the graph looks like this if i ask i guess i mean probably [Music] is the product of okay maybe a picture is going to be like to laborious to draw um but i guess it'll probably look like something like this like minus minus plus minus plus minus plus minus plus probably and like i guess we maybe uh we just did like this on some [Music] vertex whose label was minus maybe this one and you see and did we did like we kind of averaged up its three neighboring values minus plus and plus those indeed averaged up to one-third so like k-u or sorry kf um uh probably like okay if this is u s sorry keep writing this thing u this is phi s when s is you know one two and like k phi s um i think it'll be like probably uh we'll see that'd be like minus one-third plus one-third minus one well yeah minus one-third plus one-third um i'm just like what i'm doing here is i'm looking at this thing and like averaging its three neighbors it has one neighbor that's like minus one neighbor that's minus one neighbor that's plus so the average value is minus one-third okay red on red is not my smartest move minus one-third that's going to be plus one-third one will be like plus one-third so you see what happened here like um every time over here you had plus like it got changed oops it got changed to like a minus one-third and everywhere here you had minus it got changed to a plus one third so actually k phi s as a function is sort of the same as phi s up to a scalar multiple like the value at each corner like got multiplied by some value which value is that negative one-third yeah indeed so what we kind of just saw here is that this is like literally negative one third times phi s and like that literally means that like negative one third is an eigenvalue of k whoops okay with eigen function or eigenvector if you will phi s yeah it seems like that one third value is like related to the size of s exactly indeed it will be uh yeah you should do one more example just for fun so for example um let me just try to quickly draw it so let's say um [Music] we do a different i don't know phi sub t say phi sub t uh where like t has cardinality one so basically this is you know you look at the coordinates which are plus one plus one minus one for example and like phi t here it's just like outputting one specific coordinate value maybe the third one so like uh phi team might look like the following like it might be like you know plus one on all the back faces and like minus one on all the front it faces look like that and then again if we like draw k t or k phi t okay so what's the operation like the value here is like you average the three neighboring values which are plus one minus one and minus one so i guess the average of plus one minus one and minus one is minus a third so like here you'll get like oops here you'll get like minus a third at this vertex this is like k phi t and here we have minus one so i think long story short in this case again like every value here will be like one-third the value it was back in the original picture i think for this kind of phi t i think like one third is an eigenvalue we'll get an eigenvalue of k with eigenvector this phi t which as you said like for t is like cardinality one does that make sense yeah that makes sense so i'm trying to think about what the general formula would be ah let's think about the general formula for like two minutes maybe we won't do all of it but like okay let's try to think in general now so um let's say d is d and s is s so like we're looking at we have phi s so okay this labels the vertices of the hypercube with plus or minus ones according to like the product of the coordinates associated to the set s so k phi s at a point u is like average or expectation over a neighbor u of phi s v so like this will be like you know it'll look like you know v two times v three times v seven times v8 times v9 times dot dot you know if like for example s is the set 2 3 7 8. um and if like u i mean like this specific u happens to be like i don't know plus plus minus minus plus minus plus minus minus minus plus minus dot dot you know we'll be looking at like v2 times v3 times v7 etc now typical v will look like plus plus minus minus plus and then like maybe in what one location like this one oops it'll like be different this is like this is like a typical v you know there's going to be d such v's but like it's going to agree with you like on all the coordinates except for one so yeah what to think about this quantity i guess it's like v2 times v3 times v so i guess we could like condition on whether the neighbor is or i'm trying to split it up for the cases on whether the thing is flipped due to s uh yeah so like v so yeah this like phi s v okay it's like if you know hypothetically it's v two times v three times v seven etcetera so here's v two in this box this is v three v7 is one two four five six v7 oh i just missed the place where it was different v8 v9 so let me ask you this uh in this specific case where like s happened to be like two three seven eight nine et cetera and where like u and v had the property that they differed in like position six um [Music] how would you describe what phi s of this specific v is it's just phi u right very good exactly so like it seems like it's quite often or like potentially the case that like this equals by su you know like sometimes at least sometimes um [Music] oh yeah rather maybe not sometimes but like i better say for some neighbors b yeah i guess we want a condition on like whether the bit that was flipped in [Music] going from u to v is in s and then that's how you get the size of s involved yeah exactly so like you'll see indeed like if um you know v and u differ on coordinate i that's ins then what goes inside the brackets uh negation of right you're getting like minus phi s u else like if i is like not an s you're kind of counting plus phi s u yeah nice yeah so the average well the average i guess maybe c is like i mean the key is that it'll be like some multiple of phi s u indeed as you say maybe depending on the size of s and that's good like that's the kind of thing we're seeing where like you know k uh where's my thing k a phi s is value a string u was like some multiple of like phi s u that's what you want for eigen vectors now of course we've been talking about k all along and like oh we originally got talking about um l but i guess you know one can put the pieces together here yeah nice that's good that was helpful to think of k i guess rather than l um yeah that's the thing like somehow uh like k is like a an operator or like a transformation that has like a nice meaning like l is just this thing that facilitates you know its quadratic form being like the quadratic form that we care about i mean of course they're not like you know vastly different it's like just this but like there's no inherent reason in life to like do this move except for the fact that um well like f l f is this nicely interpretable quadratic form um okay another question we could also get back to i suppose 7.3b but we could also do another question all right well perhaps uh 7.3 b are any any thoughts on this yeah so i guess it's like to start off with i just thought he could potentially write out the whole thing like in terms of instead of in terms of l in terms of like l prime and then expand in terms of l so okay what do you mean by that what do you suggest writing like like um on the right like first we can write out the maximum eigenvalue in terms of the the um rayleigh quotient again and just in this time instead of using l we use l prime first okay so what would what is this in terms of like it's like the maximum of something maximum of the inner product between f and [Music] um l plus diagon okay l prime uh um yeah so this is l prime yeah f and this maximum is over one addition yeah the same um f have uh constrained to be one what's constrained to be one yeah yes that squared is concerned okay yep true um so here my intuition was we had to do i mean so the diet l plus diag of h has no like interpretable form in terms of the the graph stuff so i was thinking if maybe there's a way of like no rewriting this sorry no doesn't it um so i guess so so i guess like this doesn't like give us like some the nice kind of quadratic form of l i i guess maybe you could like use linearity of um inner product but i'm not sure if that like helps at all um maybe actually it's a little bit similar to what we were just talking about like here we see l prime f so what is the definition of l prime f oh it's um l plus diag h and then i minus either yeah maybe we can think of l as i minus k let me maybe i'll write capital h just so i don't have to keep writing h alphabet soup here um it's diag okay so again like what type of object is this a vector oh yeah a vector function yeah um and uh okay so it's a function so if i were to evaluate it at a vertex u what would it give me s minus k f u plus um h f u yeah f of u minus k f of u plus okay h f of u right okay half of u we know okay a few we actually talked about this is the expected value over v a neighbor of u of f okay what's this it's uh another function that just scales each it's like a component-wise scaling of each of the uh each of the notes all right each of them modes by like h i or something or is it like yeah yeah well yeah indeed that's a good question what is literally okay so this is a number what uh number is it i guess it's the um the f of u times h of u where like h u the index of like yeah yeah very good i don't know if we used function notation or like vector notation but yeah basically they're equivalent i would say here yeah they're equivalent yeah so we can put i suppose we could put it together it's like 1 plus h of u f of u minus this average yeah all right that should be a u here okay and uh let's see we got to this because well then we were like interpreting this with f so we have this kind of long and mildly disgusting expression here so what's this uh i mean how to write what this is i see so we could say that this is i plus h and so wait um uh let's first remember this what does this mean oh yeah it's the expectation of f times g yeah yeah yeah i see so we can expand that uh yeah we could we could indeed so then this would become [Music] well expectation [Music] you drawn from pi it's a little bit intense f of u times well i suppose like this above expression [Music] it's like a little bit messy yeah um although one thing to remember is like apparently if you just do this if you ignore the h you know we know that this is f l f we know this is like apparently like a nice quantity that's a collective form so i see actually you could imagine like subtracting these yeah yeah i mean that that's kind of like the linearity of like the um the inner product you could yeah we could we could have said like the same thing where it's like an inner product of f l f and then plus f hf in some in some sense yeah that's right that's right so like this thing let's call it star as you say equals f l f which we know about or like at least we have some thoughts about plus inner product of f hf right what is what is this piece that is the um [Music] that is going to give us the expectation of f f e f f f squared h f u sorry f squared and h yeah f u squared h of u so let's see what can you make if anything of this expression is h of u independent of like there's some notion of independence um [Music] maybe i mean h of u is uh okay h of u is like some labeling of the graph like h is some labeling of the graph if h is you know in in white i don't know it could be something like this i don't know if this is allowed and then f is some other labeling of the graph maybe if f is in yellow you know so this is like going over every vertex i guess according to its weight square f's label you times it by h is labeled and you average that uh um we want to use the condition that we know for h what is the condition we know for uh i think it's at zero the expectation can you use like cashew swords or oh yeah i don't know it's going the wrong way i think um yeah potentially you're saying this i believe this is a condition yeah that's true okay yeah um yeah you're right in the sense that okay these are good thoughts in the sense that okay here we are facing expectation of like the product of two things and it's kind of a drag because you know yeah maybe you know some things about one and the other but it's a bit of a mystery what to do when these things are multiplied together but well what are we trying to prove by the way that this has the same um that you can you can say op g is less than or equal to half times the top eigen value of l prime yeah and i guess we we did show like like this right so essentially you want to show that the second term is zero in some sense yeah i mean not necessarily zero but um less than i suggest you like okay i mean i suggest you write down the proof of this well i guess you gotta write down anyway then uh reflect on how much that proof changes now that we have this like h business in there i see well maybe i'll leave it there because i guess we're kind of out of time um any super quick questions before we wrap it up um\", metadata={'source': 'GHwobff5Yhs'}),\n",
       " Document(page_content=\"okay so in the last part of this lecture having showing you some applications of expander graphs explicit expander graphs I'm gonna start talking about actual constructions of explicit expander graphs and there's actually several ways to make such constructions and I'll survey them a little bit unfortunately there won't be time to actually prove that any of these constructions work but the proofs are not too bad actually and maybe we'll explore some on the homework okay I must also confess that I got to do a bit of a bait-and-switch on you so in the two applications we talked about we used bipartite expander graphs but I'm gonna tell you about a constructions of regular old non bipartite graphs which are expanders and so let me just briefly tell you how you can take a regular old regular graph with some expansion property and convert it into a bipartite graph with the kind of expansion properties that we use in applications so I'll just sort of do this by some quick pictures let's say you have a regular old graph G this will be a D regular not necessarily bipartite graph and it will always have n vertices these will actually be two standing assumptions for all the graphs we talked about today so let's say you have a graph like this and you've managed to show it has some good expansion properties how can you get a bipartite graph out of it that hopefully has good expansion properties there's actually two nice methods so we're trying to get like a new bipartite graph over here oops with some bipartite with some expansion properties what method is called picking the double cover of G and what does that mean it basically means you take your copy of G and you take another copy of G and you know each copy of G has some edges and basically you make the edges go across instead of within G okay and if G is d regular then this double cover will also be d regular and an other way to explain it an easy way to explain it if a is the adjacency matrix of G then the adjacency matrix of the double cover it has two n vertices so it's twice as big and it's bipartite and it kind of looks like this you put a here and a transpose here and zeroes here okay so if you think about it that makes a bipartite graph which is kind of like the one I just described so that's one way to turn a regular usual graph into a bipartite graph and it often gives you the good expansion properties or preserves the expansion properties another way is to make what's called the line graph and this doesn't give you a balanced a bipartite graph but it gives you something pretty balanced if D is constant for the line graph you just make a a bipartite graph where one set of vertices is associated with the original vertices V of G and the other set of vertices is associated with that's not a very good e is associated with the original edges of G and then you put an edge between a V vertex and an EVO text if and only if that V vertex touches that a vertex back in the graph and G okay so each edge is of course connected to two vertices so this graph will have regularity two on the right-hand side and each vertex V is connected to D edges so you'll have regularity D on the left-hand side okay so let me just leave it at that it's a couple of ways to get a bipartite graph which hopefully is a good expander out of a good expanding usual not bipartite graph the other aspect of the bait-and-switch was you know in these bipartite graphs we talked about vertex expansion where you know for every set of vertices here let me just switch colors briefly every set of vertices here if you look at its neighbors you know if we call this s and this the neighborhood of s then n of s had to have large cardinality whenever s was not too big and that's called vertex expansion but now when I go back to talking about usual none bipartite graphs you can vortex expansion but it's gonna be easier and it's a bit more common to talk about edge expansion so let me just write these two things here edge expansion so from now on I'm talking about not necessarily bipartite graphs versus vertex expansion and so an edge expander is one that satisfies something like this for all sets of vertices s of cardinality and let's say most half the number of vertices and the fraction of edges on the boundary of s is a good proportion of s itself or to put it in this probabilistic way if you choose a random edge UV the probability that U is in s given that well let me just say sorry that V is in s given in that V is not an S given that U is in s is at least epsilon ok it took me a couple tries there but what I want to say is that another way to put it is if you pick a random vertex in the set s uniformly random and take one step in the random walk the probability of you escaping s is big it's at least epsilon that's edge expansion in vertex expansion is a condition that looks like this for all sets s again may be of size at most n over 2 or some constant fraction of n the cardinality of the vertex neighborhood of s is at least some epsilon times the maximum it could be epsilon times D whoops times the cardinality of s ok and as it turns out and the same expander' graphs it's more convenient to study edge expansion it's a little bit easier but they're not unrelated now in fact I have to pull one more switch on you which is that finally in the study of expander graphs it's even more usual not to study this edge expansion parameter epsilon but rather to study the second smallest eigenvalue of the graphs laplacian okay so let me repeat that it's even more typical to study what you might call lambda one this is the second smallest eigenvalue of g's laplacian your graph laplacian which let me just remind you L it's normalized laplacian it's the identity matrix minus the normalized the Jason C matrix okay so we're in the setting of regular graphs now here so a is the adjacency matrix of the graph one over D is the normalized adjacency matrix and the laplacian is the identity minus that and as we saw in the spectral graph three lectures this matrix always has an eigen value of zero that's equivalent to saying that a always has an eigen value of D and what controls the expansion is the second smallest eigen value which we call lambda one okay and what we saw in previous lectures was this thing Jeter's inequality which relates this lambda 1 to the best Epsilon you can put here this is really I mean the best epsilon you can put here again is this minimum conductance of the graph G and we know that these two things are related up to a square or up to square root depending on how you put it so in particular let's say relates this lambda 1 and epsilon so in particular actually it's even harder in some sense to construct a graph whose lambda 1 is big then it is to construct a graph whose edge expansion parameter epsilon is a big this is the easy direction of chiggers inequality if you construct a graph which has a lambda 1 that's large then well in fact just lambda 1 is at least Epsilon sorry wait a minute wrong way around number one is at most Epsilon maybe there's a factor of two or a half er sorry if I'm getting this slightly wrong but what it means if you've managed to construct a graph whose lambda one is large then automatically its edge expansion is large and that's the easy direction of Tigres inequality let me just put a little squiggle here to denote up to a constant Fagor the hard direction of teager's inequality is that also if lambda 1 is large then epsilon the edge expansion parameter is also kind of large up to a square okay so this two-sided equality says that if you're interested in education then you're being a bit qualitative about things then it suffice is to study this eigenvalue and that's what we'll do because in some sense the second smallest eigenvalue has a polynomial time algorithm for computing it so it's somehow easy to understand mathematically it's characterized Abul or since we saw this a minimum edge expansion parameter this minimum conductance is np-hard to compute so it's hard to get your finger on it mathematically and so what we're eventually going to talk about constructing and what everybody are most people when they're constructing expand degraaf's strive to do is they strive to get what's called and n comma D comma epsilon spectral expander and what does this mean this means a D regular in vertex graph G with a second smallest eigen value of the laplacian lambda 1 at least epsilon so sorry I'm using same parameter epsilon here let me call this epsilon 0 in this epsilon 0 okay and if you have such a spectral expander then by the easy side of chiggers inequality it's even a good edge expander you can also show this is not too hard that if you have a good spectral expander it implies you have pretty decent we just put that in quotes vertex expansion as well this was originally approved by alone in and it's consequence of the tight form of the expander mixing llama which I'll mention shortly but all of which is just to say that I want to convince you that if you can construct these spectral expanders then they give you you know good actually expanders and therefore and well and also good vertex expanders and also you can get goodbye for ten expanders and so forth and so on so this will be our task for the while talking about this will be our task for the remainder of the lecture and let me just mention here that it can also be good to think about the eigenvalues of a or this matrix K so this is a me erase this up here this was I minus K where K is the transition matrix for the graph whatever UD times a and this K had eigen values that we called let's see what do we call them okay Kappa I think we called them Kappa zero through Kappa and minus one as well and they satisfied well they're just one minus the lambdas so they satisfied that one is equal to Kappa zero rather equal to Kappa one greater than or equal to dot the dot they're equal to Kappa and minus one okay and so just because just as L had a trivial eigen value of zero K has this trivial eigen value of one and so the key parameter is Kappa one in particular the key interest is like how far away from one it is so we're interested we call it you know an expander a spectral expander when Kappa 1 is less than or equal to 1 minus epsilon 0 that's equivalent to this condition on lambda 1 being at least epsilon 0 okay that was a little bit messy but uh let me now say we're just focused on constructing explicit and vertex graphs that are n D epsilon 0 or epsilon spectral expanders and as I mention before what we really want to do is pick some D like a really fixed constant like I don't know three or four or eight or 50 and then get some epsilon 0 which strictly positive constant you know point oh one it's kind of okay for us and then what we want is n comma D comma Absalon spectral expanding graphs for arbitrarily large end okay we don't have to give them for every end but like maybe every n that's a power of two or every n that's a square or something would be fine for us but we want you know like a large infinite family of these graphs and what we really want is for them to be deterministically constructible and poly n time and what we really really want is for them to be fully explicit which kind of means that you could even get like a 2 to the N vertex such graph with its adjacency matrix or adjacency list uh computable in poly n time ok so on with the show let me tell you about three constructions and the first construction it's really great because it's like unbelievably explicit it's like so explicit you just write it down and the proof that it's a spectral expander is not so bad so this construction basically originally due to Margules from 1973 actually the day I'm recording this mark ulis won a bell prize so congratulations Gregory and thanks for the expander graphs he's actually gonna show up in construction 2 as well but uh there's a subsequent paper by gabber and Khalil from 81 which kind of tidy things up a bit and so you know we don't need to give Margulis too much credit so these are often referred to as the Gaborik allele expanders and the nicest analysis of them was done in a subsequent paper by Jimbo and Maru oka okay so without further ado what is this graph family of graphs so you pick a parameter m and then the vertex set for your graph V will be the integers mod M squared okay so you think of it as like a grid here's an M equals for example and you know these vertices are indexed by pairs X where x and y are integers mod m that's the vertex set and what's the edge set well you connect a given vertex X Y to the following eight vertices have to copy them off my ice cream here X plus y X X plus whoops plus y comma Y it's the first neighbor of X Y X minus y comma Y that's the second neighbor and you also do X plus or minus y plus one comma Y that's the third and fourth neighbors and then you do a symmetric thing so you do connected to X comma y plus or minus X that's the fifth and six neighbors and X comma y plus or minus X plus one and that's it so D equals eight and this is a wonderful expander family of expander graphs one for each M so you have a if a expander graph for every n which is a square for several reasons first of all well let me just state the theorem the theorem is that this is an M Squared comma 8 , you know epsilon 0 spectral expander for some strictly positive constant epsilon 0 which you know you have to kind of squint into the Gaborik allele proof or whatever but I think maybe like epsilon 0 like 0.1 or something okay so the actual expansion parameter is not like amazing it's point 1 that's okay but it's got so many other great properties first of all it's as I mentioned strongly explicit or fully explicit I mean it's as great as you can get what that means is you know there's absolutely no trouble in life for a n time algorithm imagining one of these graphs and working with one of these graphs on two to the N vertices you know because these you know names of vertices are just pairs of you know numbers on em so you can imagine if like M is let's say 2 to the N so these vertices are just named by pairs of n bit strings and you know if I give you a pretty good a pair of n bit strings it's super easy to compute the eight neighbors of these strings you can do that in linear time over n time it's just some addition so these are brilliant strongly explosive graphs the degree is 8 that's very nice it means if you want to take a random walk in this thing even this exponentially saw is graph then you know every time you take a step you use three random bits to pick a random neighbor out of the egg possibilities so that's that's great and that's all I'll say about it I mean the analysis of it is kind of a drag but it's like two pages of elementary linear algebra that you can look up online James Lee and Luka Travis on have good expositions of it for example okay so ah that's the first expander I want to tell you about the second family of expanders I want to tell you about are actually the so called ramanuja graphs and let me warm up to them so as I mentioned before a random D regular graph is an excellent expander it's got great edge expansion the only trouble with it is it's not explicit but it's it's way better even than these kind of expanders we've been talking about so like here epsilon 0 is 0.1 but for a random graph the second smallest of the eigenvalue of the laplacian is super close to 1 like it's amazingly large perhaps a better way to look at it in the context of random D regular graph let's say you have a random okay let's say we have a random D regular a good way to get such a graph by the way when n is even is to pick d random matchings on the N vertices it's very clear how to pick a random matching and then just superimpose them this is a good way to get a random D regular graph in a random deregulate let K which is okay let's call it G let K be 1 over D times the adjacency matrix a as usual so this as I mentioned before has eigenvalues 1 that's the trivial eigenvalue kappa 0 and then the next i can value is capital 1 next one is kappa 2 down to kappa and minus 1 and to be a good expander we want this kappa 1 to be far from 1 found it away from 1 and it turns out from this point of view in a random deregulate graph not only is it kappa 1 down and away from 1 it's super close to 0 and in fact all the eigenvalues other than capita 0 are super close to 0 and this is a theorem a very famous and difficult theorem due to friedman from 2000 maybe it's famously like a 100 page paper although it's been recently simplified to I don't know 20 or 30 pages but the theorem is that in a random d regular graph all of these eigenvalues capo 1 through kappa and minus 1 in magnitude are at most basically two times root D minus 1 over D okay plus little of one that says n goes to infinity yeah oh sorry with high probability with probability also one minus the low of one okay so our Freeman's theorem says with high probability you pick a random D regular graph G and you look at this eigen values of its transition matrix you're hoping that they're all other than the trivial one they're all bounded away from one and in fact they're all super close to zero so this is the exact magic number but you know in your head you can just think of this is asymptotic to 2 over root D okay so if you make D like a big number this is really close to 0 a big constant okay that's kind of awesome and what we'd like to do is get explicit graphs that are close to this and in fact for some values of D you can get this bang on and this is called Ramanuja graphs okay so what is a ramen eating graph it was um fancifully named by Lubitz key Philip since NAR nak in 88 when they constructed these graphs that are explicit expanders basically they're explicitly regular expanders that have exactly this eigenvalue property and they're actually constructed independently at the same time bar mark ulis also in 88 and it's not that they're also implicitly constructed in the late 60s by e hara um and as I said what are they they're d regular grass G and they have all of these Kappa eyes at most this exact magic number two root D minus 1 over D literally exactly with no plus little of one so these are actually kind of amazing because they're in some sense like better than random graphs okay by little of one but that's nice but here's a little bit of a catch the construction of these requires very heavy duty number theory like super hard super deep well the construction itself is actually not very complicated okay you know the vertex set is like all the two by two matrices with ones in the diagonal and the other zero down here and some number of mod p and the other entry and the edges are not hard to say what they are but the analysis that proves that they're expander graphs and that I have this expansion property is like super super duper hard it uses some deep number theoretic results of Pettersen which are generalizations of some results by Roman ijen so that's why they were called ramanuja graphs and there is another number theory catch which is that they only show them when you need that D minus 1 is a prime which is congruent to 1 mod 4 okay so you don't get them for every value of D well you get them for like a bunch of values D whenever do you - one is a prime and actually this was subsequently improved by Morgenstern in like 94 or something to the case where any case when there's a finite field of size D minus one so D minus one being a prime power so that's great and these are also uh they're definitely explicit I think they're strongly explicit - or maybe there's like some tiny caveat but let me just say basically they're also strongly explicit and that's great - so these are wonderful graphs again they're let's say strongly explicit basically explicit for any de - one that's a prime power you just write them down and writing down the the graph is not hard the proof that they're a great expander is spectral expanders and super hard but they're actually like optimal Specter expectable expanders it's actually known that you cannot get anything better than this magic number - Rudy - 1 over D basically 2 over Rudy and that's you know it's really close to zero so it's like the epsilon zero in the definition of spectral expander is really close to 1 it's like 1 minus 2 over root D yeah so these these are great as well another thing that is nice about them there's a corollary of their number theoretic construction well this also also takes work but I like to mention it because it's like a super cute explicit construction of an expander graph that results after you work through all the number theory in their paper it's this consider the graph where you let V be the integers mod P where P is a prime and now what are the edges well you connect vertex or number a - 8 one and a minus one and one other thing but so far a plus 1 and a minus 1 like makes just a ring a cycle and the last thing you connect a to is 1 over a mod P ok and the P tents will ask what about if a is 0 well you treat 1 over 0 as 0 so you can just like put a self-loop on 0 ok and so this gives you like chords you know 1 over na is uh if you do it twice you get back to a so this makes an undirected graph and it gives you some chords and they're pretty pseudo-random as it turns out and indeed this is enough to give you a three regular explicit expander ok so the theorem is that this is like a and comma 3 why suppose I should say P P comma 3 comma epsilon zero spectral expander and the epsilon zero is really not great I think it's I don't know like 1 over 100 or something ok it's a positive constant K remember that means that all the eigenvalues the second largest sorry second smallest eigenvalue of the laplacian of this graph is at least 1 over 100 or the second largest eigen value of K equals one third a is that most 99 over hundred ok so that's all I wanted to say about Rama Newton type expanders and the last family of expanders I want to talk to about is what might be called zig zag esque expander graphs okay so these convicts paragraphs were developed by Rheingold vidonne and we gerson a little while ago and what's cool about them is uh they're like an explicit iterative construction of larger and larger expander graphs which is nice and the analysis is pretty combinatorial so you don't need I mean the crazy hard number theory of the Ramanuja graphs and you don't need this somewhat long and painful it's so bad but kind of long and mysterious linear algebra based proof of the Gabbard Lila spanner graphs and one more key aspect of this zig zag style expander graphs is actually these are the only known way to get those like really good bipartite expander graphs that we use in the applications well we only really needed it for the coding application if you remember way back when when we talked about these by part of expanders we use the fact that the expansion from the left side to the right side was at least point eight times the maximum possible thing like every set s here expanded to a set of vertices of size at least 0.8 times D times cardinality of s and getting that point eight is pretty hard and this zig zag based method is the only known way to achieve it so that's another good thing about this exact based method now I'm not gonna tell you exactly what is this like zig zag method for constructing expander graphs because subsequently just appearing will realize there's like slightly less complicated ways you could get similar iterative construction of expander graphs maybe again not with these ability to construct these great bipartite expanders but like sort of easier to analyze and one method uses what's called the replacement product for graphs and another method which was recently developed by dan Spielman in his course lecture notes uses some kind of like a line graph product method and probably one of these will be on the homework I won't have time to prove everything but these are sort of like slightly simpler than the zigzag product based method of constructing expander graphs so let me briefly sketch for you the idea behind the replacement product construction and this replacement product of grass is a way to take two graphs and put them together to make a bigger graph and it originally was analyzed in the paper prior to this exact stuff by Jam song totally in Bogota and well let me just quickly say here's the idea which perhaps we'll be exploring on homework what G be a well n n comma capital D comma epsilon sub G spectral expander and let H be a capital D little D epsilon sub H spectral expander okay and the funny thing here is the number of vertices of H is equal to the degree of G so think of G as like a big graph and think of H is like a little graph and they have this compatibility condition that the number of vertices of H is equal to the degree of G and there's this thing called replacement product so it's like our in a circle that takes G and H and makes like a new even bigger graph this is a G replacement product H and it's gonna have some nice properties so I can't even tell you how this replacement product graph works even though I said G was a big graph let's try this graph for G this is an example here [Music] okay this is not even a regular graph but okay don't worry about it so much so this is a three regular graph so you would need to that's capital T equals three so two replacement product it with H you would need a little three regular graph maybe a triangle that's age and how does the replacement product work basically you put a copy of H at every vertex of G so let's do that and then you put edges between these copies in the way the jihad' edges so we need some like edges here and some edges here and some edges here and here and here and maybe also some self loops and you know every vertex of g has degree 3 and now you know the Associated kind of cloud of vertices has three vertices so you can actually associate each vertex here with one of the outgoing edges and so like every vertex in one of these clouds has like one outgoing edge okay and so every vertex gets like one outgoing edge okay hopefully that was kind of clear and in fact to make it simple I lied it has one outgoing edge you know each of these in H the degree is little D so in each one of these copies of H every vertex has de sort of inter internal edge internal neighbors and instead of having each vertex have one external neighbor you make it have D parallel external neighbors D parallel edges as its external napola so you actually just like make you know three copies of each of these edges and the reason to do that is so that when you take now finally a random walk in this big graph every vertex has like little D internal edges and a little D like little crossing edges okay and so that's set up so that when you take a random walk and every step you have a half chance of doing an internal step and a half chance of doing an external step okay and okay when you do this construction what does that what are the properties of the new graph G replacement product age well it has d times and vertices because well we have G had n vertices and H had capital D vertices and it's new degree is two times little D that's because I said remember you know little HS d is little D regular and but we also for each vertex in and have D like external edges so every vertex has D plus D edges that it touches and the theorem that's basically in this charm son totally if we go to paper is that it also has pretty good spectral expansion its spectral expansion is epsilon H times epsilon G over 16 okay so if epsilon H and epsilon G are both like you know constants you multiply them together you still get like a positive constant over 16 and you know the idea is that you know Jesus expander so it's kind of like well connected and H is also expander so it's kind of well connected then you you know put them together in this way and you know it should also be pretty well let's connect it so this is a way of taking two smaller graphs you think of G is like pretty big H is like a constant size and making like you know constantly bigger graph so what happened here the number of vertices went up the idea is we're gonna start you know with some expander G and we'll do this replacement product with H and try to get a bigger expander so the number of vertices went up that's good just to make bigger graphs and the number of the degree actually went down if you think about G's degree was D you know now it went down to two times little D so that's good we kept the degree nice and small the only trouble is that the this you know this eigenvalue went down I mean let's say if you think of your big graph G is the main graph you know it used to be epsilon G and now you multiplied it by epsilon H over 16 so you kind of shrunk it and that's bad um but not so bad okay we kept the degree small we made a bigger graph okay the the spectrum went down let's do a different move to make the this eigenvalue bound go back up and one thing you can do is if you have a graph G another move you can do is square it so the square root of a graph has the same vertex set and you put an edge between U and V if there's a length to path from u to V in G actually this is not totally accurate what you should do is put if there are L different lengths two paths from u to V then you should put L parallel edges from u to V another way to put it is simply about the adjacency matrix gets squared that's exactly what's going on here and what happens when you do this well the number of vertices stays the same the degree goes from D to D squared because that's how many lengths two paths you have and well it's not hard to show that the eigenvalues of a squared are the squares of the eigenvalues of a and that's also true for the transition matrix K the eigenvalues of K squared or the squares of the eigenvalues of K so every like Kappa I just goes to Kappa squared okay so if these kappa eyes were all at most 1 minus epsilon in absolute value then now they're all at most 1 minus epsilon squared and absolute value so the gap went from the so called gap from one went from epsilon to well around two Epsilon okay maybe two epsilon minus epsilon squared but never mind about that basically two Epsilon so the point is this made the eigenvalue parameter this eigenvalue cap go up okay so this gap went up by a factor of two that's great the number of vertices stayed the same that's fine and the degree went up well that's kind of bad but we can up with these two moves together right because the replacement product makes any number of vertices get bigger that's good and it brings degrees down so if you make degrees go up then you can bring them back down with the replacement product and the replacement product makes the eigenvalue bound go down but squaring makes it go back up so that's good and you can kind of enter it iteratively interleave these and get good results actually more generally what you can do is look at the teeth power of G where T is an integer parameter this is the graph whose adjacency matrix is a to the power of T or you put L edges between U and V if there are L paths of length T from u to V and in general this has n vertices if the previous one had n vertices and the degree goes from D to D to the power of T and the eigenvalue gap epsilon basically goes up to around T times Epsilon okay that's ignoring this like lower returns okay so right so you can now iterate this as I said so how does this iteration work I'm just sketching things here but perhaps the full details will be on the homework let's say if H so H is this like small expander graph that you're gonna start with at the beginning and keep using over and over again as your replacement product friend and G is like a graph that's getting bigger and bigger and bigger so let's say H is a graph that has little D equals four okay for regular graph and let's say it's number of vertices is eight to the power of T and it's second smallest eigen value of its laplacian lambda1 is at least like 16 over T or something okay what I claim is that such a graph H exists so just by that claim for now so what happens so now if we interleave we start with a graph well we can start with a graph G and now we interleave the replacement product and this power of T operation and what happened so we replacement products and the degree becomes two times little D which is eight and let's see right let's work this out okay let's go back to a replacement of product here good so this capital D which is the number of vertices of age was like eight to the power of T so when we do this replacement product the number of vertices goes up by this constant factor eight the T and the degree is 2 D and the eigenvalue bound goes down by epsilon H over 18 and this is like the epsilon H sorry epsilon H over what I say 16 this is Epsilon H so this goes up by it goes down by this over 16 which is about eigenvalue gap goes down by around 1 over T okay but then when we power of T the eigenvalue you know power of t eigenvalue gap you know basically goes back up by T so basically the eigen value gap stays constant when you do these two operations and the number of vertices is getting larger and larger by a factor of 8 to the T at each step okay and the point is that right since the degree of the graph at each step is 8 when you raise it to the power of T in this step the degree goes to 8 to the T which is the number of vertices in age okay so I got a little bit muddled there but I think it all matches up and then when you you know replacement product this goes back down to 2 times little D which is 8 okay so the degree goes from 8 8 to the T back down to the 8 and so forth and the eigenvalue gap that goes up by a factor of so i goes down by a factor of 1 over t but goes back up by a factor of T and so forth and so on ok and uh this is great it makes bigger and bigger graphs constant degree 8 I can value is like some absolute constant and actually really all you need to make this work is like a starting graph H so you just need like a starting graph H which has these properties for some tea with these properties for some tea the required properties for some tea and you just let the argue that that graph exists because then you know an algorithm can find it in constant time and indeed such a graph exists an H exists it can be shown with T being I don't like 30 or something I somehow convinced myself of that like a random graph will have these properties okay so actually this last step is uh makes it not so practical that you have to like root for search for this but actually there are other ways you can take like a little mini expander or using one of the other constructions like a Ramanuja graph or something just to assure yourself that this finite graph exists and then you can use it in this iterative process to make bigger and bigger expander graphs okay so this is basically the idea by of the zig zag product this is the replacement graph version and as I mentioned you know one great utility of this is it it's the only known way to make some of these most sophisticated bipartite expanders with really great expansion so those are three ish different ways you can get explicit expander graphs for your many applications coding Theory D randomization and not\", metadata={'source': 'j6JzqPkvRHM'}),\n",
       " Document(page_content=\"Oh okay let's get started this is lecture 17 we're starting a new kind of unit here short unit on like linear programming and semi definite programming which will extend into a longer meeting or longer unit on constraint satisfaction problems and hierarchies and some other things okay so as always I'll put up some resources here a nice simple book there's many many books but a nice simple book about linear programming is this one by Metuchen Gartner and if you want the full details on every possible aspect of the theory of linear programming you can check the Croce Lobos Shriver book at the bottom okay so linear programming I know you probably know something about it but we're gonna go over it again because it's definitely the greatest polynomial time algorithm let's just be honest about it it's fantastic actually there's a bit of a I miss not misnomer but I said it quite correctly our programming is not an algorithm it's a problem but the fact that this problem is solvable in polynomial time is you know in my opinion the greatest fact in Hawkins theory because so many things can be done with linear programming and it's really great okay so what is the linear programming problem the answer the input to the linear programming problem is a list of inequalities okay so you know when you're solving a system of linear equations you have a list of equations in variables x1 through xn here we have a list of inequalities in equations and let's say n variables and M unknowns so you can think of everything happening over the reals but since you know you're supposed to be given this as an input to a digital computer we assume that the coefficients the A's and B's are all rational numbers represented as just pairs of integers numerator and denominator okay that's the input and there's a geometric interpretation to it so for every inequality like this a 1 X 1 plus a 2 X 2 plus a plus a and X n that's geometrically 1/2 space the set of the points in real space that satisfy this is a half space so if you had equality like a 1 X 1 plus dot dot plus a n xn equals B that's a half plane or a hyperplane okay and the inequality gives you everything on one side of the hyperplane and so K is supposed to represent the conjunction of these so we have a sort of a conjunction or intersection if you will of hyperplanes and their intersection is the set K it's a it's like a polytope represented by these inequalities now sometimes you might have lots of inequalities but the intersection of all of them might be empty so I put this green inequality up here and if you look at the green inequality it misses the triangle that was formerly K so it's possible that the intersection of all these inequalities leaves nothing and in some sense this is the linear programming problem to tell is there a you know is this K the empty set or is there at least one pointed okay so that's the input and yeah so it looks a little bit oversimplified because I only have inequalities but you can make a few variants on this as you possibly know so if you want to have less than or equal to other instead of greater than or equal to well you can do that just by negating the inequality if you want to have equality instead of just inequalities or a mix of inequalities and equality's that's no problem as well you can just put in two inequalities in both directions less than or equal to and if you want to have strict inequality well can't have it sorry about that but that's one thing that's not allowed we always want to just have greater than or equal to than these linear programming problems and you might be most familiar with linear programming not as just you know you're given a bunch of inequalities and you want to know you know can they be satisfied or not is there a point in the intersection of all these half spaces but rather it's a problem of you know given a bunch of the inequalities to maximize a linear form like this C dot X here subject to these inequalities and this also called linear programming and you can do that too but we'll get there a little bit later for the purposes so far we'll just stick with this question about you know given some inequalities can they be satisfied simultaneously or not okay are there any questions at this point okay not hearing anything hopefully you can all still follow along as I go ok oh right so you know you can leave this as a decision problem given these inequalities just decide yes or no is there a solution to all of them but you might ask for a bit more for example if the answer is yes there's at least one solution K is not empty then the natural question is to actually find a point X that satisfies all the inequalities and interestingly enough if K is the empty set you shouldn't you know ideally just say yep it's empty what would be really great out of an algorithm given such a K would be to output a proof that there's no solution to these inequalities and in fact we're gonna talk about how that's possible but we'll come back to that exactly what a proof is I should remind you or what I want to tell you though is the this problem has stated the linear programming problem is solvable in polynomial time and that's a wonderful fact that we're gonna actually take for granted in this lecture but we'll talk about it in subsequent lectures this was first proved by kutcheon in 1979 it was the USSR time so he's I suppose a Soviet person although you can tell he's of Armenian heritage from his surname oh he was actually a PhD student at the time when he solved this it's a little bit funny though because he solved it using an algorithm called the ellipsoid algorithm which we'll talk about later you know in the 50s and 60s when this was studied you know a lot in America and other places they didn't know if it's polynomial time solvable or not but they had a different algorithm the simplex algorithm which they seem to think worked well in practice it was a big question of whether you could actually solve it you know in P and you can thanks to the Koch John's work as I said though you use this thing called the ellipsoid algorithm which is a ridge developed by Nam Shore in the early 70s and uses a tool for convex optimization by nemirovsky and Union in the 70s and I think they were like more practically minded and they weren't really thinking about like Oh lists you know see if it's shows that linear programming is solvable in PE on a Turing machine so the idea is and techniques date back to these works but caught Xian published in 1979 proof that it's in P actually he didn't give the proves he just gave the sketch of how it worked and it wasn't until um everybody you know in the West got excited about this and particularly these two Hungarian guys who are a nice bridge between the East and West Goths and low vod's Lovaas they kind of read his work and figure out how to fill in the gaps in the proof so maybe they were the first to publish a proof although in anyone caught UN publish the full details of this proof and I should also mention shortly thereafter Gautreaux lavash and Shriver showed that you can do many other related things in polynomial time like find vertices and optimize even with a separation Oracle and all these other things that we'll eventually get into uh I just want to briefly say that you know it's funny to think about but it was what forty years ago it was really big news back then so this is the front page of the New York Times in November December 1979 you can say down here it was front page news a Soviet discovery rocks the world of mathematics it was about Koch ends paper so you can I don't take a look at these slides afterwards actually I thought it was a little bit funnier to show like a follow-up article from I think a month later or even less than a month later this is from Tuesday November 27th 1979 front page of the science section and it was about coffee on himself Soviet mathematician is obscure no more it says and yeah that's the top top story clue special Sh yeah but about Cauchy on is I just think it's funny you know it starts out by talking about the mystery author of a new mathematical theorem and the dark haired bachelor says he was somewhat surprised by the enthusiastic response and so forth and it ends why this is the end of the article and says you know his absorption in his work appears to be total I tried to take up karate as about three years ago he said but now I do mathematics instead so good times yeah I was another follow-up article from New York Times and it's also you know when you know Cotchin showed us the new program isn't p it made the front page of The Guardian and Der Spiegel so it's really big news so I'm you know happy to tell you about it today actually probably Koch eons daughter is more famous than watching on himself right now you can follow her on Twitter about 46,000 followers okay but enough history lessons let's go back to the math so here's the linear programming problem again and actually what I would actually talk about today we will talk about the polynomial time algorithm yep we're going to talk about sort of the theory of it and why it's possible that you can solve it in polynomial time in the next lecture on Thursday we're actually going to talk about applications of linear programming in combinatorial optimization but today will be about sort of the theory of linear programming and let's actually one way where I want to say something about that you know the fact that this problem is solvable you know the fact that when K is empty you can find a point X in polynomial time one interesting thing that that implies is that a solution if it exists can be written down with polynomial in many bits and that's actually not obvious so that's interesting we'll get into that and you know on the other side what if K is the empty set well we'll think about that too so let's now to sort of dive into this question about what is meant by a proof that a system of linear inequalities\", metadata={'source': 'DYAIIUuAaGA'}),\n",
       " Document(page_content=\"okay all right so what does it mean by a proof or a certificate that a system of linear equations like this has no solution well suppose this was your input instance and I suppose I just told you hey add up all these inequalities you know if there's a solution to all these inequalities then that solution X should also satisfy the some of the inequalities and imagine like by some weird miracle when you just formally add up the left-hand sides like all the coefficients sums add up to zero the left-hand side cancels out completely and you get zero on the left-hand side and the right-hand side adds up to like the number one or some other positive number okay then that would show that no matter what X is if it satisfy all these inequalities it would also satisfy their sum and in this miraculous setting the sum is zero greater than or equal to one which is you know not true and that would be a proof or that would be like a demonstration that these inequalities have no solution you can generalize this a bit more generally you know if you have an inequality you can always multiply it by a non-negative number lambda I and it'll remain equivalent and suppose I pointed out some non-negative multipliers lambda 1 through lambda M and you know you multiply the inequalities up with these numbers lambda 1 through lambda M and added them up and then the miraculous thing happened which maybe is less miraculous if you choose these lambdas carefully that everything in the left-hand side cancels out and everything on the right-hand side becomes the constant 1 okay this would be also demonstrating that there's no solution to these inequalities and this is the kind of proof of insert unsatisfiability that we're gonna be talking about for linear programs right so you'll multiply each of these inequalities by lambdas that are non-negative and then you'll add them all up and all the coefficients will come out to 0 on the left hand side and on the right hand side you'll get the number 1 so let me do something here ok great if such lambdas existed whenever the system was unsatisfiable then it would mean you have like a cool succinct way to prove that unsatisfiable systems are unsatisfiable and this always happens and this is the content of what's called the Farkas lemma of Farkas lemma or it's basically the same thing as LP duality and it's this theorem which I'll show to you that whenever a list of in equations has no solution so the K that they define is the empty set there is a proof of this sort these non-negative lambdas which when you multiply them against the in equations and add them up they give zero greater than or equal to one such lambdas always exist okay this is proved by Farkas in 1898 basically also proved by Gordon in 1873 but somehow he never gets the credit he's actually probably most famous for being I mean there's PhD adviser these days okay I'm getting a question here let me check it out on chat the question is does that mean that if all the coefficients on x1 were positive it's always satisfiable yeah that's exactly true so it is true that if all these equation there co efficients on x1 are positive then it's always satisfiable and the reason is you have a solution where you just said let's say x2 through xn all to 0 and then set x1 to be some enormous number positive number big enough so that I would exceed all the right-hand sides B 1 through B M ok so this relies on the fact that you know our in equations are all of this form the he'll have greater than or equal to signs so that makes sense ok good cool good question all right and this also by the way this was known in like the 50s and stuff in 40's and basically if you ignore these things about bit complexity it tells you that you know obviously if a list of equations has a solution then you know you can just show the solution X and so that proves that linear programming is in NP and this also Farkas lemma also applies that when there is no solution you can just show these lambdas and it's sort of a witness or certificate that there's no solution so if people knew all along that linear programming was in NP intersect qohen P if you remember this complexity theory stuff and that kind of hinted that it was perhaps just in polynomial time solvable in P which indeed proved to be true okay so I'm gonna show you a proof of the Farkas lemma it's sort of a proof by example and the proof uses this thing called the Fourier Motzkin elimination algorithm which in a sense is an algorithm for solving linear programming but as we'll see it's like an extremely inefficient algorithm but it's an algorithm that is useful for theoretical reasoning good so I suppose this is your input there's just three variables here so instead of X 1 X 2 X 3 I'll call them x y&z and so you have these 5 inequalities okay and we're gonna try to reason about whether or not they're simultaneously satisfiable so just like you know you're taught in you know high school or whenever to solve systems of linear equations by this technique of first eliminate the first variable then eliminate the second variable then eliminate the third variable you can also do that with linear inequalities and that's what for today Motzkin elimination does so the step one is to someone take this system of linear equations and find another system that doesn't involve X at all and whose satisfiability status is the same as the satisfiability status of the original system so how are you gonna eliminate X so let me let me tell you first thing you do is you multiply all the inequalities by positive constants so as to make the coefficient in front of X either plus 1 or minus 1 okay so whenever X had a plus 1 or a positive coefficient before it's gonna have a plus 1 coefficient here on the the new set and whenever I had a minus 1 coefficient before we're sorry a negative coefficient before it's gonna have a negative code and negative 1 coefficient on X in the new set ok so that's easy and it doesn't change the set of solutions in fact now don't change the in equations at all yet but mentally think of them as being upper bounds and lower bounds on X so whenever you have like positive X you can think of this as an equation on the left here that's like X is bigger than some stuff involving y and z in constants and whenever you have an equation whose coefficient on the right is minus you have minus X you can think of this if you move the minus X to the other side and the constant to the left side you can think of this as an in equation of the form X is less than or equal to some stuff great so and if you have some inequality in the right system here that doesn't involve X just like temporarily you forget about it for a moment so we'll just think about what bounds are implied for X here now you see it's not too hard to understand you have some expressions that X is supposed to be bigger than and some expressions that X are supposed to be smaller than and like think of y&z I just put the original system back up there think of y&z as numbers temporarily think of buoyancy is numbers but X is like a variable and then over here your constraints on X are then X is bigger than some numbers and X is smaller than some numbers and you see there is a legal or an acceptable solution for just the variable X if and only if all the lower bound numbers are smaller than or equal to all the upper bound numbers right so this thing involving x over here on the left is satisfiable or has a satisfying x if and only if all the lower bounds are smaller than the upper bounds so we can just kind of take a cross product here of all possible pairs of a lower bound and an upper bound and make a new an equation about it okay so we have the red lower bounds or reddish lower bounds and the greenish upper bounds and we'll sort of take this cross product so this is actually to make a lot of mean equations as we do this but as I mentioned this algorithm is not efficient and on the right now here we have foreign equations which are satisfiable if and only if this set on the left for given y&z there satisfiable or satisfied if and only if for this kind of left to exists satisfying X this make sense so this is what I said there's a satisfying X on the left if and only if we have all these upper and lower bounds you know square with each other and let me just take them and simplify them push you know collect all the terms so I've done that here you can take my word for it and now we can just remember the original system we were thinking about we also have this one in equation that didn't involve X at all so let's stick it back in and we've actually done it we've actually shown that this initial system with the X's is going to be satisfiable if it only at this new system with no X is satisfiable okay so we've successfully eliminated X ok any questions about this ok great ok so obviously the next step is to eliminate Y and I won't go through that whole process again but I did it I probably made some numerical errors but I believe when you eliminate Y in this specific example you get down to just a bunch of inequalities that only involve Z and actually when you're down to one variable it's easy to check if it's satisfiable or not but we can even go like the whole Megillah and eliminate Z itself eliminate the last variable and when you do that you get down to just a bunch of inequalities involving numbers okay and then it's real easy to check if they're satisfiable or not you just check hey are all the right-hand sides negative and actually in this case they're not and that implies that the original original system was not satisfiable okay but on the other hand if all the right-hand sides were negative then that would imply that the original system was satisfiable so what I've shown you here is a highly inefficient algorithm for deciding the linear programming problem there's a certificate right there well this is a witness that it's unsatisfiable let me also just mention that you can always multiply an inequality by a non-negative number and it preserves the inequality so you can always get your final you know you know unsatisfiable an equation to be 0 greater than or equal to 1 if you'd like and now I'm gonna make a key claim what a claim is that every inequality that you generate in this whole Fourier Motzkin elimination algorithm is a non-negative combination of the original original inequalities and in particular if that's true that it means even this last one in the unsatisfiable case even this last one 0 the greater than or equal to 1 is a non-negative combination of the original inequalities and then actually is going to prove the Farkas lemma right that's what we're trying to show when it's unsatisfiable there's some way to add up the original inequalities with non-negative multipliers to get the blatant contradiction 0 is greater than or equal to 1 and let me just illustrate this claim for you well I mean I'll kind of go over the steps quite fast here was the original system on the left the first thing we did was multiply the inequalities by positive constants to get like all plus or minus 1 coefficients on X so that's multiplying them by just non-negative coefficients and then we eliminated X and if you think about it if you this is the previous version with X this is the version with X eliminated and if you think about it actually we got all these first foreign equations here involving y&z by taking just pairwise sums of the plus X in equations and the minus X in equations on the left okay so for example we got this first one involving y&z by just summing the first in equation with a plus 1 on X in the first equation with the -1 X okay so that indeed demonstrates that after this elimination step all the in equations here indeed were non negative combinations of original in equations okay and then you know in the next step the when we eliminate Y and get down to Z is just Z's all those just Z's acquit inequalities will you know be non-negative linear combinations of these in equations here on the right which in turn were non negative in equation or multiples of these in equations involving XY and Z in the original case so on all in all as you go along you maintain this invariant that everything is a non-negative linear combination of the original in equations does that make sense any questions okay oh great so but um you know nutshell proves this for cash Lamar this LP duality statement now let's make some more observations let's go back to this for a Motzkin elimination now we're gonna just think about a little bit more in fact you know I told you this was a good thing to think about when you're trying to reason about what happens when the system is unsatisfiable you'll eventually be able to make zero greater than or equal to one as the linear combination but actually if it is satisfiable and in the final step you get all down to zero greater than or equal to some negative numbers it's an easy exercise to show that actually you can now sort of go back one variable at a time and reconstruct a solution in fact a rational solution because I don't know for example let's say you've you know inductively done this so far and you have some solutions for y and z and now you just need to figure out how to set X such that all these equations on the left are satisfied well the way it's set up right with these wines even now being numbers the whole point is that and being satisfiable means there is a value of x which is between all the upper bounds and the lower bounds so you can just choose any x it's between these upper bounds and the lower bounds that you worked out and that'll be an acceptable setting for the variable X okay so you can inductively build back a solution from the eliminated variables one by one in particular that shows that you know if you start out with rational coefficients which we always assume then you can actually get back a rational solution okay so it's a minor comment but there's always a rational solution most like said this is a highly inefficient algorithm it's not the polynomial time algorithm for linear programming and in particular you see if you start out with like let's say M inequalities you're doing this elimination step maybe half of them have a positive coefficient and half of them have a negative coefficient you take like the cross product between them you'll generate M over 2 squared inequalities so even eliminating one variable kind of squares the number of inequalities you have so if you eliminate n variables you get a doubly exponential time algorithm like m to the 2 to the end okay which is of course very bad but we learn some things by thinking about it we learned the Farkas lemma we learned there's always a rational solution when there is a solution so it's still worth thinking about right so is there a polynomial time algorithm for linear programming well there is as I mentioned but we'll get to there in the end after next actually maybe two lectures from now but before getting there we should actually you know again come back to this sort of technical but interesting to me question before wondering if there is a polynomial time algorithm we need to wonder if there are even polynomial bit solutions okay so remember a solution when it is feasible when K is feasible or non-empty you're looking to find a quote-unquote feasible X meaning and exit satisfies all the inequalities or else you want lambdas which prove that x equals the empty set and here you know what do I mean by bit size well remember you know we're working with digital computers and you know either the word RAM or the Turing machine model but in any case every number it's you know can't work with it by magic you have to store it's gonna be a rational number so I'm gonna use this notation just in this lecture it's traditional the bit size of an object I put in angle brackets and it's the number of bits needed to encode it and like a standard method okay so for an integer it's the number of you know base two digits of X it's a rational number we're just gonna store it as like a numerator integer and a denominator integer you know if it's a vector then it just means you know the sum of the encoding lengths of all the numbers in the vector same as a matrix etc etc and an important theorem for than your programming which is sort of necessary in order to show that there's a polynomial time algorithm for linear programming it's to show that the output that you're looking for you know the feasible X if it is a satisfiable and the proof of infeasibility if it's not satisfiable can be written down with polynomially many bits so um luckily that's true so here the theorem that working a reason about it says let's say you're given a system of inequalities K and its traditional and linear programming to write capital L for the number of bits needed to encode K so there's a number that's like M plus n plus the encoding length of all the numbers and the inequalities and it says you're in good shape so if K is non-empty there is at least one solution then not only is it our rational solution X but there's one that you can write down with poly n bits and if K is MP said then there exists a rational proof of these lambdas that also have only polynomial in almaty bits and what is the proof of this well that's what I want to talk about for a little while first thing I'll say is actually it's a physis to show bullet point one it's Spice's to show that when there is a solution there's a solution with Palio many bits in it and what is the reason for this well if we suppose we knew it was true for bullet point 1 how could we reason about polynomial bit size for the lambdas in the case where it's unsatisfiable well and this is a bit of a detour but the reason is once you know it's unsatisfiable or when you're interested in this the question of given K are there lambdas which certify that K is unsatisfiable this itself is a linear programming problem in the variables lambda so I'll talk about why that's the case in a moment but this is called the dual linear programming linear program and this dual linear program in the lambdas it's bit size is about the same as the bit size of your actual system K and so this is why it suffice as the show bullet point number one assuming this this fact about there being a linear programming for the exist linear program for the existence of the lambdas you know to get bullet point two well you know you just use bullet point one but applied to the dual in your program about the lambdas whose bit size is about the same it's also about Al okay so uh yeah why is this question of finding the lambdas satisfying unsatisfiability of K why is in itself a linear program well let's just look back at the Farkas lemma which is here you know this is our input data these in equations involving the X's and A's and the Farkas lemma says that if K is empty said if these are unsatisfiable there's these non-negative lambdas which when you multiply them against the inequalities and add everything up you get zero greater than or equal to one so let's just a bit here's we're looking for lambdas which are non negative which is a good start right you're allowed to have these as constraints in a linear program think of now the lambdas is the variables and the X's are kind of gonna go away and the A's are gonna remain constants and the B's are gonna remain constants so yeah so now if you think about this a little bit more you know this to say that let me go back a little bit to say that the lambda is when you multiply them against the inequalities and add them up to say that you get a coefficient of 0 on X 1 it's just to say that lambda 1 a 1 1 plus lambda 2 a 2 1 plus lambda M plus a M 1 is equal to 0 and that's encoded by this vector matrix multiplication you know that first constrain is saying that this row vector of lambda times this column vector of s equals 0 and we want a similar thing for all of the columns we want the row vector of lambda times the second column of a's equals 0 dotted with i should say and it should be true for all of the columns it should equal to 0 and then we also want that the dot product to the lambda is with the bees should equal the number one so really what we've got here is a bunch of equations on the variables lambda plus the constraints that the lambdas be non-negative and this is indeed a valid linear program and this program is the dual linear program it's the one about certifying unsatisfiability of the original K any questions about this\", metadata={'source': 'aGqLfpkiYRM'}),\n",
       " Document(page_content=\"so good so as I said this means for our theorem here about bit sizes it suffices to show this you know one state number one that when you have a list system in equations and the bit size is L takes L bits to write down your input then assuming there is a solution there's a rational solution that can also be written down with poly almani bits okay so let's think about how to prove this try to draw a picture here this is uh RN well I guess and it's kind of like to on my slides but it's like RN and here we have like three inequalities depicted in their intersection is non-empty it's this triangle okay okay so the first thing that I want to claim is that when K is not the empty set so there are at least you know there's at least one point inside K then there's always a feasible not just a piece of all point X inside but a feasible vertex which I'll call X star and what is the vertex it's like a corner so this is an example it's you know one of these corners okay it's like a polytope you know it's like a vertex it's like a corner of the polytope and this is actually not true due to like a little annoyance but we'll come back to this stupid annoyance later so just for a long prove that this is assume this is true because it's true in essentially all cases or in the knife's case okay and these vertices which are also called extreme points or basic feasible solutions are very important in the theory of linear programming which is why I guess they have a threefold synonyms here vertex extreme point basic feasible solution these are all kind of the same thing in fact we're gonna reason about not only well there exists you know a solution ax with pong little bit size but actually one of these vertices in fact all of the vertices of all the corners will have pala need a little bit size needed to write them down so what is the vertex remember in this picture right you have a bunch of M linear in equations and your your initial K and if you turn the inequalities into equality's and you get like a half space these are sort of depicted by these white lines over here in the diagram these are Hopf spaces and the the inequalities are the half spaces and we're in n dimensions so a vertex is really what it means is it's the unique solution of some n linearly independent half space equation so if you think of like an inequality as an equation and in your mind and you that it's defining a half space or like yeah an equality you know we kind of know unless modular much low annoyances if you have n equations in n unknowns then there's much low annoyances you know a unique solution it's true when they're linearly independent and these are affine have spaces by the way and meaning they don't necessarily go through the origin there's a nonzero right-hand side and these solutions are these vertices like the corners of K okay so in this we're an is-2 these yellow lines which are your house spaces the intersect here two lines to a in two dimensions that intersect at this vertex of K and therefore it means that this vertex here or any vertex X star is the solution to an N by n system of equations like a subsystem of the M in equations that you're given you view them as equations and then the vertices like this vertex is the solution to an N by n system of equations and this means that the Syst the solution this vertex X star can be written down with polynomial in L you know many bits and the reason for that is basically because you know solving systems of equations is a polynomial time algorithm you know we have Gaussian elimination you take some rational system of equations and variables and knowns any one such a system that defines a vertex you solve it and you get a vertex and you know since the polling wheel time algorithm you only have time to output polynomial many bits which means that X star the vertex can be written down with Palio many many bits so quotation marks here but we'll come back to that in a moment actually I just want to mention but I'm a little bit alighting some complications here because in fact the fact that Gaussian elimination is in pong mealtime is pretty non-trivial fact to prove my Edmonds in 1967 it needs a little bit of sophistication and needs Kramer's rule so the you know the algorithm you remember from linear algebra class you know takes polynomials steps in and if you're solving an N by n system for sure and cube steps maybe I have a question here the question is what is Gaussian elimination Gauss elimination is the you know classic algorithm that you learn in linear algebra class for solving like an N by n system of equations usually you kind of just Gaussian elimination refers to the algorithm you take like the matrix for the equations and you kind of it's kind of similar to for a Motzkin like you kind of clear out you know a whole column you try to eliminate the first variable and eliminate the second variable and so forth so basically it's the algorithm you learn for solving systems of equations and yeah this as I said this you know involves N squared or n cube steps or something but the thing that's annoying is checking you know these rational numbers that you generate as you go along that they don't you know blow up and require exponentially many bits to write down the numbers that are generated but it's true um and perhaps you'll see that on the homework so let's just take God for a given that if I give you a system of n variables and unknowns all the coefficients are rational you can solve an appalling mealtime and so the answer is probably no many many bits to write down okay any further questions about this to make sense okay oh so what's this annoyance that we need to come back to well you know it says that sometimes there are no vertices so here's a perfectly valid linear program n is 2 and the number of n equations is 1 it's just like X 1 plus X 2 is greater than equal to 1 if 1/2 space here's the feasible region it's everything above you know to the northeast of that and there's no vertices it's just a it's just a line uh so that's a problem if you have vertices if K has at least one vertex then you can conclude that it has you know this vertex can be written down it's polynomially many bits you can include that there's a polynomial size solution you could have these like terrible LPS with no vertices but the only way this could happen is if like they're unbounded which is perfectly normal thing that can happen but we want to get around this so the preceding proof would be fine about vertices and so forth if K always included what I call big box constraints this is not a real term but it's just a term I made up for this lecture what are big box constraints I say you have unquote big box constraints if your linear program you're seventeen equations also includes for every variable X I two-sided equation saying X is between negative B and B where B is like some really big number okay it's like saying oh I my linear programming this K but plus like all I'm only want to worry about X's that are inside a gigantic cube a side length to be okay and be the kind of B's I'm gonna be thinking about are ones whose representation size is polynomial in L but remember that means that B itself like a P is an integer it's actual magnitude is 2 to the Palio but that's okay because you write it down in base2 so the number of bits needed to represent it is Polly L so uh we're gonna like it if you have big box constraints because if you have big box constraints then you know like this then even if your original K we're like unbounded in some directions you know the fact that you have a big box forces the intersection of K in this big box to have some vertices okay so what I want to show is basically whenever you have a linear program K with bit complexity L hmm you can sort of without loss of generality put in big box constraints for a big value B that takes polynomial and alle many bits to write down okay and once we show that then all this argumentation about there exists the vertex is true and therefore there exists a rational solution and that takes pawn and really many bits to write down is true okay does that make sense any questions whoops okay great uh okay and yeah what's nice about this is these are give you always give you n linearly independent equations if you imagine converting through the equations they just look like X I equals B and X I equals negative B you have one for each variable so it's kind of like you have you know and linear independent equations which is another reason you'll be able to find you know and linear independent equations making a vertex now I want to show you that you can always have these big box constraints and it almost seems like I'm making like a circular argument here because you know this is sort of what we're trying to reason about in the first place that we were trying to reason that you could a priori you'll sort of only worry about numbers that are at most polynomially many bits large but you know I'm gonna show you how to make it non circular so in order to do this I have to talk about converting a linear program of the type we've been interested in before just with all in equations like this which is called standard form linear program to an equivalent program and what's called equation form and LP is said to be an equation form if they're basically no inequalities you only have equality's well that can't quite be right because I mean then it would just be a linear system of equations but you have all variables constrained to be non-negative so equation form is a special kind of LP where you have only equality's but you insist that all the variables be non-negative and what I'm going to show you now is that given any standard form LP you can convert it into an equivalent equation will form LP okay where it's like easy to go back so they have the same satisfiability status and it's easy to go back and forth between a solution for one and a solution for the other okay so how are we going this is just some simple tricks for converting this to an equation form so trick number one is in K the excise the variables are not constrained to be non-negative but they're supposed to be in this final version K Prime the X lies are not constrained at all per se they're just involved in the in equations well this is no problem the trick here is the following you take each variable X I and you replace it with two variables called X I plus and X I - well you don't replace it with that you replace it you introduce two new variables X i+ X I and negative and you replace X I with their difference and you see if you plug you know this into some in equations it turns the one in equation into an another in equation just in twice as many variables and you are gonna constrain X I plus and X I a - to be non-negative in your final K prime as needed and this is wise its valid it's just valid because you know any real number can be written as a difference of two non-negative real numbers regardless of what the initial real number is positive or negative or zero you can always write it as the difference of two non negative numbers and conversely the difference of two non-negative numbers has no particular constraint on its sign so this is a valid move so that takes care of this aspect the other problem besides the variables not constrained to be non-negative is that you know your constraints are in equations and they're supposed to be equality's okay and the way to get around this is also pretty easy you take in each in equation like this you know like this a dot X is greater than or equal to zero and you replace it with a dot X minus B equals s this s is called a slack variable it's a new variable that you introduce and you know we see it has to be constrained to be non-negative so indeed si is constrained to be non-negative and now what you see that this equation with the new slack variable you have one of these for each original inequality this new equation together with the constraint that the slack variable is not negative this is equivalent to the original inequality that said you know a dot X is at least B so same as saying a dot X minus B is non-negative good so if you put these two fixes together it does the job it converts this K that has only inequalities so this K Prime that has only equality's but with all the variables non-negative constraint to be non-negative any questions about this okay good so let's go back here right so what I've told you now is that we can take any standard form LPK and convert it to an equation form LP that's equivalent and let's write this equation form LP in a bit of a simpler form oh let me just also remark that you can obtain K Prime efficiently from K and you know the number of K problem is very similar to K so the number of bits you need to write down K prime is probably linear the number of bits you need to write down k and they're polynomial related so we can whatever given K remember what are we working towards here we're working towards trying to show that given any feasible LP or even infeasible LP any LP you can sort of without loss of generality put in these big box constraints without affecting satisfiability and what I show you here is you know given K you can just convert it to equation form and think about that okay so let's write K prime a little bit more briefly so X's we're gonna call the variables X now even though really they're you know it's like the exile us and - and also the S is but I'm gonna kind of forget all that and just say okay let's just call the new variables X and the newbies B and so yeah an equation will form LP looks like this it looks like a it's a matrix here so it looks like ax equals B that's your a bunch of equations and X square equal to 0 and this is a shorthand for the coordinate wise statement that each of the components of X X 1 through X and X greater than or equal to 0 we're going to continue again to call the number of variables n in the number of constraints M even though it actually changes a little bit when you go from standard form to equation or form okay so let me simplify this a bit more okay and now once you get to equational for more once you have an a help you in equational form you can make a few other simplifications or a little observation so ah let's say a is M by n you know there's n variables and M constraints and another thing you got assume without loss of generality is that a has full rank so if you remember your linear algebra what does this mean it means each equation is linearly I mean the leading equations are linearly independent of one another no equation is you know non-negative sorry is a linear combination of other equations like no equation is automatically implied by other equations so all the equations you know are linearly independent they stand on their own and this is without loss of generality because if you have some like redundant equations or equations that are applied by other equations you can just eliminate them from K Prime and it won't change K Prime's nature so we could assume without loss of generality and an algorithm can actually algorithmically do this efficiently if it wants again by basic linear algebra stuff how's the elimination again in fact from row reduction type stuff so we can assume that aunts full rank and so actually the number of rows of a will be less than or equal to the number of variables okay you'll have n variables and you'll have either you know 1 2 3 4 5 . and equations for some value n between 1 and n and so this ax equals B this is like a subspace it's an affine subspace and K Prime is saying you know K Prime is like everything that's on this subspace I'll draw a picture in a second we where all the coordinates are non-negative ok so I'll draw a picture in a second but now let me just say I want to show that once we're in this equation will form like this whenever you know K Prime has a solution you know it has a solution where the bit complexity is not too big and therefore we can equivalently add these big box constraints okay so here's a picture I'm gonna try to draw a picture when n is 3 and right so in K Prime you have this non negativity constraint so all your solutions should be lying in this non-negative for fans I'm gonna supposed to be you're supposed to imagine that like you're sort of looking at the origin the non-negative org fan is like close to your face okay and then what the picture looks like depends on what M is like the number of equations involved in K Prime so let's imagine case one that M equals n so you have like n equations and n unknowns and they're linearly independent okay in that case K prime is just a point it's the solution to the n equations and K prime being feasible or you know satisfiable simply means that this point is in the non negative or that like all its coordinates are non-negative oh so you're right this point is just the solution to ache x equals B in this case and you know just for these reasons we talked about you know since solving a system of equations can be done in polynomial time the solution to a system of linear equations can be written down with polynomially many bits so in this case where your equation form solution has M equals n your solution will have Polly many many bits and therefore it's okay to take you know B to be a number that's a bit you know that also needs can be written with polynomially many bits like an integer that's like a bit bigger than the maximum possible a solution to a system of equations in your your LPS okay does that make sense so I want to argue that like of you you're having an alkene equational form maybe you've converted to an equation form you got rid of the redundant equality's if you have you know n equality's then the solution provided exists as inside a big box but not too big one that can be written where the side lengths can be written is polynomially many bits okay so case two what if M is n minus 1 well in that case you have n minus 1 equations and n unknowns the solution to that is a line it's like an infinite line and then K Prime is like the intersection of this infinite line with the non- orphans which you know the line might or might not pass through the non- orth and thus the question of whether K Prime is satisfiable or not it has a solution or not but if it does then the picture sort of looks like this if you can try to imagine what I'm drawing here it's like a line piercing the non-negative or Thant and making like a line segment it'll at least be a half infinite segment if not a finite segment where I want argue is it has to hit at least one of the coordinate planes the three coordinate planes X 1 equals 0 X 2 equals 0 x 3 equals 0 you can imagine it fits this line is exactly parallel to one of the axes like it might hit only one coordinate plane assuming it does hit the earth in it non-negative or thin but there's no way like you can hit the or Thant yet like Miss all the coordinate planes so what I'm arguing there is the algorithm doesn't have to know this or anything and now we're gonna just have to know this but whenever you have a feasible K Prime like this where M is n minus 1 so you have this line that does pierce the non negative orphaned it has to hit at least one coordinate plane and therefore there exists some equation like X I equals 0 that you can throw in pathetically without changing satisfiability well once you've done that you've reduced the case of M equals n ok and the solution is just a point it's the solution to the whole system of equations together with this new X I equals 0 so again you know yada yada yada you can the solution with this X I equals 0 will be expressible with polynomially many bits and the big boxing thing is ok ok and this generalizes so if M is n minus 2 then K prime is the solution to n minus 2 equations and n unknowns it's like a two-dimensional flat intersected with the non negative or fence and again there's no way you can hit the non North is not a negative or fan without hitting at least one with these coordinate planes X I equals 0 therefore again you can throw at least some equation in the form X I equals 0 which gives you like a new equation and it reduces m to n minus 1 and then you can like inductively argue that you can reduce from em n minus 1 to M etc and so yeah what I'm trying to say is no matter what actually M is whatever the rank is if there's a solution then there's some fixed be integer B with polynomial a many many bits so you can write down bigger than any output of Gaussian elimination or a linear system solver that's sure to can include at least one feasible point okay so the summary is given a system of inequalities K with bit size L its feasibility unchanged if you add these big box constraints or be us pulling them away many bits I think even quadratically many bits is sufficient and this even now we've proved it when the LP is in equation will form but it follows kind of immediately you just think about how equation form and standard form or equivalent but it's also true for the original standard form so you can put in these big box constraints without changing satisfiability okay and in light of this what we can conclude is that when K given you know LP without these big box constraints K is given if it has a solution that in fact has a vertex solution and this vertex can be written down in polynomial size and if it's empty there's a proof lambda and the dual and the proof can also be written down with Paulino many many bits okay this actually formally shows that linear programming in is in NP intercept Co NP if you remember your complexity theory any case I mean as we all know and we will show later in this class it's in fact in P but now we're sort of fully warmed up to the fact that has the potential to be solvable truly in polynomial time any questions right now\", metadata={'source': '8AqRkD9T_EE'}),\n",
       " Document(page_content=\"good so uh in the last part of the lecture um we're gonna talk about moving from well the decision problem that we've been talking about so far whether or not a given system of linear equation inequalities K has a solution or not to the more familiar problems like you know if I if I give you a system of linear inequalities and you figure out that it does have a solution like how about finding a solution so so far what we're gonna show actually in the in a couple of lectures is that there is an algorithm ellipsoid algorithm by touching on adapted for linear programming that can solve this decision problem given K is it feasible or not but now in the lecture I'd like to talk about well what about the search problem like when K is feasible you know can we find a point X in the in the polytope and probably the most familiar aspect of linear programming is the optimization version and that's where you know under the assumption that K is non-empty you also have like an objective function which is a linear function of the exes like CX like c1 x1 plus CN xn so you want to maximize linear programming is usually thought about like this like you're given the inequalities you want to find point X that satisfies all the inequalities while maximizing this linear expression C dot X and I'm going to tell you in the remainder of the lecture oh I should also mention I said maximizing here but you can also equivalently do minimizing because if you want to if you know how to maximize C dot X it's the same as minimizing negative C dot X okay so what I want to hear tell you in the reader of the lecture is that you can get from just the emptiness decision problem to these more complicated problems using polynomial time reductions so the way it's typically formulated is like this let's imagine that we have a quote-unquote Oracle for solving this feasibility decision problem you know we have some magical Oracle that can given a set of inequalities K it'll correctly tell you yes or no is I suppose this is a bit wrong the feasibility question is whether K does not equal the empty set or not so it'll tell you you know K contain at least one point or not okay and eventually we'll implement this Oracle the ellipsoid algorithm will let us show that you know there is a polynomial time algorithm for this but let's assume now but we have this algorithm and we're going to show I'll show you now how you can use it to solve the search problem and the optimization problem using sort of like black box use of this Oracle okay so that you know once we get this Oracle for solving the feasibility problem we'll have everything we want to have and we'll be able to move on to the glourious you know algorithmic applications of having this efficient algorithm for linear program okay so let's start with the search problem assuming we have a feasibility Oracle okay so the thing that in mind is we have some magic box that can tell us of a system of equations as a solution or not and now we have some actual system of equations you know K that we care about or maybe we use the Box one time and the box says yep it has a solution and you're like great now I don't like to actually find a solution so how can we do that okay well the first thing we can do is put K into equation alarm we saw that was without loss of generality and I also mentioned that like if you have some K not an equation form you put it into equational form you find a solution to the equation form it's easy to transform it back into a solution to the like standard form I mean really you can just disregard the slack variables and you have these X ID pluses and X I minus is and then to get X I just subtract them okay so we can assume that K is in an equation form which is like sort of a nice form for all this reasoning and so we think of K is looking like this again ax equals B X is constrained to be non-negative and a is some M by n matrix you assume it's full rank so has no like redundant equations and the number of irredundant equations is M that's at most M this picture like again n is 3 maybe M is 1 in this case we have this two dimensional hyperplane and K is like everything in this two dimensional hyperplane the intersect the non- orphaned okay um and you know we can call the feasibility or Oracle once and find out that this K has a solution so now we kind of know that this two dimensional hyper plane does have at least one point in it that has all non-negative coordinates but now we'd like to find such a point so you know typical scenario is maybe M is like I don't know n over 2 ok it's neither a particularly close to n nor particularly close to 0 just to mention that we set everything up and M is like - so again by this reasoning we talked about before right if if K is feasible that normally does have to pierce the non-negative or Thant it has to hit at least one coordinate plane X I equals 0 ok I mean it could be parallel to a coordinate plane but it cannot be simultaneously parallel to all of them so there must exist a coordinate plane X I equals 0 which is consistent with K I mean you intersect K with this coordinate plane and you still get a non-empty K now the point is you can use your Oracle to find such an eye ok so you can just use the Oracle end times take K together with you know x1 equals 0 and ask the Oracle hey is this feasible or not yes now and if it says now then try throwing in x2 equals 0 instead is this feasible yes no maybe no maybe K is parallel to the first coordinate plane and the second coordinate plane well you can do this you know n times which is fine and at least one time you'll have to get a yes answer ok and once you get the yes n so you look great now I know that it's consistent with k2 throwing this X I equals 0 that I found you throw it in there you've reduced now you've got one more equation so you sort of reduced to the M equals n over 2 plus 1 case and then you can repeat you just repeat repeat find like a coordinate plane that you can consistently throw in and finally you'll get down to M equals n you'll have a system of linear equations and you've maintained that like it's always like a subset of K a it's always in the non-negative or that and it's feasible so then you just solve a system equations and you found your X okay and um one thing that was actually confused about myself and it's making this slides I was thinking you know I mean uh why don't I really need the Oracle like this kind of gives me a recursive algorithm given the initial K for like finding a solution just try the end possible excise and then you know recurse but why do you need the Oracle like why is this not a polynomial time algorithm well it's simply because you know this recursive algorithm it has like branching factor in and there's potentially and depth so if you just try to execute this algorithm without the Oracle you can do it but it'll take you like n to the end time because you really have to recursively try all the excise but with the Oracle you have to try all the excise well you try them once but like you know which is like a successful branch to recurse down okay so you only have to make N squared Oracle calls here you know one for each level to successfully narrow in zoom in on the the X I equals zero that are consistent so that make sense why not end to the end instead of two to the N is a question is it two to the end I was just thinking okay like there's a branching factor of N and you have to do it like up to n times its end to the end maybe it would be 2 to the N I mean it's it is true that's sort of yeah no way you know if you fix X I equals 0 you can kind of eliminate one variable or actually you can just take yeah you can probably make it 2 to the N I mean you can fix a variable or you know you can just consider all subsets of X I equals 0 's to throw in to get em up to n so you can consider all like n choose n minus m coordinates to potentially be these like X I equals 0 coordinates so yeah good question and you could get this down to only a 2 to the N okay ah great so that's actually the end of search so if you have feasibility Oracle you can actually find points in the polytope and last UH one we'll talk about is optimization using a feasibility Oracle okay so this is the problem again you just imagine you have the ability to decide feasibility of K or we can also imagine we have the ability to solve search but really only need feasibility and here the task is you want to find a point X that maximizes a given linear form CX or you can also just think about the task of finding the maximum value of CX achievable not necessarily the point X that achieves it but we can do both so if you have an idea for how to do this a quick idea and type it into the chat and we'll see if you get the right idea but here's a picture let's say we're in two dimensions and we use our feasibility Oracle we're like yep okay is feasible it's you know it's not the empty set and now we want to maximize Z X so C is itself an n-dimensional vector and you can think of it as an arrow and you know all the points in r2 that make a given value for C dot X they look like they're on these like dotted lines or in general like all the points in X X and RN that make a given value for C dot X or on a hyperplane okay I'm maximizing C dot X over this K is really like about finding like the farthest extent of the polytope K in the direction C so the point you're trying to find is the Maximizer okay and the Maximizer always occurs at a vertex there's a little caveat there which we'll come back to in a moment well it's a quick moment I guess it's right now maximize R always occurs out of vertex unless K is unbounded in which case it's possible that the maximum is infinity okay so here's a picture of this in r2 here's an unbounded K that's a perfectly valid feasible set and in this case the maximum value of CX is infinity you know this CX there are X's that can make CX arbitrarily large okay so that's actually you want to be able to detect that if you can if you're trying to maximize C dot X um but if the maximum is not infinity then the Maximizer will occur at a vertex and we know that these vertices can be written down with polynomial Lee many bits we've seen that before all the vertices can be which means that just given K there is some number M kind of like the big box uh value B actually um of size 2 to the poly K which means you can write em down with polynomial in many bits as well such that B max is either at most M or else it's infinity so let me kind of know that if the maximum is infinity fine it's infinity but if it's not infinity then it has to be occurring at a vertex and this vertex is you know polynomial bit complexity so the maximum value is either infinity or at most some number M that you can write down in polynomial time yes I see somebody in the chat had the right idea for how you can do this optimization spoiler it's gonna use binary search but a little bit more actually so this is good so actually this means we can use the Oracle to detect if the answer is infinity are not quite easily we just have to figure out this number M just polynomially many bits and we then ask the Oracle hey is it true that K together with C dot X is at least M plus 1 is feasible okay and if this is feasible then it must be that the maximum is infinity which is great so we've used the feasibility Oracle to decide if the maximum is infinity and then if the Oracle on this new set K Union C dot X is at least n plus 1 comes back and says infeasible then you're like great the solution is going to be finite and a particular it's going to be at most Capital m and now we can get started as mentioned with binary search to try to find this maximum value okay so let's now assume we're in the case where the maximum we know is at most some M which can be written down with polynomially many bits and we can also do the same thing just to get started for binary search we can ask if C dot X is able to go to minus infinity actually even if it's not we can just put in the some big box constraint so let's just assume we also have this number minus M that we know them the maximum is at least this and now we can binary search so we can use our Oracle on K together with a single inequality of the form C dot X is at least T for various values of T so we can start T at like the big capital M and we can also try it at the like really small - Sam and we can start having right so we'll use the Oracle ask hey can you get pointing K that's beyond this half space C dot X equals T no and you ask about this one and you find out yes k has a point that's bigger than this hyperplane something you ask in the middle this K have a point bigger than this hyperplane yes this can have a point bigger than this hyperplane yes okay have a point bigger than this hyperplane no so you go back and you know you know how binary search works and all the while you're narrowing in zooming in on the optimal possible value for C dot X among points X that are inside the polytope okay and that's great because you're like having you know the the range of number of values let's start say M starts out as like an integer that's 2 to the poly size of K you know it's has probably many many bits so you can kind of you know get one bit of accuracy per having on the optimal value of CX I got a question in the chat does this always return a vertex no that's a good point so as you're going along we'll come to this in a second as you're going along you know once we let's say try this one I have one more example and we decide oh you know K together with C dot X's at least this T is feasible we know using search we can get some point in this little triangle so we get some point in X which achieves at least this value of T but we'll see this question about whether we can actually pick out this vertex that's the Maximizer we'll come back to that yeah as Misha says in the Chad this can only return things which are of the form integer over 2 to the K that's right if you assume that like M starts out as like um an integer and you're always sort of binary searching with this T value T will always be in the form like integer over 2 to the K it'll be like a dyadic rational and you can see you know you'll be getting like one more binary digit of precision you can see that you know if you know K involved like some numbers like 3 and stuff the optimum might not be a rational number whose denominator is power of 2 so there's a little bit of a wobbly thing going on here but to that the point is that you can get within epsilon of the maximum very efficiently in a number of steps it's polynomial in the input size polynomial in the number of bits of K and polynomial in fact linear in the logarithm of your desired accuracy Epsilon okay or in other words if you want like our digits of precision our binary digits of precision you can do that in pali are in fact order are multiplicative running time factor here which is really great you have the super fast you know quadratic convergence you're getting extremely good answer accurate answer for the optimum possible value of CX and you can use search to find a point X in K which achieves this near max value great so if you're happy with that great I mean you can just take that and in many cases like definitely impractical like I don't know optimization industrial optimization and applications of linear programming this is perfectly fine well it's gonna say you're probably not doing this in industrial applications you're using this of a pre-written hyper optimized package but anyway if you're happy with getting within you know like the correct answered within 2 to the minus n squared which you can certainly do by taking epsilon to be 2 to the minus n square that'll only incur an extra N squared factor in your running time then that's great but sometimes you're really greedy and you're like man I really wish to know literally the exact rational optimum and I'd even also like to know like a vertex which achieves this exact optimum this you can do as well but kind of surprisingly and it's maybe a little known fact uh you need some hackery at the end in particular you need some like number theory so I'll just talk briefly about this I'm not gonna get into the details but I'll just talk briefly about it for interest sake basically the story is we kind of know in advance right that the vertex is like a rational number can be expressed as with rational numbers whose denominators and numerators are integers of polynomially many bits hey but we don't know what that denominator is and you know you can't try all denominators or anything because there are exponentially many possible denominators by the way this is like a really pedantic points like I love pedantic points as you know it's a really pedantic point so like this is kind of not interesting to you then just forget it but if you if you're interested what we can get is like an extraordinarily good you know binary representation of this fraction a over B this optimizer so we're kind of you know there's this unknown fraction the optimal value of C dot X we kind of know that the denominator can be written with some you know polynomial and bits and we can get like an amazingly accurate like decimal representation or a base to representation of this and we just kind of order like round it off to the nearest fraction whose denominator has at most n bits and this is another like number theory tasks that can be done in polynomial time like I give you like a denominator upper bound and it's like in digits and I give you an extremely close like poly an close you know decimal or binary representation of the fraction and I say like tell me the actual fraction this can be solved using um continued fractions it's actually not that hard it's basically I'm not gonna get into it but it's basically like doing Euclid's GCD algorithm but in an approximate form and suffice it to say you can do it so I'm gonna skip that but like when you really want to get an exact optimal value of C dot X you get like super super super close to it with a dyadic fraction and then the last step is some like rounding thing and it's one more bonus exercise for you I'll let you think about this one you might wish to also find a vertex which achieves this optimum value and once you know the optimum value you might think oh I'll just throw in a constraint saying C dot X equals that optimum value but this doesn't quite work what could happen is the C might actually be parallel to some facet of K so your your solver that gives you back a feasible solution might give you something in the middle of the facet instead of a vertex but sighs for you you can actually find a vertex by like very slightly perturbing see okay so everything on this slide in the last slide are kind of very pedantic points I just sell them to you for your own interests sake but the implication is that you know all you need to be able to solve is this feasibility problem and then by reductions you can get everything you want you can get the solve the the search problem you can find the optimization problem you find the exact optimizer the exact optimizing vertex and so forth and let me just end with like one slide about this number theory thing there's even further advances in linear programming we'll talk about that actually when you want to solve the linear program without actually knowing the linear program you just have some like a separation Oracle it's called so some general general case and in order to solve that you need like some very sophisticated number theory problem that you need to solve something about Diophantus emission and like here's a part of a handwritten letter from it was like Rocha lo vas and Shriver who did this originally in the 80s and here's a handwritten letter from lo vas to Martin Groeschel saying dear Martin I think I found upon Mila they didn't know how to do this like last I fancy an approximation in step this very sophisticated number theory in polynomial time he said I think I found a polynomial time algorithm for something nice approximation consequently to compute the offline halt of a rational polytope give me a strong separation Oracle I'm closer right oh it's actually this is the origin of the so-called llll where than this number theory algorithm about geometric lattices and there's all developed well Lots he'll of us develop that he's one of the ELLs in lll who was all developed to get over this like last weird annoying hump in like a linear programming problem okay so I'll stop there I guess I'll in fact stop the recording there but I'll stick around so if anybody wants to ask questions I'll still be here\", metadata={'source': 'GNbPWTABLTM'}),\n",
       " Document(page_content=\"okay so this lecture is lecture 18 it's about linear programming it's sort of part two and whereas last lecture was a little bit about like the theory of linear programming this one is it gonna be a bit more about like applications of linear programming sort of how you can use it for some combinatorial optimization problems so maybe as always or maybe as we've been doing before if you have questions type them into the chat and I will periodically look down into the chat and answer the questions okay so right this one's gonna be a little bit more about practical problems and indeed linear programming was originally discovered and developed by practical people in several different areas one of these people was this person George Danzig it's a photo of him when he was a bit younger so dancing was a mathematician slash operations researcher in the working in the 40s and 50s and he developed the simplest out simplex algorithm which is a practical algorithm for solving linear programs efficiently it's not actually in polynomial time in theory but it's works well in practice and he was really interested in applying it in practical scenarios and there's a little anecdote I want to tell about him that's relevant for today's lecture so you know as he was developing this theory who was really excited about it and he was a younger brother addition he was invited to this mathematics conference at the University of Wisconsin and he gave a lecture about the topic called programming in a linear structure which I guess meant linear programming some of the applications were for like the US Army and like they really thought the term programming was cool so the mathematicians agreed to call it that which is why the linear programming has this sort of a funny name anyway dancing was there and in front of all these famous mathematicians he delivered his lecture about linear programming and you know then at the end when there's time for discussion this other famous mathematician / economist hoteling stood up and like you know wanted to give a response to the lecture apparently he just said you know well this lecture was all nice but you know as we all know the world is nonlinear and then supposedly he just sat down and that was his robust to the lecture and dancing I guess was a little flustered by this and did know what to say but then luckily the the great John von Neumann was in the audience and he stood up and said you know I'd like to you know respond on the speaker's behalf and Vaughn diamond says you know dance it gave us the title of his lecture is like linear programming he stated his assumptions clearly if you have a problem that's you know the linear programming form then great you can just use you know his work it's not of this form then don't use his work and this made you know dancing very happy and relieved and indeed apparently stop look into this he had this picture posted outside his office door happiness is assuming the world is linear and I bring this anecdote up because indeed this maximum that von Neumann said it's gonna be quite relevant so sometimes you have a problem that's exactly formulated as a linear program and terrific then the fact that you know linear programming is solvable in polynomial time it means you can solve your problem efficiently and you know a little bit analogous to this assumption here when you're programming is so great that like even if your problem is not linear you should still see if you can like corral it into some kind of linear program or use a linear program to try to help you solve it anyway and so we're gonna see some examples of this later in the lecture as well okay but let's start with the problem the max ST flow problem that genuinely is going to turn out to be just a linear programming problem so what's the max ST flow problem the input to the problem is a directed graph G so here's an example and you'll notice that there's some numbers written on the directed edges here and these are called capacities okay so these are denoted C sub u V for each edge UV in the graph and these are positive numbers okay and there's two special vertices called the source s that's over here and the sink or target T it's over here and what's the point the point of this problem is that um you're imagining that you're trying to ship stuff from s to T and you can ship like any amounts of stuff you know fractional amounts as well but the amount of stuff you can ship across the directed edge is upper bounded by the capacity that labels edge and you're trying to get as much stuff from s to t as possible okay so in fact I believe the first person to say this from was a chap called Tolstoy but that Tolstoy but another Tolstoy who lived in the USSR and was working on it in 1930 and his directed graph was really you know the vertices were like cities and towns in the Soviet Union and the directed edges were railroads and you know maybe the the number of the capacity represented the amount of stuff in tons that the the railroads could ship particularly was interested in cement so like s was maybe like a city that had a cement factory and T was like a city that you know needed cement okay and you know these railroads I can carry a so much tons of cement and he's imagining you know what's the best steady state if you know this edge here represents that s can you know transmit up to three tons of cement to a city a in a day then in the long run like how much you know transmission of cement should we have along all these edges okay and there's important constraint here that I haven't mentioned yet it's part of the title which is the flow conservation constraint which basically means that you know s is you know generating cement or stuff and T is accepting cement but all the other cities are not generating or you know taking in cement they're just transmitting it so the point is that for every vertex V other than the source and the sink the amount of stuff you're shipping to that vertex should be the same as the amount of stuff you're shipping out so the incoming flow should equal the outgoing flow and now you're naturally trying to you know ship as much stuff out as you can so it turns out this yellow numbers I've written here is like the optimal solution for this specific instance so UCS ships out to you know units to a and one unit to B and one unit to sea and like a gets to incoming units from s and it ships one of those out to D and it ships one of those out to be and B gets one incoming unit from s and one incoming unit from a and it ships zero out to a and two out to e so you see that every vertex the amount of yellow coming in is the same as the amount of yellow going excepted S&T so s is shipping out two plus one plus one just for and indeed t is getting in three plus one which is 4 and so as I told you this is the MEC the optimal solution so the max SD flow here is 4 ok this is a natural optimization problem actually you'll notice there's something additionally special here about this particular instance I mentioned that all these numbers can be rational they can be fractions but a special thing actually happens in this problem when the capacities are integers and the case of integer capacities kind of makes sense like if you imagine maybe these are like air you know flights from cities you know a flight can carry an integer number of passengers right you cannot ship a fractional number of people and as it turns out something that we'll eventually see is that when all the the capacities are integers the optimal solution also has integers but um that's just a special case and for now think of this as like a divisible good so fractions make sense you know like you know amounts of tons of cement ok good so the great thing about this problem max max sta-flo problem is it's literally exactly an instance of optimization version of the linear programming problem ok so let's see why that's true so in the linear programming problem we have variables and in this problem max st flow we're gonna have a variable called f sub u v 1 for every directed edge UV in the graph and this is supposed to represent the amount of flow along that edge amount of stuff being shipped along that edge okay now we have some constraints and because it's linear programming these have to be inequality constraints so one constraint we'll have is that these flows are non-negative you can only ship non-negative amount of stuff on an edge of course you know the main constraint the obvious constraint is called the capacity constraint oops this is says that the you know amount to flow you ship along the edge should be at most the capacity so these seas are not variables there fixed numbers that are part of the input to the problem so there's also just an inequality on the variables f and this is the most important constraint this is the hello conservation constraint so let's take a quick look at it it says we're gonna have one of these constraints which is actually an equality constraint we're gonna one of these for every vertex other than the source and the sink no it's okay to have equality constraints in linear programming this is just the conjunction of less than or equal to and greater than or equal to and what does it say for this vertex of V you look at all directed edges from u to V in the graph and you sum the flow variable f sub u V for each of these directed edges so this left hand side captures in your solution defined by F the amount of stuff flowing into V and on the right hand side here the sum over W of such that V W is an edge in the graph of f u v this represents the amount of flow coming out of V and the flow conservation constraint says that these two quantities should be the same okay remember here these we think of these variables as taking values and the reals so it's perfectly possible to have you know a real number you know a fractional solution although of course if the capacities are rational you'll get the optimal solutions will be rational and it's not just one of these feasibility problems where you're trying to decide if the there's a solution or not there's always a there always is a solution to this problem you can set all the flows to zero and then everything is fine but you have an objective you're trying to maximize this represents the amount of flow out of s okay let's see the thing you're trying to maximize it's gonna be equivalently the amount of flow into T actually arguably you should write another term in your objective function where you subtract the amount of flow into s you know our graph doesn't have any directed edges into s because they're kind of pointless you would actually never want to ship flow into s if you're trying to maximize this so you should probably actually subtract that I'm gonna flow into s if you want a you know to understand what the correct objective is for any flow that satisfies all the constraints we're trying to maximize there's no point in shipping stuff into s there like circulating things from s back into s so we can just write this as the objective function okay looks like no questions so far but just you can flag me down if you want to ask a question okay so in some sense that's it we're done this linear program exactly captures a combinatorial optimization problem access T flow and so once you know know that the linear programming can be solved in polynomial time then you include that this optimization problem access T flow can be solved in polynomial time as well okay so that's a great application of linear programming now as it turns out there are nonlinear programming based efficient algorithms for this problem as well indeed it was always shown by Ford and Fulkerson and others like in 1954 that there's a polynomial time algorithm for solving the max st flow problem this was done well before Katya and 79 showed that linear programming is solvable in polynomial time it just goes to show that like you know linear program is like a cool like hammer you can use to try to solve a lot of optimization problems I mean we just saw that this Max's T flow problem is a special case of linear programming so as soon as you you know that learning program is in P you can solve this and other optimization problems efficiently just a quick aside on this subject in particular you know you two can do it in real life you know all these programs that I implored you to use in the first lecture like maple and mathematica matlab there's all packages for all of them to solve linear programs there's dedicated you know software like SEAPLEX that you can get access to for solving hugely your programs so indeed they're very practical algorithms to I mean they don't use the theoretical P algorithm for solving this ellipsoid algorithm for solving LPS they are very optimized practical algorithms so these problems are really once you show something can be solve this linear programming it's truly practical to to do it even on hugely over hams with you know thousands and millions of variables okay so that's all you're gonna say about max SD flow for a little while actually let me mention one more little anecdote I mentioned that Ford and Fulkerson in 1954 we're writing about this max st flow problem in english in america and they also used like the railroad example as their generic example but they made it very generic they just said oh imagine you have a railroad system you're trying to ship things from here to there and so forth and they and their paper thanked a guy called Ted Harris who worked at the RAND Corporation for suggesting the problem to him referred to Ford and Fulkerson and interestingly it turns out that Ted Harris's himself was specifically studying the railroad network in the Soviet Union and that's why he cared about this problem and why you asked for it and Fulkerson you know for their mathematical ideas on it and why the Americans were just in studying the railroad network in Soviet Union you might think about it as we go on well in some sense come back to this later\", metadata={'source': 'VODQYm_FpvA'}),\n",
       " Document(page_content=\"okay so let's switch to another problem now where you can use or you can try to use linear programming to solve it and this is a problem called oops well bipartite perfect max perfect matching okay it's actually in one of the lectures about polynomials we discussed the perfect matching problem in a graph and here we're going to look at a generalization of it where you want to find the best highest weight perfect matching in a bipartite graph this problem can also be solved using max-flow but we'll ignore that for the purposes of this discussion so what is this problem you have a bipartite graph so it has two sets of vertices UV I'm gonna draw them like you on top and V below and for the application you can think of you as being and people and you have the other side of vertices V it's gonna also have exactly n vertices and you can think of this as n jobs and it's a graph it's an undirected graph so you're gonna have edges it's a bipartite graph that connects people to jobs and on top of this these edges are gonna have weights okay these weights are unit one for each edge W sub e for edge e I think of it as a rational number and it doesn't have to be positive or negative but what it when you think of it as meaning you think of it as being the value of assigning you know this person to that job so the weight from vertex little you that little V is supposed to represent you know how good it is overall if you assign four you know person you to job B and eat again this was this specific application and came out of like army applications where they really had you know and soldiers and end jobs they needed to do and they wanted to figure out the best way to program their their tasks for the day okay and naturally the goal is to find the maximum weight perfect matching assuming there is a perfect matching so what you want to do is you want to find a perfect matching for sure so like every job has to get done and you have n jobs and n people's every person has to be assigned some job and we assume that there exists a perfect matching if there isn't then the algorithm should report you know there is no perfect matching but typically we're gonna be in this scenario where there is such a matching and the task is to try to find the maximum weight perfect matching the one that maximizes the sum of the W is over all edges all n edges that you include into your matching good so how can we try to use linear programming to solve this problem what we're going to do well this is not if you think about it for a moment this is not obviously a linear program because in linear programs you know usually the variables represent real numbers and these variables are gonna somehow be associated to whether or not we choose an edge or not and you know in the statement of the combinatorial problem you cannot fractionally choose an edge you know a person either works a job or they don't work the job but we're just gonna press on and develop something that will temporarily call the integer linear program ILP and then the idea is we're gonna relax this integer linear program I'll say what that means later to just a linear program and solve this linear program we know how to solve this efficiently and see what we can do okay so what do I mean by an integer linear program we're going to again have a variable it's like we do in linear programs and we're gonna have a variable I'll call it X sub u V for each edge U and V in the bipartite graph and there's like an int int here the intent is that X u v is supposed to be 1 if you've decided to take the UV edge if you decided to match vertex u to vertex V and it's supposed to stand for 0 if you're not gonna match U and V and we're gonna call what we're gonna set up at first to model the this max way bipartite perfect matching problem we're gonna model with an integer linear program and so we're gonna set up the constraints like this the first constraint we're gonna have is that this XUV should be an integer that should be 0 or 1 okay and this is not like a legal constraint in linear programming this is why it's called an integer linear program we're gonna allow ourselves this constraint but just let's roll with it for now and let's add the following constraint this is an equality constraint so it's perfectly fine in linear programs for every vertex U for every person you we're gonna sum up XUV for all edges UV for all jobs v that u is attached to and we're gonna constrain that to be 1 okay so the meaning of this constraint if X UV are supposed to be 0 or 1 is that um each person is assigned exactly one job and we're gonna have a symmetrical kind of constraint here for every job little V we're gonna sum over all people eat little you connected to V X UV and insist that this should be 1 okay this is insisting that every job is done by exactly one person and so forth together these three constraints these two linear constraints together with this integer constraint indeed means that any valid solution X sub u V and you know any vector X of x UV x' corresponds to the indicator of perfect matching in the graph ok and then we have a linear objective right we're trying to maximize the sum over all edges u v of the weight given to that edge W sub u V times X UV which again it supposed to be the indicator that you've chosen this edge UV in the perfect matching okay and this program this into journaling your program if you could optimize it it's exactly characterizing it's exactly equivalent to the max weight bipartite perfect matching problem this makes sense so that's great the only trouble is it's not an LP as an integer linear program and it turns out that solving a general integer linear program is np-hard so that's a shame so just you know because you can formulate your problem is an integer linear program doesn't mean you're done because in general it's it's a np-hard problem to solve these but you know with Dan SIG's you know picture of Linus in mind let's just try to force the world into being linear and do this thing that I call a relaxation or it's called relaxation so what does it mean to relax this integer linear program it's just to UM relax this constraint that the X's be integers and so we'll form an LP just by merely insisting that these XUV should be between 0 & 1 but now in the LP they're allowed to potentially be real numbers and you keep all these constraints and you keep this objective okay so this LP is you know these constraints that XUV is between 0 & 1 together with these constraints together with this objective function and if you kind of interpret this it still sort of makes sense you could imagine that like xu v z-- represents sort of like the fractional amounts of effort that you know person u devotes to job v and then you know this constraint is saying that everybody's fractional effort is used up to an extent of a hundred percent and every job gets completed with like a person's fractional effort of a hundred percent and this objective kind of is you know the sort of weighted value we end up getting but um let's imagine that the problem is such that you're not allowed to do that you're not allowed to like have people fractionally assign their their efforts like you know you just have to find an exact perfect matching of people to jobs so we have to ask ourselves well what's going on like why why do we what what benefit do we get out of relaxing this integer linear program which exactly captured the problem to a actual linear program which doesn't exactly capture the problem um well uh before we do that we just draw a picture of what's going on so when you make this relax linear program you get some actual poly tope you have actual constraints and this dashed lines indicate the interior of the polytope so this is the LP relaxation this is the feasible set of X UVs you might imagine but in the integer linear program this grid represents is a bit you know pictorial it's not exactly drawn to scale but it's supposed to represent all the zero one solutions although or integer coordinates solutions okay so we're really trying to maximize over the dots that are contained in this poly tope let's see integer linear program whereas the LP would maximize where the whole poly tope okay so if you have a point inside the poly tope which is not actually one of these integer dots it's sometimes called a fractional solution okay and we have an objective which again you can think of is like a direction this is like the the vector of W values so the LP is trying to find near the extreme points of this poly tope in that direction and maybe the optimal solution is here of course this might not be an integer solution and so the true optimum for the original problem the integer linear programming optimum might be this vertex here okay so again in polynomial time we can find this vertex here the LP optimum but what does it really have to do with the thing we're really trying to find the integer optimum one might ask okay well what we performed this relaxation of an integer linear program to a linear program there are some things that are always true so one thing that's true is that if the linear program turns out to be infeasible like that polytope is empty then certainly the integer linear program is infeasible there are definitely no integer points or zero one points inside an empty poly tope okay so that's good I mean you can use linear programming to determine if the polytope is infeasible and if it is you know the integer program is infeasible well what can you say if the LP is feasible well there's two possibilities it could be that this poly tope is non-empty but there's no integer point inside it at all so it could be that the integer linear program actually has no solutions but usually in a lot of applications you can kind of tell in advance well sometimes you can tell in advance there will be solutions and if the integer linear program is feasible then what you can deduce is that the optimum value for your true problem for the integer linear program is that most the LPS optimum value that's because it's a relaxation right I mean if you go back to the picture of the big pollito the farthest extent you can go in the C direction inside the polytope is always at least as far as you can go when you're restricted to stop but like an integer point inside the polytope this is for a maximization problem if you had a minimization problem we're trying to minimize something the LP is value the LP optimum would be only lesser or equal to the integer optimum okay because you're allowed to use fractional solutions so they're not equal that would be amazing they're not equal but at least you have a one-sided inequality so what merit of this is that LP opt it's easy to compute you know we can compute it in polynomial time and at least it's an upper bound on the optimum it's better than nothing okay so you can compute it maybe you compute you know your specific problem and you compute that the LP off is eighty three point six and now you have like a proof a certificate that the best true solution has value at most eighty three point six you know and your problem may be if your problem all the weights are integers then you would know that Oh even 83 is a maximum on the optimal solution okay so you know heuristic Lee it could be useful it means if you somehow came to like an actual valid you know perfect matching or actual valid integer solution and it's its value as close to eighty three point six then you'd be like okay I mean I've done a pretty good job here I have like a definite certificate on the upper bound upper bound on how good you can get now sometimes that's all that you can do when you take this paradigm of picking an integer linear program and relaxing it to a linear program but it turns out we actually have quite a lucky situation for this bipartite max perfect matching problem which is that it's a theorem that all the vertices of the polytope are actually have integer coordinates they all have 0 1 coordinates but because you have this constraint that the variables are between 0 & 1 so this is a theorem which we're going to prove and that's a terrific sort of lucky situation because it means as a corollary first of all whenever the LP is feasible has at least one point in it then it's gonna have at least one extreme points and the extreme point by the theorem is gonna be integral which means that there's a the integer linear program is feasible which means that the original problem has a perfect matching and furthermore although we know that in an LP the bounded LP the optimum was achieved at an extreme point at a vertex of the polytope which means that um the LP optimum is actually equal to the optimum you can actually with a truly integer solution achieve the LP optimum okay and that's great so now you can once you know this theorem you can use linear programming to find a the optimal extreme point the optimal vertex and it's going to be an integer solution so it's going to be an actual valid solution to your original bipartite max perfect matching problem okay and that means happily that the this maximum weight bipartite perfect matching problem is also boats all in polynomial time okay any questions about this any questions so far okay so we're gonna go over the proof of this it's a nice proof so let's put the theorem up here okay here's the proof so we're gonna prove the contrapositive statement and what is the contrapositive statement the contrapositive and statement is this if you have some X tilde which is a point in the linear program the feasible set for the linear program a point in the polytope and it's not integral it's uh you know it's has at least one entry which is not an integer zero or one integral and zero one are the same thing here because we always have in the linear program these constraints that each variable is between us or equal to zero and rather equal to one so integral is the same as saying that it's zero or one so the contrapositive is that if you have some you know assignments of values X tilde to each edge that's feasible but it's not integral then it cannot be an extreme point it's not a vertex of the polytope and what does it mean geometrically for a point in space to not be to be inside a polytope but not be a vertex not be an extreme point if you think about it a little bit what it means is you can write this point as inside the polytope is like a the average or convex combination of two different points that are also in the polytope it means basically you can draw like a line segment through X tilde which is contains X tilde at the midpoint and it extends a little bit on both sides okay that's impossible if you're at a vertex because you know any line segment you try to draw through a vertex you know it's gonna go outside the polytope once you get through the vertex but even if you're like on a face run on an edge remover of a polytope but you're not at a corner there's always a way to draw like a line segment that's inside the polytope and both its endpoints are also inside and X tilde is at the middle I hope that makes sense um good so I've written it here a little bit more algebraically it means that there's some other feasible points I'll call them X plus and X minus inside the polytope such that X tilde is the average of these two points and I should have added that X plus and X minus are distinct okay good so what we want to show is if you have a feasible solution X tilde but it has at least one value or variable in it that's strictly between zero and one then we can find some other feasible solutions X plus and X minus there are distinct whose average is X tilde so let's draw the picture again we have this bipartite graph and we don't need to worry about the values for this argumentation but X tilde is a vector whose values live on the edges okay so X tilde is an assignment of a number a real number between 0 & 1 to each edge in our assumption here is it's a feasible assignment and it's not integral so let's say it's not integral so it means it has at least there's at least one edge this email where X tilde is strictly between 0 & 1 so perhaps it's 0.3 okay so what are we gonna do about this what we're gonna do is take one of the two endpoints of this e call it V and we're going to remember since X tilde is feasible it means it satisfies all the constraints of the linear program and one of the constraints of the linear program is that for every job for every little V if you sum up the X tilde values on all the edge neighbors of V they sum up to one okay so now we've got to a situation we have this vertex V we know the sum of the X tilde values on its edge neighbors adds up to one and we know one of these edge values is fractional it's like 0.3 it's strictly between 0 & 1 so that means that you know the other edges add up to 0.7 so particular V must neighbor at least one more non integral edge there's no way for it to add up to 1 around this vertex otherwise so [Music] perhaps this non-integral edges this one okay but now we repeat this argument we go to this vertex u and we say uh we have this vertex u it's adjacent to an on integral edge you know maybe it's point one maybe it's point seven we don't really know but again one of since X tilde is feasible we know the sum of the X tilde values around this U has to be one we know it has one fractional edge and so it must have at least another fractional edge this limb could be one okay so we could have chased this arguments around the graph you know this vertex now has to have a fractional edge and maybe now this vertex up here has to have a fractional edge and you know it's a finite graph so eventually as we chased this argument around we'll get back to you know this original edge this original vertex V and what we'll get is a cycle in the graph we'll get a cycle in the graph where every edge in this cycle is to sign a fractional non integral value by X tilde our solution okay great so now we're almost done remember we're trying to find some X plus X minus two different solutions whose average is X tilde which are both feasible so we have you know this picture you know six edges on the cycle there's some finite number of edges in the cycle and they're all fractional there's strictly between zero and one so we can just fix any tiny tiny number epsilon which is smaller than any of these fractional edge weights and such that one minus Epsilon is bigger than all these fractional edge weights okay so just pick any nonzero epsilon that has this property the for every fractional value on the cycle this X tilde is between epsilon and one minus epsilon and great so now we're going to make this new solution X plus by just adding epsilon on the even edges of the cycle and we'll make this new solution x- by subtracting epsilon from the odd edges of the cycle actually this is not said quite right I guess what we need to do is 4x plus we'll add epsilon to every other let's say the even edges of the cycle and we'll subtract epsilon from the auditors of the cycle and vice versa for fx- we'll add epsilon on the odd edges and subtract epsilon on the even edges okay so if we you know go through this cycle edges one by one and you know we add Upsilon subtract epsilon add epsilon subtract epsilon and so forth then this is going to preserve the fact that at each vertex you sum up to one okay because we've added epsilon to one of your edges and subtracted epsilon from another one of your edges and this could preserve feasibility not all these edge weights are going to remain between zero and one that's because we pick epsilon real small so that adding and subtracting it to an X tilde value doesn't get you outside of zero one and finally you know as I sort of wrongly wrote here we can do this in two different ways we can add and subtract on even an odd or on odd and even and so this gives us our two feasible solutions X plus and X minus and indeed they're average you know the plus and minuses cancel out and we just get back the original X tilde okay and that actually completes the proof we found this X tilde sorry we found this we've shown that X tilde is not an extreme point okay I'm getting two questions so let's go through them one by one one person asked don't we need to show that if X is not integral then X is not a Maximizer we're not really actually in this problem thinking about whether or not points like X tilde are maximizes or not for the objective this whole argument doesn't have to do with the values at all it's only to do with extreme points or vertices of the polytope okay so what we're showing here has actually nothing to do with the the values and the maximization problem we're just showing that all the corners all the vertices of the polytope ignore the objective function just the constraints have integer coordinates that's what we want to show if you're a feasible points and you're well if you're a feasible point and you're extreme then your integral that's what we want to show and we show the contrapositive if your feasible point your feasible and your not integral then you're not extreme and why this was useful is because there's another theorem that says or one thing we learned last time is that optimizers of LPS always occur at vertices so the optimum can always be achieved at an extreme point and this theorem shows that every extreme point is integral so it shows that the optimal can always be achieved by an integral solution okay I got another question that asks where do we use the bipartite Ness where do we use the bipartite miss I guess we used it here we use the facts that this [Music] psycho will always be of even length it's a bipartite graph so it'll always be of even lengths because it's got to go back and forth and back and forth and back and forth and this allowed us to create this X plus and X minus by considering like the even edges and the auditors separately so you know X plus we added epsilon on the even edges and subtracted epsilon on the odd edges and then X - we did it the other way around and that allowed us to get these two solutions whose average was the original one I guess if you have a non bipartite graph it could have odd length cycles and then it wouldn't be clear how to do this okay I think yeah somebody else made that point as well the questions somebody else asked what if his subtracting epsilon makes the weight less than zero yeah that's a good question that's that's why we pick epsilon super small so small that subtracting epsilon will still keep it greater than or equal to zero so you can imagine there's six edges here maybe their point three point one point nine five point two point eight and point five then we could just take the epsilon to be like point zero zero one or something just small enough so that adding and subtracting it always keeps you between zero and one okay and more questions okay I think that's all the questions great so that's the end of that proof so let me tell you a fact and the fact maybe on the homework this is not you know I said this was like a lucky situation that happened for this max wait bipartite perfect matching problem but it happens in somewhat more generality so there's a wide ish class of linear programming problems I have this integrality property meaning that all extreme points are integral it happens for any linear program that looks like this so you have a maximization or minimization of C dot X doesn't matter at all what C is C can be any fractional it should look like this so that like you have a matrix M times your variables X you can have upper bounds being B prime for it these are B&B primer vectors and X you can also bound between some Ln U and these B and B Prime and Alan ul you should all be integral notation is not a quite right these should be integer vectors not integers and they're allowed to have infinities in them which means you know some of them you don't have to have all of these X entries and MX entries bounded they can you know omit some of these inequalities but the main point is that this matrix M should have this property called total unimodularity and totally you know modular means that every square sub-matrix of m has determinant which is -1 0 or plus 1 so that's kind of a funny linear algebra condition and it might not be so clear how to verify that matrix and that you care about and your LP has this property but there's some theory of this I mean it's an enjoyable theory that we will not get into but except perhaps on the homework but in several applications in combinatorial optimization you can verify that the integer LP relaxation that you set up has a this form where m is totally you know modular and then you get this wonderful property that all the extreme points of your poly tope are integral and then that's great it means your linear program is sort of exact you know the fact that you relax them from an integer linear program doesn't harm the fact it doesn't harm anything because the optimum of the LP will be integral itself and this max weight bipartite perfect matching is an example like this one can show that for any bipartite graph this LP that we set up has this property that the matrix M is totally you know modular but I shall leave it at that\", metadata={'source': 'Rua9Ff8znsk'}),\n",
       " Document(page_content=\"[Music] so now let's move on we've seen now two cases where we had some combinatorial optimization problem in one case it was exactly captured by an LP in one case it was exactly captured by an integer linear program and we relaxed it to a linear program and then tada something very nice happened we found out that the linear program was not much of a relaxation at all and in fact it still essentially captured the original problem now we're gonna talk about the minimum vertex cover problem and it's gonna be example where you know taking the integer linear program and relaxing into a linear program it will not be exact anymore the linear program will not give us an exact solution but still you know as I suggested earlier program is so great like you should just should just do it any way as much as you can and perhaps relaxing to a linear program will help you anyway and we'll see that you will okay so the vertex cover problem I imagine you've seen it before we're gonna talk about a slight generalization here from the version you might know where it's like a weighted vertex cover problem so what does that mean so the input is gonna be a graph or regular a usual graph an undirected graph but now the vertices are gonna have costs for every vertex B it's gonna part of the input is gonna be a cost C sub B which is non-negative and what are you trying to do you're trying to output a so-called vertex cover a vertex cover is like a very confusingly named object whoever named it in the theory of combinatorics and we did a weird job of it a vertex cover is a subset of vertices s that touch all the edges so it's kind of like these for his ease form a cover of the edges okay so you've got any east of that vertex covers the set of vertices that touch all the edges and your goal as you might guess is to minimize the total cost of the vertices you selected so I don't know maybe like you have the graph and here's an example the edges are like roads and you can put like a moderating station at each end point and like you want that every road is monitored by one of its endpoints and like differing monitor locations have different costs to install and so you want to install the cheapest cost set of monitoring stations which are vertices such that every road has one of its two end points with a monitor okay so I've given a little example here where the graph is a star and the central node has costs too and the leaf nodes have cost one and I hope you can see what the minimum cost vertex cover is here for those of you who just want to enjoy the show the optimum here is to you should select the center vertex and that touches all the edges so that's great and the total cost is two on the other hand this is an example graph which might fool the first polynomial time algorithm you might or polynomial time heuristic you might think of the first point of time algorithm you might think of as a greedy algorithm where maybe you just choose the cheapest cost vertex and buy it and they cover some edges once the edges are covered you don't have to ever consider buying their end points so then just buy the the next cheapest vertex that covers an edge that hasn't been covered by the next cheapest vertex that covers an edge that hasn't been covered and so forth that's a simple heuristic it's not very good at this problem because this algorithm this heuristic on this graph will give you a solution of six you'll buy one vertex and you'll just cover one edge and they ought to buy you buy the next cheapest vertex which is still a one it covers this edge and you'll end up buying all the leaf vertices which is six when you should have just you know boning up for the center vertex and paid to in general this shows that like the central greedy algorithm can be terrible if you generalize this to a star with n vertices and the optimum value this too but the greedy algorithm would pay close to N and minus 1 both now you know it's such a fault of the the greedy algorithm because this problem is np-hard as you well know even in the unweighted case where all the vertices have cost 1 it's np-hard so you know there's nothing you can do if you're trying to find a polynomial time algorithm that solves this exactly in a particular this implies you shouldn't hope that if you're you know one thing we're gonna do is formulate this as an integer linear program we're gonna relax it to a linear program we're gonna like solve the linear program you shouldn't hope for the last case we're like oh I hope the linear program we'll just luckily give us an integer solution which is also optimal because then you'd be solving an np-hard problem in polynomial time which it ain't gonna happen but you should do it anyway so let's see what happens when we do this recipe okay so let's formulate an integer linear program for the min-cost vertex cover problem and similar to before we're gonna have a variable X of V for each vertex and in the integer linear program will constrain it to be either zero on one and the intent is that xB should be set to one if you're gonna buy that vertex and zero if you're not going to buy it and we have constraints we have to constrain this your choices so that you form a vertex cover which really means for every edge UV you take at least one of its endpoints okay and this constraint this linear constraint enforces that for every edge UV that you choose at least one of the endpoints you'll have to choose both and we have this simple objective function the sum over all vertices of 0 1 XV for whether don't you chose it times the cost of that vertex so again this integer linear program exactly captures the min weight vertex cover problem I mean cost vertex cover problem and if you already know that sin be hard this gives you a proof that integer linear programming solving them to the optimality is also np-hard okay what we're gonna do what we did before we're gonna quote relax this and that means forming the the linear program that has the same variables and the same constraints and objective except that we relax this constraint that xve should be either 0 or 1 - just the constraint that XV should be between 0 & 1 so again you can think of it as like in the LP you're allowed to like fractionally take vertices to the some extent between 0 1 & 4 every edge you like sum up the extent fractionally to which you took its endpoints that should be at least one so that's a different problem it's not the real problem it's some relaxation of the problem but the this relaxation the problem is it exactly capturable by a linear program excuse me and therefore you can solve it and find the optimal solution in polynomial time now here we have a minimization problem right so in the LPE you're trying to minimize is something and you have you're allowed to minimize over a whole a polytope rather than just the integer points inside the polytope so here we have the situation where the lp/op will be always less than or equal to the true opt and sometimes it might be strictly less than the true opt and here's a very simple example let's say you have this triangle graph where each vertex has cost one well the optimum solution has costs two if you only take one vertex then you know the opposing edge will not be covered by either the endpoints so you have to take at least two vertices so the optimum cost is two on the other hand it's easy to check that the LP optimum the cost of the L up on LP solution is at most three halves 1.5 and the reason for that is you can just put XV equals 1/2 on every single vertex and in fact you can do this strategy of put 1/2 for X and E on every single vertex for any graph at all and you see that it will always satisfy the constraints because every edge will be half covered by one vertex and half covered by the other vertex and it'll achieve a value which is 1/2 the sum of all the costs okay in a special case it'll cheat the value three halves and it's very easy to show that actually the LP optimum is exactly three house in this case but anyway it's at most three house and this shows that there's some kind of gap I mean the LP optimum is strictly better than the actual optimum and for a more general example like this we could consider G being the complete graph KN and here I claim the true optimum vertex cover cost is n minus 2 and the reasoning is the same and a complete graph with n vertices you know you can keep buying vertices and if you've bought even if you bought n minus 3 vertices ah oh wait this OP should be n minus 1 sorry this is a little typo could maybe make this a 1 is that minus one because if you've bought uh let's see you bought n minus two of the vertices well there's still two unbought vertices and it's the complete graph and so they have an edge between them which you haven't covered yet so the optimum is n minus 1 once you buy n minus 1 vertices you've covered every edge yeah I'll get back I got a power point advice from the power point guru here I'll ask you at the end addition I have that one issue is I'm doing this not in standard presenter mode but in windowed mode anyway enough power point talk right so for this complete graph you can again assign every vertex a half and so the LP optimum value will be n over 2 which is much less than n minus 2 in one hand oh man you might think it's not so bad because only half of the optimum it's within a factor 2 of the optimum and as I said before one merit of doing this is at least this LP opt that you can compute efficiently in polynomial time serves an efficiently computable lower bound on the optimum so you know if you managed to find an integral solution that's close to the lp/op you can be like on doing a pretty good job here okay so now I want to tell you about a concept called LP rounding and it's a it's not a specific concept it's more of an idea for a concept the idea is an algorithmic idea of taking an optimal so-called fractional solution the optimal solution to the LP and somehow trying to convert it or round it to a feasible integer linear programming solution which is somehow almost as good so this is the idea between LP odd behind LP rounding solve your LP get a feasible fractional solution to achieve some value and then try to somehow convert it to an integer solution which is feasible whose value is closed okay so for example let's try to do this in our vertex cover case let's say X tilde is some feasible solution for the min vertex cover LP for a graph G okay doesn't even have to be an optimal solution we're gonna apply this reasoning to an optimal solution but for what I'm about to say it can just be a feasible solution feasible linear programming solution and now I want to imagine getting an integer solution and one way I can do this is define the set of vertices s or for full notation S sub X tilde to be almost vertices V where the fractional solution gives an extent that's at least a half so for every X tilde I can cook up a set of vertices like this as it's a sort of integer solution and this is sort of where the name rounding comes from it's kind of like I took this X tilde and I literally rounded it you know to the nearest integer so everything was at least a half I round up to one we think that's below 1/2 I round down to zero and I've zeros and ones I think of this as an indicator of a subset s ok so what's nice is the simple fact is that this s in this way is a feasible vertex cover and I'll let you think about why maybe for a moment it's because of one of the constraints that you have in the LP specifically the constraint that says for every edge has to be sort of fractionally covered so the endpoints of X tilde have to add up to at least one so here's the reason you know X tilde is feasible we know for every edge X still the values on you and me have to add up to at least one so if two numbers add up to at least one then at least one of them has to be at least 1/2 which means that S will contain at least one of you or B possibly both so that's nice now what about costs well what's nice is when you do this idea the cost of the set s you pervert produce can be shown to be not much more at most two times the cost or the value if you will of the fractional solution X tilde so let's prove that well what does it mean the helpee cost of X tilde it just means this the sum over B of the cost of B times X tilde B so this is a fraction now let's observe that this is if I take this sum in sum and only include the terms v that are in s I can do that and then whenever V is in s its X tilde value is at least 1/2 so this whole sum is at least the sum over V and s CV times a half okay just 1/2 times the sum overall V&S of CV it's the half of this cost of this integral solution and that's it I mean if I just put the two on the other side I got the cost of this integral solution ask that you make is that most two times the sort of LP cost or LP value of s and that's pretty cool because as a corollary of this we can just apply this to the optimal LP solution which we can find efficiently with the linear programming algorithm we find the optimal LP solution let's call it X star it's a feasible solution so now that an algorithm an efficient algorithm can form this S sub X star and it's a feasible integer solution and we've just shown that its cost will be at most two times the LP cost of X star but X star is the minimum solution so this cost solution so this is just 2 times the LP optimum value and remember this is where a minimization problem so the LP optimum value is always less or equal to the optimum value so this is at most two times the true optimum value but on the other hand if we go back to SX star it is a feasible integral solution it's a feasible vertex cover and it has some cost and this cost must be at least the optimum value so what are we shown with this long sequence of equations we've shown that in polynomial time you can get this L X star by linear programming convert it to a feasible solution a feasible vertex cover S sub X star whose cost is sandwiched between the true optimal in at most two times the optimum and that's pretty cool it means you can efficiently find a vertex cover s whose cost is within factor 2 of the optimal it's almost 2 times the size of the optimal vertex cover ok this is called a to approximation algorithm and it's pretty nice and in fact this is the optimal Paul mealtime algorithm known where optimal refers to like how good a factor box emission you get so it's not known in polynomial time how to get vertex covers which are guaranteed to always be let's say at most one point nine nine times the optimal solution okay this is like a cool way using linear programming and you can get like a factor two solution yeah and you may have seen a simpler combinatorial algorithm that achieves a two approximation for a vertex cover which works in the case where the costs are all the same where you just count the number of vertices there there's an easy algorithm where you just keep choosing edges that are uncovered and include into your vertex cover both endpoints well that simple algorithm does not extend to this case where the vertices have costs or there you really need kind of need this linear programming based algorithm okay any questions okay cool oh let me make some final remarks if you can squint and see this stuff I crammed into the corner there's another cool thing about this LP which we may not show well we're not gonna show now but we may see on the homework that's always this help he has a special property it's not integral but it's half integral every vertex X tilde for this LP happens to have the property that all its coordinates are either 0 1/2 or 1 so it's not integral I would say that all the coordinates are either 0 or 1 what is 1/2 integral it turns out there are lines are 0 1/2 or 1 and so this that's kind of cool and so this this rounding technique of taking all the vertices whose assignment is at least 1/2 turns out to just be taking all the vertices of the assignment is equal to 1/2 or equal to 1\", metadata={'source': 'WT2MtXI35vc'}),\n",
       " Document(page_content=\"[Music] great so let's uh go back to thinking about on linear programming duality which we mentioned last time we mentioned it last time mainly in the context of the feasibility problem like if you have a linear polytope you want to know if it's empty or not but there's like a very simple extension to optimization problems which I'll now tell you so let's suppose you're trying to solve this optimization version of an LP you have some linear inequalities and you're trying to maximize CX now here's how do allottee works suppose you're staring at this linear programming you're thinking for a long time gee I wonder what is the maximum possible value of CX assuming X satisfies all these inequalities imagine some super-smart person came along to you and pointed out some non-negative numbers lambda 1 through lambda M they said check it out if you multiply the first inequality by lambda 1 the second inequality by lambda 2 the last name equality by lambda M well you get new inequalities which are also satisfied by X any X that satisfies all the inequalities and suppose this person said check it out if you add these up you'll get another true implication of these inequalities another inequality that must be satisfied by any X that satisfies all these inequalities and imagine a miracle happens and these inequalities with these multipliers add up to the inequality C dot X is at most beta where C is exactly your objective C that'd be amazing but such lambdas would be to you a proof or a certificate that the optimum possible value is at most beta because you know in this with these lambdas like C dot X Atmos Beta is a true implication of these in any exit satisfies these inequalities has to satisfy this so C dot X is at most beta and just a very simple extension of what we did last time you can use Farkas lemma to show that actually there always exists not just certificates like this but optimal certificates like this lambdas which achieve C dot X less or equal to beta for beta being the actual maximum value of C dot X so there will always be like a certificate like this these certifying lambdas which certify to you just by like multiplying them against the inequalities and adding them up the optimal maximum value of C dot X so that's cool that's linear programming duality for optimization version of lp's and again this LP is called the primal LP and the dual LP is the LP whose goal is to find these best certifying lambdas so you're trying to find lambdas one for each constraint up here and lambdas which are all non-negative and the constraint that when you multiply the iPhone by lambda I and add them up the left hand side becomes C dot X if you think about it for one second its equivalent to saying that like the transpose of the a matrix times lambda equals the C vector and the right-hand side is just uh when you multiply by lambdas and add up is B dot lambda and you see this is also a linear it's like an optimization linear program the lambdas are the variables and B dot lambda is the objective function and trying to find like the best certifying lambdas the lambdas which certified the best upper bound on C dot X it's a linear program and in fact the the lambdas which give you the lowest possible upper bound by duality this upper bound exactly is equal to the maximum of the original LP and this this fact that they're this Farkas lemma fact that the best upper bound certificate out of the dual LP actually equals the maximum of the original LP is called strong duality this is the case when they're both finite sum like edge cases if they're unbounded or infeasible I should say that you know any feasible solution to the dual LP gives you a bead odd lambda which is an upper bound on C dot X so the fact that every feasible LP value is an upper bound on the primals LP value is called weak duality the fact that the best certificate is equal to the optimum of the primal LP is called a strong duality it's basically the same as Farkas lemma any questions about this looks like no questions so far so uh let me give you a lifepro tip in this CS theory toolkit class first of all you know proceeding life protip is like whenever you have a problem see if you can make it into an LP and the prototype I'll give you now is whenever you have an LP just always take its dual and see if you can interpret it and I'll see what tell you what I mean by this through an example so the last thing I want to talk about is I want to go back to the max-flow problem which is a maximization LP and write down its dual LP and try to think about what the dual LP means okay so if you recall this was the max-flow LP so put yourself back in mind of those railroads and the semantics the directed graph you're trying to ship cement from s to T trying to find a flow of how much cement along each edge and you have the flow constraints for each vertex it says the amount coming in should equal the amount coming out so we have these variables F sub that's a typo it should say F sub P oh no that's not a typo you want to maximize the amount of flow out of s you have a variable F UB for every directed edge you be give these capacity constraints the flow should be between 0 and the capacity edge and these are the flow conservation constraints that say four vertices other s and other SNT I'm not flowing in is equal to the amount flying out okay so what I want to imagine doing but I don't want to actually literally do is to take the dual of this to find the dual LP that we talked about on the previous couple of slides so one thing you have to do is you have to fix this LP up into the form we were talking about where you only have less than or equal to constraints but you can always do we have equality constraints we know how to convert those to less than or equal like pairs of less than or equal to constraints we have fun upper bound we can negate that to get an upper bound lower bound okay so by messing around you can convert this to the form that we talked about in the previous slides and then you can form this dual by having a lambda for each constraint and trying to find the best linear combination of the constraints that looks like objective function is at most blah you should try to have the dual help you make this blah this certificate certified upper bound as small as possible and so it'll look something like this and then you should do like the opposite of this like messing around you should somehow clean this up to an equivalent LP so this often happens when you take the an LP I'm not going to show it because it's a little bit nitty-gritty but you'll get situations where like you know you'll have you know some less or equal to constraint and you'll see oh it'll be more natural if I call this a greater or equal to constraint by negating both sides or sometimes you'll have you know a constraint with a less or equal to and this constraint with a greater than equal to and you'll say oh I should convert this to an equality constraint stuff like this so you kind of like clean it up to make it more nice and I'll tell you the results of cleaning up the dual of this max St Flo LP it looks like this it's kind of complicated so let's go through it slowly because you know in life you'll be doing this a lot you'll have your LP you'll take its dual tidy it up and now you'll be like well what the heck is this I mean can I think of this in some useful way beyond knowing that it's some minimization problem whose minimum value is equal to the maximum value of the max flow problem okay so in this LP there's two kinds of variables we have a variable called lambda sub u v for each directed edge and in some sense these lambdas for each directed edge come from these capacity constraints remember at a high level you know you multiply each of the constraints in the primal by some lambdas and those are your new variables so these lambda UVs come from multiplying these capacity constraints by some lambda but you also have like a constraint for each vertex here and so you have different multipliers in your dual for these vertex based constraints and I called them over here miss of V so in the dual LP you also have some variables called mu sub V one for each vertex and those are some how could associate it with the flow constraints good and the objective of the dual LP is to minimize the sum over all edges of the capacity u v times the lambda u v and the Meuse do not participate in the objective function as it turns out somehow a function of the fact that you have equality's here but the here are the constraints you have constraint on mus that it should equal 1 so really you don't even need to have a variable because you have to assign it to one but it's best to think of it as like a constraint that Mewis has to equal 1 and a constraint that mu t has to equal 0 so these Meuse are like assignments to the vertices you have to sign one to the source vertex if the sign 0 to the target vertex and all the other vertices you can assign them you however you like you have some more constraints relating the lambdas and the Meuse so you have some more constraints you constraint is that lambda u v should be at least mu u minus mu V I'll draw a picture in a moment and it should be non-negative okay so you're if you're trying to optimize this you're trying to minimize the sum over all the edges capacity times lambda u V so you're motivated to make the lambdas as small as you can you there's a limit how small you can make them based on your choice of the Meuse you can make them at most mu you - me a V and you can also can't make them negative so uh okay let's uh put our picture back up this is our directed graph example with capacities and really another thing you can say about this mystery LP this dual LP that we're trying to interpret is that really even though you're kind of minimizing overall assignments to lambda and all assignments to MU it's really just minimizing overall assignments to MU the vertex green vertex labels and I'll tell you why imagine you came up with some assignments for them the Meuse some green assignments for the muse and now you're like okay suppose I use these Muse like now I still gotta figure out the best lambda is to try to make this objective function as small as possible but this part finding the best lambda is given the muse is trivial because you just have some you know given the muse you have some lower bound a mu u minus mu B for lambda u v and you should just set lambda to be that lower bound because you trying to make lambdas as small as you can well there's another twist you're not allowed this could be negative and you're not allowed to set the lambdas to be negative so in that case you're like well so it to be zero so what I'm trying to say is like given muse you should always set the best lambda choice for lambda u v is to set not a you be to be the difference of the muse for its endpoints or zero whichever is larger so yeah and then what are you trying to what do you have to hey it's the products the sum over all the edges of the product of the capacity times the lambda so let's just sort of try to imagine what's going on in this example if we're trying to solve this minimization problem okay so the green represents the muse we're like obliged to put one on s and we're obliged with zero on T that's these two constraints and now let's let's imagine we're trying to put ones in zero or not once in zeros but real number is mu between one and zero on these other vertices so think about what should we give to a well there's a pretty big capacity here between s and a3 and so if we said me to be something noticeably smaller than one then we'll be a sort of a smaller we make it the the bigger will be obliged to set the lambda for this thing and if we make a 0.5 then this difference is 0.5 will be obliged to set lambda as large as 0.5 and we'll be paying three times 0.5 in the objective so because this three is big we might say ourselves gee maybe I'll just make this one one and then for this edge I can make the lambda value zero and I won't pay anything for this edge and the objective um on the other hand like you know I'm just making stuff up here but like you might say you know this capacity is one it's not so bad we kind of got to get to zero in a way and like this drop here this capacity is four I'd really hate to have a large new value for D because I have to make this lambda large and that would cost me a lot so I'd kind of like this to be close to zero let's just make it zero because you know this edge is not so bad and similarly I don't know you might reason like this but these might be good choices and it turns out that these green numbers I've put up here are actually the optimal choices for the Meuse for this problem and it's not a coincidence as it turns out that they happen to be able to zero in one so the Me's actually could be fractional but an interesting thing happened again the optimum use were zero and one and what happens in this solution is for every directed edge that goes from a 1 to a 0 you have to set the lambda value to be 1 and every edge that goes from 1 to 1 or 0 to 0 you can set the lambda value to be 0 so you basically a pay for exactly those edges in the Optima in the minimization problem I go from a 1 to 0 and what you pay is the capacity which you can also think of as a cost and finally I can say what is going on here this actually turns out to be the LP relaxation for the natural integer linear program for the min St cut problem the mean St cut problem if you haven't seen it before is the problem of finding a set of vertices capital S that includes little s doesn't include little T and sort of has a little cost across the boundary going from inside s to outside s as possible and that's sort of what this very this LP is doing if you think of the Meuse is indicating absence or presence in the set capital S so I'm trying to say here is here's the minima state cut problem written out is just an optimization problem given a directed graph with C's on the edges which now you think of as cost as instead of capacity - set that vertices the sort of a includes s but it not T such that the amount of cost going from you know if you think of this as a cut disconnecting s from T the amount of cost of the edges crossing the cut is as small as possible and if you write down like an integer linear program that exactly captures me an ST cuts and then you relax it to a linear program you'll get exactly this which says that this minimum SD cut LP is the dual to the max SD flow LP and what's interesting is that um this [Music] strong duality that we know says that the max SP flow that's exactly the max st flow is the exact solution of the LP it's exactly equal to the minimum fractional SD cards the minimum a solution to this linear programming version of SD cards and that since it's a relaxation is less or equal to the minimum true SD cuts and this actually makes sense if you think about it if you have some cut s some set of vertices s that achieves the minimum sort of the most flow it's kind of a bottleneck for the flow in the graph and like the most flow you can get out of s is equal to the total capacity coming out of the set s or at least it's the most well you can get is upper bounded by the total capacity coming out of the set s so it's quite natural that the minimum SD card is an upper bound on the maximum flow and we've seen that through linear programming duality in fact you can show that in fact the nicest proof is through like randomized rounding but actually the SD cut LP has the nice property that it's not a relaxation the minimum fractional SD cut is actually equal to the minimum St cut and this gonna be proved in a way that's sort of like the other integrality proof we did we don't have time for it today but you can prove it and from this one deduces that the max SD flow is actually equal to the minimum cuts and since the max flow can be solved in polynomial time aslam you're programming it replies the minimum SDK cuts can also be found in polynomial time via linear programming okay so that's another cool application of linear programming I just want to close with a bit more of an anecdote so this fact that the max flow is equal to the max SC flow is equal to the min SD cut was also known prior to like this theory of linear programming it was proved by Ford and Fulkerson has also proved independently by Peter Elias and Feinstein and Shannon and they ford-fulkerson remember they were in contact with this guy Ted Harris at RAND Corporation who told them about the max flow problem and you see that Harris that ran was you know his boss was I'll call general Ross who is in the US Air Force at the time I mentioned that Harrison was also in particularly interested in the Soviet railroad network and in fact he was not interested in the SD flow problem so much is the minimum ste cut problem because you know our Air Forces motivation the US Air Force motivation was to figure out the minimum SDK in the Soviet railroad network they really want to find the what is the cheapest set of cities we can like bomb to like disconnect you know the cement factory from the cement needing city so yeah a bit of a unfortunately warlike motivation for Ted Harris and Ford Fulkerson originally but yeah well I suppose it led to this nice theorem in\", metadata={'source': 'W6z-Sa4TU-k'}),\n",
       " Document(page_content=\"yeah i was going to say indeed you can um solve uh context uh you can optimize convex functions over convex sets right um given only a separation oracle for the convex set using ellipsoid algorithm except that there are some mild technicalities uh there's some like mild technicalities assumptions you need to make in order for this to make sense to give like a simple example like you know um you know you must assume that the optimum occurs inside some potentially giant box even of exponential size right right um which is like automatically true in linear programming but like you have to just like literally make it as an assumption in in complex programming now in like most typical situations it's true like so i mean it's not like it's worrying you or anything yeah um but uh that's an example right and there are still like other like weird like there's like weirder issues like if your convex set like happens to be like low not full dimensional i see but uh yeah so generally one can can do this but um [Music] you know so i come out from like you know like a like a theory-ish perspective and um well for most classical algorithms problems that use linear programming in 70 different programming this is somehow it's more or less sufficient to know about linear programming and semi-definite programming i see um yeah programming especially is the most mainly used one and there's like a nicer theory but it's it's what you say is right that makes sense i guess like just in like machine learning and some of the other spaces um there's more emphasis on convex kind of problems because you're like essentially trying to solve non-convex um problems like np hard problems and then the closest you can get yet is by like relaxing it to some convex problem and then trying to get some intuition on how you can solve the convex problem so i guess like lp is like the other end of the extreme where it's like things that we know can be solved in polynomial time yeah i mean uh it's the same spirit in both cases um i guess like maybe in like machine learning optimization education it's typically a little bit more like oh you have like very complicated non-convex function on like a simple-ish domain and the main work is maybe um [Music] i don't know real working about like how to optimize this complicated function or replace it by some kind of surrogate that is convex whereas like this linear programming stuff is more you know connected to like trying to optimize over integer points and therefore like maybe it's more like you have a simple function you're trying to optimize but over a very complicated domain you're right that makes sense um interesting yeah really um the the case where you're trying to optimize a convex function it's really not much different from like trying to optimize over a convex domain because right like a convex function you can consider like the set of points that are like it's sub-level sets or like all the points where it's like at most 100 and then this should be a convex set maybe i should say at least 100 but anyway one of these should be a convex set and so um they're related problems make sense yeah yeah i was just like interested because i was like because i was studying like convex optimization recently and i'm like well there's a lot of analogs between this and like lps but like usually like they just like kind of skip over lps qps qcqp's like and say like directly well everything can be formulated as an sdp and then we'll just work on that yeah and then use like cvx pi or something to solve it yeah well a little bit you know from my perspective uh i'm interested and yeah explaining also like uh you know why you can solve these things efficiently and like how so um it's like natural to build up the starting from linear programming that makes sense yeah um yeah we can also always as always discuss things that are not the homework as well but we can also discuss the homework um uh any of you all have some questions you want to ask about i don't know if ken has something i i think i like kind of have most of the homework already um but we can go through a problem okay yeah um about just going through 8.1 maybe because that was interesting it was like somewhat fun because i've come at it from like an svm kind of perspective and this kind of was similar but slightly different from the way i'm used to looking at it yeah actually even this last part is kind of like svm with a degree to polynomial yeah yeah exactly so i was like wow like it's like forcing me to think a little bit differently about our problem which i feel i thought i had a lot of um that i understood really well already that's cool um i don't know like any specific question about this so the first one like part a i was just like a little bit maybe confused as to that that like hint the a dot x not the hint but the a a dot x plus b is equal to zero um is that x the same as the x i think excited here because we were somewhat confused that if if a dot x dot plus b is a constrained then you can't explicitly like get any um hyperplane because it like kind of forces your hyperplanes to look a certain way like like if you have a constraint that a dot x plus b is equal to zero then your minimization will always be y i right um [Music] okay so i mean uh let's think about this case d equals one yeah so i mean the way i have it in mind is here's r2 and then you get these data points like i don't know if there's like five of them let's see uh where's yellow here maybe it's like here here here i don't know here and uh here and so maybe this is like uh x1 is okay maybe this is like minus three comma minus six right but like this is you know x1 and this one is y1 maybe this is a minus two comma minus one or something right etcetera you've got five of them yeah it's like x5 y5 right um yeah and the task is you're trying to find like a line yeah you know the equation of a line is like well in high school maybe write like y equals mx plus p but you can also say like you know a x plus b equals zero right but but in this in this formulation x contains both x and y right oh little x little x here is like a dot x plus y sorry a dot x plus b is um it's like it is going to x is going to have both x and y right because ax plus b is equal to zero is just a vertical line right uh oh wait maybe i've typed this wrong yeah yeah that was the thing i was somewhat like we were somewhat confused about oh uh well oh yeah oh yeah i've kind of typed this wrong okay this is a problem yeah i had in my head yeah that's not good so this is something here like you know a1 right exactly like a1 x1 plus a2 y plus b equals zero right that was something we were like confused about because we were like wait this is like forcing us to like because the constraint forced us to pick vertical lines which just like did not work in the rest of the optimization oh yeah okay thanks yeah i spaced out this should just i mean this should just be the generic definition of a hyper plane which everything is happening in like as you say it's really happening in like d plus one plus one that's right yes yeah this should be like this x should be kind of like x comma y where this is like d the dimension and this is one right okay yeah that's that's that's confusing okay cool now now it makes sense because we were like confused for a very long time trying to figure out like what this constraint was and whether we like needed some like vertical line or something but i guess i guess then then that thing just like that then then it's just it's just a definition of the hyperplane with like x and y um concatenated into x the little x here yeah okay yeah i'll fix that sorry about that so that's a confusing type of sorry about that yeah yeah maybe you should just like read if you know what the words mean just read the words and not the uh yeah yeah not the math letters yeah cool so so yeah and then like i guess like uh but then then it becomes like relatively easy to like see if you just like have a generic hyperplane and not like the specific kind given here yeah that you can you could just use like some variable t to be um ax plus b so sorry to be like the absolute value and you can have like t is greater than equal to um a x plus b and then t is less than equal to ax was b i think yeah we even like mentioned a little bit about that trick in the class yes yeah yeah yeah um yeah okay whoops somehow i've yeah i've used this exact problem text before and people had not noticed it so okay very good i see yeah that makes sense um and then yeah so then i guess like the second part made sense because like we were confused with the first part but then the second part like made sense so we were like wait do we just ignore like the first part because of that hint but i guess like so now that the hints resolved i think we we got the rest of it uh yeah uh okay uh yeah i don't know if ken has any questions in general but that was like the only thing that i was like somewhat like about in this homework okay uh yeah sorry i was in that public space um i i maybe just one super minor question uh for part d of the first question 8.1 uh so if we say have some ideas for abc and turned um the constraints well the things we want to satisfy into a constraint do we in this case simply just add really the c parts into the constraint um and basically called the day because uh the c becomes like a matrix and then you can expand it i guess in the extra constraints for each of for each of the rows or columns of the matrix and then um yeah probably pretty much i mean i think like once you understand what's going on [Music] well this is like a comment that's devoid of content but like once you understand what's going on like there's not too much um mysterious about this problem uh so yeah i mean as long as like maybe maybe one little thing to think about maybe not directly really the problem but you know one thing to emphasize is that this is like still a polynomial size lp and you might think about like okay we can do this what about if we allow like cubic surfaces or uh degree four surfaces or whatever um [Music] you might think about like oh how high a degree can we allow while still keeping it pollen meal size um but it's not even directly relevant for the problem but like it's just uh something to think about so so would any um so so would so not every kernel would work there so this is like a quadratic kernel but would you have like if you had like a more complex like rbf kernel or something then it wouldn't be a qp okay uh um uh lp for any more than theoretically yeah so this is a good point so it's really what we were talking about um right yeah i mean you know the size here in the lp like you're really like literally writing down the whole optimization problem on a piece of paper and so like if the kernel is getting like more and more high degree then like it will get you'll need more and more paper to write it down and therefore um solve it um but like we were saying like you know one interesting idea and that's using spms and stuff is that sometimes you can uh use a kernel that like if you were to like write it down would be like high degree or maybe even infinite degree in some sense but depending on how your algorithm for like you know trying to minimize the error operates if it like you know only operates on its data points by taking inner products of kernels and things as long as you can do those operations efficiently it doesn't matter if like writing out the kernel would be some super large or infinite uh expression that makes sense yeah interesting that's a little bit similar to i mean uh it's similar at least in spirit to the idea of like solving linear programs where you don't have the linear program explicitly written down but you have the ability to efficiently solve like the separation oracle problem like and somehow if you're only interacting with your the set you're trying to optimize over through like separating hyper planes then you need not necessarily have to write down the whole uh have the whole lp written down yeah that's interesting that's like somewhat like that's close to like how like you would think of kernels when you actually evaluate them you're trying to you just evaluate them on your test put on your data points not on your like not not like for not write it down explicitly the whole thing yeah cool i guess i could ask like just a general question um on on the duels so i understand like so so is the dual dlp essentially the same as like when we when we talk about duals and like convex off we talk about it as taking like a lagrangian and then solve it sorry um yeah the lagrangians and then like solving for like the lagrangians is that the same thing here just like in a different scene in a different way or um yeah it is the same thing so this is i mean the the let's say if you just simplify and focus on lps yeah um it's the same thing like taking the lagrangian optimization problem and taking the dual um i uh i guess yeah it's just like a different you know you can show the same thing it's like a different perspective i suppose on it um i [Music] really like in this course uh i really like this kind of proof theoretic interpretation of the the duel where um you know you're trying to if you're trying to maximize you know if the primal is about maximizing something the dual the process of duality is to try to like construct a certificate that the maximum is at most some value by taking linear combinations of the constraints um [Music] i guess um yeah i guess like in terms of the lagrangian business um [Music] it kind of becomes a little bit trivial in the context of linear programming because like all the expressions are like linear functions and therefore like their gradients and things are like very um simple um they're uh so once you get to like more complicated setups with like stps and things like the notions of um duality become a bit more subtle um there are theorem like duality theorems for semi-definite programming that basically uh you know if you're trying to maximize a semi-different program you can take the lagrangian dual and it'll turn into another semi-definite program a minimization problem that uh you know any solution to which is like a sort of a certificate for the uh an upper bound on the primal value um however it's like not necessarily the case that this like strong duality holds that um you know the best certificate that you can achieve in this way is actually equal to the true optimum um so you need like slater slater's condition or some something like some something like that that yeah so like it somehow like morally holds in like many or most cases but like there are technicalities um and so like technical cases where it doesn't hold and actually slater's condition is one popular one although like i find at least in theory if not in practice it's not so useful because in particular like um slater's condition like uh requires that the the feasible region be full dimensional because it has to have like interior points right and this is like often not true in like um you know stp is arising from combinatorial optimization like pretty soon we'll look at like the one for max cut where you're like maximizing something and you have this condition you have like a matrix x of variables and this is supposed to be positive semi-definite and you know some of the constraints are that like every diagonal entry is equal to one and you know this constraint alone is like intersecting the whole space with like hyperplane so like the whole feasible region is in some like lower dimensional setting and so slater's condition doesn't apply it's a bit annoying though because like it turns out that like um strong duality does hold for the max cut stp relaxation it's true it's just like oh you can't deduce it from this like for example slater condition i see you have to do a little bit more work for that yeah so like this kind of theory is like a little bit tedious you can actually there's like even like you know you know research papers that show you can write down like a different dual than the standard dual and it'll be an stp and it will always satisfy strong duality um oh well it's just like it's much more complicated the this other alternate duel so i think um it's as a result this may only be interesting in theory and not so much in practice yeah because in practice i think like we usually take the dual and we laugh because the tools like eating to like kind of solve usually for some reason i think because it's like an easier problem and and then like from is it my intuition is that like the duality gap is usually the difference between like how np-hard the problem is and like being able to solve it like efficiently using a convex program is that intuition somewhat like correct or do you think there's some flaws in that yeah i'm not sure i don't really think about it that way um at least here for these things like semi-definite well okay um for semi-definite programs like the duality theory is i was as i was saying kind of like morally correct even if it like maybe like there's some technicalities like for the most part in like almost all like normal circumstances like there is no do ali gap uh or more or less and this is reflective of the fact that like you know semi-definite programming is in it's related to the fact that semi-definite programming isn't solvable in polynomial time right um for maybe this is what you're alluding to is like for more uh general problems even more general problems or like complex problems uh optimization problems you can have circumstances where like maybe the you don't expect you know strong duality to hold and therefore there will be some duality gap and this will be related to or indicative of the fact that maybe this optimization problem is not um solvable exactly in the time yeah um yeah what you say is true and there's a funny thing too is that like for example with semi-different programming um the you know assuming strong duality holes which it usually literally does or at least morally speaking does including some technicalities um if you're only interested in like the the numerical answer then you know you can get it either by solving the primal lp or solving the um dual lp and sometimes these have like wildly different complexities maybe they're both like polynomial time in theory or maybe not but like you know in practice like you know solving one could be like much much faster than solving the other right um and uh well in fact um this even comes up in theory and like uh algorithms for like linear programming like sometimes you'll see in some kind of sophisticated papers like maybe they'll like set up an lp that they want to solve to capture their problem with the property that has um maybe exponentially many variables and [Music] polynomially many constraints and this seems bad yeah uh but then they'll take the dual and they'll say oh the dual has now light polynomially many variables and exponentially many constraints they have the same value and then like polynomially many variables and exponentially many constraints you can often solve these or sometimes you can solve them using the ellipsoid method if you can argue that you have an efficient um separation operation right yeah so this is like some that's cool theoretical instantiation of this phenomenon that sometimes like the dual or the primal might be much easier to solve yeah that's very interesting yeah because like that that's that's the intuition i had for um why like people care about the dual usually because one is usually the dual i mean at least when i study like svms like they claim just the dual super easy to solve so you just work with the dual and then like you reason about it with the um with the with the um primal and yeah there's some dual like that you could still talk about the dual like in terms of separation gap and um margins and stuff but usually we talk about like solving the primal but primal's so hard to solve so you convert it to dual and then you solve the dual and a dual is like super easy yeah this can happen um yeah uh maybe i have a simple questions back to just 8.1 with okay svms more like a general questions too so i was thinking about like when i was learning about svms and having kernel tricks like for example gaussian kernels and then we basically apply this kernel trick in the dual on the on the data points and so that you don't have to explicitly compute the say the projection of your data points into like a super high dimensional space and in the dual that means i think you don't need to say write down any constraints in relation to that high dimension which could be in dimension or something but in i'm just trying to wrap my head around um what this means for the size of the lp the corresponding lb in the in the primal space which i think we briefly touched on like i i imagine if you have like a super high dimensional kernel um you would and i imagine the size of lp would have to do with the number of constraints you have and i would imagine that somehow your primal lp will have a very large size but somehow with infinite dimension i would assume that would go to infinite size but somehow that's still in polynomial relationship with the size of the dual key and this is i guess i didn't quite get this yeah i'm not sure if we're exactly talking about the same things um [Music] i guess i mean i'm not like a super expert here so i might be like maybe what i'm about to say like doesn't make sense and what you're saying does make sense that's quite possible um i guess this story with uh i mean uh i mean this is like okay this problem here is like a little bit like um trying to find you know the best fitting hyperplane in this like sort of expanded feature set where you know you are using okay degree two polynomial kernel and um you know in this problem we're like explicitly writing everything out and so therefore we're like introducing um you know quadratically many new variables and um [Music] i guess okay wait a minute i guess no i see okay i see what you're saying uh so like here let's think about it we have like quadratically many variables let me write in white but i guess we still have um the same number of constraints right yes and yeah so if we were okay good so if we were gonna like do i don't know like a degree 10 kernel or something we'd have like maybe i don't know n to the 10 um variables but still like i don't know n constraints and yeah i guess like what would uh we're just discussing like um if you take the dual of this you'll have you know n variables running out of room here you'll have like the dual will have like n variables and like n to the 10 constraints and somehow like this is usually um a lot better of a situation like for solving because um [Music] you know typically like you'll always be if you imagine like i don't know any algorithm whether it's like a lp algorithm or any kind of optimization algorithm usually like you know maintain like a solution explicitly and then try to get like better and better solutions uh through some kind of iterative process and so you usually be like writing down or like holding your hand like a current solution and see much rather like that be like n variables uh and it's maybe you know better for you or it's like more okay for you if your constraints are like very many because like maybe you don't like always sort of explicitly write down your constraints you just like think about your solution and think to yourself hmm like okay i want to like improve it or like understand if there's any constraint that's violated or like some constraint that i should fix and often if you know like you're in a pretty like nice algebraic situation like you might be able to figure out like what sort of constraint to worry about next by some smarter method than just like oh i will now enumerate all the constraints write them all down and see how i'm doing on each one so it depends exactly on how your algorithm you know tries to improve its current solution uh vis-a-vis the the constraints or the things you're trying to optimize but like yeah this is typically like a much better scenario than this dual one than this primal one um even though if like you didn't know anything or you're working like a perfect vacuum you might be like well they're both the size n to the 10 so they're equally good or bad um yeah maybe i should have thought about like or maybe i should have asked like you two you know this this problem over here asks you to like take the the duel of some particular linear program and just think about it like yeah maybe i should have also done that for this one like take the dual of one of these linear programs and think about its meaning although perhaps if you're like coming from a linear programming background maybe or from i should say from a machine learning background maybe maybe you've thought about this already but if not that could be like a nice exercise for you uh maybe just very quick a very quick question so when we say polynomial size lps we mean both the constraints and the variables combined together like yeah here i'm just being very simple i'm talking about like the number of variables the number of constraints and even i suppose you know to be like real theoretical about it like the number of bits you need to write down all the numbers in the constraints i see um this is like another probably like difference from like you know theory and practice right like maybe in practice you'll just be like well all my numbers are just i don't know floating point with like 16 bits or 32 bits or something and you know if like some error accumulates what are you going to do we'll just uh proceed and so there you know only you would only be probably interested in counting the number of variables and number of constraints see i was i guess my uh confusion earlier was say um saying the in primal you have n constraints in the door you have a lot more constraints with higher dimensions i was just confused say because in the slides we're looking at angle bracket k and i thought it was only relating to the constraints i think yeah um that makes sense um yeah it basically means well i mean the way i think about it theoretically is like okay the constraints if you were to write them all down would be defined by like a matrix with like usually would say like m rows if they're m constraints and n columns if they're n variables and you know each entry is like a number and so if each number you assume takes at least one bit to write down then the total number of bits in the input will be at least m times n um so in some sense size of the input will always be at least the number of variables plus the number of constraints or even the number of variables times the number of constraints actually this is another like major like difference in practice i mean in practice it's often the case that like maybe your constraint inequalities are very sparse like they might mention only a few variables um so you wouldn't like store your constraints as like a complete like m by n matrix because that might be very wasteful especially you know you could have like a thousand variables and a thousand constraints but maybe like there aren't literally a million non-zero numbers in your your linear programming formulation so the sparsity of these is also important in practice yeah so so that just be then the constraints unstored as some like article or something usually like some some some like being some yeah oracle you can question to see if like you actually violate any of the constraints or that gives you the separation yeah actually that's the one way to think about like in the like the oracle model is separating hyper plane model it could be like you have exponentially many constraints or infinitely many but like you just somehow you don't write them down you don't write them all down but you maybe have the ability to question them i was just thinking like more practically like even if like you know you have the constraints explicitly like maybe you have like ten thousand constraints and a thousand variables it's just often the case that like many of the entries of the constraint matrix are zeros and so you store them with like some kind of sparse matrix format rather than a full matrix format right that makes sense oh maybe we can do like question 8.3 actually because i i remember doing it but not like actually getting everything maybe like i was like i was like maybe there's a different way of a better way of looking at this so um so so i was trying to come up with a good way of representing the matrix here and then the matrix of constraints here and it was somewhat complicated and i'm not sure if that was like potentially the best like i basically did a little bit of a pattern match between this and the max sd card kind of problem and it seems super similar so i was able to um maybe make some inferences but maybe perhaps you could do it in more like explicit detail yeah this cleaning up is sometimes yeah important yeah well uh sure i'll all right you speak so um yeah what's the lp uh me what is the the original primal already yeah i i forget exactly what the pc lp is um [Music] i'm just putting it up um yeah so it's you're given so this is the ilp that we're writing or the full album yeah we can start with the integer linear program sure yeah so this is like um you would want to minimize the sum of cv x v for x v in 0 1 and x u plus x v greater than equal to one for all uv edges yep okay you relax this one to you xv being between zero and one okay good uh so what's the next step we converted the equation from and then i don't always do that oh interesting because i that i was trying to do that and then because that like followed the duel followed from there but maybe it's not the way we talked about it in class at least i don't think oh um in class we did the what was it we took the prime llp we we wrote it in terms of um the inequality constraints so a a1 dot x less than equal to b um what was this mean there i mean it's something like you know if we have uh three x plus two y is greater than or equal to five and like uh x plus um i don't know eight y is greater than or equal to three [Music] and we're trying to like minimize uh i mean uh six x plus 10 y and the i mean the dualization process is to try to like find some [Music] number lambda 1 should be not negative yeah which we like times this by this and some number lambda two which this is on negative since we'd like times this by this and then like add them up and hopefully we'll get something that looks like 6 x plus 10 y is greater than or equal to some number like i don't know 12 or something and then we're like oh if we find the best possible lambdas that will give us like the best possible right-hand side here and hopefully that right-hand side will be as as large as possible right right i guess i was just like looking at it like explicitly from the slides that where we where we formulated in terms of that but yeah we can we can yeah we can we can think of it as basically um c dot x sorry uh c dot x less than equal to some some beta ah as the new constraints right and we set it um let's just see what it like on we can even just do it on like some little graph right close like uh let's see on this graph there's three vertices one two three let's assume that all the costs are r1 so then the minimum vertex cover lp is uh x1 plus x minimize x1 plus x2 plus x3 subject to um right 0 is less than or equal x 1 less than or equal to 1 0 is less than or equal to x 2 less than or equal to 1 0 is less than or equal to x 3 is less than or equal to 1 and x 1 plus x 2 is at least 1 and x 1 plus x 3 is at least one [Music] and uh somehow my screen is refusing to scroll oh there we go x2 plus x3 is at least one i guess for this like uh i'd probably say like oh i'm gonna multiply this one by a half i'm guessing here and this one by a half and this one by a half and these are really two in equations or inequalities right and if we want them all to be like greater than or equal to this one's really like x one greater than or equal to zero and like minus x one greater than or equal to minus one got more of those but anyway i think i'm gonna multiply this one by zero this one by zero all these ones by zero and if i finally add all that up i'll get you know i got half x one here half x one here so i'll get one x one and we'll get half here and i'll have x two you all get half x two i'll get plus x three and then the right hand side will be a half one plus half two plus a half three so it'll be at least three halves so these lambdas these are like our lambdas like these lambdas have certified that the minimum better be at least three halves i'm pretty sure that like in this specific instance like these are the optimum lambdas and that like three halves is the true answer right that makes sense okay so we would basically have to do we would basically have to come up with like lambdas for each of the constraints here and um and then and then the lp becomes the lambdas um the lambdas plus the sorry the lambdas times the constraints um added together that becomes the maximization problem then right so becomes max lambda um uh yeah i guess we have like we can break this up into three constraints x v greater than equal to minus one x be greater than zero and x u plus b greater than or equal to one and each one gets a a um lambda yeah so these are three different kinds of constraints there's um there's a number of edges of these there's like a number of vertices of these there's numbers of vertices of these and it's quite quite nice i mean it's like a good uh practice to name your lambdas with different letters for different types so i might call these ones lambda sub u v but like maybe i call these ones with a different letter these ones are like sort of a little bit similar so sometimes you might like you might call them by like similar letters you might call this like mu v and like this one like i don't know mu prime v i see that makes sense this is like kind of a good life uh tip just to um i don't know it usually works out well i mean it's helpful to have like different letters for like different like kinds of constraints that make sense right so actually if i were to add all this up what would be the coefficient the coefficient on like x i don't know w where w is some specific vertex uh so you'd have you'd have all the contributions from mu v and u prime v so okay so it'd be like mu v minus mu and w maybe oh mu sorry mu w minus mu prime w [Music] um and then all the and then all of the edges are the neighbors off um w that like the edges that contain w and for them you'd have lambda mu v sorry it's lambda sorry w v where v is a neighbor of w yeah like really this kind of should be like sort of unordered so it's like maybe like sum over v as you say a neighbor of w of like lambda sort of v w right that's right so like when you add them all up you'll get like all of this times xw right that makes sense plus and like similar stuff for every single w yeah that's right and then you'll get greater than or equal to and what will go on the right hand side uh should be minus mu um minus mu lambda or sorry mu prime w minus mu prime w for the second term for first dimension zero and the last term it would be again lambda um the the the total number of neighbors times the um lambda b w is that correct not quite so actually the first i mean i'm imagining like you multiply all these multipliers against these yeah qualities and add them up so the first thing i ask is like what would be like on the left hand side you'll have some linear combination of the x's and i just asked you what's the coefficient of this specific xw right now i'm asking on the right i'm like what is the total of everything on the right yeah yeah yeah oh i see that makes sense um yeah so so you probably get the total number of vertices times minus mu lambda v because you have to sum all of them up is that for the second equation no i think so maybe i'm getting something wrong here so so if i understand correctly we're just like multiplying the the mu and the lambdas by these inequalities and then summing them up right yeah so for the first inequality the x be greater than equal to zero all the mu b's will be zero on the right-hand side so they don't they don't contribute anything to d all the mutants will be multiplied by zero yep for the second one all the mu b primes would be uh i see so this would be the sum of all the mu the um primes for all of the v's in the in the um graph and minus of that yeah yeah and then that makes sense okay yeah and then and then the and then plus like all the lambda uv is where uv is an edge in the graph yes exactly yeah makes sense yes so like you'll have one [Music] inequality that has like this big old expression on the right hand side and like this big old expression on the left hand side where in this left-hand side expression you know so we only wrote down the the piece that included x right w right but now we want this inequality to look like a lower bound on this or rather uh an upper yeah a lower bound on this yeah yeah we want a max over some some something like the uh so we'd have the max over the sum of all of the w's where the coefficient the the coefficient is this like coefficient that we have here [Music] we're trying to make the left-hand side yeah literally look like identical to this i see um so in order for that to happen some stuff needs to happen i mean the lambdas and the muse and the mu primes have to be really well selected right right so we have to constrain the the lambdas and the muse to what would be well what's my weird's happening here with my pen all the lambdas and these should be positive that's one of the things that's true whoa something really weird is happening with my computer but i'll survive yeah every time i write it like moves the letters a little bit um well we'll live uh yeah yeah all these have to be non-negative that's true otherwise this move of multiplying the inequalities by lambdas and mus is not really legitimate right and then there's another like equality constraint where um the equality constraint comes from the [Music] fact that what was the equality country right i get i guess the quality constraint is that the lambda that the muse should the muse should be uh i'm not exactly sure what the quality constraint is here well let me ask you a question i'm going to circle something in blue and hopefully it'll oh what do we want this blue circle thing to be right right right we want this to be like c c w exactly this should be exactly equal to c w right that makes sense oh we want that to happen for every w yeah yeah so and what do we want to optimize we want to optimize the sum of um c the max sum c w x w the primal we're trying to minimize oh cwxw but like if we managed to get all these blue things exactly right right oh we want to we want to we want to maximize the right-hand side exactly yeah we want to maximize this thing in yellow right right right because if we get all the blue stuff literally exactly right then the left-hand side will be this original objective and then whatever the right-hand side is like the bigger the number is like the happier we are because we're like oh we just proved or certified that the minimum has to be at least this right that makes sense yeah so yeah we're motivated to like make this right hand side which only involves the variables um as large as possible so indeed like we've got a new linear program where like the lambdas and the musing the mu primes the variables we have like a bunch of equality constraints and we're trying to like maximize like this thing right that makes sense that makes sense yeah so so then the task is sorry i just want to say then the task is you're like yeah kind of gross looking or at least one has to try to think about like what does it all you know mean i see so this would be like the cleanup kind of stuff yeah like now you have to like kind of clean it up or at least just like think about going what's going on you really want to like find well maybe like a tidy things up if things can be tidied up right uh and then like you want to like find some like story to tell that story makes you write down this lp i see yeah my intuition was that like there was since we're doing the max um the max vortex cover here i thought that sorry minimum word type rated vertex cover like i was i was assuming that this would like be related to the dual of the graph and i need to be doing like the some some form of like max cover on the dual of the graph um or something like that because like like some something where you're selecting edges instead of selecting nodes it is definitely something like where you're selecting edges instead of selecting nodes yep yeah yeah similar to those it's like a general like oh yeah go ahead yeah it was similar to the next sd card kind of intuition where like you can think of like this them very similarly um by like thinking about like the dual of the graph or something yeah another like a life like tip or fact is that like the duel of like something that involves the word cover will usually involve the word packing turns out so somehow the interpretation of the dual of a covering problem is usually a packing problem it's really just like a linguistic clue but um yeah i guess when you're staring at this like new lp you've written down trying to understand what's going on with it um yeah it may be um something to do with packing another thing to say is like you know uh i mean this sort of just occurred to me but like you know how um one thing like we mentioned in class it's like kind of relatively well known about linear programming it's like oh you know we saw these tricks like if you have um you know greater than or equal to and you wish you had a less than or equal to then you can just negate things or if you have like a variable um that's unconstrained in sign but you wish that it was non-negative then you can replace it with like a positive version subtract a minus version right if like you have an inequality and you wanted to have an equality you can like put in a slack variable and like these kinds of tricks right this clean up but like you often have to do is frequently like undoing those tricks so you should like look in the lp you written down and think like does it look like the sort of thing where like somebody did those tricks and now i will undo those tricks because like the original form is conceptually simpler and these tricks were designed to just like put it into some like some mechanical format but like essentially you wish you had the original format like often with this like duality and cleanup like you get to some expression or some lp and you like can maybe recognize that it looks like somebody had done these mechanical tricks to something that was conceptually more meaningful so right think about if you can like undo such things that make sense yeah that's that's interesting i think i think this is like better i was i was trying to like formulate this in terms of matrix problem i think you can but the matrix is very undecipherable in some sense yeah i mean in some in some weird way like this method that i'm describing for the taking the dual is like actually like a slower and more laborious way of you know doing the thing with the matrix where you just take the transpose of the matrix but like right i like doing it anyway because like it's like i feel like i psychologically understand what's going on whereas like exactly i don't know like when i first learned this stuff like you know i learned from professors that somehow just like knew how to like do duels and like they just write down the lp and then they just like instantly write down the duel and they just had like i felt like they had like some like seven rule algorithm that they're running in their head and like oh do i have to memorize this algorithm but like eventually i finally understood that it's like it's very natural you're just like trying to get the best certificate uh of like how good you could possibly do by basically combining the constraints so then i was like oh i'm just gonna do it the slow way every time because at least i'll understand it then that makes sense yeah because i i was doing it in the matrix and like it was like effort to put it in the matrix form and then when you try and like then once you have the dual format like then you have to like kind of like decipher it which is much harder than like doing this i think yeah hopefully this will be hopefully this will be a more understandable way well thanks um okay i guess i'll wrap it up about now uh any last-minute questions all right well then i'll see you around\", metadata={'source': 'IInIsjZy-zw'}),\n",
       " Document(page_content=\"okay hello everyone this is lecture 19 it's about the ellipsoid algorithm so we've seen a couple of lectures on linear programming and refer to the fact that it is in solvable in polynomial time and the algorithm that can solve in polynomial time is this ellipsoid algorithm so we're gonna do today it's a couple of things first I'm gonna tell you about the ellipsoid algorithm for solving linear programs in polynomial time and we'll see that in fact not only does it solve polynomial linear programs in polynomial time like expected but it can even solve when your programs when they're not fully given explicitly they merely have a separation Oracle because that might not make sense right now but I'll explain what it means later in the lecture and then we're going to see how to use this technique to solve a generalization of linear programs called semi definite programs which you may or may not have heard of and the motivation for this will be from the max cut problem and trying to get a good linear programming relaxation for Freud okay so let's get started I would like to start to tell you about the ellipsoid algorithm so recall that we saw two lectures ago that the general problem of linear programming maximizing or minimizing a linear function and a bunch of variables subject to a bunch of inequalities linear inequalities it reduces in polynomial time to much simpler problem just the problem of testing whether a bunch of linear inequalities has a solution or not it's like all the emptiness testing problem together with the additional assumption that you can freely assume that the inequalities can include what I call like big box constraints so all the variables are constrained x13 excited and are constrained to be between some like minus R and plus R where R is a really large number it can have polynomially many bits in it where I've referred to polynomial in the representation of the all the other constraints so just a bit more formally the problem is you're given as input K a polytope which is the intersection of some linear inequalities like these ones together with these big box constraints as I mentioned that each variable is mounted between some minus R and R and you want to decide whether or not there's a solution for this whether or not K is empty or not okay so um what I want to do now before I get into the ellipsoid algorithm is to claim that you can make this problem even a little bit easier on yourself you can reduce solving this problem to resolving a robust version of this problem and what do I mean by a robust version well in the robust version the input also is going to include a small number R which you should think of as like exponentially small but positive so it can be written with polynomially many digits and you're promised that either K is empty or not just that K is not empty but that K actually contains a tiny little full dimensional cube in it of side length R so sort of robustly non empty so this is the problem that the ellipsoid algorithm can solve this robust version where K is either empty or contains at least some little cube and before we get to how the ellipsoid algorithm does this I want to show you one more reduction which illustrates that you can reduce to this problem also in polynomial time and since you know two lectures ago when we're really talking about all the nitty-gritty details about you know polynomial number of bits this and that I'm only gonna give you a proof sketch of this point to explain why we can reduce this robust version so the first step of the sketch is to convert the the input polytope K to equational form so if you recall this is the form you can always come use an LP to an equivalent form like this where your constraints are that all the variables should be non-negative and that they should satisfy some equations so there are two pictures here so first you know in RN you look at the positive or Thant all the points X that have all of their coordinates non-negative and this ax equals B is gonna define some subspace like a hyperplane and the case that k is empty is the case that this hyperplane misses the positive or that and on the other hand the case that k is non-empty is the case that the hyperplane hits the positive or thent and you know typically it could hit the positive or Thant you know in like a big you know range of points but the picture i've drawn here kind of illustrates like maybe the trickiest hardest case when the hyperplane just barely hits the corner for example of the positive non-negative or thent okay and so the leader programming problem you have to decide for example given k like this whether you're in the empty case or the non-empty case and this non empty case is sort of like not a robust case this here the you know in this little picture the set of points in k does not contain a full dimensional cube of some little size but maybe it's just a point maybe it's just the origin so this is the problem you have to be able to solve and what i want to show in this claim is that you can again like sort of rejigger the input such that the two cases instead of being empty and non-empty are like empty and like robustly non-empty containing a little cube so what's the idea behind this well the point is that these so if you have a hyperplane like this ax equals b and it misses the non-negative or thent then what i claim and this is kind of like these claims we was talking about before with the fact that the gaussian elimination is in polynomial time and so on and so forth is that these distances sort of the distance by which it misses the non-negative or thin it cannot be like arbitrarily small it can only be exponentially small whereby exponentially i mean in the number of bits needed to write down you know ax equals B so if the input is ax equals B and your your poly tope K these distances if they're nonzero if they're positive then they have to be at least some like exponentially small amount okay so this is why it's a sketch under saying take my word for that and what this means is the following given that you know that what you can do to kind of convert to a robust version the problem is to change K to some K prime where you take all of these equality constraints ax equals B and you relax them a little bit to two-sided inequalities where you allow a teeny bit of slack plus or minus some exponentially small R prime okay so you actually change these equalities to slight you know two-sided inequalities like be plus or minus R Prime in every coordinate and what effect does this have on the picture this little move well on one hand it sort of takes the feasible region from just this hyperplane intersect the non-negative or thent to sort of this a thin slab intersect the non-negative or thin but if you make this R prime like small enough the empty case will still be empty okay because you've made this R prime small enough so that even thickening this hyperplane a little bit won't hit the non negative or that but on the other hand when you do this thickening over here you'll actually contain a whole like full dimensional you know poly tope now if even in this you were in the worst case before where the hyperplane just barely touches the non negative or thing in particular you'll contain this little cube here okay so if you just take this K which you don't know anything about you don't know whether you need the empty case or not and you stick in these equations to like slights slack to side inequalities then it preserves whether you're in the empty case or the non empty case but furthermore in the non empty case it gives you the property that the polytope contains at least a teeny tiny full dimensional cube okay so that's sort of the end of this sketch any questions about that let me know okay so now we've reduced the problem that the ellipsoid algorithm truly solves which is the robust emptiness testing problem so let me give it to you in full details here so given for the ellipsoid algorithm is the following things the input is a bunch of inequalities defining a poly sub K think of this is just like the ax equals B kinds of inequalities and also giving us input to the ellipsoid algorithm are two numbers like big R and little R and you should think of R as being you know exponentially large in the number of bits needed to write K and little R is being exponentially small but any case they're just these two inputs capital R and little R and you're given a promise the promise that you're given in the ellipsoid algorithm is that you're in one of two cases either K is the empty set there are no solutions to the inequality is defining K or your promised that not only is K have at least some points in it it contains a full dimensional cube of side length little R and it's contained in the full dimensional cube of side length R centered at the origin okay so let me draw a picture like the algorithm sort of well the picture is like this so the algorithm gets this parameter capital R and it knows that should r exist a sort of sorry should K exist should K be non-empty then it's somewhere contained in this cube centered at the origin of side length capital R or capital to R whatever and K may be empty but it also knows if K is not empty then I see there's a question one second if K is not empty then the algorithms promise that K is it contains like a little cube of side length little R now it doesn't know where this little cube is it just knows wherever you know K is it has at least on full dimensional little side a little cube of side R in it okay and the goal of the algorithm is just to tell which of the two cases it's in whether we're in K is truly empty there are no solutions decay or whether K is non empty but in an effort to try to decide this it gets to rely on these two assumptions about capital R and little R so there's a cushion in the the chat about what's the difference between little R and capital R maybe in the description I gave subsequently will clear it up so little R and big R parameters known to the algorithm big R is the side length of a big box that's like known to the algorithm that's guaranteed to contain K should it exist and alar is like sort of like a robustness parameter like the algorithms promise that like okay at least contains a little box of side length little R okay and the running time of the algorithm should be polynomial and you know the input size of K the number of bits needs to write down all the inequalities but it should also run in time polynomial in the logarithm of capital R and the logarithm of 1 over little R or to put it another way it should run in time polynomial and the number of bits needed to write down big R and the number of bits needed to write down little R ok and this was important for you know solving all pieces we saw because in general the big box constraints capital R that you can assume exist without loss of generality involve taking R to be like exponentially large in the number of bits of K and similarly the little R that we talked about on the previous few slides that you can assume sort of exists without loss of generality is also going to be exponentially small in the number of bits that are used to write down K okay I think about this oh there's a question of that says could you repeat exactly what angle brackets K means that's a great question I forgot this this is notation from like two lectures ago so it's no wonder that's hard to remember for these lectures I'm writing like angle brackets around a mathematical object do know like the number of bits needed to write it down so like if K is just a number angle brackets is basically the logarithm of that number it's like the number of binary digits you need to write it down if K is a fraction then it's you know the number of bits needed to write the numerator plus the number of bits needed to write the denominator okay as a vector then it's like the sum of the encoding lengths for all the numbers in the back there and in this case K where K is basically you know like a matrix together with a vector it's like the number of bits needed to write down all the numbers in the in K there's a question that asks to convert K to a robust version should we have log one over a little R to be at least the number of bits of K yeah exactly right so let me go back a couple of slides I showed you here this sort of little the reason why we can sort of assume without loss of generality this little R or R prime as I call it here exists and as I mentioned back on this slide this little R you have to take it sort of quite small has to be like 2 to the minus polynomial in the number of bits of K and so logarithm of 1 over little R is like polynomial number of bits of K so in order for everything to be like a polynomial time algorithm just given K where polynomial means in the number of bits you need to write ok you hope to set these capital ours and little R's to be like exponentially large or in other words they have to be written with a number of bits that's polynomial in the number of bits needed to write okay okay these are good question keep them coming in the chat right so to summarize this is the problem we have to solve it's sort of like hunting 4k you're gonna like hunt 4k and you know it starts inside this big box capital R and you're gonna try to see if you can find a point in K and you have this like guarantee that's like you don't sort of have to hunt indefinitely like K could not be like you know infinitesimally small like if it's gonna exist it has to be at least the you know at least the size of this small cube of side length R you're quite restrictive you have to run in time polynomial and log of these big R and 1 over little R okay great so I said here that the task is to decide between the two cases either that the inequality is defining K make a non empty or you know there unsatisfiable there's nothing in them but actually I want to tell you that it's sufficient to do something slightly different when you're designing this algorithm the ellipsoid algorithm in fact it's this it's efficient to like assume to like work under the hypothesis that K is non empty and try to find a point X in K in the allotted amount of time polynomial in the number of bits needed to write k log of big R and log of 1 over little R and what's the reason for this well suppose you had an algorithm that could do this that like under the assumption that K really existed it could always find in the appropriate amount of time a point xn K well you could also use that to solve the decision problem the problem of deciding if K you know is empty or non empty because you could just run this argument that tries to find a point in K if K is non empty then this algorithm will succeed by your assumption and if K is empty this goal doesn't really say what happens if you run the algorithm but if K is empty like this algorithm is not gonna find any point in K because there is no point in K if K is empty and so you know the algorithm will just fail to find a point of K and then like the mehta algorithm can say ok like it must be that k equals empty so that's sort of how you reduce the empty versus non empty decision problem to the task of finding a point assuming that set is non-empty and now we're sort of finally finally set up so like now you can really start to get ready for the algorithm once again like you're given these inequalities defining K that defining this poly tope your promise that K is inside this giant box the side lengths are centered of the origin you also promised that K at least contains like a tiny box of side length little R and you just have to find a points in okay and of course you can like you know you're given case you can look at the inequalities that define it and so one thing you can do easily is test whether a given point is in K or not we have a potential point inside this box in mind this big box in mind x and r n like literally numbers you can easily see if it's in k or not because you can just plug it into the inequalities defining K and see if they're all satisfied so this is one thing that you can certainly do and that's gonna get us started in this ellipsoid algorithm so um true to its name you look slight algorithm involves Phillips awaits it's a little bit just because they're convenient so initially we assume that we're promised K is inside a big box but the first thing we're gonna do is like we can that assumption of our minds and just pick the sphere or ball of radius while the ball that contains this big box of radius capital of side lengths capital R so the ellipsoid algorithm is iterative in the way it works because it always maintains some ellipsoid like a stretched sphere some ellipsoid Q which is guaranteed to contain K so remember we're in the this the assumption here that K really does exist and is not empty and since you're promised that K is inside this big box you're just like well I want to work on the lips let me just put it like an even bigger sphere Q around this box and that'll like you know give me my initial satisfaction of the invariant that I have the ellipsoid q that contains K okay so the very first thing that happens in the algorithm is you've got this Q in your hands and you just check whether the center of Q is in K okay so the center of Q at the beginning is the origin and you just plug all zeros into K and see if it satisfies all needing inequalities and if it does then greates you found your point in K and the algorithm is done but usually that doesn't happen so what do you do if the center of Q is not in K well here's the thing if the center of Q is not in K it means when you plugged in the center of Q as numbers into the inequality is defining K at least one inequality was not satisfied and that sort of violated inequality or violated constraint is called a separating hyperplane why well the picture is this so you have this hyperplane which is you know defines one of the facets of K and in this picture this orange one is a separating hyperplane because it has all of K on one side sort of the right-hand side on your screen and but the origin is on the left side or is sort of outside it so this orange thing is like a half space and it's one of the half spaces or inequality is defining K and it's separating in the sense that like K is all entirely contained inside it but the point that you tested is outside it that's what the meaning of separating hyperplane is good so the algorithm can certainly find that it plugs the center into the equations duval it finding K and picks out one that's not satisfied that's what happens if the center is not in K and so now what does the algorithm know let's see if I can annotate this in PowerPoint so what does the algorithm know it kind of knows this is terribly funny annotation but it knows that K wherever it is has to be in this sort of dome the intersection of this half space with the ellipsoid that it's currently - right so the next thing it does is it just sets a new it gets a smaller ellipsoid containing K it's it's Q prime to be the smallest ellipsoid that contains this dome so that contains the intersection of Q with this separating half space which might look like this in the picture this is the Q prime this is smallest ellipsoid that contains this sort of dome here okay and let me just assure you it's like pretty simple matter of linear algebra if you like know the ellipsoid q and you know this half space that like cuts off a piece of it it's a pretty simple matter of linear algebra and geometry to figure out Q prime the smallest ellipsoid that contains this dome and then you iterate so actually let me just go back here and say what I mean by iterate so now you're hanging on to this you know blue Q Prime you do the same thing you you know it's Center so maybe it's sent there as perhaps right here perhaps right here maybe it just misses okay and so you check whether the centers in K if it is then great you found you found a point in K otherwise you know you when you plugged it into a day in equations defining K there's at least one inequality perhaps this one where you know k is all on one side of the inequality but the point you tested here the center was outside so then again you know that K wherever it should be is you know inside this region the intersection of your current Q prime is a great drawing and your hot space okay and then you pick the smallest ellipsoid that contains this new region which I don't know what it looks like maybe okay that's a terrible side okay I can't draw with the mouse but someone lip side that looks like this it contains the dome that you just cut off and you iterate okay so let me tell you about the analysis the analysis is there's a very easy lemma which you know I won't prove but like really it's easy when you write down you know a little bit of analysis about this Q prime that says that the volume of Q prime will always be a little bit smaller than the volume of Q it'll be smaller by at least a 1 minus 1 over 3 n factor where n is the dimension it's not hard to prove and that's great because it means if you repeat this algorithm order n times then the volume will go down by like 1 minus 1 over 3 n to the power of n and you know that's like expose minus 1 over 3 n to the power of and it's like a constant so repeating it like a constant times n number of times the volume goes down by a constant factor so you're really making a lot of progress and you can do this even more times to now like start driving the volume down geometrically so now it's a pretty easy calculation as well again this is like a proof sketch but the volume of the initial sphere Q and a show it's like you know the sphere that's like an N dimensions and like it's got this radius that's like capital R so it's at most you can check like R to the N times n to the end you know that K contains this little cube of side length little R which has volume little R to the N and therefore like as soon as the ellipsoid you're hanging on to has volume smaller than little R to the N like the algorithm has to stop because like it you know you know you can't have like a an ellipsoid the FLE deliver so you'll always fully contains K so like it's volume cannot get lower than little R to the N without you finding a point in K there's a question about whether K has to be come back so I'll get back to this question in just a second um and therefore you will find a point in K after how many rounds of this well you know you have to drive this initial volume down to this volume and like it halves every quarter and rounds and so the number of additional rounds is like logarithm of this which is good this is gonna give you the log R term you'll collect up some extra poly n terms which are fine and you'll also get the log one over a little R okay and this is what we're shooting for poly n Times log of capital R over little R steps okay so in the actual details of the algorithm there's like a little bit more pain because you know you have to you actually like the numerical arithmetic algorithms to keep track of Q prime and so forth but that's very boring and this is just the idea of the algorithm the question from when seumas does K have to be convex it's a good question this algorithm kind of looks like you could be hunting for any shape the critical aspect of it being convex okay that's a wonderful picture of a version that's not so crazy yeah is the existence of this separating hyperplane so for this volume to go down you really need to be like chopping by hyperplane is you need this property that like whenever you draw a hyperplane through the center of an ellipse and then take the smallest ellipse that contains that chopped off part the volume goes down a little bit and yeah so you need this shape to be convex because you need that whenever you test a point and it's out the shape K there should be a hyperplane that separates K from the point and so if K has some like weird non convex parts like you know this then you would have a point like the center here which is outside K but there's no like hyperplane which separates the point from K hopefully that answers that question okay we have another question in the claim the question is about in the claim that the volume of the initial sphere Q in it being at most R to the N times n to the N where does the N to the N come from oh this is because you need to know like if I ask you like what is the volume of a sphere of radius one in n dimensions there's a formula for that and it's something like and to the it's it's n to the order n so I didn't make that point but that's the case yeah I forgot the formula off the top of my head but it's a it's like in ten dimensions the volume of a unit sphere is like n to the N and then the capital R to the end is because it's like a radius ours here okay great so now let's get actually back to this point about whether or not Kay needs to be convex because it does need to be convex but a related thing is a crucial observation and this crucial observation is that the algorithm doesn't really need to know K per se it doesn't really need to have the inequalities defining K like written down in front of its face necessarily how did the algorithm really interact with K well it needed to know capital R and little R and it needed what's called a separation Oracle what's a separation Oracle separation Oracle is a way where you like you know like some kind of like black box that gives you information about capital K and a separation Oracle I'm defining it here it takes in a point Z and RN dimensions and it outputs one of two things it either outputs like just this correct statement hey this is e is in K or and that's like you know when you test whether or not the Center is in K you want to know this question or when Z is not in K the separation Oracle gives you back a separating hyperplane explicitly it says nope not only is Z naught and K but like here's an inequality that Z does not satisfy despite the fact that all points in case satisfied the fact that this half space contains K so this is a definition of a separation Oracle and it's really the only thing the ellipsoid algorithm needed in order to do its algorithm to interact with okay this is actually a really useful fact for algorithms yeah this just note also points out this in equation does not have to be a facet of K you know the way we're showing it it was always like an equality inequality defining a facet of K but it doesn't have to be just any separating hyperplane and the reason that this fact is so cool is because it means in certain sense you can actually solve linear programs with exponentially many constraints or in fact even more than exponentially many constraints as long as you can efficiently implement a separation Oracle so we won't get in this too much but this idea has a lot of applications in algorithms and the idea is like an algorithm can maybe like imagine an LP that it wants to solve that has n variables that's maybe a like you know polynomial but it has maybe exponential and M many constraints and so the algorithm cannot explicit write they write it explicitly write the LP it wants to solve down but as long as it can solve this task of like given a points testing whether it's in the LP and if it's not finding a separating hyperplane then you can use the ellipsoid algorithm to solve the linear program and this is exactly what's needed for this new kind of algorithm called semi-different programming or this new kind of optimization problem which is a generalization of linear programming called semi definite programming and that's what I'm going to get into\", metadata={'source': '5HWTpadNa_E'}),\n",
       " Document(page_content=\"so let's go on and start to talk about semi definite programming which as I mentioned is a kind of generalization of linear programming which is very useful for algorithms for especially for constraint satisfaction problems and other optimization problems and to motivate this so many different programming we're gonna think about a very simple optimization problem the max cut problem which we even mentioned before already in this course so the next problem is quite simple and the max cut problem you're given a graph G an undirected graph and unlike min cut you're trying to do the opposite so you're trying to find a subset of vertices s like these ones circled in yellow so that s as a cop has cuts as many edges as possible so remember that a and edges had to be cut if it has one endpoint in the set s and one endpoint outside the set s so in this particular example almost all the edges are cooked this edge is cooked it goes from that inside outside this edge is cut this edges cuts this edges cuts and despite the picture this last edge is not cut because it ten points are both in s okay so this it cut four edges and that's actually the maximum cut in this graph it's impossible to partition the Pentagon into two sets such that it costs all the edges because the Pentagon is not a bipartite graph okay so max cut is given the graph find the cut of largest cut size and as you know this is an np-hard problem and a particular that means unlike the min cut problem which we saw last time can be exactly solved efficiently using a linear programming relaxation you kind of know that's not gonna happen for act max cut because otherwise you'd be solving an np-hard problem in polynomial time okay so what that means is you know if you try to do this paradigm of like writing down an integer linear program that captures maximum cuts relaxing it to a linear program and hoping that's not such a bad relaxation maybe solving this linear program getting optimal sort of fractional solution and then rounding it to a true solution s this paradigm we talked about last time well you're not gonna get the situation that this always gives you the optimal maximum cuts because then you know max cut would be in polynomial time and in fact this paradigm fails really badly for maximum cuts so I is a great strategy in general facing an optimization problem you know formulate an ILP relax it to a linear program but for max cut doesn't work well at all and if you try it the only thing you'll really be able to cheat proved about this strategy is that the size of the cuts it achieves whatever you you know however your strategy works the size of the cut it achieves the number of edges from s to s complement will be at least a 1/2 fraction of the optimum which is okay but we saw like a variety I mean there a variety of really simple algorithms that can achieve the same approximation behavior so you don't really need linear programming to get this kind of weak result in fact in the D randomization lecture we talked about how you could just pick a random partition you know it cuts half of the edges half of all the edges which is certainly half at least the optimum max cut in polynomial time and this was easily deep randomized pool okay so um that's a shame that this powerful linear programming technique does not work but it's going to be the reason we extend this version called semi definite programming and it's a bit of a spoiler using this tool of semi definite programming we'll actually be able to get a polynomial time algorithm that achieves a much better so-called approximation factor it'll give you a pong real time algorithm that returns a set s that always achieves at least eighty seven percent or eighty seven point eight percent of the maximum cuts which is pretty great and in fact it's known that there's evidence that doing better than this eighty seven point eight percent cannot be done in polynomial time it's unique games hard' and will say what this means in a much later lecture of the course okay so this is just like you up for trying to develop a generalization of linear programming that will help us solve max fat okay so even though I told you it's gonna fail let's talk about this standard paradigm for max code of trying to write down an integer linear program that exactly captures it and relaxing it to a linear program okay so you know an integer linear program we usually introduce variables we're gonna introduce variables X v1 for each vertex with the intent they're supposed to be 0 or 1 and the intent is that XV should be 1 if vertex V is gonna be in the set ass and 0 otherwise okay so X be in the integer linear program is supposed to be the indicator set for a set s well now we need to start talking about whether or not s cuts or doesn't cut severe cut various edges and so after a few moments or in fact inspired by the min cut LP we talked about in the last lecture you would be motivated to introduce variables Y so V W for every pair of vertices V W that are supposed to be 0 or 1 and again with the intent that Y be W should be 1 when X when thought of as a cut actually cuts the edge VW or in other words y VW should be 1 if and only if XV differs from XW that's gonna be tricky to enforce with linear constraints but let's just go on for now what is the objective well we're trying to maximize the number of cut edges so if we could set all these variables up like so the objective function would be the to maximize the sum over all edges VW in the graph of y VW I would exactly just count the number of edges cuts by the set indicated by a s and that's a perfectly linear function it's just the sum of some of the variables okay but what we have to worry about is this constraint that we're trying to any capture I mean we of course we have to worry about the fact that this is an integer linear program and like we cannot eventually insist that XV + y VW are just 0 but like this is the standard worry that we've always talked about in this paradigm where of course gonna relax these constraints to make an LP we're gonna try where we're gonna let X V be a real number between 0 & 1 and y VW be a real number between 0 & 1 but even if we do this we have this difficulty that it's not clear how you can enforce this constraint with linear inequalities and it's quite funny actually if you think back to min cuts I know we didn't talk about it too much but at the very end of last lecture we did see a linear program that was capturing min cuts in a not linear program you see one way to try to express it is to say that y VW should be at least the absolute value of X to the minus XW okay and think about these variables as taking zero one values this sort of exactly works you know as long as X vm x w are different this right hand side is 1 and so you're obligated to make y VW 1 but it's if they're the same you're allowed to make y VW 0 and 4 min cut you and of course always be trying to make these wise as small as you're allowed now this at first does not look like a valid linear inequality for linear programs because it has these absolute value signs but actually it's fine because if you think about it this is an equation is can be converted to two proper inequalities that capture it you know you have to just say that y VW is at least X V minus XW and y VW is at least the negative of X P minus X W so this is a shorthand for two inequalities that you really could include on the other hand form X cuts you might wear some X cut you're always trying to make them wise as big as you can and you could enforce this if you could write down that like y VW is that most absolute value because then again that would say that if X V and X W get different 0 1 values you're allowed to set y VW to be 1 but on the other hand if X via next W are the same then you're obligated to set y VW to be 0 but unfortunately like this to solder this inequality you just right it as like this you can't convert it to to like normal inequalities right if you just think about it you know this doesn't work so that's the difficulty so we're a little bit stumped at this point even if we're willing to go to an integer linear program we're a little bit stumped as to how we can cap encapsulate this constraint any questions there's a question question is is it known whether we can express yvw equals the indicator that X V naught equal to X W through a set of linear constraints say by defining a few more variables short answer is no you might say like oh yeah maybe I'll try to play around I'll try to add a few more variables and I'll exactly encapsulate this I guess basically the answer is is no but you can still have a smart idea the question is is there a result that proves this is impossible it's like saying if you're allow yourself to have an integer linear program can you capsulate this um with the inequality is I guess I would say probably probably no I don't want to try to prove it on the spot but I would say probably provably II know probably not without exponentially many inequalities that's my guess but don't quote me on that my guess is not without exponentially many inequalities well okay there is gonna be a smart idea though and the smart idea came from circa 1990 in some paper of two people Delorme and pull Jack and let me try to tell you about their smart idea first thing this is not the essential to them but to explain their smart idea we're gonna make a notation change it's kind of my favorite notation change instead of using 0 and 1 for the X keys we're gonna use plus or minus 1 okay so now switch in your mind xB it to be like plus or minus 1 and now also switch in your mind yvw should also state be plus or minus 1 and you should try it should try to be the product of X V times X W okay and so there notice that means that like Y V W is supposed to be plus 1 if X V and X W are the same because plus 1 times plus 1 is 1 and minus 1 times minus 1 is 1 and y VW should be negative 1 when X V and X W are different plus or minus 1 values so again it's just changing notation this is going to be like the constraint that is very hard to encapsulate but I wanted to switch notation like that and with the switch notation the objective function is still fine with the switch notation you see this little expression here 1/2 minus 1/2 times y VW remember this y VW is 1 if X V and XW are the same so when you plug 1 in here you get 1/2 minus 1/2 which is 0 so this expression is 0 if X B is the same as XW and if X V is different from XW then Y VW is minus 1 so this expression is 1 so with this new notation and this parenthesis expression is the indicator that X V differs from XW and so this objective function which is still a linear objective in the variables y VW exactly encapsulates Mexica ok so what I'm trying to say here is you have to switch notation so far this program if you could carry it out exactly captures maxcut okay so let me throw in a few more equations that are fine certainly if you have this set up then yvw should equal ywv so I can throw that in that's a linear inequalities or in linear equality so that's fine for LPS and again if Y VW is supposed to equal X VX W and X these are suppose to be plus or minus 1 then Y V V should always equal 1 and that's another linear equality so there's no trouble in throwing that into an LP and let me just pause again to say that like now this program also captures maxcut exactly okay I no longer have to explicitly say XV and xw r plus or minus one valued because well if you have the Y V W equals x VX W and you also know that Y V V is one then means X V squared is 1 which means X V is plus or minus 1 okay so still you know this is not like a linear program or anything but it still captures maxcut exactly uh and furthermore this piece without this tricky constraint is a linear program it's a linear objective linear equality in facts well that's good okay so here's really the smart idea of the lormand pull Jack it's gonna say let's take this LP kind of forget about this equation but instead imagine that we put this LP together with this weird constraint and the constraint is given the wise just that there's some should exist some real numbers X V such that Y V W equals x VX W ok so there's a weird constraints but let me call this for reasons we'll eventually see the moment constraint and I still want to say that this linear program LP together with this moment constraint exactly captures max cuts you know this moment constraint is some weird constraint it's not a linear inequality constraint but we're still at the point where these two things together exactly characterize max cuts okay so now I can finally say their idea and their idea is to try to enforce moment using linear constraints just on the yvw is so the only variables are Y VW's and smullin constraint says there exists real numbers like X such that y VW equals x vxw for all vnw and that's not a linear constraint but like I just try to encapsulate it with some linear constraints now we're not gonna be able to succeed in doing this exactly because we could succeed in doing this exactly then well the conjunction of everything would be a linear program which we could solve in polynomial time and then max cut this np-hard problem would be in annual time but just like with this sort of ILP to LP relaxation paradigm we're gonna kind of try to do a good job of captivating these moment constraints so let me talk about this heuristic Li for a bit and then I'll get talking about it properly later right so this is the idea to try to enforce these moment constraints using linear and constraints linear inequalities on the Y VW's well suppose we didn't try to enforce them at all and we just took the LP constraints and we gave those the ellipsoid algorithm the ellipsoid algorithm can you know solve linear programs and I'm gonna be a little bit um slightly heuristic in the following discussion for a bit we handed those you know the LP inequalities actually their equations off to the ellipsoid algorithm without this moment constraint at all and said hey ellipsoid solved this for us okay so ellipsoid would solve it and you know maybe it would return some values like yvv all the YV B's are one that's a constraint it's got to satisfy that and maybe all the other Y VW's are like some tiny value actually I think I'm - right here- of - to the poly and not - to the - poly n I meant to say some like super negative value remember the objective is to know I should have written like well anyway imagine it gave you back a bunch of like negative values for the Y's well it turns out that you know giant negative values for the Y's we're not gonna solve solve the we're satisfied the moment constraints and so imagine they look so I did this to you that's what I want them did this to you and then you came back and said hey um just a moment ellipsoid algorithm I forgot to tell you one constraint in the ki you should be solving and it'll give back imagine you tell the ellipsoid algorithm I've actually got a separating hyperplane for you it's the sum of all the Y's should be greater than or equal to zero I forgot to tell you that constraints sorry and it's actually legitimate for you if you're trying to capture the moments constraints to say that this is a valid constraint what I'm trying to say is any y v's but y BW is that satisfy the moment constraints must satisfy this inequality and the proof of that is quite simple you see this sum of all the yvw is if the really are X is real numbers X is that satisfy yvw equals x vxw then the sum of all the y VW's is equal to the sum over b and w of x v xw which by arithmetic is equal to the square of the sum of the X V's and the square of a real number is non-negative so this is sort of like a validate equality for any solution Y that satisfies the moment constraints so you can um you know if the looks like algorithm returns some Y's that don't satisfy this inequality you you can just say oh sorry ellipsoid algorithm I forgot to mention you should also satisfy this constraint ok the ellipsoid algorithm will be like Oh fine it's gonna like put that constraint in and like chop off like half an ellipse oh I didn't like it'll you know keep going okay we're using the fact here that looks like a girl it really only needs this separation Oracle we're sort of trying to build a separation Oracle for the set of all wise which satisfied this moment constraint this is a bit tricky so let me know if you have questions okay so maybe you hand that off to the upside algorithm and maybe now it comes back and it says okay I have a new solution for you why one one is one in my solution and y12 and y21 or 100 and y22 is one and got the dot and you say well gee host a moment ellipsoid algorithm i forgot to tell you a constraint on the y's I forgot to tell you this constraint that y1 1 minus y 1 2 minus y 2 1 plus y 2 2 should be non-negative and I'll actually force the SDP sorry the ellipsoid algorithm to go back and revise its solution because you know 1 minus 100 minus 100 plus 1 is not non-negative okay and again this is valid for any Y's that satisfy the moment constraints and here's the proof if you have Y's that satisfy the moment constraints then this y 1 1 minus y 1 2 etc etc is equal to X 1 1 X 1 minus X 1 X 2 minus etcetera and again my little bit of arithmetic this is exactly the square of X 1 minus X 2 which of course is nonzero a non-negative so Y's that satisfy this moment constraint will also satisfy this linear inequality on the Y's I'm getting a question the question is could you reacquaint constraint so the moment constraint is it's some kind of constraint on a collection of variables called y1 y VW you have one variable for each pair of vertices in the original graph and the constraint on these collection of Y's is that there should exist a collection of real numbers called XV 1 for each X of little V 1 for each little v vertex in the graph such that y VW is equal to X B times X W ok so not all collections of Y's are such that the you can find X's that satisfy this but some collection of wise have this property it's kind of just hard to characterize them but what and I'll explain us a bit more later but when we're trying to do is find some linear inequalities which are true of all collections of variables Y which satisfy this property it's pretty tricky but that's what's going on okay so maybe now you tell the ellipsoid algorithm oh you know I have this separating hyperplane so go back and find me a new solution so it looks wide will be like alright and I'll you know chop it's ellipsoid with this half space get a smaller ellipsoid keep going and maybe now it'll give you the same thing but why y12 and y21 being negative 100 but then we're gonna say oh I forgot to tell you a lip soit algorithm here's another constraint that the Y's I forgot to tell you are supposed to satisfy and notice this new solution doesn't satisfy this new constraint and it's a valid constraint again in the sense that any Y's satisfying moments satisfy this inequality because of well you can change these minuses to pluses question I guess this moment constraint is quite confusing the question is are we quantifying twice over little V in the moment constraint I don't think so I don't think I've written anything wrongly this means like the moment constraint is that there exist a vector of X's one real number for each vertex in the graph such that for all pairs of vertices in the graph VW y VW equals the product of X V and X W ok so uh as a side note I just explained to you why any Y's that satisfy this very I guess hard to understand condition it is hard to understand condition why why is it satisfied it's hard to understand condition at least must satisfy these two inequalities also the inequality that there's sum of all the Y's is non-negative that was the first one and notice actually that this inequality here since we also know that one of our constraints is that y1 2 equals y2 1 and another one of our constraints is that like y1 1 equals 1 and Y 2 2 equals 1 these imply that 2 minus 2y 1 2 is non-negative similarly here and together these imply that Y 1 2 is at most 1 and at least -1 and that's nice so I'm just observing here that you know the equality's we have that y VW equals y WV but we can put into the linear program and the Equality that yvv equals 1 that we can put into the linear program plus these inequalities which are consequences at the moment in equality enforce that all the variables Y are between minus 1 and 1 which is reasonable thing is something that um should be true ok but um going on here I mean this is definitely a tricky thing I've been telling you about so let me try to explain what's happening or what was doing with these examples where it's imagining running the ellipsoid algorithm and sort of implementing a separation Oracle where I kept telling the ellipsoid algorithm oh I forgot to mention this inequality I forgot to mention this inequality and so forth and so on so one thing that was always happening is we would take some constant some real constants CB and we would observe that for any real values X if you sum up C V X V and square it it's non-negative that's I mean that's a fact of life let me just expand that out oops there's some sort of weird PowerPoint bug here that should say C sub V not a V super C we try to draw that in here scribble this out okay so if we expand this out it means like just this is a fact of life about real numbers for any green sees and therefore if wise our numbers which satisfy the moment constraints if wise are such that you can find reals X and X next be one for every vertex such that this equation holds then this constraint will also be satisfied by the wise okay to repeat any Y's that satisfy the moment constraints must satisfy this green box constraint for every choice of real vectors C and notice that these are our linear inequalities in the Y variables so what I'm saying is any Y's that satisfy the moment constraints must satisfy infinitely many different linear inequalities all the ones that look like this for some choice of real numbers C sub little B one for each vertex and this what's called the SDP constraint or well call it s DP constraint is to imagine adding all of these linear inequalities for all vectors of real numbers see now this is actually infinitely many inequalities that we're gonna imagine including into a linear program but remember I told you like you know the ellipsoid algorithm is so great you can solve LPS even if you have like exponentially many or even infinitely many inequalities as long as you can do this separation Oracle I'll talk about this in more detail but that's where this is going so this SDP constraint is actually infinitely many constraints on the wise linear inequality constraints one for every choice of real vectors C which are satisfied by any wise lets satisfy the moment inequality moment constraints you so this is what we're going to imagine doing if you recall we set up an exact some kind of system that exactly captured max cot it was some linear program involving these variables Y a and it exactly captured it if you threw in these moment inequalities fact let me go all the way back to that and just remind you of that it's right here so this is the LPE this part that's circled in white it just involves the variables Y and we argued that if you could sort of solve this while enforcing this thing which is the moment inequality moment constraints then you see first of all all this facts together with the moment constraints would imply that this these X B's have to all be plus or minus 1 and then you know Y V W will just be plus or minus 1 depending on whether X V and X W are the same or not so X V so the X values now can be thought of as like a cuts let's say all the plus 1 vertices are in the set s and all the minus ones are in the complements y VW is plus one if the edge VW is not cut and it's minus one if it is cut and then this expression is one if the set defined by X cuts the edge VW and it's zero otherwise and so this sum represents the number of edges cut by the set indicated by X so just as a reminder this LP together with this moment constraint exactly captures max cuts and what we found is that like this moment constraint implied like infinitely many inequalities but linearly in inequalities and somehow we're gonna try to add all these infinitely many inequalities to this LP and be like oh I have an LP now with like you know N squared variables and infinitely many inequalities well I'm going to try to solve it anyway and hopefully this will do a good job of capturing the true thing that we are trying to encapsulate this weird moment constraint okay so let me go back to where we were okay so again we're gonna try to enforce this moment constraint which if we could exactly capture it would give us max cut exactly captured with this SDP constraint where we're gonna imagine taking our base LP and adding infinitely many constraints in our minds one for every real vector of C see you got this constraint and this constraint is implied by the moment constraints so just to repeat here are the key facts the real main key fact is that if we if like an algorithm imagines the original LP e together with these infinitely many constraints we can actually solve this LP efficiently and the reason is we're gonna actually be able to implement the Associated separation Oracle I'll remind you what that means shortly but that's what's gonna happen second remember these moment constraints imply the SDP constraints but it's a fact that it's not they're not equivalent okay so what we've done here is made a relaxation we had a program that exactly captured maxcut the original LP with the moment constraint we relaxed it like the new LP or the original LP with this SDP constraint which is now in an LP with infinitely many constraints and we can solve it so it's not gonna exactly capture Mexica but just like we saw in the last lecture where you you know take an integer program relax it to an LP and hope it does like a reasonable job of capturing the problem that's what's gonna happen here so a key fact is that this new thing that we can solve will give a good relaxation from maxcut okay and so for the remainder of the lecture I won't explain this key point one oops how we can satisfy this efficiently oops we got on the right slide here how we can solve this infinite LP efficiently and in the next lecture I'll tell you how you can use it how you can like round the solution that it gives you to give you this excellent approximation algorithm for maximum cut which always finds a cut that's within eighty-seven point eight percent of the best optimal cut\", metadata={'source': 'a2Z3vMju3Y4'}),\n",
       " Document(page_content=\"okay so the last thing I want to talk about is how you can impound real time solve this like infinite LP that has and squared variables the y VW's if n is the number of vertices in the graph but like infinitely many constraints and as before in order to do that we just need to implement the we can do that using the ellipsoid algorithm as long as we can implement a separation article one that can give in some n squared numbers why it can either check that all infinitely many of these inequalities are satisfied in which case it's like done you found wise or it can find a violated inequality which is to say it can find some real numbers C which make this inequality fail I claim given these wise there's a polynomial time algorithm that can do that task and this polygon time algorithm comes from linear algebra and this SDP really stands for a semi definite program and this notion of positive semi definite matrices is exactly what you need so a slide or two about linear algebra some linear algebra that you may or may not have seen before in your life so a symmetric matrix Y Capital y and n by n symmetric matrix Capital y of numbers is said to be positive semi-definite or PSD if it has this property but for any real vector C you know C transpose times y times C is non negative so that's infinitely many inequalities and if you just multiply this out in coordinates C transpose times y times C is exactly this thing sum over V W going from 1 to N of C V W sorry this is a typo this should say C V times C W sorry about that some W go from 1 to N of CV C W times this matrix entry y VW okay this is like infinitely many conditions on a matrix Y for it to be positive semi definite but there's gonna be some linear algebra to the rescue that'll make it less confusing so matrices Y that have this property are called positive semi definite PSD and luckily they're actually easy to characterize even though there are infinitely many constraints there's a polynomial time algorithm polynomial in the number of bits you need to write down why such a given the symmetric matrix Y of numbers it either says yep why is PSD like why satisfies all of these inequalities simultaneously or it finds like a counter example it finds an explicit oh I still have some typos in this slide sorry it finds an explicit vector C such that C transpose YC is not greater than or equal to 0 such that it's less than 0 okay so it's like sort of tests if a matrix is positive semi definite and if it's not positive semi definite sort of finds a counter-examples vector C that proves it and ah this is based on a linear algebra algorithm that depending on your linear algebra background you may have seen before I'll just tell it to you in a proof sketch this is not a linear algebra class per se the algorithm is quite similar to Gaussian elimination actually you take this Y and you kind of do Gaussian elimination but like a symmetric form where you know Gaussian elimination you try to sort of like clear out columns like make them zeros by taking linear combinations of rows like in the symmetric form like whenever you clear out it like the first column of zeros except for the diagonal entry you also simultaneously clear out the first row and make it all zeros except for the first diagonal entry and then you like you know pivot again on the second diagonal entry and you clear out this column all zeros in this column all zeros and you keep doing this and if you have a symmetric matrix to start you can sort of symmetrically do Gaussian elimination to sort of form linear combinations of rows that like transform the matrix into a diagonal matrix I say this if you're kind of half knowledgeable about linear algebra maybe it'll help the linear algebra term for this algorithm is cheol-su decomposition and what you're trying to do eventually is to write Y as like this diagonal matrix sort of times a lower triangular matrix on the left and it's transpose and upper triangular matrix on the right okay so there's given a symmetric matrix this chose s key decomposition algorithm which is like symmetric Gaussian elimination actually expressed as y as L times D times L transpose where D is diagonal and L is lower triangular this sometimes called like the LD u decomposition but the point is Y will be PSD if and only if when you do this the diagonal entries of D are non-negative so it's a fact that if you succeed in doing this and all the D entries are non-negative then the original matrix Y is PSD and it's also a fact that if you feel like you come up as you're doing this you get some diagonal entry that's negative you can easily extract from the algorithm a vector C such that C transpose Y C is less than zero okay so I'm not gonna get into it further than that but if you do a little linear out for a practice and maybe it'll be on homework this proves to be true and this is also a polynomial time algorithm because it's like Gaussian elimination and it's like an Ncube time algorithm for doing this there's a question can we use eigen decomposition instead great question so it's a fact that if you have a symmetric matrix another linear algebra fact is that if you have a symmetric matrix it has all real eigenvalues and another way to define PSD positive semi definite is to ask or is to say that all these real eigenvalues are non-negative that turns out to be equivalent so you might ask oh great so given uh symmetric Y I'll just compute its eigenvalues and if they're all non-negative I say great its PSD and in fact if you find an eigenvector with a negative eigenvalue then that eigenvector will actually be this C that has C transpose Y C less than zero in principle that works but actually if there's numeric problems because there's not really a polynomial time algorithm for exactly computing eigenvalues right these not even don't even have to be rational I can values so like you have to get into like ooh what if like the eigenvalue is something like negative irrational number that's like super close to zero can I like tell whether it's zero or just like slightly negative but like maybe as like ee exponentially many zeros after the decimal point you get into some weird issues like that so this is actually as far as I know the only truly polynomial time way it's a test of a matrix is PSD what intuitively you can say like yes compute the eigenvalues and if they're all non-negative its PSD great question so this can actually be is this algorithm could be used as the polynomial time separation Oracle right because it exactly given some explicit numbers for Y which are symmetric it'll either say good job ellipsoid algorithm you found some Y's that satisfy all the infinitely many constraints such that Y is PSD otherwise it actually finds you an inequality this is a linear inequality in entries why because this is like sum of CBC W times y VW expression that's violated so this is really takes advantage of the fact that the lips are dogyum really only needs to run you know a polynomial number of variables and the ability to find separating hyperplanes when you think of a point that is not satisfying all the constraints and so it's a fact of life that even forgetting this L as the Mexica thing you can use the ellipsoid algorithm to efficiently solve any one quote semi definite program where a semi definite program is like a linear program over variables that are arranged in a matrix together with the constraint that the variables in range in a matrix form a PSD matrix there's a little asterisk here which I'll talk about but there's another question here question was for these exponential or infinite sized LPS with separating Oracle's how do we know what value of little R or big R we should give to the ellipsoid algorithm I guess since the number of bits to describe this Cathy erratically is infinite because I simply infinitely many constraints superb question it exactly gets into this footnote for these semi-definite programs where you like take an LP plus you put it in like these infinitely many constraints all the stuff that we said about without loss of generality in a linear program this capital R with poly mini bits and this little R with poly mini bits exists and you can throw in the big box constraints etc those are no longer true technically this is a very technical point but they're no longer true when you have a semi definite program and so there's two consequences one you cannot do this to many different programming at all unless you explicitly have big box constraints you can't just say like oh since I have you know an LP I can put in big box constraints without loss of generality in the PIA in the STP case there's some different primary case it's no longer without loss of generality so you have to explicitly put them in however for many applications that's perfectly fine like in this Mexican application we know that all these little Y's we're looking for are supposed to be bounded between minus 1 and 1 so we happily put these in that Y all the y VW is are between minus 1 and 1 and then you know our big R can be 1 it's also true that we cannot when we have these infinitely many constraints we cannot have any little R that works there's a consequence the ellipsoid algorithm can't like keep going into like some finite time and then be like oops I know it's either empty or it's not empty and so that goes into this footnote you can really not literally solve these SDPs you can only solve them to additive accuracy epsilon in poly log 1 over epsilon time but you know it's usually fine to solve your SD peas up to this numerical accuracy so this is like mostly a highly technical footnote so I'll leave it at that so just real briefly why if you succeed does this imply that the matrix Y is positive semi-definite well let me just put everything out there it's because if Y is really expressible as LDL transpose then for any vector C if you look at C transpose Y C it's C transpose LDL transpose C if also the diagonal entries are non-negative then they have square roots so you can write D as square root natan square root D which is the diagonal matrix with the square root of the diagonal entries on the diagonal entries and so now you can write this and see now this part here is the transpose of this part here and so this C transpose Y C is equal to this vector times its transpose which means it's equal to this it's equal to the squared length of the vector which is always non-negative so this proves that if Y really can be written like LDL transpose where the diagonal entries are non negative then it is PSD and in fact if you manage to do this gets LDL transpose representation you can write it like this and now this matrix root DL transpose is an upper triangular matrix this matrix is a its transpose a lower diagonal matrix and so we see that if you have a symmetric matrix Y it can be written as y equals u transpose U for some matrix u I mean because these ELLs and DS exist and conversely by this proof that still written down here if you ever write Y as some matrix u transpose times u then this proof starting from here goes through and you concluded that Y is PSD so it's another basic linear algebra fact that a symmetric matrix Y is PSD if and only if it can be written as u transpose u for some matrix u in fact this matrix u doesn't have to be upper triangular it doesn't even have to be square there either always will be au which is both upper triangular and square which you know satisfies Y you you transpose you well conversely this proof doesn't need you to be upper triangular or square you know there should be like this will be you see transpose and this will be u transpose C so that's why this proposition is true and so it means if you have a symmetric positive definite matrix Y you can write it as u transpose u and if you think of the rows of so if you think of the columns of U as vectors and the transpose has these same vectors as rows and what it means is that the IJ entry of Y little Y IJ is the dot product of the I u vector with the J that you back there so finally and this is my last slide here it's also basic linear algebra proposition that whenever you have a positive semi definite matrix capital y its entries can be expressed as UI UJ like the IJ entry sorry there exist some vectors u1 through u n such that Y IJ equals UI dot u J and actually this is super similar to the moment condition remember the moment condition that we were trying to capture with PS deenus was that though there are real numbers u1 through UN we called them x1 through xn but there are real numbers u 1 through n such that Y IJ equals u pi times u J and we replace this moment condition with this relaxed version this PSD condition and the PSD condition is equivalent to saying that there are vectors u 1 through u n such that UI dot u J equals y IJ and so next time what we're gonna see is how we can solve the Mexica SDP we've cooked up and get these vectors and then round quote-unquote round these vectors to integer values to get good maxcut solutions okay so that's it if there are any quick questions I'll answer them now otherwise I'll stop the recording and then I'll answer non quick questions so feel free to hang on after that I saw the recording and asked any questions you may have\", metadata={'source': 'Pkp8kBz4IlQ'}),\n",
       " Document(page_content=\"okay so this is lecture 20 and in this lecture we're going to talk about csps which means constraint satisfaction problems and we're gonna talk about approximation algorithms for them so when we were last talking we were thinking about the maxcut problem which is gonna turn out to be a particularly simple kind of constraint satisfaction problem or CSP and we spent a lot of time developing a semi-definite program for it which is a generalization of a linear program and we got as far as understanding that this would let us attempt to solve a maxcut problem by finding vectors associated to each vertex that had some properties and now we're gonna talk about how we can try to take these vectors and convert them into an actual cut of good value so let's recall what was going on let's phrase the max cut problem in the following way so you're given a graph and undirected graph G and you're supposed to partition the vertices into two parts often called s and s complement but in this case we're gonna call them all the vertices assigned value 1 and all the vertices assigned value minus 1 so we're gonna assign values XV to the vertices plus or minus 1 and what we're trying to do is maximize the number of edges that have their endpoints on different sides of the cut so in other words we want to maximize the sum over all edges VW just of the indicator that we've assigned different cut sides to the two endpoints B and W ok so I'm getting some can everybody make sure their mic is muted please Thanks ok so um right so last time we developed what was called a semi definite programming relaxation for this problem and what was that well here it is up on the screen we want to we're gonna have in this time a different program we're gonna have variables and squared variables one for each pair of vertices and we call these little Y's some VW and we think of them as being arranged into a matrix and the idea is that yvw is supposed to stand for the product of X V and X W sorry I'm still getting people who are unmuted okay can everybody still hear me I've tried to mute you all okay so right so yvw is supposed to sin 4x v x times x w which is this quantity up here and so we have a linear objective function involving the variables Y V W and since X V and X W are supposed to both be plus or minus 1 XV x XV should always be one so we can put this as a linear constraint Y V V equals 1 and also X V is the same as X W so we can put the constraint that Y V W equals y WV okay I just wrote that down here because the main constraint we had was that Y is a positive semi definite matrix which we talked about last time and where did this is it's a relaxation of this condition that we want so it's a relaxation of the condition that there exists real numbers one for each vertex X W such that y VW equals x v times X W okay so as we argued last time you can solve this semi definite program and find the optimal y VW's here in polynomial time okay may be opposite to some like exponentially small additive accuracy but we can ignore that great so we also talked last time about what is a positive semi definite matrix and square matrix is said to be positive semi definite assuming it's symmetric matrix if and only if we have this condition this is one of the equivalent conditions we talked about last time if and only if there exists some vectors little u1 through unit little u n such that the VW then' tree of the matrix is equal to the dot product of the veeth vector with the W's vector okay so the existence of these vectors these dot products all whose pairwise dot products give you the matrix Y is equivalent to being PSD and I've written here um these vectors of which their end should themselves be in n dimensions this statement is true it's also true if I eliminated this condition that they be in n dimensions so just if you have vectors in any dimensions and vectors and any dimensions that have this relationship their dot products give you the entries of Y then Y is PSD but on the other hand if such vectors exist as we argued last time they can all exist in n dimensions now what's interesting here in the what's you can you can look back at this complain that it's this SDP condition is that it's a relaxation of this goal that there should exist numbers X be such that y VW equals x v xw if we could somehow insist that these vectors be one dimensional vectors ie numbers then this PSD condition would be exactly equivalent to what we were shooting for and if we had this exact condition about numbers XV this would exactly be capturing the maxcut problem but alas we cannot solve that program in polynomial time but with this relaxation where we allow you know these X V's which are not calling you V vectors to be vectors in like a large dimension it almost looks like a more complicated problem but interestingly enough we can actually solve this program in polynomial time any questions right as always you can type them into the chat and I'll catch up with them what I need to okay so let me just make a few changes of notation instead of writing Y some VW down here and insisting that the matrix of Y is positive semi-definite I'll just use this equivalent constraint about positive semi definiteness so this maximization problem is now maximizing over vectors one vector per vertex in the graph so given the graph and this program is now defined vectors one vector for each vertex in the graph and what are the constraints we have well the only constraint really really had left was that the Y V V was supposed to equal 1 so that's saying that the beef vector dotted with itself should be 1 and it's like saying the length of the V vector squared is 1 and so that's just equivalent to saying that the V vector is a unit vector so I'm just changing this notation a bit more but now we can see this SDP relaxation which we can efficiently solve is seeking a unit vector for each vertex in the graph and it's trying to maximize this objective function the sum over all edges of you look at the vectors for the endpoints use of the vector and use of a W vector and you dot them together and you take 1/2 minus 1/2 of that now let's think about this a little bit more as well so these are unit vectors so they're dot products are going to be between minus 1 and 1 well actually I'll come back to it in a second but let me do this as it says on the slide this relaxation check I want to check that this program is still a relaxation so first of all what it means to be a relaxation is that the optimum the actual max cut in the graph should have value less than or equal to the semi definite programs optimum you know I clean that's true because suppose you have any cuts some subset of vertices s which achieves the maximum cut that achieves this optimum value now what solution can we get for this SDP well I'll just take all the vertices that are in s and assign them like the fundamentally one dimensional vector that's like plus one in the first coordinate and zeros elsewhere and all the vectors sorry all the vertices that are not in s I'll assign them for a unit vector just minus one okay so these are definitely unit vectors they're all lying in line and so they're a feasible solution since they're unit vectors you see if you have an edge VW where V and W are on the same side of the cuts then the vectors associated them will be the same and so the dot product will be one and so this expression half minus half the dot product will be zero so you'll indeed sort of get like zero points for this edge where the end points are the same but if the end points are different the dot products will of course be minus one this linear expression will be one and you'll get like one point indeed for this edge okay so this says you can always take a feasible like integer solution like a cut and turn it into one of these vector solutions that has the same value but you know this SDP op could be bigger because well these vectors don't have to be one dimensional that can be and dimensional so maybe you can make things bigger and in fact you can is we'll see on the other dimension since all the feasible solutions use unit vectors this thought product will always be between minus 1 and 1 a particular it'll be at least minus 1 so this linear expression 1/2 minus 1/2 times the dot product will be at most one oops this is a typo it should say optimum is at most and the number of edges okay that's because you gets drawn white here you get at most one for every edge okay so let me make now a really kind of slightly gross but useful abusive notation before I was calling these vectors one for each vertex if the vertex was V I was calling the vector use of V vector now I'll do something like a little wacky and for a vector sorry for a vertex named V I'll call its associated vector V with a vector sign kind of a weird piece of notation but I it's actually kind of nice so now what I'm gonna say is we on a unit vector for each vertex and you know the amount that you get for it is just for the edge with endpoints V and W is just the dot product of the associated unit vectors a let's think about this dot product a little bit more if you remember high school algebra the dot product between two vectors is the product of their lengths times the cosine of the angle they make but these are unit vectors so the product of their lengths is just one so the dot products is really nothing more than the cosine of the angle between them as we've seen before like you know if V vector equals W vector then you get plus one and and if V vector is minus W vector then you get minus one but for example if V and W are orthogonal which they can be once you go to higher dimensions you know their angles 90 degrees the cosine of 90 degrees is zero so like if you have two vectors that are orthogonal and they're forming an edge you kind of get half a point for them you get 1/2 minus 1/2 times 0 so this do you strictly like these victors you have a victor for every vertex and it's like there you know and people antipodal you get like one point if there's an edge you know if they're the same you get zero points if there are thought and all you get half a point and everything in between so this is what the plot of cosine theta looks like as theta goes from 0 to PI okay so you're trying to embed this graph into like the unit sphere such that the edges are as far apart as possible their endpoints are as far apart as possible so let's do an example pretty simple example but kind of an interesting example so this example is a five vertex graph it's just the five cycle graph and I've called its vertices a B C D D and E and so in this problem you're looking to assign a unit vector to like five vertices such that the ones connected by edges are sort of as far apart as possible turns out the optimal solution for these vectors occurs in three dimensions and it's actually got a name it's called low Voss's umbrella because lots of levels came up with this for well relating sort of problem in coding theory so yeah so it's he called it an umbrella you see so this peak of the umbrella is the origin in three dimensions and then like a five really called some things that the umbrella spokes knots folks there's word for them anyway these five spokes in the umbrella are the five unit vectors okay so I guess they're all pointing you know sort of down and I actually said which vectors associated to which vertex and you see actually it's not the first thing you might think of it's not like you just put the vertices like ABCDE around the spokes of the umbrella ribs I think they're called ribs the ribs click umbrella because you want if it's an edge in the graph you want these unit vectors to have as big an angle as possible so in fact let's say you call this vector here a let me get the highlight around oops not the highlight or the laser pointer let's say you call this first vector a you may as well do that without loss of generality so then you now let's decide where to put B so you actually since a B is an energy what could be as far apart from me as you can now bearing in mind that I've already told you this the optimal solution happens to look like this it means that B should be one of these two edges here sorry vectors here let's say this is B and then B and C have an edge so you want a/c to be sort of as far from B as possible and you don't mind if C is close to a because they don't have an inch so perhaps this is C and then you indeed also be far and you want e to also be far and this is it this is actually the optimal solution I mean it's not obvious or anything but it turns out to be true and you see that in the solution every edge in the graph is associated to two unit vectors whose angle is well as far apart as possible in this umbrella and if you do a little geometry it turns out to be a 4 PI over 5 aka 145 4 degrees so this is the optimal solution to the semi definite program for this 5 cycle graph and cosine of I mean now we we take a look at what it achieves it turns out that cosine of 4 PI over 5 is negative 5 over 2 where fi is the golden ratio 1 plus root 5 over 2 and numerically this is about negative point-eight so it's pretty good right so like every edge in the graph all 5 edges in the graph are associated unit vectors whose dot product is around negative 0.8 which is good I mean the smallest it could be which is what you're shooting for in terms of dot products is negative 1 you got negative 0.8 so 1/2 minus 1/2 times negative point 8 is positive 0.9 so like this sort of vector solution I kind of like 90% cut each edge in in a weird way and so forth is therefore uh oops this was me messing around I don't know why that's on the slide ignore this right so what's the integer solution the actual solution here the optimal cut as forages because this is not a bipartite graph so it's impossible to find out partition that cuts all the edges but you can get 4 to 5 is there a question ok but the SDP Opta is this like approximately 0.9 times 5 because every edge achieving this 0.9 so the SDP up is around 4.5 okay so this is an example where the SDP really is a relaxation in the sense that the SDP gets a value that's bigger than the optimum but it's still kind of non-trivial and just out of interest sake the ratio here for over 4.5 is around well it's like eight nights it's around 0.8 eight or more accurately it's exactly this number right okay so this is an example of a solution to the semi-definite program for max cuts on this graph and now let's talk about this concept of rounding so again when we were solving problems before with linear programs we take an integer program relax it to a linear program get some kind of like fractional solution and then what we tried to do is like convert or round this fractional solution to an integer solution like a real solution where the the the value of that solution was as close to the LP value as possible okay we're gonna try to do that again here except instead of like a fractional solution we have a vector solution why worry again I'm gonna try to round these vectors down from n-dimensional vectors to like one-dimensional numbers plus or minus 1 and this will give us a cut okay so there's a very beautiful uh natural and I look at way to do this given by Goins and Williams in this famous paper from 1994 and it's this you have all these vectors in space that you've gotten out of solving your SDP you want to partition them into two parts so what you can do is pick a hyper plane through the origin and since you know we don't really know much else to do or like maybe the easiest thing to do is just pick a random hyper plane through the origin one that's like rotationally symmetric so just random hyper plane through the origin and yeah let's split your vectors into two parts and therefore it splits your vertices into two parts and just let's call those our parts s and s far so this is a cut now we have to analyze this or we would like to analyze this and understand you know expectation at least what number of edges can we expect this to cut it just does a little call back to i'm gonna lecture three or something vertically how do you actually pick a random hyper plane through the origin well hyperplane you can associate a hyperplane with its normal vector and you can get a random normal vector just by picking all the end components to be gaussians I remember this most important fact about gaussians is if you pick n of them independently the resulting vector is like rotationally symmetric so algorithmically the way you actually execute this this line of the code is to just pick n gaussian x' and consider the hyperplane orthogonal to them in particular actually you put every vertex into s if it's vector V vector makes a positive inner product with G vector and a negative if you put it into as far if it makes a negative dot product with G vector ok so let's talk about the analysis and what do we really want to do with this analysis so we haven't given some vectors there's two things going on first of all these vectors achieve some value for the objective value for the semi definite program namely they achieve the sum over all edges of this expression 1/2 minus 1/2 times the cosine of the angle between the endpoints and we have this algorithm now that produces a cut and we were interested in like how many edges it cuts and we want to bound that in terms of this sdp expression an objective expression so that's all we care about the expected number of edges cuts by this random hyperplane rounding algorithm ok so we can write this as the expectation over the random hyperplane of the sum over all edges VW just of the zero one indicator random variable that s cuts the edge of VW that one of V is in s one of you W is in s and the other is not in US ok so as always in life when we see this expression you should switch the expectation in the summation and then you'll have expectation of an indicator and you the expectation of an indicator is a probability so you'll get to this expression so it's the sum over all edges or the probability again over the choice of the random hyperplane through the origin that well s cuts this edge according the algorithm if and only if the two vectors V vector and W vector associated to these vertices by the SDP are an opposite size of the hyperplane okay and what's nice is we can now analyze this on an edge by edge basis so we can like fix of an edge VW in our minds and the associated unit vectors V vector and W back there and ask like what's the probability that a random hyperplane will split these two vectors so here's a two-dimensional picture of course it's really an N in n dimensions but uh actually in another sense it's really in two dimensions as you'll see so here's the unit sphere and we have two vectors V vector and W vector and we're picking a random hyper plane through the origin and in fact really it only it really is two-dimensional in the sense that like if you think about this geometric layer for a minute whether or not a hyper plane you know splits these two vectors I don't know if these my hand gestures are gonna make a difference it really only depends on whether like where the normal vector to the hyperplane is when projected onto the two-dimensional plane spanned by BMW and say that's super well but the upshot is I mean picking a random hyperplane and once you've fixed V and W in your mind picking a random hyperplane you can really do it analogously just by picking a random well a hyperplane or line in the two-dimensional plane where V and W live and like this you know orange line that I'm just fun here and once you have this geometric realization you know it turns out there's a very simple expression for this probability you see there's basically like 180 degrees of possibility for how this orange thing rotates and the event that they get split you know occupies an angle of the angle between V and W so what I'm saying here is this probability once you fix me in W the probability that they're split by a random hyperlink to the origin is precisely their angle divided by PI or 180 degrees you know once you get down to the student case I think that should make sense right like a V and W are at angle 0 and the probability that this hyperplane would split them is 0 and if V and W are at angle 90 degrees see if I can draw here a V and W are at angle 90 degrees like B was this okay then you know the probability that the orange line would split it would be like sort of you know only if the orange thing was going like this it's a 50% chance and again if V is like this if V is actually opposite to W and there's a hundred percent chance that they'll get split by random diameter of the circle like this so make sense any questions good so this is kind of cool because we have like a an edge by edge expression for both things an edge by edge expression for the SDP objective value that's this yeah the nitrogen for like the expected contribution to the tonal cut value the probability that the edge gets cut by the algorithm and so and they're both in terms with just the angle between I mean this is also key just involve the angle between the two vectors so now it's just like some numeric comparison between this magenta thing and this green thing so let me put this back up here expect the number of edges cut by the algorithm is the sum of this purple or magenta expression and you can just plot these two quantities as a function of the angle theta so you know any two vector unit vectors their angle is somewhere between 0 and PI 0 180 degrees and you know the fact the probability that these vectors get cut by the algorithm is like a linear function of their angle if they're at angle 0 at 0 if their angle PI it's 1 so it's just this expression up here theta over pi well the sdp the contribution of the semi definite program's value is this nonlinear thing it's 1/2 minus 1/2 cosine so it looks like this curve the point is that these two expressions are not the same but they're pretty close they're not too different in particular if you look at the ratio between these two expressions you know you're you're worried this stp opt could be bigger than the actual thing you found but could never be like that much bigger this green line is never that much higher than the magenta line and in particular the worst ratio between them occurs at this point that's really really close to 3/4 pi it's a teeny bit less it's here and even here the magenta point is like 0.87 8 times the the green point another way to say it is if you multiply this green curve by 0.8 7 8 which I've done in a charmingly homemade fashion here freehand you get this slightly darker slightly thinner green curve point 8 7 8 times the expression with the cosine and this thought this new curve fits underneath the magenta line which means that the magenta expression is always greater than or equal to 0.87 eight times the green expression okay what I'm using here is the fact that this magenta line is above this like this lower green thing point eight seven eight times the green expression okay so now you can bring the point eight seven eight outside this sum and you'll get just exactly the STP objective function so you conclude that the expected number of edges cuts by this algorithm is at least you know 87 or 88 percent of whatever the SDP optimum is and you know the SDP optimum is always bigger than or equal to the true optimum that's because the SDP up the SDP is a relaxation of the maxcut problem so this is in turn it is bigger than 0.8 seven eight times the optimum the optimal max cuts okay this is like the end of the analysis of the gunman's williamson SDP routing algorithm and it says or one way to succinctly say what's going on here is this Komen's williamson algorithm this GW algorithm is a so-called point eight seven eight ratio approximation algorithm for max cuts so at least an expectation and it's it will give you a cut whose value is at least eighty seven point eight percent times the maximum cuts okay then you can run this many times and take the best cuts and you'll get this hold not just an expectation with high probability you can even do randomized this using Nissan small set generator a small space generator and some other ideas if you like okay any questions about that okay there's a question is there any particular reason why we consider our to the end instead of some other dimension in the relaxation say our to the login it's still a relaxation great question so remember yeah in the relaxation the STP relaxation it eventually turns out to be you know find a unit vector for each vertex to maximize this expression and so to answer the question it's sort of not up to us what dimension the solution ends up being in if we could specify not only do I want this but I want these vectors to be in dimension D and we'd actually be solving an np-hard problem because we could say oh I want them to be in dimension one I either I want them to just actually be scalars in that case as we saw the I mean the vectors since you have that there dot products is one that's of a unit vectors they would have to just be the numbers plus or minus one and then you would exactly get that the SDP captures the maxcut problem which is np-hard so in some sense it's like not really up to us we have to allow them to be sort of any dimension it's weird because by like saying like you know I give up you know they can mean any dimension it actually makes the it polynomial time feasible to like find the optimal vector solution and as a side note it's not like they're gonna be in like exponentially many dimensions if you you know think about the nature of positive semi-definite matrices for a moment as we did last time if if some dot products can be real i if you have n vectors realizing some pairwise dot products in any dimension and they cannot be realized in n dimensions as we saw last time if you you know do this cheol-su decomposition on the y matrix you get out of the semi definite program you get back n-dimensional vectors good on the other hand related to the question about whether they could be in lower dimensions it's also true that if you solve the semi-definite program and unfortunately get vectors in n dimensions you can rent it's like a known fact this Johnson Lyndon Strauss lemma if you've ever heard of it that you can randomly project these vectors down to around log n dimensions and the dot products will with high probability not change by very much so actually you can get them down to like log n dimensions but in some sense it doesn't help that much okay great\", metadata={'source': 'T8hD0ofzDP0'}),\n",
       " Document(page_content=\"okay great so now I want to go on and as the title of the lecture suggested talk about constraint satisfaction problems more generally in some sense Mac Scott which we've been focused on for a while is the almost like the simplest constraint satisfaction problem so now I want to talk about them more generally and about approximation algorithms for them more generally and I'll start calling them CSP is that is a stands for constraint satisfaction problems okay so constraint satisfaction problems are a pretty wide class of algorithmic problems or tasks and many well-known algorithms problems are examples of constraint satisfaction problems so the max cut problem is a constraint satisfaction problem and the two set problem and the 3sat problem in the case that problem are csps trying to three color a graph is the CFS be trying to solve a system of linear equations as a CSP not every problem is a CSP about a lot of cool problems are CSPs so in order to say you know instead of just directly diving into the definition of CSP which is a little bit elaborate let me talk about two SATs specifically for a moment and why it's a CSP it'll help us like to learn some of the terminology so remember the two sad problem I am sure you've heard of it before ah the input looks like this it's like a to CNF formula have a bunch of variables x1 through xn and a bunch of clauses each of which is like the Auror of two literals yeah and you're trying to find a truth assignment but you know ideally satisfies all of these clauses so we're some of the ingredients of this two-step problem or the input well first of all the one ingredient is the variables x1 through xn another ingredient is what I like to call the domain the domain is like the values that you're trying to associate to the variables okay so sometimes in you know the sat problem you call them 0 and 1 sometimes you call them true and false let's just call them true and false for now so in you know to cite you're trying to assign true or false to every variable and well the other main ingredient in too sad is the clauses which we're gonna call constraints because you know I'm going to start talking about CSPs and each one is a constraint it's because like they're ANDed together you know this clause is a constraint like you want x1 and x3 have a true or and you want this clause thought it was a constraint to be true of your assignment and you want the this clause to be true of your own assignment I should say by the way uh it's gonna come up again but like there's an unfortunate behavior in algebra and constraint satisfaction problems be saying that like people don't make a clear distinction between a variable and an assignment to it a variable we're gonna try to keep those two things very separate in our heads in today's lecture the difference between a variable and an assignment to a variable okay so even to sad it would seem that you just say okay yeah uh you can have any number of clauses or constraints and each one should be the or of two literals but in order to define constraint satisfaction problems in general I really don't want to talk about literals if you recall a literal is a variable or its negation but I want to talk about negation I just want to talk about variables so in fact I don't want to say that the constraints are ORS of two literals I'm going to say it differently so let me call in my little example here and we call the clauses or constraints this let's start to call them C 1 C 2 C 3 and so forth so let's take a look at C 1 C 1 is nothing too funny C 1 is the or function the or predicate or the or constraint or the or relation those are all synonyms applied to variables X 1 and X 3 okay what about C 2 I want to claim that C 2 is also some predicate a relation little function applied to variables X 2 and X 4 but I don't want to mention literals so does anybody know what a predicate is being applied or is constraining X 2 and X 4 here type it into the chat all right well enjoy the show it's NAND right this constraint is specifying that they can't both be true x2 and x4 cannot both be true because that's when this clause fails or fails yeah very good somebody got it and and there's just a little lag I suppose between when I asked him when people quickly type great so what about C 3 C 3 is another clause the simple constraining X 2 and X 11 what variable or what predicate is constraining X 2 and X 11 here check in the chat guess it's like some implication or something like maybe implies or something but I want to stop talking about it this way let me just call it this it's like the or function except you negate the first input okay that's what's going on here in other words it's the you know the predicate with this truth table yeah I suppose that's commonly called like implies but yeah very good somebody said it on the chat but yeah I think what's best is to think about it is like actually in 2sat there's like four kinds of constraints you might have each one is sort of like the or constraint on two variables except that you are allowed to have negations on inputs okay so instead of making a literal into a first class object I just want the variables to be first-class object and talk about changing the constraints so in the two set problem you don't have like one kind of constraint like or of two variables or literals I want to say that you have a set of four allowed constraints you know or or but with the second input negated or but with the first input negated and or but with both inputs negated and each of these four constraints has a well called arity to arity means just the number of variables to which the constraint applies it actually to be super technically correct here the CSP I'm describing is usually not called 2sat it's usually called e to set this e referring to the fact that every constraint involves exactly two literals the right minor point well it's not so minor but it's the technical point usually when people just say 2sat they allow clauses of length up to two now we're describing you know the version where every clause has to have exactly two constraints so instead of writing the input like this is like first of all I'm not gonna write as a conjunction I'm just gonna write it as a list of constraints that I want to be true and yeah each of these constraints I'm gonna say okay it's one of the predicates sore constraint types from my set applied to in this case two of the variables okay and these pairs of variables are called the scopes like in this first constraint look or with no negations is the predicate and the pair x1 x3 is the scope of this predicate and again just as this minor point in the formal definition of CSPs the scopes are required to involve distinct variables which is why this really only lets you have it's like II to say you only have words of two distinct literals you can't do this trick of like oh if you want to just have the constraint x1 I'll just do or of x1 and x1 because I want the Scopes to be distinct variables okay so unlight of that if you want to capture just plain 2sat where you're allowed to also have clauses of length 1 and liberals of length 1 then you have to add two more predicate severity one which you could call or and sub one and or someone with negation or like I guess it's like then the identity function on one variable and the negation function on one variable okay so let me get the formal definition now and this is like a it's like a sort of annoyingly complicated definition so if you have any questions definitely type them into the chat so in general a CSP is defined by a couple of things first of all the domain which I'm going to call Omega and remember this is like the values you want to assign to the variables so this domain was true and false for too sad or Sat like things but like for 3-coloring the domain is gonna be like red green blue and it's most often just like the integers from 0 up to some Q minus 1 and often Q is 2 so it's like often you're concerned about like binary constraint satisfaction so I should say boolean constraint satisfaction problems where you're trying to assign like 0 and 1 to every variable but the main thing that defines a CSP is the set of predicates that are allowed so the set of constraint types that are allowed on call this capital sign and this set can have one or more predicates in it like for e2 said it had four predicates and every little piece I in the set is a mapping from like an R tuple of domain elements to 0 n 1 and here 0 n 1 means satisfied and unsatisfied and R is the arity of the constraint so remember like for the two set for e 2 sad CSP Omega was true and false all the arrow T's were two so our constraints little psi were supposed to map a pair of true false values to 0 & 1 meaning satisfied or not satisfied and indeed like you know the first one of the four constraints was the or function which Maps a pair of truth values to 1 unless both those truth values are 0 false and you can have different error to your size and your big gets that kind of different era T's if you want like if we want to capture 2sat remember we needed some or type predicate severity two and two for type it's of arity one that's fine and ah that's it so I want you to find the domain and most especially the set of predicates that defines a particular kind of CSP like as we'll see max cut or 3-coloring so that's like the problem but now you have like an input or an instance of the problem the input or an instance of constraint satisfaction problem is well it's like a set of n variables so it's called V together with a list of constraints and the list of constraints is the main thing and I think of like the clauses and our 2sat instances each constraints see as a pair I usually write the other way sigh ass I'm not sure why I wrote SSI here anyway saya is one of the predicates from the set it'll have some arity R and s is the scope and the scope remember is a tuple of distinct variables and there should be R of them for a predicate of already are it's all the terminology involved in a CSP and my computer is really chugging and freezes at the start of each new slide okay so let's see some examples so one I said you know one of the simplest possible examples is max cuts and that's true so max cut well on one hand sometimes we describe it oh you're giving a graph and you want to partition the vertices into two parts to cut as many edges as possible but you can really think of that as a CSP as well where the vertices of the graph correspond to variables and the edge of the graph corresponds to constraints so it's like a funny way to write a CSV so they're the domain you should say plus or minus one it's a little bit of a funny typo the domain is plus or minus one if you imagine you want to assign let's say plus one and minus one to each vertex and the predicate set only has one predicate and that predicate is the not equals predicate which maps a pair of plus or minus 1 values - you know satisfied or unsatisfied ok it's really like capturing max code right you want to assign two values to all each of the vertices and you want to every edge is saying like please cut me it's like saying please assign my vertices not equals satisfying values ok ah what about 3sat so 3sat is one of my favorite CSPs classic and you can called still mein true/false if you want but let me switch now to maybe the more standard computer science a way of describing where the domain is 0 1 so you're trying to sign 0 1 values to all the variables and remember you know now - I can't really capture 3sat you need like a lot of constraint types you need all because you're not involved along literals you have to build it into the constraints you're allowing them you can or together three variables or you can hoard together three variables but like put a negation on the third one and there's like eight ways to do this ending with you can order together three variables while negating all the variables and then there's all the because in three sight usually allowed clauses of size two and one so there's all the four or type functions already - and there's all the to type or functions of parity one okay so 3sat when you write is the CSP in this way like the predicate set capital size 14 predicate sin it well if you care about III sat not again just refers to all the clauses having to have length exactly three and so then so I would just have the 883 kind of owers little pretty cool constraint satisfaction problem is called nae 3sat and there again it's kind of like three set the domain is 0 and 1 and sy is also gonna have like eight constraints but I'll just say it's like you take the na III predicate plus you allow literals and what does nae it since we're not all equal so the not all equals predicate is satisfied if well the truth value is x y&z are not all equal so it's unsatisfied if they're all equal and 1 otherwise okay so it's kind of like a weakening of 3sat like an 3sat you know an or is satisfied if you get at least one of the literals to be true and not all equal 3sat you want at least one literally to be true and all at least one literal to be false maybe I guess it's a strengthening of three set anyway three coloring is a very popular algorithms problem is also a CSP so here the domain again it says like a you think of it it I'm one here in is like a graph you're trying to color the vertices with three colors but again you know you can think of the vertices as variables and the domain is like red green blue so you're trying to assign one of three colors to each vertex and each edge is just insisting that its endpoints get not equals colors again so here again the predicate said only has one predicate it's the not equals predicate although formally this is not an equals predicate that takes a pair of colors and outputs whether or not they're unequal we tell you one more a CSP there are plenty of others I'll tell you one more it's not quite a standard name but I'll call it by jex in parentheses Q so and this CSP the domain is just a set of q you call it zero three Q minus 1 and what predicates are allowed you actually allow a lot of credit kits they're all binary predicate s-- so they take two values what's called them a and B and any SCI is allowed so long as it has what's called the ejection ejective property and that means um it should be that for every little a that you plug in for the first value from side there should be a unique B that makes satisfied okay so for example max Scott has this property because with the not equals constraint you know over plus or minus one domain like no matter what you assign for like a first vertex there's exactly one way to assign the other vertex to satisfy the constraint okay but it doesn't have this property in 3-coloring right because you know if you sign one vertex read there isn't a unique you know correct way to assign the other one you can do green or blue but max code is an example with this property or if you have like an a constraint that's like an equation X I minus XJ equals seven mod Q this is also a bi Junction constrain XY minus XJ equals seven mod Q because for every any assignment to X I will be a unique value assignment for XJ that will differ from it by seven mod Q and this CSPs aa real name or most usual name is this weird name unique games okay and it's related this unique games conjecture you might have heard about and which I'll talk about at the end of this lecture briefly okay so this is a bunch of CSPs and uh okay what are you trying to do with the CSP right so given an instance I this is my weird capital cursive I I are trying to cope with an assignment what is an assignment it's a map F that assigns to each variable a domain value okay we say that a constraint C sicom s is satisfied if you know when you assign the variables in its scope according to F you know the constraint outputs one okay so F of S is my shorthand just for F apply to each of the variables in the scope as further definition we say the value of F is I'm gonna call it the not the number of satisfied constraints by for prefer to call it the fraction of satisfied constraints pretty much the same different same difference so this will always be I do that because I want the value of an assignment to always be between 0 and 1 and so like a funny way to write it would be you average over all the constraints of just the predicate applied to the scope with this assignment F and that works because I have my my predicates outputting 0 or 1 for whether or not they're satisfied ok so then if you just average all these your one values you indeed get the fraction of satisfied constraints importantly I'll say the optimum of the CSP denoted opt of I is well it's a natural thing it's the maximum possible value you can achieve over all assignments F okay so we you're trying to find the assignment which maximizes a fraction of constraint set aside as wise okay so like in Mexico right you're trying to max find a partition that maximizes the number of constraint there are edges that you cuts and you can't even think of things like 3 sad as like a sort of optimization problem where you're given a 3-cnf instance you know one thing you might think about is just whether or not it's there's an assignment that satisfies all the clauses but more generally you could try to find an assignment that satisfies as many clauses as possible okay and we say that an instance is satisfiable if its optimum is 1 meaning there's an assignment satisfying 100% of the clauses or constraints okay so I'm going to talk about three tasks three algorithmic of tasks associated with CSPs so one task give it a CSP is the satisfiability task yeah that's a simple task you're given the instance and you just want to decide if it's satisfiable if the optimum value is one this is like the classic task associated with like the 3sat problem you give an instance you know is there an assignment that satisfies 100% of the clauses / constraints and you know you might also ask you know if you find out that it's satisfiable you might want to find a satisfying assignment but actually um if you can efficiently decide of a CSP is satisfiable now you can also efficiently find satisfying assignments for it and that's by a self reduction trick you know you just try setting the first variable to some domain value and ask yourself again hey is it satisfiable if so great freeze that assignment if not try the next assignment you go through all the variables like iteratively and find an assignment that preserves the fact that the CSP is satisfiable and if you're you know don't mind about polynomial time then this reduces the problem of finding a satisfying assumption assignment to just deciding whether or not an instance is satisfiable wait the second task associated a CSP I wanna talk about is optimization and you know nothing in this life is perfect so maybe you run a satisfiability algorithm where you try to and you find it's not satisfiable we still might want to try to find the assignment F whose value is as large as possible ideally whose value is equal to the optimum value that's optimization and now this third one is maybe the kind of task you thought about least but it's equally important I call it certification and it's this task you but it instance I output a number beta which is as small as possible but you have to be guaranteeing that you're outputting an upper bound on the optimum so like an oven heater is gonna output like a number like 0.9 together with like an ironclad guarantee that like I analyzed this instance and I'm confident the optimum is at most you know 0.9 90% of the constraints even more ideally it would be for you to output like some kind of like vivid certificate that the optimum is at most beta but as long as you like output a number that's like a hundred percent guaranteed to be correct and we call you a certification algorithm okay and of course your your goal is to output a beta of this as small as possible ideally one that is equal to the optimum value okay so let's talk about each of these three tasks in turn because there's interesting things to say about each of them so let's start out with CSP satisfiability so absolutely the most interesting question asked here is what's going on with you know the satisfiability tasks because sometimes for some CSPs you can do it in polynomial time and for some CSP is is np-complete and like what's the deal so for example the satisfiability problem for mac scott i claim is in p why is that well in the satisfiability version you're not sort of trying to find the optimal cut you're just trying to ask is there a partition that cuts 100% of the edges and that's exactly equal to deciding or equivalent to deciding if the graph is bipartite means there's partitions vertices into two parts such that all the edges go between the two parts and as you mentioned you know there's a simple polynomial time algorithm for that I'll even say what it is in just a moment on the other hand like three sad you know it's the most famous np-complete problem I give you a 3-cnf a squid satisfiable or not that simple complete okay this is not all equals three sub-problem is also np-complete three coloring problem is np-complete so sometimes CSP satisfiability said be complete but sometimes it's in P so in fact it's like digestion problem I mentioned is in P and while deciding if an instance is satisfiable is in P and then the reason is basically some kind of propagation algorithm this actually includes an algorithm for max cut because max cut is a special case of this buy action problem why well you can just take any vertex in a bijection instance and try all the possible assignments for it and for each one like assuming it this variable participates in some constraints the by junction property of the constraints means as soon as you decide on a domain value for one variable all the constraints by the way are binary it forces a unique choice for all of the assignments the variables that it participates within constraints so like forces some other values if you're gonna be fully satisfiable and those have turns forced some other valuables variable assignments if in turn you're gonna be satisfiable and you can just propagate this and check if you ever get like an inconsistency it's like breadth-first search and as long as you do that for like each connected component of the constraint graph as long as you like try every possible assignment for one of the vertices or one of the variables in each component then you can decide efficiently whether or not such an instance is satisfiable in polynomial time okay so yeah so how can we tell the difference so for example two sad NP or NP complete well this is definitely non-trivial but you probably know that it's in P actually you can decide if a to CNF instance the satisfiable or not three Lin mod two this is a CSP where you have like linear equations each linear equation involves three variables and looks like this but everything is happening mod two this is a CSP all the constraints of already three the domain sizes to the CSP is in P because it amounts to solving a system of linear equations over a field to determine if it's completely satisfiable or not about one out of three set where you know in each clause instead of trying to get at least one literal true you're trying to get exactly one literal true NP r-- np-complete turns out it's np-complete what about this CSP called order ten this is the problem where every variable can be assigned a value between 1 and 10 there's only one constraint type it's the less than constraint if a bunch of you think of it like its vertices and you have a bunch of edges directed edges and the directed edge implies that like you know this head of the directed edge should be less than the tail of the directed edge you have to assign a value once return to each vertex can you check if this is satisfiable or not this is also in P you have to think about why would you eventually take this instance and do a topological sort of the graph and see if it has depth at most ten okay so you can see that like there's lots of different kinds of csps and sometimes they're satisfiability problem is in P and sometimes it's np-complete and like what's the deal so let me tell you a little story about the situation so in 1993 Federer and VAR d were thinking about this and they made a conjecture and they made the following conjecture they conjectured that every CSP satisfiability problem is either NP or NP complete you know uh it sounds funny because I didn't really mention like a third possibility before but there are third possibilities you may know this theorem in complexity theory called Ladner's theorem which says that zooming P does not equal NP there exists NP intermediate problems problems which are neither in P nor are they np-complete well the federal Rd conjecture says this does not happen for csps every CSP that conjecture is either NP or np-complete so there's like the easy ones are the hard ones and this became known as the dichotomy conjecture and a much later 12 years later blue table Jonathan's in croakin who had thought about it well many people have thought about for some time they proposed a strengthening of it called the algebraic dichotomy conjecture which basically told you which CSPs should be in P and which one should be np-complete like they gave you a rule and this rule was very sophisticated it was an algebraic rule using ideas from an area called the universal algebra but they basically gave you some like you know abstruse algebraic condition such that if it was satisfied they conjectured that this CSP satisfiability problem is in P and if it's not satisfied they conjectured us and P complete and actually strangely this part of the conjecture was the easy part that was like known for sure if you know your since P doesn't have this problem he was probably np-complete what was missing was the hard part is that these csps that do have this property are NP so it's kind of a strange situation where people were conjecturing certain problems those that satisfy this condition had a polynomial time algorithm even though they didn't know one it's almost always just the opposite and complexity theory and algorithms right if you have like some problem and you can't figure out a polynomial time algorithm for it you're like wow probably doesn't be complete but here they're like we believe these CSP satisfiability problems are P although we don't know how to prove it so there's lots of work on this conjecture and let's see it was proven when the domain size is - back in 1978 actually before was even conjectured by Schafer I think there's two F's in Shafer this is maybe a typo we prove that every boolean CSP is in PR np-complete and then in 2006 it was proven for CSP for the domain is of size 3 and then in 2017 bullet table and independently nuke proved it so fantastic so the dichotomy conjecture and in fact the algebraic dichotomy conjecture is the theorem it's true everyday CSP it's in P Ward's np-complete and this weird algebraic condition tells you given a CSP which one it is interestingly enough the meta problem given a predicate sets I does it satisfy this algebra condition or not that makes you in P that met a problem is np-hard I believe but well anyway good so that's all I want to say about CSP satisfiability but it's an extremely interesting area\", metadata={'source': 'zqNEtGfGGmA'}),\n",
       " Document(page_content=\"let me move on now to talking about CSP optimization which is like the second task so uh in contrast to CSP satisfiability if I ask you to find you know the optimum assignment the one that satisfies the most constraints even if that's not a hundred percent of the constraints this is really hard it's empty hard for all but the most simple CSP I mean it's already NP hard for max cuts as you well know so uh in light of that when things aren't be hard you have to try some mitigation strategies and one of them is what's called approximation so let's see mitigation strategy I'll talk about in this lecture so let me make a definition let's say an efficient algorithm a for a particular CSP is an alpha-beta approximation algorithm if it has the following property for all instances where the optimum is at least beta huh the algorithm a is guaranteed to find an assignment whose value is at least alpha okay that's alpha-beta and had this little mnemonic I think of beta is standing for best like that's the best assignment achieves beta and like for me alpha stands for algorithm it's like what the algorithm is promised to achieve in this case okay let me put a little pause here because there's a question I was asked even if it is np-hard presuming uh-ah even as np-hard presumably meaning this met a problem of deciding whether predicate at least a p-problem or np-complete problem is it in exponential time it seems like the description of a CSP is usually very short for the problem we care about right yeah so it is an exponential time it's probably in P space and then some version of it if like there's some simple property of the CSP there's like a simple and natural property of CSP s called being a core some algebra thing which I forget which if it happens then the meta problem is in P but yeah it is true that like for the CSP s like we actually care about you know they have their descriptions like finite size you know they're like 14 predicate it's the domain is of size 3 so like you don't really worry about you can you practice decide if it's a p-type or an np-complete type of CSP yeah so this is a meta problem is sort of a theoretical interest only ok let's go back to CSP optimization so this is my kind of slightly complicated definition of what is an approximation algorithm and I built efficiency like pong real-time into the definition just so I can stop saying oh and efficient approximation however an efficient approximation I'll just assume when I say an approximation algorithm it means that follow meal plan 1 and also I'm going to allow randomized algorithms in which case the guarantee should be that it finds a value an assignment of value at least alpha and expectation and one thing I want to mention is that one algorithm a can simultaneously be an alpha beta or proximation algorithm for many betas and let me give a clarifying example what is the what do we prove the first part of this lecture we proved that thus Komen's williamson algorithm for maxcut when we prove is at some point eight seven eight beta betta approximation algorithm for every beta right when we prove there is that whatever the best value is beta the Goans williamson algorithm is guaranteed to get you at least point eight seven eight times beta okay that like a point eight seven eight ratio approximation algorithm means this that you're like point eight seven eight beta comma beta for every beta but it can be interesting to look about you know guarantees that are not just of this type like ratio factor x beta comma beta for every beta okay so let me give a couple examples there's simpler example or algorithms for maxcut than governs Williamson one for example just randomly partitioning the vertices or just like doing local search in a greedy way going through vertices one by one and putting them on the better side and then being done and as we saw before at least a random partition these are like a half comma beta approximation algorithms for every beta I should add here like for all beta because they're actually guaranteed to cut at least half of the edges no matter what the optimum is okay and here's another area like example using this terminology there exists 1 comma 1 approximation algorithms for too sad but what is a 1 comma 1 approximation algorithm it's an algorithm that's guaranteed to find you an assignment of value 1 100% whenever the optimum is 1 which it means the satisfiability algorithm or satisfy our gonna find satisfying assignments when they exist should mention that alpha will always be at most beta and in fact one it's not often stated this way in my undergraduate classes but one algorithm that is polynomial time satisfiability algorithm for too sad is to write down the natural LP relaxation and just check whether the optimum value is 1 it turns out that LP out there is 1 if and only if the two side instance is satisfiable ok all I want to say about optimization for now any questions if not I'll talk about this third task the certification task which you might have heard about a little bit less so here's my definition again I'll say a certification algorithm for a kind of CSP he's any algorithm that give it an instance I it outputs a correct statement like a true statement of the form optimum of AI is at most beta star so it outputs a number beta star which it guarantees is an upper bound on the optimum value and ideally beta star should be as small as possible and now we can make a similar definition we say that a CSP certification algorithm is an alpha beta certification algorithm if the following holds whenever the optimal value is less than alpha the certificate that the algorithm is guaranteed output will be less than beta okay it's always good when the certificate value is smaller so we get some algorithms that explain this and the most canonical example of a certificate certification algorithm let's forget this alpha beta stuff for a moment most canonical example of a certification algorithm it's just an algorithm that like takes a CSP considers it to be an integer linear program relaxes it to a linear program or semi-definite program and outputs the like LP optimum value right we're always talking about maximization problems so the LP optimum value is always an upper bound on the true optimum value and you can compute it in polynomial time and so like that's a great notion of a certification algorithm like just outputting the LP relaxation value and even better would be you know with LPS they have a dual and the dual is like sort of a very explicit certificate of a statement like this it's really just like it's some multipliers for the constraints in the LP which when you add up the constraints with multipliers turns into the inequality optimum is at most some number good and just to get a little warmed up for this alpha-beta notion let's think about what is a 1 comma 1 certification algorithm it's nothing more than a satisfiability algorithm why because what is a 1 comma 1 certification however them supposed to be an efficient algorithm that has the following property whenever the optimum value is strictly less than 1 so whenever it's unsatisfiable the algorithm outputs like beta star less than 1 so output a number less than 1 together with the guarantee that the optimum is less than 1 so it's an algorithm that whenever the instance is unsatisfiable like is guaranteed to like say i certify this instance is unsatisfiable ok and so in that way it can be used as a satisfiability algorithm you run the algorithm if it satisfies unsatisfiability then you're like great it's unsatisfiable and if it fails to if it outputs like 1 as its number then you're like well it must be satisfiable ok so now uh in the reader lecture I want to talk about this kind of confusing point like what is the difference and what's the deal with approximation versus certification because turns out they're both very important problems in the study of CSPs and algorithms and they're different problems but like the difference is kind of subtle so let's talk about this now an alpha-beta approximation algorithm searches for an assignment it's like a search problem for an assignment ok and just to remind you the guarantee is that when the instance happens to have value optimum value at least beta the algorithm will give you back a solution of value at least alpha an alpha beta certification algorithm also kind of searches for things but it searches for like a tight upper bound on opt ok so again it has a property that when the optimum is strictly less than alpha it certifies the statement like optimum is less than beta okay now let me try to really clarify the difference here for the max cut problem the goldmans williamson an algorithm I first saw the SDP II get the vectors then do this random hyperplane rounding to convert them to like an actual cuts and output that cut that's an approximation algorithm right it finds good solution and in particular like whenever the optimum cut is beta it guarantees to find a solution that's at least point 870 times beta on the other hand there's like a lesser thing you could do you could just solve the SDP don't you know maybe you didn't even figure out this random hyperplane thing you just figured out oh I can write down this SDP which is a relaxation and therefore the SDP optimum value is an upper bound on the true optimum just solving an SDP and outputting the STP OP number is I claim a certification algorithm actually that's clear that it's a certification algorithm it's just because the SDP op is always at most R or yeah so I can upper bound on the true ops I also claim though that it's a certification algorithm that's like point eight seven eight beta comma beta and let's really understand why this is the case so let me dive into this question I claim that outputting the STP value is at point eight seven eight beta comma beta certification algorithm for every value of beta so why is this true well fix some number beta fix a number beta and suppose you have a graph G whose maximum cuts is strictly less than point eight seven eight beta could it be that when you write down the SDP for this graph and compute its optimum the optimum is at least beta I claim no and if my claim is true then I have shown that this outputting SDP opt is a point eight seven eight beta comma beta a certification algorithm because I've shown that like whenever the opt is less than point eight seven eight beta the TP op will be less than beta and therefore the certified amount that you'll output will be less than beta so why is it true that if the true office is less than point eight seven eight beta the SDP opt will also be less that it could not be atleast beta the reason is we happen to know the Goins Williamson algorithm hyperplane rounding algorithm exists and we know that the hyper prime rounding algorithm has the property left the SDP opt is at least beta it would find a cut achieving value at least point eight seven eight beta but that's impossible because we're assuming the graphs max cut is less than point eight seven eight beta okay so therefore the existence is hyper priming rounding and it's guarantees tell you that when the opt is less than point eight seven eight beta the SDP op will be at most beta so just outputting the SDP opt is at most is a point eight seven eight beta comma beta certification algorithm so probably like you wouldn't know this fact about the algorithm that just outputs the SDP value unless you figure it out this hyperplane rounding stuff but nevertheless that's the case and in general the same phenomenon always holds and you can you know check for yourself that this is true in general whenever you have an alpha beta approximation algorithm it's also an alpha comma beta certification album [Music] good so the approximate trying to conclude here is that the approximation task is strictly well it's at least as hard as a certification task I got a question here which said do we have to talk about the D randomized version of the hyperplane rounding instead of just the randomize rounding good question I've kind of been brushing the difference under the rug actually for making this deduction we don't because actually even if you just know that the hyperplane rounding has the property that when the sdp opt is beta the expected value of the cut is 0.87 at least point eight seven eight beta then you can say oh by the probabilistic method if the expectation is at least point eight seven eight beta there must exist a cut yeah whose values at least point eight seven eight beta which contradicts opt to less than point eight seventy beta so yeah so you don't need it to make this deduction and one reason I'm glossing over it is like basically um whenever you have a randomized approximation algorithm like this for CSP you can convert it to like a deep randomized deterministic one um very easily in particular like you can run it like n times and pick the best approximate of argument that I'll let you think about shows that your with high probability you'll find a solution whose value is at least the expected value minus one over N so I'm always glossing over like I don't mind if the algorithms are randomized and we only care about the expectation okay so uh let me conclude with a one long slide it's a tale of the gums Williamston algorithm one two graphs I should I should mention that uh you know I only have like two minutes left so I'll give this slide and then I have a little bit more content like one or two more slides that I'll add to this video at the end in which I you know tell you what is the PCP theorem and what is the unique games conjecture among other things but let me end just this portion by talking about the government's Williamson algorithm on two graphs because I think it will assess difference between certification and well it'll illustrate like another confusing point potentially competing point so the first graph I want to talk about is looks kind of like this the vertices are the corners of a cube centered at the origin but I don't put in the normal edges think of this cube is sitting in you know high dimensional space I connect two vertices by edges if like the origin is here and I consider connecting two vertices if the angle between the vectors going to those corners is at least three-quarters pie or 135 degrees okay so this vertex you know it's connected to like the vertices that are kind of like sufficiently far away from it so the dotted things are not the edges the yellow things are the edges and you have the same picture for every vertex so I'm not I'm just going to tell you some facts but are not necessarily easy to prove but they're true so the first question is like what is the maximum cut in this graph the maximum cut in this graph you might guess it if you actually take any axis parallel hyperplane it turns out you can prove that this is the maximum cuts and once you know that it's not hard to show that the actual optimum value a fraction of edges cut is basically this quantity 1/2 minus 1/2 coasts 135 degrees also known as 1/2 plus 1 over 2 root 2 which is about 0.85 so in this graph the best at least in high dimensions the best cut cuts about 85% of the edges now what about the SDP algorithm well remember it in the SDP algorithm you're trying to assign a unit vector to each vertex and our graph is already kind of geometrical in fact it turns out that the optimal SDP assignment of vectors of vertices is like the identity mapping that just assigns each vertex to like where it really is in space and you can show that the SDP opts value is equal to the true up the SDP opt is 85% so here like the SDP has done a perfect job of certifying the solution it says it's at most a 5% and indeed the OP is 85% but if you do the goldmans Williams some random hyperplane rounding it will not produce an axis parallel hyperplanes it'll produce some like you know random one and you can show that it an expectation will give a cut that only cuts on a 3/4 of the edges it's the same 3/4 so it only cut 0.75 of the edges and by the way this point seven five over point eight five is really close to this point eight eight value point eight seven eight value so this is a case where the optimum SDP ops value gives a perfect certification but the algorithm only gets the point eight seven eight factor that's like a search optimization problem on the other hand there's the opposite occurrence for a different graph it's an interesting so you can take another kind of geometric graph where you take a smear in high dimensions the unit sphere and you set the vertices to be quote-unquote all points on the surface of the sphere okay you have there like to make it a fine a graph you have to discretize this instance but just imagine like you put in all points and you use the exact same edges for every two points in the sphere you connect them if their angle is at least 135 degrees so each point is connected to like a bunch of points that are like quite far away for this graph some other interesting things happen it's perfectly rotationally symmetric so the best cut in this graph this is hard to prove but it's true the best cut in this graph is any hyper plane through the origin cuts and any hyper plane through the origin cuts you can show cuts about 3/4 of the edges so opt is like 3/4 if you give this to SDP it turns out again that the best SDP solution for associating a unit vector to each vertex is like the identity mapping you just let the vertices be themselves and then you can show the SDP value is bigger it's this 85% number so this is a funny case where the SDP is like a relaxation it's like as far away from the optimum it's like we need 7 a weight 8 away from the optimum it's bigger than this 0.75 but ironically if you do a random hyperplane rounding on the vectors you get back you know you get a hyper plane through the origin and those are optimal so this is a case where the random rounding algorithm actually will find you an essentially optimal cut a value 3/4 even though the SDP opt value is off ok so this is a situation is a good pair of situations to keep in mind when you're trying to understand the difference between sort of certification algorithms and approximation algorithms ok so let me end the lecture there as I mentioned I'll add like five minutes on to this lecture later I'm going to stop the recording now but you can feel free to stick around and ask me any questions here's the last bit I wanted to add it to the okay so I want to just tell you some known complexity results for alpha comma beta approximating various simple csps so let me start with the 3sat problem or maybe more accurately the ìiî sap problem where every clause has to have exactly three literals per or and we can think this is an optimization problem by also imagining the version where you're trying to find a truth assignment that satisfies as many clauses as possible okay so the first result here is that 1 comma 1 approximation is np-complete and just remember that 1 comma 1 approximation means that when the best truth assignment a satisfied 1% of the clauses the algorithm should find a truth assignment satisfying Honda decided that clauses in other words it's the satisfiability problem you have to find satisfying assignments when the exists or else decide that they're non-existent and so this is just the fact that you know the 3sat problem with a three-set problem is np-complete that's been known since time immemorial but there's still an NP hardness result for this even when you're not so ambitious as to try to find perfectly satisfying assignments so imagine that you're given a perfectly satisfiable III Sat instance and your only goal is to try to find an assignment that satisfies 0.99 nine nine nine nine nine nine nine nine a fraction of assignments okay some 1 minus epsilon fraction of the assignments for constant epsilon turns out that this 2 is np-hard and this is exactly equivalent to the theorem known as the and the PCP theorem very very famous theorem in theoretical computer science first fully proven by a LMSs that's a lone sorry Aurora loomed well twonny sudan and ii ii and 1992 and i was presented for the first time in 1992 at the fox conference in pittsburgh so it's a great result and subsequent to this result people try to make this point 99999 etc um smaller to show that even trying to get let's say 99% of the clauses satisfied is np-hard or 95% of the clauses is an hard and the ultimate result along these lines is obtained by hosted in 2001 but I'll tell you about that in just a moment let me first I'm gonna tell you what the limit to this kind of result is a positive result if I'll give you a satisfy a bull instance of III Sat and I ask you to find us an assignment that satisfies at least seven eighths of the clauses this is super easy you can do this in polynomial time and one way to do this is to simply pick a random truth assignment now this will be a randomized algorithm so it's not literally in P but if you imagine picking a random truth assignment then for every clause which is the or of three literals the chance that that Clause is satisfied by random trees assignment is 7/8 and therefore an expectation this randomized algorithm which doesn't even look at the input actually will satisfy 7/8 of the constraints and it's not too hard to do randomized this algorithm and therefore this 7 7/8 comma 1 approximation is in P and this randomized algorithm idea was due to Johnson in 72 so that's the positive result and then as I mentioned the ultimate hardness resolved which matches this was proven by hosted well around the turn of the millennium and he showed that for every positive epsilon constant trying to get 7 eighths plus epsilon fraction of constraints satisfied on instances that are fully satisfiable is np-hard ok so taken together these two results are sort of a perfect understanding give a perfect understanding of the complexity of approximating the ìiî Sat problem unsatisfiable instances you can get if I give you a satisfiable instance you can get seven eight so the clause is satisfied in the worst case in polynomial time but also in the worst case is np-hard to get seven eighths plus epsilon fraction of the clauses just aside on the topic of the 3sat problem where it's the slightly more usual version of the 3sat problem where you also allow clauses of length one and two actually so the hardness results still stand for this because 3sat is more general than III sad so since the 7/8 plus epsilon is hard for III sad it's also hard for three sad what about the algorithm well for example to pick a random assignment algorithm doesn't quite work anymore because actually clauses of length one and two are only satisfied by a random assignment with probabilities 1/2 and 3/4 which is less than 7/8 so that doesn't quite work it's funny you think that like clauses of length one and two should make the problem kind of easier but formally they don't it is known that you would get 7/8 also in the 3sat case when clauses of length one and two are allowed in polynomial time but this turns out to be extremely extremely difficult it's an extremely difficult analysis of a semi-definite programming based algorithm due to Karloff in Zwick from 1997 in fact they didn't even quite prove it they just gave extremely good numerical evidence that was true and only subsequently Zwick was able to give a computer-assisted proof that the algorithm was correct and that it gave a 7/8 comma 1 approximation for the 3sat problem ok up about 3sat let me move on to maxcut which is an even simpler in some sense well even simpler CSP so Mexico as we all know is np-hard or at least that's something you learned early on in complexity theory but that really means is finding the optimum cuts is np-hard when the out phone cut is a hundred percent of the edges the task is actually not at be hard that's testing if a graph is bipartite but if you inspect the proof that you know a classical text book proof that machs cut is np-complete what it shows is that for some specific constant you know between 1/2 and 1 let's say 3/4 if I give you a graph or the maximum cut cuts a 3/4 structure in the edges it's np-complete to find or I'd be hard to find such an optimum cuts so now let me tell you about a few more results that are known on the subject of trying to find a cut that cuts 3/4 of the edges in a graph so the first thing I'll tell you is that this is sometimes hard if I even if I give you a graph for the optimum cut is essentially 4/5 of the edges so for any epsilon greater than 0 if I give you a cut we're 4/5 minus epsilon with the edges can be caught so seventy nine point nine nine nine percent even in that case it's still hard to find a cut that cuts three-quarters of the edges that's a result building on host odds work due to Travis ons Sorkin Sudan and Williamson on the other hand if this is a little bit of an exercise but you can use the Commons Williamson algorithm to get that in graphs where the optimum cut is 1/2 plus 1 over 2 root 2 fraction of the edges or this point 8 5 fraction of the edges that we've seen before then it is possible in polynomial time using Goines Williamson to cut find a cut that's out of cuts 3/4 of the edges okay so if the graph is like 85% cuttable you can get 3/4 but if it's only 80% cut a bowl or 79.9% cuttable you cannot get 3/4 unless P equals NP what about in between let's say between 80% and 85% there's 83% or 5/6 and here's a fact of life is it possible for 3/4 comma 5/6 approximates max cut this is unknown it's a gap in our knowledge I give me a graph where it's possible to cut 5/6 of the edges can you find efficiently a cut that cuts 3/4 of the edges this is open and it's a problem just like factoring or grass up graph isomorphism you know one of these mystery status problems where we don't know this in polynomial time and we don't know that it's n be hard so that's still a great open mystery about max cut so let me tell you about one more problem is this problem I mentioned before called max by ejection it's the CSP where all the constraints involve two variables there to be assigned values between 1 and Q where Q is you think of as maybe a large number and they have this project of property that constrains that whenever you set one variable to a certain value and participating in a constraint was another variable there will be exactly one unique value to set this other variable to that will make this constraint satisfied so as we mentioned before if I give you a 100% satisfiable instances instance this Maxima ejection problem then in polynomial time it's easy to find a 100% satisfying assignment you notice this is a propagation algorithm I mentioned on the other hand it's np-hard in general to find the optimum assignment and a max bisection problem really this actually follows from the fact that max cut is np-hard in Mexico does a special case of this max by ejection problem when Q is 2 but in case it's true that for every epsilon greater than zero it simply hard if I give you a 1 minus epsilon satisfiable instance to find a 1 minus epsilon satisfying assignment now I'm not sure this is a very interesting problem it's connected to this thing called the unique games conjecture so let me tell you a couple of facts along this line so this is not the unique games conjecture but the first thing I wrote here in white if I give you a 1 minus epsilon satisfiable assignment and I ask you to let me just take this back to find an assignment that satisfies half of the constraints so it's almost perfectly satisfiable but I just want you to find an assignment satisfying half of the constraints you can do this in polynomial time if Q is 2 but the problem gets harder the larger Q is and if Q is 3 or higher than 3 it's not known if you can do this task getting 50% of the constraints satisfied even when you promise that 99.99% of them are satisfiable and finally unique games conjecture so asking about this exactly when Q is large so the unique games conjecture was initially opposed by Cote in 2002 was the conjecture that for all epsilon greater than 0 there's a large enough Q depending on epsilon such that even if I give you a 1 minus epsilon satisfiable instance of this max by ejection problem on w of size Q it's NP hard even to find an assignment that satisfies an epsilon fraction of the constraints so this is an extremely strong conjecture and it was really not clear if there was evidence that you know such an easy constraint satisfaction problem could indeed be so and be hard and thoughts of about whether it's true or not back and forth over the years let me mention that it's it's known that this is equivalent to the harder task of finding an assignment that satisfies half of the constraints on instances which are one minus Epsilon satisfied well that's requires a theorem but if the getting half is np-hard then even getting epsilon is also np-hard that's known as I said you know opinions went back and forth over this over the years at first maybe people thought okay sure why not Subash code knows he's talking about maybe it's gonna be hard then people thought some more about algorithms and they got the idea that maybe it's not gonna be hard maybe it's in polynomial time but the latest salvo in these wars came just recently and it swung the tide in favor of maybe hardness so this was not quite proven but sort of a version where the half is in the other place was shown to be np-hard so building on several previous works over the last few years code mints are in Saffron 2018 showed that if I give you a one of these max bisection instances where the optimum solution satisfies at least half of the constraints so it's not like almost satisfiable is not 99.9 percent satisfied well but it's half satisfiable then indeed it is np-hard to find even an epsilon satisfying assignment assuming the domain size Q is large enough so this is sort of considered a half the unique games conjecture this is a consequence of the two-to-one conjecture being proven true by Cote mincer in Safra\", metadata={'source': '9nMwIr6y8NM'}),\n",
       " Document(page_content=\"okay so this is lecture 21 it's one more lecture on the subject of linear programs and some different programs for constraint satisfaction problems and other optimization problems but it's going to also mix together with another topic in computer science theory namely proof complexity and proof systems so a good resource for this topic is a really long monographs 100 pages by Fleming Kothari and Petoskey called semi algebraic crus an efficient algorithm designer just came out about six months ago and this is Professor professor Qatari who will in fact be teaching a course all about the sum of squares proof hierarchy system and the topics were gonna talk about today next semester so do check that out if you're interested okay so we're gonna be talking about once more about this paradigm of trying to use let's say linear programs to the lunar programming to solve some hard maximization or optimization problems so remember this paradigm that it's very popular algorithmically that we set up you start with like an instance of a hard maximization problem and you sort of exactly express it by an integer linear program a linear program where you have an extra constraint that the variables are supposed to be integers I'll say specifically 0 and 1 and this part is exact you haven't lost anything yet of course solving U is 0 1 into your linear programs is np-hard so the point is that you relax this integer linear program to just a linear program where the variables can be any real numbers now and so no longer exactly captures the problem but hopefully it's reasonably close the point is that you can solve this linear program in polynomial time which is great you've got some number lp/op let's assume it's a maximization problem and a good thing is that this number Elte opt is guaranteed to be an upper bound on the true odds I mean ideally it'll be maybe close to the true off but at least it's guaranteed to be an upper bound on the true optimum and if you're not sure of just satisfied by the algorithm spitting out a number and saying like I promise you this is an upper bound on the true optimum nothing you can do is you can solve and set up and solve the dual linear program which we talked about in the first lecture and the dual linear program basically the way it works is it finds some multipliers lambda 1 through lambda M that are non-negative and with the property that when you multiply the LP constraints by these lambdas you get a new inequality which is of the form you know objective function that you're trying to maximize is less than some number and we know that this LP dual tries to minimize this number and LP duality tells us that I'll actually achieve the optimum of the original program so in other words these like lambdas will be like a very vivid proof if you will that the linear programs value is at most lp/op and therefore like a very vivid proof that the upper bound of your optimization an upper bound for your optimization problem is this number LP odds good so I want to start thinking about this less in terms of like a linear program and more in terms of like a proof system so I want to talk basically this lecture about proof systems or efficiently bounding the optimize the optimum of like an integer linear program the hope that maybe we can find other proof systems that we can work with efficiently that come up with hopefully even better upper bounds for our integer linear programs so let's do an example it's the max independent set problem which is basically the same as the min vertex cover problem except you subtract 1 the answer from n so in the max independent set problem you're given a graph and what is the task you're trying to find a subset s of the vertices and an independent set is one where there are no engines with both endpoints and s ok so you're trying to find and so the maximization problem you're trying to find a larger subset of vertices as possible S especially it still has no edges inside it ok so in this very simple example the optimum is oh boy I've already a mistake here the optimum is 1 not 2 because we try to take any two vertices in this graph then you'll have an edge between them okay so the largest independents that you could take and you can take any of the three vertices would be to take well any of the three vertices and you get one it has no edges inside it okay great so first in this paradigm we write down this exact integer linear program for it and there we usually introduce a variable you know X I for each vertex I okay and this is a little example there's only three vertices and in the in your program these are 0 1 variables and meaning they're supposed to take values that are 0 or 1 and the intent is that like X of V is supposed to indicate whether the variable V is in the independent set that you're taking now your instance actually defines a bunch of constraints so in you know the maximum independent set problem the constraints you have a constraint for each edge and the constraint is that like you better not choose both endpoints of this edge okay so for this edge here between a and B we got this constraint which effectively says XA plus xB is at most one it effectively says that you can take at most one of vertex a or vertex B and this edge here gives you this constraints and the last edge here gives you this constraint okay so on all in all the that just give us these constraints and let me just say that the the task is to maximize the sum of the x i's okay so that exactly captures this max independent set problem any zero one solution that satisfies all the constraints will be an independent set I mean the you know this paradigm of relaxing to a linear program what we'll eventually do is relax this assumption that the X variables are exactly 0 & 1 now if you set this up as on your program you can look at the as I said the dual linear program and again the dual will try to find a linear combination and non-negative linear combination of the constraints which equals to you know objective is that must something in this case the thing to do is to take half times each of the three constraints and if you do that you know separating variables appearing in two constraints you'll exactly got XA plus xB plus XC is that most three-halves okay and this XA plus XB plus XC is the objective function it's what you're trying to maximize okay and so this you know these multipliers this dual solution if you will are like a vivid proof that the independence the max independent size set sizes at most three halves of course it's actually at most one its but well that's why self relaxation it doesn't always give you the optimum the true integer optimum i should mention as always if you have any questions while I'm going can pipe up or type them into the chat I'll keep an eye on that and answer them as they come in okay so the linear programming is in dual linear programming a way of formalizing it but now I want to just change our language a little bit and talk about this more like a proof systems it's not gonna change things per se I'm just gonna rename them so um just for clarity I'm just gonna start calling the variables in determinants instead of variable well that's the same thing and I'm gonna call these sort of given constraint inequalities I'm gonna start calling them axioms because they're like you know facts that are given to us like XA plus xB you should be at most one and in this dual solution where I sort of combined these axioms to derive like a conclusion I'm gonna call this an inference rule or a derivation rule okay so let me just say this a bit more clearly I mean this you know sort of proof system based on linear programming we're gonna have some in determinants one for each variable vertex that is supposed to be representing zero one values and we're gonna have something called proof lines so you know when you write a mathematical proof you know stylistically you write down a bunch of lines and each one follows from previous lines according to some rules and hopefully the last line may be the first lines are some axioms and the last lines are your conclusion that you've drawn from these axioms and then this sort of proof system based on linear programming the lines every line will be a linear inequality and our per system is gonna have an inference rule and that inference rule is that you can derive from previously achieved inequalities or lines non-negative linear combinations of these lines okay so you can add a couple of previous lines and get a new inequality you can multiply a previous inequality that you've got by a positive number you can add up several previous lines and this allows you to derive more and more lines and equalities which are consequences of the axioms you start with and at the end you're trying to you know drive a line that helps you bound in this case XA plus xB plus XC so we can throw in these axioms too I didn't mention them before but all these variables can be bounded zero and one so this is like six axioms but you would always you could always put in if you were you know working with a proof system where the variables are supposed to represents your own one but given an instance of like a problem like this max independent set problem the instance divides some like you know additional axioms as I mentioned these three axioms corresponding to the three edges okay the goal is to derive from these axioms according to these inference rules and inequalities it looks like this XA plus xB plus X C which represents the objective function here is that most some beta for a beta as small as you can make it okay so one thing you could do is you know take the three axioms XA is at most 1 X P is at most 1 X C is at most one and add them all up and deduce that XA plus XB plus XC is that most three but as we saw there was a better thing to do using the instance axioms that gave you one and a half okay so I'll call this proof system the linear programming proof system it's not really different from linear programming duality but you know it's the phrasing it or in terms of like a proof system okay so in this proof system just to repeat it you know we would have these lines that are our axioms and our proof took this you know linear combination of previously derived lines well axioms cowntess derived oops we have a little typo here and this should parentheses should go there and we derived from them this line which when you add up gives you this objective bound now in this lecture I'm going to talk about taking this proof system and making like I'm sort of more powerful proof system that lets you that gives you like more kinds of maybe the derivation rules and more complicated lines which will let us get better proofs on things I want to take this opportunity to mention um a different proof system that you could introduce at this moment which is studied in proof complexity it's called the cutting planes proof system and it takes this proof system and adds and inference rule and this new inference rule is you know whenever you have a linear inequality if you're left inside coefficients are all integers and your right-hand side number is not an integer then you're eligible to round it down you're allowed to round it down to the nearest integer okay so you would take this and I would derive from this the same thing but with less than or equal to one okay and then actually that is the optimum solution and this is like a sound rule as I say it's it's legal because you know XA xB XC there's what's the sign for integers so like if you have an integer linear combination of them you'll get an integer so you could always write round down the right-hand side so the very interesting proof system to study but we're not gonna study it in this lecture we're gonna study different kinds of proof systems today but I thought it was a good opportunity to mention this coming planes one\", metadata={'source': 'q6bm-PeTv_M'}),\n",
       " Document(page_content=\"okay so uh I'm going to now introduce some kind of LP type brew system I won't be quite the same as this LP brew system I've talked about before we introduced it for an example instance namely an example instance of the max 2sat problem okay so consider this particular instance of 2sat for max 2sat i have four variables here called a b c and d and i have 5 clauses 3 of them have length two and two of them have length 1 and you know it's a maximization version of this problem so I want to make truth values for a B C and D such that as many of these clauses of our satisfied as possible and I hope you can easily check that the optimum here is 4 it's impossible to satisfy all four clauses perhaps you can think about why one reason is you can view these clauses in the middle as like implications if you remember your basic logic and this one is the same as like a implies B this is the same as B implies C and C implies D so it's impossible to have all five clauses satisfied because if a is true and a implies B is true and B is apply C and C and by D are all true then D would also have to be true but this Clause is asking for D to be false yeah we had a question in the chat is max 2sat and B hard and we had an answer in the chat yes it is and B hard so deciding if to side instance is 100% satisfiable is solvable in polynomial time but you know if you solve it and find out no it's not 100% satisfiable and you'd like to know well what is the largest fraction of clauses I can satisfy her the largest number of clauses I can simultaneously satisfy that harder task is np-hard ok great so uh we might try to use like a proof system to establish an upper bound on the number of simultaneously satisfiable clauses okay if we were not like smart enough as humans to just see that the answer is 4 so we're gonna again have 4 indeterminate here standing for the truth assignments 0 1 truth assignments to the 4 variables and we're gonna have axioms we'll always have these axioms that they're between 0 and 1 although we won't actually need those I don't think and notice there's no additional axioms here unlike in the independent set problem there no additional axioms about how the X's are allowed to be you know that we're imposing on how the X's are allowed to be satisfied but our objective is more complicated here so let me just explain the objective function I wrote down here I wrote down some linear expression although it's not well sorry it's not linear I wrote down some expression it's not linear though it looks like a polynomial of degree two because it has these terms and what I've done here is sometimes called arithmetic a Sat instance so you see this part of the polynomial corresponds to this constraint it's just X a it's one if a is true and 0 a is false so that represents the indicator that this constraint is satisfied the more complicated thing is here so let's look at this expression one minus XA + x8 hunt B so I just wrote this down is that like a little polynomial which captures with a value of zero or one whether this constraint is satisfied or not ok so imagine let's say that a and B are both false that satisfies the constraint and indeed if you plug in XA and xB to be 0 here you get 1 minus 0 plus 0 which is 1 or a and B are both true so they're both 1 the constraint is again satisfied and you here get like 1 minus 1 plus 1 times 1 is 1 so you get 1 so again it's correctly computing that this constraint is satisfied and the point is that like an in the unsatisfying case if a is true or 1 and B is false or 0 the constraint is not satisfied and indeed here you get 1 minus 1 plus 1 times 0 which is 0 ok so this little expression the whole point of it its degree 2 expression is that it captures whether or not this Clause is satisfied ok and so we've written down this expression for all the clauses here I wrote down 1 minus XD to capture that D is supposed to be false okay so this you know if I didn't have this this relaxation or if I didn't imagine that the rather if I really insisted that the x i's were always 0 or 1 then this would sort of exactly capture this max 2sat instance okay so they'll be different though because they you have inequalities here involving a degree two polynomial well let's just run with it for now so in this new proof system this new LP type proof system inventing now my blinds I'm gonna allow degree two inequalities okay my axioms happen to be degree one what's I'm gonna lot of the Greek two inequalities and uh well let me talk about this a bit more so actually if you just you know expand this out and rearrange it a little bit you get that it's exactly equal to four minus this expression I put in parentheses and I'd like to you to observe something here so take a look at this expression XB minus XA x x.b i cleaned up four zero one values this expression is always non-negative no matter how you set these two variables to zero one values there's four possibilities one way to see it is if you just factorize this a little bit it becomes one minus XA times XP you see that like X B's value is either zero or one particularly that's non-negative and one minus x a's value is either zero or one that's also non-negative so the product is non-negative okay so it's like a little fact of life that if these X's are supposed to stand for 0 1 variables then this expression here is non negative we have the same expression here the same expression here so you see if our proof system could somehow just realize this like simple fact that you know whenever you have something that looks like xB minus XA times xB its non-negative then it would you know sort of corn code realize that this whole thing in the parentheses was non-negative and therefore the objective function is 4 minus something that's non-negative which you know implies that the objective function is at mass 4 which is great because that's what we're trying to do we're trying to bound this objective function upper bounded and for is actually the correct optimal bound so what are we gonna do let's just add this being non-negative as an axiom to our proof system a bold move what are we gonna do here are sort of developing like sort of an ad hoc way like a new more powerful proof system and try to motivate it as I'm going along and then at some point I'll pause and define it properly but here I'm gonna sort of imagine sort of heuristic aliy start building up a new more powerful proof system okay so this is sort of the proof system we started with you have a bunch of in determinants maybe these are the axioms so far the lines were gonna allow our degree to inequalities and let's make some new allow some new additional axioms that you're always given regardless of the instance that look like this you know 1 minus XA times xB is at least zero well let's make this a bit more general let's say that you know you have a Mac Siam's and we're gonna have an axiom that looks like this for all pairs inj and you know we could it's also logical to add the axioms that look like this just on again on the theory that if X I and XJ are supposed to be 0 and 1 then you know this should be non-negative and also let's add this one so if X hire an exterior supposed to be 0 & 1 then 1 minus X I on 1 minus XJ are also supposed to be 0 and 1 so their product is not negative so these are all like you know sound axioms to add sound here meaning that they're true assuming you're trying to capture the fact that X is supposed to stand for a 0 1 value and you know if you have n variables this is like on the order of N squared extra axiom so that's you know n squares a polynomial so maybe it feels fine to add these in and now we're gonna keep the same deduction rule that you can take non-negative linear combinations of axioms but now you potentially have more power because you can you know I don't take all these axioms and combine them in some cool ways hopefully but taking on linear combinations and actually I'm not gonna prove this but it's a non-trivial thing on that this proof system I sort of just explained to you is complete for the problem of 2sat satisfiability okay and what I mean by that is precisely the following if I give you any 2-sat instance which is a high-low say as M clauses and is unsatisfiable you can always derive from these axioms by taking on negative linear combinations you can always derive opt is at most M minus 1 and where opt here represents like the sort of arithmetician of the two side instance okay so this some sort of new proof system with degree two inequalities is really good if your whole goal in life is to you know detect satisfiability of two sad instances okay any questions okay so we consider similarily such proof systems for maxcut so our favorite basic CSP so here's our favorite simple instance it's a three vertex graph triangle and as we know the maximum cut here is of size two it's no partition that cuts all three edges so ah here again we can imagine you know this proof system or you know x1 you know contrast in the previous lectures let's go back to X the X is standing for 0 or 1 it's a bit more annoying here but I wanted to stay consistent but the X's stand for 0 on 1 so again you know we'll have a variable for each vertex Y or a one that represents whether it's sort of in the color outside that cut and now a rhythmic way you cut an edges have been annoying so like this and share between a and B I claim that this expression captures in a 0 1 fashion whether the cut defined by X and X be cuts this edge okay so you see if they're both 1 you get 1 plus 1 minus 2 times 1 which is 0 not good if they're both 0 you get 0 plus 0 minus 2 times 0 times 0 which is 0 not cut but if 1 is 1 and 1 is 0 you get 1 plus 0 minus 2 times 1 times 0 so you just subtract 0 so you get 1 ok so this little degree 2 expression encapsulate s' whether or not this edges cuts ok I've written the analogous expression for the other two edges here so we have an objective we're trying to upper bound which is like a degree 2 polynomial in the variables I just sort of rewrote it a bit here and our dream is to somehow you know establish the dream that it's at most 2 and in this LP type roof system I've been talking about I mean we have all those axioms that's a you know like X a xB is at least 0 and XA times 1 minus xB is at least 0 and so forth you might hope by taking a suitable linear non-negative linear combination of those you could derive you know the inequality that looks like this expression is at most 2 well it's not very easy to see this but I'm here to tell you it is a fact that you cannot do that there's no it doesn't work the best upper bound you can get is not to and in fact there's no way to deduce from those axioms that this maxcut isn't most - so we could you know just grin and bear and say like well I guess this proof system is not so good about knowing the maxcut and a triangle graph but we could also try to you know go on and say like well maybe we can put in some more axioms that are true of 0 1 variables and that'll make proof system more powerful better at proving upper bounds now what you could do it's sort of almost like cheating you could say like well it's a true fact that we all know that this expression here is that most - because you know we can look at this trying a little graph and be like I assure you the max cut is it must - and this is like an inequality involving the variables it's like a in fact that is almost - listen you really have to do with max cut it's just a true fact that this expression is that most - for all choices of 0 and 1 for X a xB and X C so we could be like hey let's just make that an inequality an axiom feels a little bit like cheating because you know it's it's done to exactly solve this although um let me actually say we're gonna do something kind of different but let me actually say it's not so much cheating it's not a totally illogical thing to add and it's not like it's only purpose in life is to you know get this triangle exactly solved so here's an exercise for you if you throw in this axiom for all triples of variables in determinants then for any cycle graph on n vertices where n is odd the true max cut is at most n minus 1 because odd cycles are not bipartite and actually you can derive this fact in our little degree to inequality proof system using only the original axioms Plus this new axiom about triples and variables ok so that's a little bit of sense in which it's not totally um just like begging the question but we're gonna go in a slightly more principled direction in this okay so here's the bold new axiom idea we're gonna keep the same inference rules and the in determinants that are supposed to stand for 0 and 1 but we're gonna do is add more axioms in some sense so we're gonna do is pick a parameter K and then I'm gonna allow you as an axiom any true inequality that involves at most K variables I think of K it's like a constant like 2 or 3 or 5 and we're just true me and it true means it's like a true inequality about zero one valued variables okay so you might be concerned like wait a minute there's like infinitely many true inequalities about you know three zero one valued variable so feels a little weird to say like well I'm gonna add all of them in so I'm gonna claim on this slide that you actually only need finitely many of them so why do I mean why do I say that well let's say looks like K is 3 and suppose you want to use uh you know this axiom let's say P is a polynomial involving the variables X 1 X 2 X 3 and suppose it's a fact that this polynomial is non-negative whenever you assign you know do one of the eight truth assignments to X 1 X 2 X 3 with 0 & 1 and so you know up here I'm saying like oh you can that's true you can take that as an axiom um well um think for a moment P of P known as a formal polynomial but think of it as really a function that map's 3 0 1 bits to a non-negative number it must do this because I'm imagining that it's it's truly non-negative for all these eight choices and it's a fact that any such function is uniquely expressible as a multi linear polynomial in fact the proof of this is the exact same fact is the sort of interpolation proof we did for Fourier expansions in the Fourier analysis analysis of boolean functions lecture we you know where the key theorems is exactly this fact except that we use the notation plus or minus 1 instead of 0 1 for our variables and you know this greater equal to 0 is not necessary every function mapping and bit plus or minus 1 strings to real numbers was a uniquely expressible as a multi linear polynomial it's same is true for 0 1 bits you can just do linear shifts to change plus and minus 1 into 0 1 but it's also I mean clear if you just look at it like this what I can do is write down these eight sort of degree 3 polynomial expressions and I can multiply like this 1 by the polynomials value at 1 1 1 you see this polynomial expression is 1 if the bits are all one and all these other ones are going to be 0 so I multiplied it by the value of P out 1 1 1 and I can do a similar thing for all eight truth assignments and since we're assuming this polynomial is non-negative on all eight truth assignments these coefficients here are actually non-negative ok and now remember in this proof system we were imagining I was giving you as axioms that each of these yellow things is non-negative you know this expression greater than or equal to 0 is an axiom in this next expression greater than equal to 0 is an axiom and you know derivation rules allow you to add up axioms with non-negative coefficients okay and so you can derive this polynomial is greater than equal to 0 presuming it's true from the axioms so somehow what I'm saying is like these 8 axioms are sort of like a basis for all true inequality it's about 0 1 variables about these 3 0 1 variables does that make sense okay so now I'm going to get more precise about things and make a proper definition so I'm gonna define for you a proof system well I'm not going to find what proof system means but you'll get the gist on this particular proof system for historical reasons is called the degree k-01 Charlie Adams proof system sort of due to two people called Charlie on Adams um and there's one for each value K actually you think of K as a parameter you get a different proof systems and okay how does this proof system work well you always have like n indeterminate says an ingredient in your proof system and they're supposed to stand for 0 1 variables and your proof lines are gonna be degree K polynomial inequalities given your parameter K and maybe there should be multilinear may mean meaning you don't have squares or cubes or anything just monomials without squares and cubes and so forth and your derivation rules can be the derivation rule we've always used that you can take nanami negative linear combinations of previous inequalities and deduce those as new inequalities and the axioms are gonna be all true inequalities that involve at most k out of n in determinants okay in the sense that we talked about in the previous slide if you will and as we saw before you know that seem fiddly many inequalities you can take as axioms but they're sort of like this basis of them that finite basis which sufficed and for all you know you have sort of one of these basis things for all first of all you do n choose K of the determinants and then like for each one you can either have the variable or one minus the variable in this product so it's like another to the K possibilities okay so like in the sort of the real finite version like all of these and choose K times two to the K inequalities are your indeterminate sorry are your axioms that that are non-negative but its equivalent to like allowing any true axiom about any true inequality statement about k0 in variables and that's the proof system you can take it and you know start to go to town deriving facts that we are useful for combinatorial optimization problems and I should mention that like you can also like in 2sat we didn't have any additional axioms based on the instance but like sometimes you do so like in the independent set problem given a graph we added like additional axioms that specify that you know each edge should not have both of its endpoints selected you know those are those like XA plus xB is at most one axioms okay so you're allowed to do this you're sort of allowed you know through system to add additional accent if you want we are trying to derive you know true consequences of all your axioms you notice that you know you should try think of K as like an absolute constant like two or three or five or ten and then you know and choose K is something like n to the K times two to the K so this whole expression is it's good to know that this is polynomial in N okay it's constant okay so at least you know the number of axioms you have is like polynomial in you know the relevant parameter N and up for revision we're gonna call this the SI parentheses k Sharlee items k proof system okay so uh you know we have lots of lectures on this particular proof system let me just tell you some properties of this proof system and they're kind of like proof theory properties of it so just for a little bit of fun I'm gonna use some proof theory terminology uh so first of all it's a sound proof system you would never want to have an unsound proof system but what a sound mean it means that every thing you derive is true so like when you're imagining a world where all the variables are supposed to be zero or one you know our axioms are set up to be true and our derivation rule which you know takes non-negative then your combinations of true inequalities like preserves truth so that's good so anything we derive is true and then you might ask well what about the opposite which is called completeness is it true that like everything that's true you can derive well so I mentioned before in the context of like max cut at least when K was two that's not the case actually this is an incomplete system which means that there are in general are true inequalities about 0 1 variables that you just cannot get no matter how much you try to combine the axioms at least this is truth K is that is smaller than him in fact you can think about this and well maybe implicitly prove it later if K equals n so you're allowed to have lines that involve all of the well degree n polynomials and your axioms can involve all of the variables well actually that case it's obvious if K equals N and your axioms are allow you you know make any true statement about all N variables and you can just anything that is true you can just derive as an axiom but in general even if K is n minus 1 will be some things that you cannot derive you another trip another property of this proof system is that it's called a static I won't get into this very much but static basically means that the derivations are without loss of generality one line long so this is a little bit weird it's more natural to have like what's called the dynamic proof system where you kind of you make your derivations gradually you derive some lines you draw it's more lines you draw more lines and finally you drive your conclusion and in this charlie Adams proof system you don't really need that because like the only derivation rule is taking non-negative linear combinations of previous lines and so everything is a non-negative combination of the axiom it's ultimately so any convolution you can derive you can actually derive in like one shot which is a little bit weird but um we're gonna stick with it I mean we're gonna still think about it as a proof system even though it has the slightly odd property by way of contrast that cutting planes proof system I mentioned before that was like similar to this except you could also do this like round down the right-hand side to an integer if all the left-hand side coefficients were integer that's definitely a dynamic proof system in the sense that like in order to get real mojo out of this you have to do like an interlaced combination of like you know take some non-negative linear combinations to get a new inequality do a round down take some more linear combinations do a round down take some more linear combinations do a round down like you can't do the whole proof in one shot okay and one more property of this Charlie Adams proof system which is sort of the main important property about it the reason I'm talking about it is it has this probably called automatize abode it's very hard word to say you can practice it in your spare time otama sizable automatize a wall yeah same thing advertising wall and what does that mean it's a property that not a lot of proof systems have what this proof system does have it which makes it great and that means basically if a proof exists you can efficiently find it okay so the property of Charlie Adams proof system is that like for any line which is derive abaut you can efficiently an algorithm can efficiently find a derivation from the axioms okay so like for any particular for optimization problems like whenever a statement like oh the objective polynomial is at most seven can be derived at all then there's a an efficient algorithm that finds it and here efficient means as a function of the number of axioms so polynomial on this number of axioms or n to the order K let's play why in a second why it has this advertisement property so it's sufficient you know if K is any constant you know it's efficient in theory land it's a polynomial time algorithm it's still interesting if K is like you know login you get like a quasi polynomial time algorithm oh I should mention that this proof system gets sort of stronger and stronger the bigger K is it makes sense because you're getting axioms about more and more collections of variables simultaneously and that's why it's called the proof system hierarchy and the Charolais adams hierarchy because shirali items one is weaker than charlie items two and that's weaker than Charlie I'm three and so forth and so on so you get this hierarchy of more and more powerful proof systems and you can find derivation zin them efficiently provide they exist in time that's like n to the order K so it's kind of nice if this like tunable per hour parameter K so it takes more time to work with this things but you get more and more power so let's understand briefly why is this Charlie Adams K proof system automatic volt uh.well powerpoints having a little hiccup here so Amy let's say a Sharlee items k equals three proof looks like the following okay so like you know because it's got the static property like the proof really you might imagine it like going you know in several steps you can do it in a one-shot way where you take a linear combination another give linear combination of all the axioms like you know like all the axioms you know these are K equals three so you like look at all and choose three times two to the three mini axioms maybe you're like minimum or maximum independent sets they have some additional instance axioms anyway like a proof will look like a non-negative linear combination of all these things that adds up to an inequality that looks like beta minus the objective polynomial is at least zero you know aka the objective polynomial is at most beta and so now if you imagine for yourself like searching for a good proof that looks like this and it also searching for these lambdas in such a way that when you do this add up you get beta which as small as possible so uh what's nice is you know this is like the LP dual this is like a linear program because you're you know beta is a variable and these lambdas are a variable and you're looking for non-negative lambdas it's the linear constraint and to say that this big giant sum holds it's sort of to say that it holds on a coefficient by coefficient basis so like for every coefficient or so I should say on a monomial by monomial basis so for every monomial like X 1 X 2 X 3 you see you get accommodate contribution of like lambda 1 here you get a contribution of minus lambda 2 here and like you got some contributions from some of the lambdas and like those some perfect some of the lambdas should equal the coefficient on X 1 X 2 X 3 in the objective polynomial if you get like a bunch of equality's involving the lambdas and so yeah that's why searching for and the number of lambdas is like one per axiom so it's like n to the order K and so searching for the best proof can be done with an LP in n to the order K time so that's cool we have these more and more powerful proof systems and an autumn ization algorithm for finding proofs in them so what is this good for I mean is this actually help us solve like new combinatorial optimization instances that we couldn't solve before I'm gonna taking K larger and larger so let me give you a few examples to show that the answer can be yes so this first example is really nice it's gonna connect to something we're gonna talk about in next lecture so let's say you have some instance I of a constraint satisfaction problem any constraint satisfaction problem there's a notion given any CSP of what's called the primal graph for the instance and the primal graph is just an ordinary graph where you have the vertex set is identified with the n variables in the graph and you put an edge between two variables or vertices if these two variables R vertices are in some constraint together okay so like it's like a max 3sat instance you have like a variable for each you know four vertex for each variable and you have like a triangle for each constraint plunked in or if it's like a three coloring instance like the primal graph is the same as the actual graph so it's a theorem providing write in Jordan I think we're there first to prove it that well let me read here if the primal graph of your instance G has tree width at most T I'll say what that is in a moment then the shirali Adams proof system with K being T plus one can it's sort of complete forcing the satisfiability question it can derive that the instance is unsatisfiable provided this is true okay so there's a proof system it's sound so whatever the proof system can derive like the earth notice a tion of the expression is unsatisfiable that's true and conversely if it is unsatisfiable has this sort of amount of completeness that if it is unsatisfiable and the graph has this tree with property T then this proof system with K being T plus one can certify this unsatisfiability okay so you get a satisfiability checking algorithm that runs the time n to the order T for any instance whose primal graph has tree with at most T now what is tree with well I'm exactly going to talk about that in the next lecture it's some parameter of graphs and measures sort of how similar they are to trees the lower the T the more similar they are to trees in fact the graph has tree with one if and only if it's a forest a collection of trees and that's true with two basically if and only if it's a series parallel graph which I'll say what it is next time there's some sort of measure of simplicity of the graph okay so it's sense of you look kind of a simple enough constraint second structure on your instance then this rally items proof so a system will be good at deciding if it's satisfiable okay so that's one example of how you can use Charlie Adams proof system there's another one that I believe will appear on your last homework consider the max case that problem definitely np-hard max case at okay so just to arithmetic t'v function which involves clauses on K variables you need to have lines that are degree K inequalities so the first thing you could imagine looking at is the shirali Adams proof system with parameter K for max case time and it's actually not too bad you can prove it as the following guarantee no matter what K is whenever the true optimum value the maximum number or fraction of satisfied little constraints is beta sorry is 3/4 beta the proof system will certify that it's a most beta okay so the the best upper bound of Charlie Adams can certify will be no more than four thirds times the true upper bound so that's pretty pretty alright well let me give you one more uh known fact about it - you know illustrate why you might care so recall one thing we talked about in the helpee lecture was that for the max cut problem you know the basic LP relaxation that your write down really stinks it's not very good at capturing max cut and for example here's a true fact about the extent to which it stinks if you take a random graph with n vertices and like end to the 1.01 edges so it's not very dense average degree into the point a1 then first of all the maximum color on this graph will be very close to half the maximum cuts will cut barely more than half of the edges like half plus a little of 1 but if you write down like the natural LP relaxation and solve it you will only certify that you know the maximum cut is at most 1 minus little of 1 like it'll notice that it's not literally bipartite but it'll be like could be very close to bipartite even though it's like its optimum was close to minimal as possible so that's the suckiness of LPS for max cuts and I should say that the natural linear programming relaxation you were right down from max cut is exactly the degree to Sharlee atoms linear program ok so I like here I'm talking about the failure of the sort of K equals 2 shirali atoms to do well it L max cut but here's a theorem I was proved with SRAM in 9 in 2019 just last year if you choose K to be a very big constant then the K degree K Sharlee atoms proof system for these random graphs does do a pretty reasonable job of noticing that their maximum cut is barely more than 1/2 so for a fixed constant above 1/2 like 0.500 1 there exists some constant K such that this K sure Ally items proof system basically you know well can show that the optimum is at most point 500 won now this is not all peaches and cream because I mean if you dig into the proof this constant KS look very bad maybe it's ten to the hundreds so maybe you have an end to the 10 to the 100 time algorithm for solving it which is your every polynomial time not so great and actually another reason this is not so good you know they have this extremely slow appalling real time algorithm is that there's a B by now maybe to you simple polynomial time algorithm semi definite programming that does the job and even does it a little bit better so it's a theorem is awake actually whenever you have a graph whose optimum is whose Mexican optimum is like barely more than a half the SDPD optimum will also be just a little bit above a half okay even with like a this little low of 1 with respect to n going to infinity so yeah so this is one reason why ok there's some things in life that are better than the Shelley Adams proof system but\", metadata={'source': 'DYILu_eL5RI'}),\n",
       " Document(page_content=\"but really that should just motivate us to make an SDP version of the trolley Adams proof system which is what I'm now going to talk about could we come up with can we like you know throw so many different programming into the mix since we know that so many different programming can be done in polynomial time so let's look back at the Charlie Adams Cape roof system where we have an axiom of the form you know any polynomial statement this is slightly bad notation it should be about any k out of the N variables if it's true that this polynomial is non-negative on 0 1 values then you can take this line as an axiom and here's a claim I want to make this will be sort of motivation for our like SDP extension I claim that this inequality P of x1 through XK greater than equal to 0 is true for all 0 1 values of the variables if and only if P okay is ignore this multi linearization for a second if you know if P is the square of a polynomial and well okay so what is this multi linearization so if you square a multi linear polynomial it might not be multilinear anymore because you have some squares in it but this multi linearization refers to like just whatever you see X I squared change it to an X I which is a reasonable thing to do because we're all agreeing these excise should be 0 1 and that's like a sound rule like the square of a 0 1 value is itself ok so this uh what I'm claiming is that this condition about P being non-negative is equivalent to being the multi linearization of AX squared polynomial let me give you a quick example about two variables so this is a polynomial P of X 1 X 2 I just wrote it here the first of all I claim it's non-negative whenever the X variables are 0 or 1 so why is that true well if they're both zeros is 9 minus a bunch of zeros that's 9 that's not negative let's say you if one of them is 1 and one of them is 0 you get 9 minus 5 minus 0 plus 0 so you get 4 that's also non-negative maybe the trickiest case if x1 and x2 are both 1 you get 9 minus 5 minus 5 thats negative 1 so far oh but plus 1 so it's exactly 0 okay so this is non-negative on a mule my claim it's the multi linearization of a squared polynomial and here it is I work this out on my computer here's a polynomial you square it you get this long thing but now also multi later eise's so this X I squared change it to an X I you see when you do that you get like minus 6 X 1 plus 1 X 1 which gives you minus 5 X 1 okay all right look at this X 1 X 2 thing we have minus 4 X 1 X 2 but this X 1 squared is really an X 1 so this is plus 2 more plus another 2 more plus another 1 more you get this plus 1 X 1 X 2 ok so this is like an illustration of this claim I just made ok so let's prove this fact well one direction is obvious if your a squared polynomial is definitely non-negative on all inputs because it's like the square of something and multi linearization doesn't change things for a zero on values so if you're the square Multi linearization of a squared polynomial you're definitely non-negative so why is it the case that we have a non-negative polynomial it can be represented in this sort of squared way so here's the proof kind of harkens back a little bit to this Fourier analysis of boolean function stuff so even though P up here is like a formal polynomial think of it now as a function mapping k01 values to a non-negative number now as I mentioned before every function mapping K bits to numbers has a unique multi linear polynomial representation so keep that in the back of your mind and this is sort of the Fourier analysis fact but for 0 1 values now this is the funny statement I claim that since P's as a function in its range is non-negative numbers the functions square root of P exists this is the function R which Maps a you know bit string of length K to the square root of peas value makes sense because P takes non-negative values so now R is a function mapping K bits to real numbers and so it has a multi linear polynomial representation let's call that the Q that we're seeking and so um now Q squared equals R as a function they take the same values on 0 1 inputs equals Q squared equals P as a function because Q squared as a function is square root of P squared which is P and so the multi linearization of it has to agree with P as a polynomial these these two the multi-layer ization really is a function and so by this uniqueness property they have to also agree as polynomials okay so basically the recipe for doing this given P is like tickets polynomial representation which looked like you know P of 1 1 1 times X 1 X 2 X 3 plus you know etc square root each of the coefficients and that's it sorry that make sense yeah that's it okay good so uh this claim established let's just rewrite the definition of charlie items okay let's say that Charlie Adams K is a proof system we're instead of saying you know you have the true statements of axioms let's say as an axiom you get to have Q squared is at least zero for any polynomial Q depending on it must K variables okay and really I mean the multi linearization of this okay so good so this is like a new definition of the Shirley Adams Kay proof system same as before same uh you know derivation rules you can take non-negative linear combinations of the lines you've arrived just so I'm saying the axioms in a different way I'm saying that the axioms you're allowed are you can take any polynomial on at most K variables out of the end and square it and multi linearize it and saying that that is nonzero is an axiom you're allowed to use there's a question square root the outputs right yeah on the previous thing how to affect this you square root the outputs good so that's surely Adams and now like I took away this question mark here I can tell you about this SDP extension of shirali Adams it's called the SOS proof system it also has a parameter and different authors choose to write either K or two K here it's just a question of notation and I'm gonna write to K but SOS with parameter to K is comparable to Cheryl I on Tralee items with parameter K and it's a generalization of this proof system where as an axiom you can take that a squared polynomial is non-negative for any polynomial of degree at most K so here the polynomial whose squares you're allowed to assert are non-negative or any polynomial depending on it most k variables in this SOS proof sister you cannot polynomial that depends on all n variables it just has to have degree at most K and this SOS stands for sum of squares and this these ideas date back to lacera and Perillo from the year 2000 sometimes called the lacera proof system but people have started to get stuck on this the name SOS proof system okay so this is uh well one thing is this is like only more powerful okay because every polynomial depending on it most K variables can be represented with degree K but now in the SOS proof system you're allowed to have axioms involving all the variables as long as you know it's like a square of a degree K polynomial now what this is good for we'll see later well shortly that's the definition ah okay another fact is that if you take this primary - so like K equals 1 so SOS to reload to have as a axiom the square of any linear polynomial is non-negative it turns out to be exactly equivalent to the semi definite program for gunman's williamson that governs williamson analyzed for the max cut problem ok so I'm not gonna prove that but this is a nice for connecting it back to things we've seen in particular semi definite programming so the semi definite program for Mexico is exactly equivalent in power really in setup to this version SOS to wear as axioms you can have the square of any linear polynomial okay and now one more important theorem which is sort of the reason we really really like in combinatorial optimization this proof system SOS K is that it's also a Tama Taizo below in n to the order K time it doesn't matter if you write to K or K here because I have a Big O here okay so in this more powerful proof system still it's the case that whenever an inequality can be derived from the axioms there's an algorithm that derives it in n to the order K time which is polynomial time if K is a constant okay there's a little asterisk here you know because you use you need send me enough into programming to do this it's sort of a semi definite programming version of Sharlee atoms and so this automatize ation algorithm uses semi definite programming and then there's like these you know slight caveats here about you know solving the precision epsilon and poly log on Ruslan time and some big box constraints but like just don't worry about that so that's great so it means we have an even more powerful proof system that potentially can derive you know it's we're powerful than shirali atoms it can potentially derive you know more and more like tight bounds true facts about the combinatorial optimization runs we care about and still we can find these optimal bounds in until our decay times a polynomial time if K is a constant okay so what is this good for I mean is this you know it truly better than shirali atoms well one yeah it is as we saw you know this rally Adams sorry this SOS with ke SOS two so equivalent to the semi enough of a program in for max cots which was already good at like realizing you know that a random graph the max cut is very close to half and so forth here's another very classic example of what this SOS proof system is good for so remembering the spectral graph theory lectures we studied a lot this parameter of a graph Phi of G the minimum conductance which was sort of like the worst bottleneck in the graph quantitatively for the purposes of random mixing so it's defined to be you like minimize over all subsets of vertices you know non-empty and at most half the graph of this quantity this quantity the probability when you choose a random edge that U is in the set s what V is outside the set s so basically it's the probability that in a randomly take like a random step and then you get outside of the set s okay so if there's some set a square this probability is really small like being inside that set is like a real bottleneck for the random walk okay so that's why we cared it about it a lot it's five G property the quantity it's a number between zero and one and I mention that is np-hard to compute it exactly and in fact this np-hard to approximate it up to a factor of ten well actually that's not known but it's de facto hard nobody knows how to approximate up to a factor of 10 or factor of 100 or even a factor of a thousand but here's a cool result of Aurora Ravis irani typically called ARV from 2009 just degree for SOS does a good job of certification actually maybe it's called degree 2 because the axioms here you allow yourself that the square of any quadratic polynomial is non-negative so this proof system it's a fact that whenever the minimum conductance is at most beta this SOS for proof system certifies that it's at most root login times beta or 2 relogin times beta gets order root log in you know theoretically this diverges to infinity as n goes to infinity but so slow that you know rule again is a very very small number even for enormous n ok so it's pretty good so I mean this is like almost like a constant factor approximation in or I should say certification algorithm for minimum conductance just as a technical point I'll repeat it again we're cheaters inequality lets you compute well it relates this minimum conductance the smallest eigenvalue of the laplacian it says it's a MOS the square root of that and you can compute this smallest eigenvalue of the laplacian efficiently but even though it's like qualitatively an approximation you can't you can't you know put any specific factor here because you know if if beta is really well because the square roots of the conductance itself could be like an arbitrarily small number ok that was a weird digression never mind the point is that this proof system SOS for is is great for understanding the conductance of a graph ok just a reminder you can compute this certificate in polynomial and end time and for a long time prior to this arv paper you know there is a linear programming basis a certification Haugan that had order log in here okay and after a long last I got to order root login okay so that hopefully illustrates the power of SOS someone come on PowerPoint okay so one cool thing about SOS it's been sort of empirically discovered over the last 10-15 years is that actually almost all known certification algorithms for combinatorial optimization problems fit in this SOS system so what I'm saying here is there's like many many many approaches to like you turistic for trying to certify bounds on combinatorial optimization problems or procs finding approximately good solutions as a combinatorial optimization problems and sort of discover that like almost all of them can be sort of captured by these uh proofs in this SOS proof system any certification algorithm that's like polynomial time almost anyone can also you can get the same quality certification with like SOS proofs so they're sort of like you know the go-to tool whenever we have like a new common Tory optimization problem and in principle or in theory you can say try the SOS automatization algorithm on it hopefully it'll give you great certificates and so it's always sort of like the algorithm to try to analyze and try to beat what else what else is interesting is that um it's humanly possible although often very difficult to actually prove statements about the failure of SOS to certify something there's a question from the class do you know a natural example of an algorithm that does not fit or at least is not known to fit yes there's one very canonical example which is the optimization problem of solving systems of equations over a finite fields for example over f2 for simplicity the field of size 2 so like you know solving linear equations mod 2 so this is like a constraint satisfaction problem you have like all these variables you're trying to assign 0 and values to them and like the equations mod 2 are constraints and what I want to say even applies if like every equation only involves three variables so it's really like a CSP like it's like local each constraint only constrains three variables in certain way in there is a polynomial time certification algorithm like 1 comma 1 certification algorithm meaning one that can detect unsatisfiability correctly and it's Gaussian elimination it's like trying to solve the linear system if you solve it great a satisfiable if Gaussian elimination fails to solve it he'll it'll certify that it's unsatisfiable but it's a known fact kind of interesting that there are such linear systems where even every constraint involves 3 variables it's like linear equations mod 2 where it's very unsatisfiable but SOS even with like k equal to point zero zero zero one times n so k like really close to n fails to certify anything at all just says like well the optimum is at most 100 percent so these like linear equations mod a primer over a field are maybe the one exception to the great and glorious power of SOS um but what I'm saying is it's also possible sometimes to prove that SOS fails to do something it's pretty tough to show that like there's no proof like you for a specific instance that there's like no proof in this SOS proof system that its value is that most US and such because it's a very strong crew system but thanks to like um the notion of duality which I'm going to mention at the end of this lecture um you know showing that like there's no proof of something amounts to showing that like a duel to the st-pierre the LP is satisfiable this is very vague high level but like you know means you only need to find like some satisfying thing for the duel to establish failure of this SOS proof system okay what I really want to get out here is though like these two facts taken together form a nice combination because you know it's we don't never know how to prove something that's not doable in polynomial time okay we cannot prove P does not equal NP and we have nothing like that what are the contexts of like optimization problems you can kind of say like look pretty much the best algorithm for certification for anything is this SOS proof system with higher and higher values of K you know that's almost all the algorithms fit in this framework and so if you prove that you know the SOS proof system fails to do a good job on a certain kind of problem that's like a really valuable negative result it kind of says that like any of the heuristics people have thought about it or almost anything here six people have thought about are not going to work so it's not like proving that something cannot be done in polynomial time but that's sort of the best thing we kind of have these days for for proving negative results in combinatorial optimization and you know the most intriguing open problem in combinatorial optimization is this unique games problem which I mentioned at the end of last lecture and it's a big open problem you know what does the sum of squares proof system do for unique games problems or for this max bisection CSP that I talked about in previous lectures and pretty good let's open you know consider the following kind of task for any large value of Q domain size I give you a max by Junction instance when the optimum is like very small like the best of the assignment satisfies like one percent of the constraints your only goal in life is to satisfy that there's no solution that gets like 99% of the constraints can the SOS proof system with parameter for always do this so just drawing from axioms that look like squares of quadratic polynomials can you always make this kind of deduction and when it's true ah this is unknown and in the early days of SMS people are really hopeful that maybe it was true we still don't know but maybe these days we're less hopeful that it's true perhaps as so most for doesn't do this it's just really really hard to find a specific instance where SOS for fails to do it okay so for the okay there's another question is there a special reason why for why the degree is 4 is it just due to previous results um good so SOS for you hear a lot about because this SOS the parameter is usually chosen to be an even number because it's like this SOS 2 K thing so that SOS 2 K is like where are you your axioms are like squares and degree K polynomials so sort of like SOS 2 is the first one SOS 4 is the next one SOS 6 is the next one and so forth and SOS 2 is basically always the same as like the basic SDP relaxation that you would always write down and this is very well understood usually like it's not that hard to understand how well or poorly the basic SDP relaxation does for a wide variety of optimization problems but like just the next rung up SOS for it's very hard to understand very hard to analyze at least currently maybe in 20 years we'll figure it out exactly well I really understand it well\", metadata={'source': 'ihYQpj1mrs4'}),\n",
       " Document(page_content=\"okay so in the last time bit of the lecture I want to talk about this idea of how you can fail to prove or show that SOS or some proof system fails to do a good job and it's through looking at the dual so I'm actually not gonna talk about this failure to prove anything so much it's just I'm gonna talk about the dual proof systems here the dual so these proof systems and just I think it's like simpler we're gonna go way back to the shirali adams proof system which is just about linear programs i think i think about do lp's remember I sort of told you this life tip before you know whenever you write down a linear program you should also write down the dual linear program and try to interpret it so remember we showed that the shirali items per system is atomic izybelle meaning like whenever you know some inequality can be derived there's an efficient algorithm for deriving its and that efficient algorithm was like a linear program searching for like sort of the best proof and so what's going on with the dual of that linear program let's try to interpret it so I'll say sure Allie Adams you know K equals three proof looks like this it's like a copy of this previous slide you know you know these are all your axioms and maybe have some additional axioms if it's like you know the independent set problem and the proof is just multipliers lambda is which are non-negative which you multiply all the inequalities by to try to get like some inequality about the objective function okay so this was the LP that solved the out the autumn ization problem that tried to find the best proof like this the best lambda is in the smallest beta and one thing I wanna remind you of is that like you know it had a variable lambda I for each axiom and had a constraint enforcing the sum identity for each monomial so you can take the dual of this LP and that's always a duals what's gonna happen is you're gonna have a it's gonna be the opposite you're gonna have a variable in the dual for each monomial and you'll have a constraint for each axiom so I'm not gonna explicitly work it out I'll just skip to the end and tell you what the dual is that has constraint for each one oh meal sorry a variable for each one we want to constraint for each axiom and this proof is this proof system optimization algorithm is actually a minimization problem it's trying to find the smallest upper bound so the dual will be a maximization problem so bear that in mind as well and by LP duality by the Farkas lemma the smallest beta that you can get as your upper bound will actually equal the largest value in this maximization problem so let me even just simplify down to the K equals 2 case okay so ah that Charlie Adams K equals 2 dual LP I'll tell you one thing about it something about it it has a bunch of variables and here's one of the variables one of the variables is named eat wittle bracket X 1 X 2 close bracket so indeed like the name of the variable usually unlike the SOPs like you know variables called like Y or something or like Y 1 2 and here it's called each wittle bracket X 1 X 2 bracket that's the whole name of the variable just kind of stressful but like hang in there the pronunciation of this variable the way you actually pronounce this variables name is pseudo expectation of the monomial X 1 X 2 I'll explain this shortly and in fact you also have these variables for every pair of X eyes and also for every single t'n okay so you have like n choose 2 plus n variables in this dual LP I know it's like a little zany to be getting into this little detail right at the end of the lecture but putting in there so these are the names of your variables in the dual LP and uh see in the dual out P you know you're gonna be searching for real values for these variables and like a dual LP solution and it's gonna assign like a real number to each of these variables and one thing that we're always gonna be doing is like once you've assigned some real numbers to these variables you can create you can sort of extend the notation and give like a real number value to anything that looks like this eat Whittle of a polynomial and you just do that by linearity in the natural way given the values assigned here so like if you have assigned values for all these things by your LP then you and your mind like assign a value for this expectation pseudo expectation of this polynomial in the natural way so that's the variables and now I'll tell you the constraints in the dual LP and the constraints are you have one for every axiom in the old Charlie Atum system and the constraint is that the e twiddle value for this axiom should be non-negative sort of makes sense so remember like in the charlie items to write like the axioms looked like this expression involving in determinants should be non-negative and in the LP the dual LP for rally items you have this constraint that this should be non-negative and really the real way to write this constraint is like a total of X 3 which is genuinely the name of a variable - e twiddle of X 3 X 7 which is genuinely the name of a variable is at least 0 so that's like the difference of two LP variables is non-negative it's a valid LP constraint and as I said the dual LP is a maximization problem and what are you trying to maximize you try to maximize this each Whittle of the objective polynomial so that's a little mind blowing let's do an example so let's look again at the maximum independent set problem in this little triangle graph so there's going to be six variables and there's Sharlee items - dual LP maximization problem named e total of XA each one of xB and also these degree to each widows and the constraints are that all of these each one else of axioms should be non-negative and remember max independent set also is one of these csps where they're like additional input axioms coming from the instance from the graph okay like they've said that like XA plus xB is at most one so you'll get some additional constraints here that say like the e twiddles are at most one okay so this is an LP with six variables and like I don't know six constraints and your objective is to maximize each Whittle of this expression which remember is sort of representing the size of the independent set okay so this is an LP and I just want to say like what's up with this LP remember it's a maximization problem and if its maximum value is some beta and that's also the minimization problem of the sa-2 proof system which means that like it's a certified upper bound so this maximum of this LP should always be a bigger than the true max independent set and indeed I claim this to LLP is a relaxation of the minimum maximum independent set problem when claiming is this true opt is always at most surely Adams to do lofts the off of this LP so why is that well it's a simple reason let's say you have an actually optimum solution like the actual optimum is to choose this this independent set a so like little XA is one little xB is zero little XC is zero well then you get an LP solution achieving the same value where the II twiddle of a polynomial of the determinants it's just what you get if you plug in your actual zero one solution to the polynomials that will sort of clearly satisfy all the constraints because well you're plugging in zero and values so this indeed will be non-negative this will be non-negative this will be non-negative it's really an independent set so little X plus a plus a little xB will really be at most one and like little XA plus a little xB plus a little XC is the actual optimum value okay almost done here I'm going overtime if you need to go I will not be offended uh indeed before we go on I want to mention that um more is true let's say you have not just one optimal solution but several optimal solutions if I tear we did have several natural optimal solutions right the independence it could be just a or just be or just see they all have value one and let's say you even have a probability distribution D on optimal solutions I'll give an example of shortly then I claim you can take as a solution to this dual LP you can take for a total of P the actual expectation of the polynomial P on the actual solutions little X a little X B little X C which are all zero one solutions drawn from this distribution and really it's because LPS are convex right so if you have some feasible solutions achieving a certain objective value then their average or like a probability distribution over them or a convex combination of them achieves the same value and is still feasible so by way of illustration let's consider the probability distribution D on true solutions that puts one-third probability on like a is the independent set one-third probability on B is the independent set and one-third probability on C is the independent set so what I'm telling you is that you can get like a feasible solution to this dual LP out of this and it's just equal to like the real expectation under this distribution okay this is where the whole weird pseudo distribution suta expectation and notation comes from so here each total of XA will be one-third because like an expectation XA is one-third these will all be one-third these will all be 0 because with probability 100% under D XA times xB is zero this is zero this is zero all of these constraints will be satisfied this is zero this is zero minus zero this is zero and these constraints will all be satisfied these will all be 2/3 which is at most one and the value here will be one the expected value of XA plus xB plus XC is always 1 so that shows that it's this dual LP is a relaxation of the true optimization problem but you know as always in life it's not perfect it's really a relaxation you can solve it in polynomial time so it better not always be giving the exact answer and indeed there may be sometimes better pseudo solutions so I'll just give you one example of that here so here's a solution to this dual LP which is not corresponding to a real expectation over zero one solutions I claim that you take all the single variable e twiddles to be a half in all the degree two variables X 2 to X patient Nations to be 0 and I claim that all the constraints are satisfied like this is 0 that's not negative this is half minus 0 which is not negative this is the trickiest one this is well I have it down here it's 1 minus 1/2 minus 1/2 plus 0 which is still non-negative so it satisfies all the constraints and now the objective value is 1/2 plus 1/2 plus 1/2 it's 3 halves so here are this dual LP gave you a larger value than the true optimum but it's because it's a relaxation and just by Alti duality the largest solution to this dual LP is also equal to the best upper bound the actual proof system can certify for the instance which i think as we saw before was three-halves so this my last slide you can also do this duality for the SOS proof system it's basically the same so you'll have all the same variables for monomials up to size K and the same sort of situation the constraints include that the pseudo expectation of any squared polynomial is non-negative for the final degree at most K and the dual SDP asks you to maximize the pseudo expectation of the objective subject to all these constraints and that can be done in n to the order K time this is a relaxation of the original problem it's like a better and better relaxation as K gets larger and larger and its value exactly equals under 99% of circumstances the best value you can certify by the proof system so this is what you might want if you want not just a certification algorithm that certifies upper bounds but like an approximation algorithm that tries to get like some kind of quasi solution and convert it into a real solution but the last thing I'll say is you know this is the task of rounding taking some kind of fake solution like a rational solution or like a vector solution and trying to get like a real solution you have these like each Whittle solutions and we want convictive an integral solutions but there's no as of yet in my opinion a good theory of how this can be done in general so that's something waiting to be discovered okay I went way over time there sorry about that about the extra five minutes you'll find online in the video and I'll also take any questions if people have them now\", metadata={'source': 'YLnMyNeXV0E'}),\n",
       " Document(page_content=\"all right let's uh what's chad how's it going with the homework good good um i it's very interesting sdps are um a little tricky sometimes to get right i think uh that's true yeah can we go through maybe problem 9.2 um it's one 9.2 yeah yeah so so actually before 9.2 because 9.2 is like probably harder and 9.1 um can you yeah maybe i'm missing something here but like to me isn't this just a chebychev center kind of problem or is there something um is that is that like not the right intuition because i mean i don't know what chubby chef center means so maybe it is oh sorry yeah it's like the convex optimization thing where it's like the uh like i mean i just i copied it from like boyd and boyden vandenberg um it's the what is it um it's like the maximum inscribed it's the distance off to the closest point in the exterior of c uh yeah i'm not sure yeah so so basically my intuition there was just like you have to um you just have to formulate uh lp where the lp is um based on the how far away from the constraints you are at any given for any given phase uh yeah yeah so i guess i guess that one's okay maybe we can just do 9.2 then okay uh so i think i got like a and b were relatively simple but then in c um finding the stp relaxation i was kind of struggling to show how um the part b like ended up giving giving uh being equivalent to like an sdp constraint like a matrix stp constraint okay um let me re-read it because you know i haven't looked at it since i wrote it okay we got the jobs they really got the constraints these are like the betweenness constraints uh all right all right suppose so we are given a perfectly satisfiable instance prove that the following quadratic program has a feasible solution uh okay did that work out for you the um part b part b so so that for what i understood that was just like arithmetizing this kind of problem in some sort just like converting x x just becomes a vector where the vectors hold the j one for j n and then like just the constraints like if you just like just like write out the constraints then then this just becomes arithmeticization of the problem pretty much i see so like what is x i supposed to kind of stand for j i and actually j maybe uh at least like that kind of because it's like x i minus x j then have to be at least an integral um integral distance apart so the square must be at least greater than equal to one so that one was just satisfied by time slots being integral and the sec in the second one either um either one of this is negative sorry either x or minus x s is negative or x t minus xs is negative and the other one's positive um when the constraint is satisfied uh okay so well let's just make sure we're on the same uh page here so like uh [Music] right uh [Music] so what what a when you have a perfectly satisfiable instance like what's how will you assign the xi's to get a feasible solution so the excise would be one where so x i would be j i right like that that's how i was thinking about the way j isd is the is the like um yeah it's just like the name of a job though oh like that's not a number wait but then like this isn't the constraints of the form jr mine less than equal to gjs so how can you have that name i see well i guess you're thinking you're saying that you're like as you're supposed to i guess suppose um i guess you can say you're supposed to assign numbers to the js yeah yeah yeah i mean i'm not sure because like the question itself says if you have the constraint of the form jr less than js this is my assumption was that it's like the number assigned to j okay and so so i was just like maybe abusive notation okay i got you yeah yeah yeah okay um all right so yeah maybe i was just thinking x i would be the um time slot assigned or the schedule assigned to um job j okay drop by sorry yeah uh okay good yeah so um so yeah i mean i think if you do that then the first constrainer is satisfied by the fact that like the time slots are integral distance apart pretty much yeah and the second constraint is satisfied because um [Music] this works out or yeah yeah the sign the sign will like work out because either one will be negative and the other will be positive right okay cool yeah so um so that was that one but then for like formulating the stp uh we were thinking about how to like come up with a matrix that represented this and um and it's it was slightly because you can't necessarily use the same like x vector shown in part b because that's one vector for the entire um schedule versus like in part c we want like one vector for each job um so we were thinking of losing like one hot vectors for the jobs where the um where like job i gets a vector um where it's one uh a vector in n off n dimensions where it's one for the position schedule then in some sense that makes sense uh okay so you're formulating like you have the idea that you want to set it up so that if you have some vector uh j vector i which looks like yes yes exactly one zero one yeah okay so this is your idea to have this as a feasible solution yeah yeah yeah like like like have this like you know have the vector assignments end up being some feasible solution so like yeah okay and and then like the idea was if you stacked all of the j vectors together and then did like you know some j transpose j or something like the matrix of the j vectors um it transposed with its and multiplied by itself then potentially you'd get like something that satisfies this except like it's hard to do ordering on ordinals um like sorry it's hard to do ordering and one hot vectors like this yeah yeah exactly so it's like we call this maybe like yeah script j or something then j transpose j yeah yeah uh okay yeah so it's either that or like yeah yeah exactly that that was like the idea and then kind of um yeah yeah we weren't that's kind of like i think what we had um and then high enough piece so so this this would kind of give us um [Music] the the first constraint would then be this is equal to sorry um uh the first constraint or something like j script j transpose of j minus i is positive semi-definite i think oe yeah um [Music] okay uh so it's your idea here that like you want to find some [Music] positive semi-definite constraints that will be satisfied by this proposed solution yeah like like like basically the idea would be if you are able to reformulate part b's like objectives in terms of some semi positive semi-definite kind of um matrix constrained then it could just be like written as an sdp in that form yeah well i mean one thing you know uh this could be good although it might not be answering the question directly as asked which is to like formulate an stp relaxation of like this quadratic program meaning like this one so actually you know it could be fine um yeah you're formulating a semi-definite programming relaxation it's kind of asking you to formulate a particular one um but you know i i don't have to be picky about it but uh it is true that like i guess the intent was that you'd be like motivated by part b right i see so would this be like so the stp relaxation of this am i the part b semi-definite outside sorry the part b quadratic program would that be like based around the idea so so when we're formulating the sp relaxation that that's usually like um assuming that the x i's which are um real numbers are vectors instead of um that that's usually that that's like how maxcut did it sometimes is that like sorry uh yeah kind of although if you remember maybe if you remember how we came to the the semi-definite program from max cut originally you know we can't thought it was like a quadratic program we're trying to maximize like this expression sum over edges right uv some quadratic expression like x u x v right we had like x u squared is one for all uh and uh [Music] you know the idea there was we wanted to like you know since this is a quadratic it's not linear it was a problem so we wanted to introduce right some variables like y u v um that we're supposed to standing for supposed to stand for uh for x u times x v right and you know then you know this business became well equivalent to like maximum sum over u be an edge of this which looks linear yeah and we have this linear constraint as well y u equals one but then we try to enforce this other like last constraint which is like you know that sort of exists you know reals xu one for each vertex such that right you know y v u oops u v equals xu xv right that makes sense and um yeah this was not a condition but we kind of we relax this to this semi-definite uh programming condition or this psd condition that like the matrix y formed by the yuv's is psd and i guess we argued that like okay if you have this property dagger that the wise the y u v you know equals x u x v for some real values x then this matrix is positive semi definite not necessarily the reverse but like that's why it's relaxation um i see and um the thing is like once you if you if you do this relaxation then you see one thing you can see is that um you know if you're gonna like just accept the fact that all right all right we're gonna take this as relaxation then in your original program like you can have the expression like xu dot xv appearing anywhere like if it's gonna you know be replaced by yuv then as long as it's like i shouldn't say anywhere but like as long as like these expressions are appearing linearly then you can have lots of constraints involving them you can have um the objective function involving them [Music] so this is kind of like the generic way to relax a quadratic program to a semi-final program right that makes sense i mean i'm like a little confused about the relaxation from dagger um why again does the relaxation like work like why why why do you have like if there exists uh x u v um sorry if you have like y u v is equal to x u of x v then y is positive like why why does it necessarily have to be positive semitone it's only if these if they're assuming x u and x v are um integral right uh no not necessarily so um so let's do one direction so if like the y u v's are such that there exists x u's with y u v equals x u x v then this y matrix is positive semi-definite uh i suppose the easiest way to see this or like a linear algebra e way to see this is um if you write the x vector here oh i st and then write like this x one dot x is yeah you will have this i mean what will this will equal y ah okay okay that makes sense because like the y u v entry here is like x u times x b right and so this gives a way of writing y as like a matrix times its transpose and that's one condition for being positive semi-definite right okay the most standard of all condition way to phrase it is you know why is psd if and only if well i guess the most usual definition is like for all vector z z transpose y z is greater than or equal to zero right but if we have y if we have this equation then we could say well for this particular y you know z transpose y z equals z transpose x times x times z so maybe i'll draw it like z is a column vector that would be like z and then this would be like z transpose like this and then uh this is indeed like greater than or equal to zero as like a number because it's like uh yeah well it's the inner product between z and x squared is the simplest way to see it yeah that makes sense so that's one direction um but that's that's the only one that we really need right because that's the relaxation that's a relaxation but then to connect it up to like uh vectors um you know one thing i might do is okay let me go back to this true fact that i just put a check mark by um let me just draw a little dot there that stands for multiplication of two numbers right this is true so now the converse is not necessarily true that if you have some real numbers such that uh sorry if if y is psd then there have to exist real numbers that make this true okay but what is true is that if y is pst then there have to be some vectors right in s right this is true where now this dot means dot product that makes sense yeah okay that makes a lot of sense cool yeah yeah so so that me so then like going back to the 9.2 yeah that like kind of um that kind of inspires me to think of the the x i minus x j as potentially being some um some sound like vector because then technically we're talking about the like it seems like there's a dot product between vectors being less than zero and then there's like a vector squared which would be uh right like like the relaxation would be it's it's like it seems like it could be x i minus x j becomes some vector um index by i i n j does that does that seem um like because because then it seems like it's like somewhat similar to the a max cut kind of stp formulation yeah well the most mechanical way to do it if you have like uh any quadratic program well first i guess oh maybe i didn't make uh things very clear first of all i guess i would say like um did i define what really is a quadratic program or if so do you have in your head like a good definition of what a quadratic program is yeah it's it's like the it's the minimization over some like some some quadratic like sub some quadratic program which is like x x i mean i'm thinking hessians and ellipsoids in general and like minimization over that well those are more like algorithms actually so maybe okay maybe i didn't even explicitly say this in class so it's maybe good to see it i mean for me like quadratic program is like well basically same as a linear program a linear program except the constraints that's not good spelling constraints and objective can be of degree two i see it that makes sense so a linear program they're supposed to be of degree one so like you know you have constraints that look like you know seven x one plus three x two minus five x three is less than or equal to nine and then some other constraint two x one plus x two minus 8 x 3 less than or equal to minus 4 and then maybe have some objective that's like you know maximize or minimize i don't know x1 plus x2 minus x3 yeah so these are all like linear expressions in the x's and a quadratic program they're just allowed to be um quadratic so you can have i mean degree two so you can have okay squared or something yeah you kind of like x1 squared and you can have like you know x1 times x2 and you have like x1 times x3 and like in the objective function you can have like x2 x3 plus 5x1 x2 minus x that's not an x x 2 squared or something like this i see that's a quadrant and then program that makes sense so i guess they the the relevant thing here is you can think of like a quadratic program it's like you can also say it's like a linear program okay maybe i'll be a little informal here but it's like a linear program but where but where the variables can be like products of two variables that doesn't quite make sense but maybe i guess you know what i mean yeah yeah so um uh and this is a situation where like so max cod is like an example here is our quadratic program because it looks like a linear program except that like you know where you would expect to have like one variable you have like a product of two variables right that makes sense yeah but whenever you have such a a quadratic program like you can try this idea oh okay so so like you can write it in terms of some um yeah yeah okay that makes sense yeah that that so so yeah that that follows like with what i was like thinking i think my intuition was because now it seems like the quadratic like part i mean i guess you could expand this in terms of um and the x i minus x j the whole squared gives you some quadratic thing and the x r minus x s times x t minus xs will also give you some some like quadratic constraint and then each of the terms there give us like some some like quadratic constraint and that that sorry they're they're at least they're off or degree two it gives you two degree two kind of um polynomials and that we can probably exploit using this trick yeah that's exactly right yeah this was so go ahead sorry yeah i was just asking if that sounded like right correct yeah that was in fact exactly what i was going to ask you i think you answered it without me even asking it which is you know so it's actually not like extremely plain on the face of it that this is a quadratic program like okay i say in the thing that it's a quadratic program but um it doesn't look exactly like this thing i just said here there's the definition of a quadratic program but as you said yeah you can just i mean if you expand this out like you know x i minus x j squared greater than or equal to 1 this is the same as x i squared minus 2 x i x j plus x j squared is greater than or equal to 1. and this does look like you know the kind of thing that i said you're allowed to say yeah okay that makes a lot of sense cool yeah that makes sense interesting i was thinking about this in a very like different way i was trying to create like a matrix um that would like constrain this but this seems like a much more straightforward approach thanks yeah sure yeah um so yeah i guess you'll find that like if you take this uh idea ahead then like generally you can think of like a semi-definite program is like a linear program but where the variables uh can be well maybe it's combining several steps but like where the variables will be like dot products of mystery vectors right right because if you sort i'm just going to have you replace this with y i j and this with y j j and this with y i i and then you do this relaxation then a solution y will be pst and that means you won't necessarily have like real numbers x's such that you know y u v is x u times x v but you'll get vectors uh such that y u v is x u like dot product x v um just cool yeah that makes sense thanks that's really useful okay cool um all right let's pause there we can come back to it if we like um is there another question somebody would like to ask about um i was stuck on point 2 d or part d i see okay this one here yeah uh okay let's see what does it say assume that you can use the ellipsoid to find a feasible vector solution given a perfectly satisfiable instance um show how to get this approximation algorithm okay so it's some kind of like rounding question so maybe you can tell me like at least according how you did it like uh what like a what is like the meaning of like a perfectly satisfying vector solution like maybe geometrically or or in any way like what did that mean or you didn't get to that part yet wait i just was just thinking that oh this one this like i wasn't really thinking about in terms of a vector solution but more so like there is a you can schedule the jobs in such a way that you satisfy all the constraints but i didn't really extrapolate that to like the vector solution uh i see um well uh maybe we should go back to c then i mean did you solve c yes okay so it gives you uh some kind of vectors right right well my thought was in part b i kind of took the the variables to mean like the uh the time in which the job should be scheduled right so i wanted the vectors to i was hoping that the vectors would represent that as well um perhaps yeah uh well in some way um but maybe we should talk about i mean what was your formulation [Music] oh so i mean for part c all i did was just like take like um all the variables and so any like x i times x i would replace by y i i in a matrix y and then x i x times x j would just be replaced by y i j and then i had the constraint that y had to be positive semi-definite and then the vectors would just be um since every position in the and the matrix is the dot product of two vectors each job is just one of those vectors okay yeah okay so what do the but i guess what i'm saying is maybe it's what what property is these vectors what properties do you know they satisfy i know they satisfy that well just sorry i think i only thought like every like basically what i said before that every position in this positive semi-definite matrix was the dot product of two vectors and that's about it um [Music] yeah okay so all right let's let's write down the stp you have in mind so uh maybe you tell me what to write and i'll write it um so i basically have the expansion of okay so let me just say so i have y i i um plus yjj minus 2 y i j um and greater than equal to 1. and then y r t minus y s t minus y r s plus y s s is less than or equal to zero and then why is symmetric and positive semi-definite okay okay cool great yeah um okay so now uh yeah the next question is what does this have to do with vectors um so every position y i j is equal to like vectors i guess x i dot product xj okay good so yeah this right this is equivalent to it's like equivalent to the existence of some vectors x1 through x and such that this holds [Music] so in particular for example that means that if okay if you solve this and you get a solution you get some vectors then for example we know that x i vector dot x i vector plus x j vector dot x j vector minus two x vector i dot x vector j greater than or equal to one right um so okay we might ask ourselves like maybe more intuitively what does this mean about these two vectors foreign sorry is there supposed to be like some property about vectors i should know um well there may be some simpler way to think about this oh we can do an example though like here's r2 let's say i don't know this is x i and uh i don't know it says length two i'm just making this up and uh i don't know this is xj has length one i don't know so uh do these two vectors satisfy this now uh okay why on okay so the dot product represents the magnitude rate so i mean if you take the dot product of itself oh it's a squared magnitude yeah wait sorry okay um my linear algebra is kind of shaky um all right so all right well how much is this this uh four yes how much is this one and well i didn't exactly say like what are the angles here but let me start with this is this positive or negative this is negative yes so then this is positive so this is like five plus a positive thing so that's bigger than one right that's good what if xj were like uh what if this was also xj what if they were both this vector then why this would also satisfy oh sorry it wouldn't satisfy since it would wait sorry the angle oh sorry yeah um this wouldn't satisfy since x i dot x j would just be equivalent to x i dot x i or x j dot x j okay so this would just equal zero yeah this is before this was before this would be four so you get like four plus four minus eight which is zero so actually wouldn't satisfy because zero is not bigger than one there's not at least one does that make sense yes okay um yeah okay so hmm what can we i mean still is a little bit mysterious i guess i mean if i said like what about this one if this was like xj would it satisfy perhaps a little hard to say um actually uh yeah so what is the i mean uh what can we make of this left hand side um so it's kind of like the the sum of their squared magnitudes minus two times their angle between them ish yeah that's true actually how do we come to get this expression in our hands anyway where did it come from just from the original constraints in part b which was what that which basically said that they had to be at least a distance of one from each other true uh that's true the original constraint did say that like as real numbers like the distance of x i from xj had to be at least one well did it say that actually yeah i guess it did um [Music] what's the distance between x i vector and xj vector i think it's the dot product of two vectors sorry me like xj we want to know the distance between them it's like the length of this yellow dashed thing um yeah it's the length of a certain vector which vector the yellow vector yeah how do i express it in terms of x i and x j x i minus x j the dot product of x i minus x j times all right x i minus x j and then dot product by x i minus x j yeah that's the distance uh squared this is like the length of a vector of v squared is like the dot product of v with itself this vector here if we like orient it like um this this is x i minus x j or maybe it's other way around uh i think it's x i minus h x j um yeah so the distance like a squared between these two things is this expression um how does that relate to this expression um it's the same thing yeah it's the same thing so this this thing here is uh this thing that we're kind of getting uh [Music] also the same as like the the distance between x i vector you know it's tip x j vector um squared is greater than or equal to one so that's a little bit more enjoyable i guess if we looked at these like pictures we're doing some examples here uh well i guess we first looked at like this case this was one and this was xj yeah it seems pretty clear their distance is bigger than from here to here it's bigger than one and then we looked at this other case where like these were both xi and xj and uh i guess the distance from like here to itself is zero so that's not bigger than one um [Music] okay so that's nice what about this one what's going on with this one um i mean if we did something similar to what we did with the first constraint just translate it back to um like x r minus x s dot product with x t minus x s yeah x t minus xs that's right it's less than or equal to zero and i guess like it's just so we have it you know in our minds this could also be written x i minus x j dot with itself sorry this is getting messy x i minus x j is greater than or equal to one so yeah it's kind of like um [Music] you know if we have a solution to this semi-definite program which is like this matrix of y then it's equivalent to like having some vectors that satisfy i need like a nice color here like this whenever rstr is a constraint and also this [Music] so you see it like depends a lot on like these differences so like let's say like say we just have three variables or three jobs and like i don't know one constraint it's like r is supposed to be less than s supposed to be less than t and okay so then maybe we solve this seminar program and like get some vectors x let me just call them okay one okay r let's say there's only three of them so r vector an x s vector and i don't know x t vector so yeah how can we i mean what what okay so we apparently they satisfy these properties in pink boxes but maybe we should just think a little bit about what that sort of means so let's start with this like pink box um i guess it said the distance is squared or at least one so i guess in this picture it means that like apparently like this distance is like at least one like this distance is at least one and i guess this distance this diagram is getting messy at least one and uh what about this one what is this one kind of saying let's let's rewrite it over here it's like x r minus x s dot x what was it um t minus x s okay minus x s less or equal to zero yes okay let me erase this green stuff okay so what's x r minus x s here i guess it's a little hard for you to say because like you can't look at the picture but it's a it's some vector here which one um like the vector between like like where the arrowhead of xs and xr are yeah so it goes like from here to here and what about this one um between like the vector between going from xs to xt uh perhaps the other way around or maybe it is the way you just said all right i should get this right from xs to xt yeah no i think you're right that's right okay okay so that's this vector what's this vector what does it mean like their dot product is not negative does this have like some kind of meaning um that their angle is at least 90 degrees yeah that's right and they make like an oblique angle as they say so actually you know like almost everything that we know only depends on these difference vectors i guess it's like it's almost nicer to think of these vectors these yellow vectors to just identify them with their like endpoints their tips or whatever so like i don't know if this is the origin maybe that doesn't matter like this xs vector is maybe more like this point oops i was going to draw it in yellow on this xr vector it's more like this point and this xt vector is more like this point and uh the things we knew you know that like x s minus x r dot x s minus x are at least one kind of so that like this distance is bigger than one like this distance is bigger than one this distance is bigger than one and then this constraint kind of tells us that like like this this vector this green thing and like the screen thing make an oblique angle actually they don't make an oblique angle in this picture looks like they make an angle slightly less than 90 degrees so i guess it means like they this actually wouldn't be like a solution that the semi-definite program that was feasible for the semi-definite program i guess to like draw a solution that's like feasible for the semi-definite program you'd need like you need like okay you need like uh i guess you'd need like a triangle like three points that like made a triangle and like there should be like an oblique angle at xs and i guess the side lengths of the triangle should be at like at least one and i guess you'll have some kind of like oblique angle constraint for all the like constraint strains that you had in your your uh original uh problem even kind of makes sense with like the original version where like we were plotting all these j's positions like on the line oops i don't know what i'm saying here j5 maybe this is like j4 j6 was here you know if we had the constraint that like j3 is supposed to be between j5 and j4 which we got right it is sort of like actually i mean if you look at it the you know the angle between like this vector and like this vector is oblique well it's not just oblique it's like 180 degrees but that's some particular oblique and the side lengths are all at least one this is like one and this is one and this is two although actually you know like this constraint we're about j6 then you know this well these lines like would still be at least one and this would be longer so like the i guess like you know feasible quote unquote vector solution is like similar except that like these vectors don't have to all be in like the line they can be like sticking around in space but somehow the condition is about i guess these triangle side lengths being bigger than one and like having this look oblique angle constraint whenever you have um one of these betweenness constraints that makes sense okay yeah so okay this doesn't really solve anything but like i mean it gets to i guess you know this is like the key step or a key step is to just i guess sort of like figure out what it whoa sorry figure out you know this feasible stp vector solution like what it really means or like you know how to think about it so sometimes there's still the remainder of the problem like okay so like it's kind of like okay if you had you know it's like the stp solver probably comes back and gives you these vectors or these points in space where are they i can't find them anymore oh here they are well nope okay i can't find it um oh there we go yeah it's like the scp solver may like come back and like proudly give you all these points in space and say look i did such a great job like all these points maybe they're a distance at least one and like whenever you wanted me to have you know j6 between j i don't know j3 and j10 i managed to find like i managed to set it up so that like the angle between these two lines is greater than 90 degrees you know you might say thanks very much uh ftp solver that's great but like i really wanted to have actual schedule you know i really want a picture more like you know this where like they were all on like a line um so now you need some bit of creativity like if you have all these points in space how can you actually kind of get a real solution where they're actually a scheduling a schedule for these js at these various times that ideally like satisfies all the original betweenness constraints now you know if you look at the problem you're actually not it doesn't insist that you find a perfectly valid solution you only have to find like a pretty good solution where like at least half the constraints are satisfied and um you know this could remind you maybe a little bit of the gomen's williamson max cut thing where it gave you some vector solution you're like well this is great but i really want like a an actual integer solution or like an actual partition of the graph into two parts you have to like figure out some method of like taking these high dimensional solution and converting into a regular solution that's maybe in some sense not that much worse yeah okay i think that makes sense yeah this is very helpful okay great yeah so some creativity is still you know required but um yeah hopefully you'll have some maybe somewhat geometric ideas maybe that that could help okay thank you sure uh [Music] do you want to ask about any other question no that was the only question i had okay well i guess we can uh call it a day then thanks for coming by and i'll see you tomorrow\", metadata={'source': 'VtfY2YOo9Gc'}),\n",
       " Document(page_content=\"okay so this is lecture 22 it's kind of a daily amount or segments of lectures on constraint satisfaction problems and linear programs and so forth scan be more connected with constraint satisfaction problems and it's actually related to something I mentioned in the last lecture the topic of tree widths okay so oh it's an interesting phenomenon that on certain kinds of graphs np-hard problems can become easy and this is a no more notable than on trees almost every problem you can think of on a tree is actually doable in polynomial time so let's take a little example here of how that works let's consider again this problem called the max independent set I'm trying to find a subset of vertices in a graph that's an independent set meaning there are no edges with both endpoints in the set you've chosen and to make it a little spicier we can imagine the weighted version where the nodes have weights so I drew a little numbers inside each of the nodes in this tree example and you're trying to take the largest weight independent set well as I imagine you can guess you can solve this problem in polynomial time using dynamic programming or recursion if you will so let me briefly illustrate how that goes maybe the dynamic programming mindset so I'll use a little notation here imagine we're doing this max independent set graph problem on a tree and let V be some vertex like this vertex here and I'll write T sub V to be the sub tree hanging off of this vertex V and we're gonna do dynamic programming we're gonna make a table I think two tables which we'll call M is plus and M is minus in any way s plus brackets V this entry in the table is supposed to be the size or the weight the maximum independent set just in the subtree rooted at V that includes V okay that's what the including V is what the plus stands for and M is minus V is gonna be the same thing it's supposed to be the maximum pennant set in T V the sub-tree hanging on V that does not include V okay what we'd like to do is like fill in this table figure out this value for every vertex V in the tree and once we have that we can take a look at the MIS values for the root and you know any media patent said either includes the root or doesn't include the roots so the maximum of M is plus for the roots and mis- for the root will be the maximum independent set we're seeking and the point is that it's easy to build this quantity in like a bottom-up fashion so it's easy to fill this in for the leaves of course it's a zero for M is - and it's the weight itself for M is plus and then in general if you're trying to fill this out if you have some vertex V and it has some loops has some children V 1 W 1 through WD and how do you fill out Emily s + of V well this is when you're going to be taking V into your independent set so in one hand you get the weight of V because you're choosing V or the other hand you're not allowed to choose in your independent set any of these children because it's supposed to be an independent setting you're planning on thinking these you come in with the children so you sum over all the children I don't know why this says goes up to D minus one it should go up to D you can scribble that out anyway it goes up to D the sum over all the children of the M is value for the child which you've already computed where you don't include the root in the other hand if you want to figure out that my ass value for the subtree root of V where you don't include V well then you're allowed to take any of the children if you want into your independent set you don't have to it's not necessarily beneficial so again what you're gonna do is you're gonna sum over all children and again you can take the MIS value for the children when it's optional to include the root or not by máxima taking the max of mas plus of the child I'm Elias - of the child okay so as this uh thing I've flashed up indicates this is a simple correct algorithm for computing the maximum independent set on a tree and it's a linear time algorithm which is great okay let's do another example your example to this is like a much more general example and it's relate to something I talked about in the last lecture so let's say you're given any kind of CSP instance well to find this quantity called the primal graph we need to find it last time this is a graph with vertices and edges the vertices are identified with the variables of the incense and when you put an edge in the graph you put an edge between two vertices aka variables U and B if they participate in some constraint together okay so this is some kind of like graph underlying the instance of a CSV okay now suppose that you're in a situation where this underlying graph is a tree now to be honest that's a bit of a very particular situation we think about it that means that you must be working with the binary CSP when where all the constraints only have era t2 because if you have a constraint involving three variables well then the primal graph will have an edge between all pairs of them so it'll have a triangle and won't be a tree but anyway there's some interesting CSPs a Verity - like the three coloring problem for example and one thing that's true is that if you're given a CSP instance and it's primal graph is a tree then you can decide whether or not it's satisfiable in polynomial time and again in the algorithm I'll kind of leave it as an exercise but you solve it with a dynamic program and again you could imagine filling out a table which i've called s of a v comma a where the V entry stands for a vertex our variable and this a entry here stands for a domain element the remember Omega is the domain of things that you're trying to assign to the variables in the CSP and the intention is that this s become a should be true set to true if there's some partial assignment again to the set of variables or vertices hanging out in the tree rooted at V uh some portion of such assignment where V gets the value a and false otherwise okay it's not too hard to show that with this definition and again you can you know build up this table from the leaves up to the roots in a top-down fashion or a bottom-up fashion I shouldn't say and once you finally filled it out then you can just look at the root vertex V and go through all possible domain assignments to it and see if s V comma you know I is true for any domain assignment I okay then that way you'll learn if the instance is satisfiable or not okay so that's an indication that uh you know on trees many many np-hard problems are actually easy can be solved in polynomial time and what we'd like to explore today is this question on the slide here um is there some more general family than just trees that admit polynomial time algorithms like this for np-hard problems okay so that's what we'll talk about today as always if you have any questions uh you can pipe up but you can also type them into the chat window and I'll answer them as we go okay well this is a very natural problem has been thought about it for a long time say since at least the 70s and I should 70s they defined some people defined a class of grass such I'll describe now called series-parallel graphs and it's more general than trees and what does it series parallel graph well it it's kind of has a recursive definition and what is the series parallel graph so uh this is not an aspect of the recursive definition well the first thing to say is that in a series parallel graph you should always think that there's a vertex called s in a vertex called TS and special source and target or S&T vertices and so now I'll give you the recursive definition so base case is the recursive definition is that you know a single edge with you know the endpoints being called s and T that counts as a series parallel graph okay and now for the rule by which you build up larger series-parallel graphs and how you do it it's sort of built into the name so imagine that you have two series parallel graphs this green one and this pink one and they have their own s and T vertices s and T and this one has s Prime and T Prime then you can get new serious parallel graphs in two ways first you can join them up like this well this is the series way of joining them up so you take 1s vertex and one T vertex and you glue them together okay you line them up like this and that's a series connection okay so if you applied that to just an initial edge you would get you know to a length two paths it's not so a very good drawing should really set up a system where I can draw with my pen right now I'm drawing with my mouse which is horrible okay and the other possibility is a parallel connection where you take the T's graphs and you actually kind of glue together both the s vertices like s to s Prime and you also glued together T and T Prime okay that's like the parallel connection so if you did that with just your base case of two with a single edge then you would get like you know graph a multi graph that has two parallel edges okay but you can combine these rules in multiple ways and bigger build a bigger and bigger graphs and all the graphs that you can get in this way are called the series parallel graphs okay so this here is an example it's nice running example of a series parallel graph it sort of briefly explain how we got it like you can see for example it's got a piece that's like this for cycle how do you get a for cycle well you know first you can take this edge and then you can do a series connection to make a path of length two and then you can do another series connection to do a path of length three and then you can do a parallel connection with a single edge to get like a cycle of length four okay actually let me go back briefly because there's a question question was in the bottom diagram of the sequential graph between s and T prime is the intermediate node s Prime yes and this diagram here this is both T and it's also s prime so in a series connection you take like the T of one series paragraph and the s of another seders paragraph which I call that's prime for clarity and you like glue them together and then that glued together node you know it's no longer a special anymore so we're back into like a plain old white node and like the former s and the former T prime we're like sort of the new s T the new terminals okay so back to this graph you can sort of see that you'll get like an you can get a force icon this way and here we have like another for cycle and so we can produce both of them and then like connect them in series one of the example of what I was just talking about it here it's everything sort of you should mention it on its side like here we took a series parallel graph cuz you know s was f and whose T was G and another one his s was G and whose T was D and then we attached them together and in serious and then the last up here was we you know took a path of length 2 which you can easily construct and we joined it in parallel so I hope that makes some sense let me erase this annotation here okay so uh actually series parallel graph has a kind of tree decomposition and it's sort of like it's like a tree that represents how the graph was created according to the series parallel rules so in this series parallel sort of tree decomposition like each node is either labeled series or parallel and or Salif and Anna leaf you just like sort of imagine in a single edge it's like the base case of a series parallel graph and then like you know it's a series node or a parallel node will always have two children and these will you know point down to the two graphs built up which one combine in series give you the original graph didn't quite say that exactly correctly but I hope you got the gist of it so this sort of tree dumping and decomposition like not only explains what the graph is but it also explains like how you use the series parallel rules to build up the breadth so here's an exercise for you you know we saw that you can compute the maximum independent set on a tree in linear time you should also show that if I give you a series parallel graph you can compute the maximum independent set in linear time actually there's a little question you might have in your mind here I'll let you think about whether you have a question and just say a little bit more so how do you do this sighs well here's a hint the hint is that actually instead of a table that has two entries you may you know M is plus and M is minus like we had for a tree you'll sort of have like a four tables which you might call M is s plus T plus there's another one that's like S Plus t minus another one that's like s minus T plus another one that's called s minus T - okay the idea is that like you know M is s plus comma T - for a series parallel graph it's supposed to represent you know what is the maximum independent set in the graph if you do include s but you don't include T okay and one can this is exercise one can like fill out these mis tables in a bottom-up fashion or the bottom of fashion is visa vie not the graph itself but the sort of tree decomposition and the explanation of you know the series parallel construction of the graph okay so I didn't you know give you all the details there but I'll let you think about it right so this is a sketch of how you can compute things like maximum independent set on a series parallel graph in linear time any questions I was hoping you would have a question there's a little question that you might have which is this exercise is easy to accomplish if you don't just have the series parallel graph but you also have the tree decomposition in your hands you know normally in algorithms world you know if I say you know given a series parallel graph you can do this in that usually the input is just the graph but when I told you how to carry out this exercise I kind of imagined that you also knew the decomposition of the graph now it turns out that you've given just a plain graph like this that is a series parallel graph you can actually compute the series parallel decomposition tree in linear time it's not completely trivial but it can be done so other you can accept that result in which case the whole exercise is fine or you can imagine that just the series parallel decomposition is given to you there is a question are there Syriana planar graphs that are not series parallel graphs that's a very good question there are planar graphs which are not series parallel graphs I will show you one one such graph is you okay for a complete graph on four vertices that really is planar because I can add the last edge like this so wait a minute oh yeah that is not a serious parallel graph we have to think a little bit about why and another example which will encounter later like even more complicated example is let's say 3 by 3 grid graph okay I didn't show the vertices here but this is a 3 by 3 grid graph it's planar and it's not serious parallel it's a good question uh there's some more questions here one person asks it doesn't matter where the s and T vertices are in a given series parallel graph yeah good question so I mean really a series parallel graph mmm wait actually you're making me question myself like if you have a series parallel graph can you now say like oh this is gonna be my s and this is gonna be my tea for a subsequent application on series parallel rules I should know this but I think you cannot I think if the remember like the specific s and T see if I couldn't figure this out on the fly because I'm quite sure that this guy k4 it's not a serious parallel graph on the other hand like this one this graph here he's a series parallel graph so if you could get this graph with this vertex being ass the top vertex being ass and the top vertex being T then you could just you know in parallel add this other edge and get K for but I guess the natural way to get this thing this is a quadrilateral with an a chord you know going from here to here is actually 2 you can 1st is the only way I can think to get it would have you know the sides as SMT so perhaps okay this is belonging my lack of knowledge a little bit but perhaps like a series parallel graph a graph with no specific s and T only cuz this is a series parallel graph if there exists a place to call s and T such that you can build it up from the rules but to be a hundred percent honest I don't know so somebody might want to Google that while I'm speaking there's another question can you construct a tree with some degree more than 3 in series parallel graph ah let me actually uh one thing I can say is shortly we're gonna sort of augment the class of series parallel graphs we're gonna like take them as our nice set of graphs and also allow you to delete vertices if you want and delete edges and there you can build like you know trees of any era T by taking a series parallel graph and deleting vertices and edges so we also wait for the diamond-shaped graph the top and bottom vertices cannot be SN to you right yeah that's my point like for this graph here let me do it in a different color so this graph that has like four vertices it's like a diamond in this like cross edge you can build it up for the series parallel rules where the degree three vertices have our the s and the T but I don't think you can build it up worth having the degree 2 vertices the top and bottom B s and T E and therefore you cannot you know add this extra edge here to make a copy of k4 okay so I guess you know without a series parallel decomposition I guess the series parallel decomposition like remembers what vertices are s and T and if you don't have that then just a clean graph would be said to be serious parallel if you can build it with some pair of vertices being s empty okay hopefully that makes sense and it's all correct eventually we're gonna define a generalization of these graphs which will let us not worry about these details but yeah it's good to get the definition right okay so this is a the other question I was answering you know this exercise where you can in linear time solve let's say max independent set on a series parallel graph it also works for sub graphs of series parallel graph basically these are only make your life you know no more difficult and therefore you can get things like you know these trees\", metadata={'source': 'YnZDAnImkdo'}),\n",
       " Document(page_content=\"okay good so uh that was you know in 70 is people were thinking about that and then 80s people are trying to like you know generalize these ideas further and they came up with the following definition so here is gonna be the main definition slide and it's a complicated definition no doubt about it so get ready it's a Altima the definition of the tree widths of a general graph okay so let's have you say you have some general graph G like this one shown on the Left I mean forget about the fact that it happens to be a series parallel graph with terminals F and D just think of it as some graph so I'm going to find for you what is a quote tree decomposition of such a graph G so a tree decomposition is itself a tree T where the vertices of the tree are called bags and I'll use the notation X for a bag okay and I will now show you the uh tree decomposition T for this graph G over here so this T is my tree decomposition these vertices are the circle so the the bags because you see they have letters in them and those letters are names of vertices from G okay and there are two rules for this tree decomposition T first of all for every edge in the original graph like okay here's an edge between e and g then e and G have to be together in at least some one bag so we have to look over here and find it and like here it is here's the bag that has a and G together in it okay or what's another edge I don't know DG that's another edge here so has to be at least some bag that has a D and a G in it and actually we've got a couple this one has it and this one has it okay let's rule one and rule two is for every vertex in the graph the bags that contain it must form like a connected sub tree of T so let's look at that rule again as well we can take some vertex like B and okay what whoops and I just hit it yeah I like here's B it's not too great here's B and here's B and here's B okay I thought that would be good but let me get a pen out instead this okay here's B here's B here's B and here's B so you see they form like okay I connected subgraph of T here we can pick another vertex I don't know if there's like an interesting vertex here like maybe F we look at all the bags that connect contain it and we see oh it's these ones these three containment and that's connected so great okay that's the whole definition it's it's a little bit complicated it's a little bit hard to like verify that this T has the property for all the edges each edges in a bag and like for every vertex each subtree two bags containing it is connected it's also hard to like it's not like immediately intuitive given a graph G how you'd come up with such a T also the T is not unique in any way there are multiple 3d compositions possible for a given graph G okay every graph can have a tree decomposition for example keeping a graph G you can just put like every single vertex into one bag and your whole tree is like one vertex it doesn't even have edges it's just a big bag with all the vertices that's allowed but as you might guess you're trying to not do that so the width of a tree decomposition is basically the maximum bag size and then you put in minus one for a reason I'll say in a second okay so the width of this tree decomposition is two because the maximum bag size is three and yeah generally you're hoping to you know given a graph G like find a tree tree decomposition where the the bags are as small as possible okay and the smallest possible width is one because the smallest possible bag size is two cuz you know for all the edges like an edge has to appear together in one bag okay and so the finally the tree widths of a graph G is defined to be the least possible width of a tree decomposition for G so although this is not even the only tree decomposition for this G of width 2 it is a tree cut decomposition for this graph G of width 2 and in fact you cannot do a tree decomposition for this graph of which one so the tree width of this graph is indeed 2 ok so somewhat complicated definition I mentioned why did I put this - 1 in here or why did they put this - 1 in here it's put in there so that if G is a tree it's tree width is 1 ok so trees have tree with one so this decomposition is or this definition of tree width has like a long history and sometimes this exact definition with the bags and the widths and supports was first made by Robert Simmons Seymour in 1986 but iron Borden proscar ski defines an equivalent concept also around 1985-1986 but then later people figured out that Holland in 1976 had to find an equivalent concept and Bertelli and Rios key back in 72 it also defined an equivalent concept so even though like it looks very complicated like it must be natural if like everybody is converging on the same definition okay so here just slightly summarized well I've summarized and repositioned our facts about treatments or our definition of treatment let me tell you some facts anyway so fact one as I mentioned is trees have three with one okay and the bags are just the edges so let me show you an example here's a tree gee and here's a tree decomposition for it so you see I took like this first edge a see I could've taken any age but I took a see I made it a bag and then I took a B I made it a bag okay and I have to do all of a sedges here here I took a D and I made it a bag I've to do all of edges here because of this rule that you know for every vertex like a the bags that contain contain it have to be connected in T and then having done that I was like all right I should also do C's edges because I'm cut see here so I did C's edges and C's bags are here and okay it's not too hard to verify you can do this for every tree but with is one okay good so in general like the tree width is supposed to somehow measure like how it's sort of similar ition the graph is to being a tree and the smaller the better actually there's some more graphs that have tree with one namely forests okay if you have like some disconnected trees then they have tree with one it's kind of a stupid well that's stupid but like generally we're not too concerned about disconnecting graphs but anyway you have true it's one if and only if you're a forest question from the audience 4.2 you mean that all I'm reading question you may not all the bags containing V lie on a single path in T ah good point they don't have to be on a path maybe all the examples I've shown have shown a path but it just has to be a subtree it has to be a connected piece okay so like the bags containing a vertex could look like you know this okay maybe I haven't shown an example what that occurs but it doesn't have to be a path just a subtree okay okay so ah another fact is you can say like that's a nice characterization of tree with one there's also a nice characterization of tree with two are pretty nice i graph its tree with two if and only if it's a sub graph of a series parallel graph okay so series parallel graph subtree with two and any sub graph as uh also tree with two and this is all the tree with two graphs it's uh this one is not incredibly easy to prove the thing about tree with one is easy to prove this fact about tree with two is not easy to prove and once you get to tree with three there's no like catchy explanation of what a tree with three graph is it's just it is what it is okay uh let me crime a few more facts into the slide I want to cram them on because you know this this definition is so long it's good to have it up on the screen so one other fact is that if you take a graph with certain tree width and then you delete some edges it cannot increase the tree width and actually that's very easy because if a tree dogged decomposition for a graph and then you delete some edges from the graph then your tree decomposition is still totally fine you don't have to do anything to it because the only thing that deleting edges does is you know puts less restrictions on you you know they used to have to be these edges you delete it you just have to be in a bag we're still in some bag but they don't need to be anymore it's also true maybe this one's a little harder to see but it's also true that if you take a graph and contract an edge this also doesn't increase the tree with I don't you remember or know what the definition of edge contraction is but like basically if you have a graph and you have some edge in it and you basically take the two end points this graph or edge and you glue them together and then you get rid of the self loop that's thereby formed this sort of makes a smaller graph and this operation also does not increase tree width that's a fact and one more fact which is not even this one is also not a totally immediate but it's true fimA graph G and you have some clique in it then what too much effort you can show that the entire clique has to be together in a bag there's no way to split it up and for example as a consequence if you have the complete graph on n vertices like a cleat on n vertices all those vertices it has to be this trivial thing I'm just like well I put everything into one giant bag and then that bag has n vertices the tree width by definition is n minus 1 ok so a tree with that's sort of like the worst case the worst possible tree with for an N vertex graph is n minus 1 and it occurs if the whole graph physically ok great I should emphasize that unlike in spectral graph theory which might be on your mind from the homework we're just considering like ordinary grass without loops or multiple edges well actually it's fine I mean series parallel graphs can naturally have multiple edges so you go to multiple edges I'd also doesn't make a difference at all for the definition of tree woods right for every edge if it's together in some bag then you can have lots of parallel edges and you're still satisfying this bullet point one ok so actually um sure it takes a while to get used to and actually I'm gonna spend some slides just like telling you more equivalent definitions of tree widths and I won't prove anything but all these definitions are just sort of warm you up to tree width and of course the point is it's gonna be that as we saw for like series-parallel graphs and trees that I later explained the point is that lots of problems on graphs or constraint satisfaction problems can be solved in polynomial time for graphs that have constant tree widths okay so a shortest way to like kind of explain in quotation marks like what is a trie Witzke graph this is not precise but I'll say some words that kind of more or less make it precise and the next slides they said like what is a tree you could think of like a tree is sort of formed by like you're taking you recursively recursively glue together a single vertices like you have a tree and then to make a new tree you like introduce like a vertex a single vertex and like glue it to our vertex well attach it by an edge to a vertex you already have or maybe you glued together a single edge onto the tree you like crafting vertices onto the trees series parallel graphs are kind of what you can get my life starting from like an edge and kind of like graphing on like edges like pairs of vertices but you know zone quotes so it's like sort of true and in some ways like tree Witzke graphs are those you can get by like recursively gluing together like graphs like at K vertices I'll say some things that uh try to make this more precise okay so in order to do I want to make another definition for graphs a graph G is called chordal if every cycle in the graph of length at least four has a cord and where's the cord it's an edge connecting to non adjacent vertices on the cycle so let me get our favorite graph back out here this is not a chordal graph because for example here's a cycle of length four and it has no cords well yeah it doesn't have a cord for a cord you need either the G a edge or the BD edge okay so that's not a chordal graph ah next definition a triangulation of a graph is just refers to the process of adding edges to a graph in order to make it chordal okay so if the graph is not coral is sort of lacking chords there's more than one way to do it but you can put edges in to make it coral so I'll give you an example for this graph just now I've added three edges here the yellow ones and I've caused this graph to be coronal now if you look at this graph you might be like wait did you make a mistake because take a look at this cycle gee this five cycle GD c f e g GD c f EG yeah it does have oh wait sorry that has a cord okay I made a mistake - my mistake so that indeed has a cord but let's take a look at this psycho mm from f EG d okay it's not like the greatest pan of all time but look take a look at this for cycle f EG d it doesn't look like it has a cord but it actually does have a cord this this edge here is the cord from f and g it just it's drawn outside you know the physical region but graph theoretically it's a cord then we try to clean that up so just f EG d is a cycle but FG is the cord so this is a now a chordal graph and the process of adding these yellow edges is called triangulation ok so here's another fact about tree widths oh wait there's a question here this triangulation have to make triangles yeah it effectively does because it basically you know takes any cycle in order to triangulate the cycle you have to keep adding chords until like the only cycles that are left are like three cycles ie triangles so like it's a pictorially clearer here on this like four cycle down here whoops such great drawing we took this four cycle and you know we added this edge in the triangulation and now we have these three triangles bgd and BA D okay if you stare long enough at this FEG DC 5 cycle first we added this yellow edge here which gives us the strangle fcd I mean just hence the name and then we still have this cycle D F eg and but this edge F completes the triangulation so we got three triangles I guess FEG and should be D F G yeah D F G is a triangle okay so a graph has tree width at most K if and only if it's possible to triangulate it so the resulting triangulated graph has maximum clique size k plus 1 okay so just on this example here this blue graph we kind of knew what had tree with two right so it's supposed to have a triangulation where the maximum clique size is at most three and that's true if you stare at this graph it does have some fleex of size 3 the triangles but it does not have any cliques of size 4 ok so that's an explanation of it so we had we like triangulated in a bad way I mean you're allowed to like add this edge to in your triangulation to get rid of to make sure every cycle has a quarter had we done that then okay we would have had a k4 down here I mean a complete four vertex graph but we can get away without doing that ok so that's a fact in some sense what this means is that like chordal graphs are sort of maximal with respect to adding edges for their tree widths okay what what that means is like if you have a chordal graph and it has some tree with t then um you know adding edges will make it no longer have you know increase its tree width so coral graphs are kind of like maximal tree widths graphs for their tree width and uh another manifestation of that is like here's a definition or a statement about coral graph so graph G is coral if it has a tree decomposition where every bag is itself a cleek okay and then these cliques you know in this situation if the graph has tree with K these clinkz will have size k plus 1 and they'll be making our bag and then the minus one will you know illustrate that you have tree with okay okay so that's another fact to kind of get you used to this and finally a graph is also called old if it has what's called the perfect elimination ordering and what is a perfect elimination ordering it's a way to take the graph and go through the vertices in some order such that when you um you kind of start with you kind of include the vertices in the graph one by one and every time you include a new vertex its edges should go to click okay so that's not super clear let me try to describe it here so the perfect elimination ordering for this particular graph G including these yellow cords which is now chordal graph is exactly given by alphabetical order I set it up abcdefgh okay so that means you start with H let me get okay you start with H and then you add g and g is you know connected to H and then you add F and when you add f it has edges to a to click and when you add e in it also has edges to a to click okay remember all these yellow edges are considered part of the graph okay and then when you add D D has also edges to it to click the edges that it has sort of currently are to F and to G F and G are indeed added by connected by an edge and then when you add C in it's connected to F and D and indeed D and F are connected by two cleek and when you add B it's got out just to D and G and D and when you add a it's got adjust to and D which are connected by edges okay so this is fir tree with two okay and if you were talking about a tree with three graph with a perfect elimination ordering like every time you included a new vertex it would have edges to a three clique that was already in the graph okay so this is the sense this is like the best sense in which like you can say that like tree with scare graphs kind of form by like attaching vertices and groups of K so this perfect elimination ordering is like so like a maximal tree with two graph like this chordal graph here is formed by like you know you have a graph and like every time you add in a vertex it has to have edges to a to clique that's already in the graph okay and then you add another vertex that has an edge to a two clique in the graph and thereby making a three clique and you add in like another vertex forming a three clique and another vertex forming a three clique and that'll give you like a maximal tree with to graph a portal graph and then finally to get all tree with you know two graphs you take these constructions and now you're allowed to delete some edges okay so that's maybe it's not immediately clear the first time you see it and it's a little hard for me to get it across this in this PowerPoint session but that's the story any questions let me give you yet another I mean you know because it's like kind of complicated let me give you yet another equivalent definition of tree widths and in terms of cops and robbers game this is a technical term cops and robbers there's different types of cops and robbers games characterizing you know different properties of grouse but here I'll tell you one so I'll explain it to you in words but like the three or so rules of the cops and robbers games are that the robber is super fast and the cops are in helicopters and the robbers can see the cop when it's trying to land so what do we mean by this so in this cops and robbers story you the graph is like you know like the game board or like some city streets and there's a robber and the robbers you know can start at some vertex so like maybe the rubber looks like an X and you have some number of cops so like maybe in this game I'll have two cops okay and the cops I'll have helicopters and they're like flying in the air and like the cop any co-op can land you can pick a vertex and try to land on it so maybe the cop and their helicopter is like I'm gonna land on vertex B and basically as the cop is landing the robber is allowed to run if the robber wants and the robber can even see where the cop is gonna land and if the robber is there it can like run away and the cops can also leave a vertex that can fly up in the air you can imagine like you know the cops got to turn where at every turn like that their land on a vertex or they have a helicopter go up in the air and every time the cop does something the robber is allowed to do something - and of course the cops are trying to catch the robber ie land a helicopter on top of the robber okay so maybe like now the the second cop will try to come in to vertex G and as it's landing the rubber spots that and you can run anywhere along the edges but it cannot run through a place where a cop is okay so if we this cop really landed here on B then the robber could run down from G to D okay and now we have a cop at being a cop at G and now maybe one of them maybe the cop at B will like fly up into the air whoops and try to catch the robber Addie but as though they're landing the club is landing Addie the robber so now we have a cop here but the robber can like run away maybe I'll run to see and so forth so actually you can tell in this graph I think it's not too hard to tell that like two cops cannot catch the robber like the robber can always evade them but it's a theorem not in any graph the tree width is at most K if and only if k plus one cops can win the game okay so we know this graph has true it's two which means it should be that three cops can win this game and it's not hard to tell that three cops can indeed win this game on this particular graph and in fact you can kind of get the strategy for the cops out of the tree decomposition this is the tree decomposition for this graph actually it's not the same when I showed before because I was intentionally showing like an over-elaborate one before but like this is also I mean I had some junk hanging off here but this is also a valid tree decomposition for this graph and I'll just illustrate how the cops win here by like looking at this tree decomposition so B D and G are like where the cops should go first so like you know the cops will go to like B and G and D okay and while they're doing this the robber you know can run around whatever the rubber likes but okay this is gonna be the cops first no so let's say the robbers you know decide to land on E okay so now the you know the cops look at this diagram they see like ease down here so they're gonna go into this piece of the tree which is rooted e of G which basically means they're gonna try to change their position to D F G so this cop at B it's gonna fly up into the air and go to F now you can see this kind of smart strategy for the cops right so like this cop that B is gonna go away it's up in the air and the robber can run around if it wants but you see G and D are blocking it from getting into this piece of the graph so the compiler can only run around like this F H E C etc so maybe the robber runs to X and the cop comes down at F so now we have a cop at F as well okay and now the cops say oh look there to see so I'll find out where C is here it is and so we can just go directly down here and we should pick up we should get rid of G and have G go to C and indeed that's a smart move it's pointless for the cops to hold on to G now so this cop in the helicopter G will fly up into the air C can run around but it's like blocked by F and D and now the cop can land on its head and win the game okay so that's like actually like a fun way to try to understand what uh the tree with the graph is like also nice to dimension you can think about like why is it the case that in a tree which has tree with one two cops can always win well you know you land a cop down the tree Robert has to go into some sub tree one of the two sub trees hanging off where the the cop is and then whichever substrate goes into like the second will fly like you know just like one edge down in the direction of the robber and the cops will keep like moving down edges and catch the robber okay so one direction of this proof is easy that if the tree width is at most k then k plus one cops can win basically you just have to make precise this thing I this story I told you about the cops using the tree decompositions as a strategy I'm going on the other direction and proved by Robertson and Seymour that if k plus one cops can win the graph s tree with at most K that's pretty hard it takes a paper shortish paper about a paper so one nice corollary of this is it helps you understand the tree widths of grids and actually grids play a very important role in the study of tree width as all indicate on this slide so one thing you can so this is a five by five grid over here and a three by three grid down here I put them up just read out something to stare at while I'm talking so if you have a G by G grid you can see from this copper utz and robbers game that it's true which has to be at least G minus one so why is that well if you only have G minus one okay so suppose uh for a contradiction that this G by G grid had tree with G minus two then G minus two plus one aka G minus one clops could win do you - one cannot cops cannot win on a G by G graph because if you have G minus one cops like you know if I have four cops here they're always gonna leave open some well I think I say you circles four cops before they're always gonna leave open some row and some column so maybe a cop here and here you know where they are there's gonna be some open row and some open column I guess maybe like here and the robber can sit on this X and basically the cops you know the robbers can always easily go to this open row and column okay so in fact well that's a little exercise for you this argued it's not quite sharp the tree width of and G by G grid is actually G so as an exercise for you you should prove that in fact even G cops cannot win in a G by G grass and so if you have this little 3x3 graph here apparently even three cops cannot win this game that's not too hard to see at least in this example of let's say you've a cop here here and here okay and the robber is sitting at the center okay well when one of the cops flies up into the air like maybe this cop flies up in the air and attempt to land on the robber in the middle well the robber sees that it's coming down at the middle the robber can run - oh well a variety of places it can run here for example okay you need a slightly more sophisticated robber strategy in order to make this work well I'll leave that as an exercise for you okay so what's interesting though is that um this is what you know some people would judge to be a pretty simple graph like a grid graph it's like a pretty easy graph like it's planar in fact it has maximum degree 4 - yeah it can have like arbitrary sort of large true it so you know you know with this do you put your great bread is a degree for planar graph and the tree width is square root the number of vertices so it just goes to show that there are you know relatively simple graphs that still have quite large tree width and they're actually converses to this theorem that basically show if you have a large tree with graph it must be because there's some kind of embedded grid in the graph so this was first brewed by Robertson Seymour with not very good parameters and then subsequently improved in like the 21st century by several papers involving Julia to Joy the most recent of which is to join 10 from 2019 which shows the following well there's some straight comma here can erase that if you have a graph with tree with T then G must quote unquote contain a G by G grid in it to full G by G grid where G is like T to the 0.1 one T to the one ninth and what does contain mean it doesn't literally mean contain but it means contains that up my as a minor if you know this terminology it means you can get the grid from your graph by deleting vertices deleting edges and contracting edges okay so you looks like that you know famous skorkowsky theorem that like a planar graph your planar graph if and only if you don't like contain a copy of K 3 3 or K 5 it's again the same sense of contain and by deleting vertices deleting and contracting edges ok so this kind of shows that like having large grids inside you is like exactly the barrier to having small tree would Canyonlands got like even a polynomial relationship between these two parameters\", metadata={'source': 'kEnDGTwSDXY'}),\n",
       " Document(page_content=\"okay so in just a moment I'm going to tell you about how you know the whole raison d'etre of tree with that how you know if you've got a graph with small tree widths and you have the actual tree decomposition in your hands then you're in great shape to solve like all sorts of like np-complete problems efficiently I mean if the graph has small tree widths but this raises the question you know if I just give you a graph can you figure out what is the tree width or you know if I give you a graph and I say like this that's true with ten can you actually find the tree decomposition you know with bags of size eleven because this is an important aspect of algorithms that are going to perform well on found a tree with graphs the answer is yes there are pretty good algorithms I mean they're quite good algorithms for given a graph small tree with like finding the tree widths and finding a low with decomposition of the graph yeah there are many many papers on this so I sort of only give you some highlights so first are these two results if I give you a graph G and asked you I mean find its tree width T first of all it was shown in 87 then you can do that in basically order n to the T time well n to the T plus two time okay and it's basically the same question of deciding if a you know you're given a graph and a parameter T you want to decide if it has tree with T and it's not too hard to show this I mean for example if you use the cops and robbers characterization you know if you want to decide if the graph has true its T therefore you want to decide if like T plus one cops can win the game you can basically like enumerate all possible cop strategies in something like n to the T plus one time because like each cop has to be on like one of the N vertices so let's pull the real time if T is constant but you know if you know T is 10 which is hopefully reasonable this is like n to the twelfth time it's pretty bad on the other hand Bolender and 96 show that you can get like a linear time algorithm linear and n for any constant T so any constant T like 10 you have an over n time algorithm now the constant is not super awesome it's like exponential and T cubed but you know it shows that you know you can decide you have a tree with free graph in linear time you can find the tree decomposition in linear time uh that's something these are the best things kind of known if you want to get the exact tree woods but actually um you know came across this tree with ten you know let's say you've a graph in its true tree with this ten and you find a true talking to decomposition of size 15 a width 15 what's also usually pretty good so in other words we don't really mind finding like approximately optimal tree decompositions and for that you can get a lot better results so this result of Bolender a drawing a druggie foam in looks down and he'll truck from 2013 give you a sort of better linear time algorithm it's only two to the order tea time singly exponential in tea time algorithm but they don't give you back a graph a tree decomposition with with tea they just give you one with with something like five times t what okay for many applications that's fine too to have like an order t kind of approximation and finally there's a relative a flag aha gie and james lee that shows in just flat polynomial time for any tea at all which is nice you can get a tree decomposition of with tea times root log t so it's like almost order tea but it looks work see in polynomial time even if t is like bigger than log n which is nice okay so long story short there are like good algorithms for finding like the good tree decompositions of bounded tree with graphs okay so let's come to the main point of the matter which is algorithms on bounded tree with graphs so I'll just tell you a couple of such algorithm or theorems along these lines so first of all is the one we mentioned in fact last lecture which is that if you have a CSP instance of any sort and it's primal graph has tree with T then you can decide whether the instance is satisfiable or not in time basically order well like n to the T basically n to the T plus order 1 ok so for example you know if I give you a 3sat instance and you look at the graph and it has tree widths like 10 then you can Impala mealtime determine whether or not this 3sat instance is satisfiable so that's pretty cool and there are analogous results for like a related results for you know problems that are not quite CSPs so there are many theorems like this but of the form if you have restrict attention to graphs of constant tree widths that in polynomial time you can decide color ability or find the chromatic number you can determine if they have a Hamiltonian cycle you can solve TSP you can solve max fleek you can solve the vertex disjoint paths problem you do lots of stuff many many things not everything there still like np-hard problems like crumbs which remain and be hard even on like series parallel or you know constant tree with graphs but most problems actually become polynomial time unbounded treatments graphs I also wanted to show you this one like even more crazy theorem you have to necessarily understand it exactly but it's pretty cool it's called Courcelles theorem from 1990 it's a theorem about it's like a meta theorem about graph algorithms so it goes like this let f be a formula and extend a monadic second-order logic so I know you all know it extend a monadic second-order logic is but I'll remind you just in case I'm joking so it looks like um you know like sort of first-order logic formulas but you can have existential quantifiers and Universal quantifiers over subsets of vertices and also subsets of edges and you can also have predicates so like decide whether you know UV is an edge or like whether given an edge Eevee like whether their incident or not okay so you can write down like a complicated formula like this one this one says there exists a subset of vertices called C such that for all vertices V and C there exists you wanting you to also in C such that you want is different from u2 and you want is adjacent to V + u2 is adjacent to B so I'll let you think about what that means in human language as I finish this slide now what is what property of a graph is this long formula expressing any way you can write down any formula like this in this like language of logic with quantifiers over edge sets and vertex sets and so forth and of course else theorem tells you that then for every constant T there's some big constant K depending on T and F such that for the problem of giving a graph of tree with that most T doesn't satisfy the formula F or not there's a linear time algorithm for deciding that okay so basically it's simultaneously giving you like linear time algorithms for like all sorts of graph problems any graph problem like color ability for example you know holes or having a Hamiltonian cycle holds or being connected holds like all sorts of properties in it you can express in this language they all have linear time algorithms by virtue of crystals theorem the dependence of this constant K on T and the formula F itself is truly horrendous but it exists so yeah if anybody knows what property of a graph this expresses please type it in the comments yes somebody in the comments got it right it's expressing just as very simple property that well they're not the graph contains a cycle okay good so let me the main thing I want to do for the rest of this lecture well there's some wrap-up remarks but the main thing I want to do is just illustrate to you how this works I won't even prove a theorem but like I'll illustrate how in polynomial time you can solve the 3 colorability problem on graphs of bounded treatments ok so like what I'm saying is you know there's an algorithm which given any graph if it has tree width at most 100 it can determine whether or not the graph is 3 colorable or not and it runs in polynomial time although it's straight up again on this you know our favorite graph which actually has tree with 2 but hopefully you'll get the idea this could be a dynamic programming algorithm what's surprisingly I'm not too different from the ones we saw for trees in series parallel graphs ok so the first step is to find a tree decomposition and we talked about how you can do that in polynomial time but the second step is to take this tree decomposition and like make it like a bit nicer and that'll like sort of the nice ChiCom decomposition will help you do the dynamic programming step at the end so what is a nice what about a nice tree decomposition it's a binary tree the tree decomposition should be binary so every vertex has at most 2 children and rooted and each node in the tree decomposition has one of four types and I'll call these types leaf introduce forget and join so a leaf vertex just looks like a bag that contains one vertex that's very easy and an introduced edge in or interest vertex in a tree decomp nice tricky decomposition is one where the bag the bags one you had the back is one child and that child is just missing one vertex from its parents so it's sort of like in going from the child to the parent you introduced the vertex and VW conversely a forget vertex is one where like it also only has one child in the tree decomposition and like when you go from the child up to the parent what's going on is you're deleting one of the vertices in the bag okay so we forgot W here and the last kind of node in a nice tree decomposition is called join and join is the only node type that has two children and is particularly simple it just has two identical children which is allowed and this is an exercise for you if you have any graph with n vertices and tree with T it'll actually always be a tree decomposition that has like at most n bags but these nice tree decomposition ones will have a few more bags you can always find a nice tree decomposition with order T times n bags okay but we're always thinking of T is a constant so like order TM is not a big deal and not only that you can convert any tree decomposition into a nice one in time like order T squared n and again T should be thought of as a constant so that's also linear time okay so uh hold on to your socks here's a nice tree decomposition for this graph okay so then you know it's nice it's nice for further purposes but not for drawing you know it's great much crazier but the whole point is like to make the structure very simple so you see it's a binary tree and I fact most vertices have only have two children most vertices have most vertices have only one child and each vertex has a type and I've Eli pointed to a vertex of each of the types so this top root is a joint vertex because you see it's two children are identical and this vertex here B D G is an introduced vertex because you know it's child is just missing one virtus vertex like in going up we introduced B this DG is itself up vertex forget vertex because like in going up to it we forgot about F and you know here's a leaf vertex at the bottom that has just one vertex in it okay so is a valid tree decomposition for this graph over here and it's a nice one I should say okay and so now I'll tell you how the dynamic program works you could basically figure it out yourself but I'll just go over it so well this is an important step we're gonna get always gonna be filling out a table and the table called s and for every bag X and for every coloring of ooh this is a mistake this should say not X we should say V actually wait a minute I think it's not a mistake hold on let's go on and leave that um for every X sorry C should be coloring of the vertices in X okay for every bag X and for every coloring of the vertices in X we're gonna find a table entry s X comma C and here's the meaning that we want ya s X comma C will either be true or false and it's gonna be true if see a coloring for just the vertices in X can be extended that's the spelling error extended to a proper coloring of all the vertices in like X and its descendant bags and otherwise it's false okay so for example here let's say this is X this guy and we're gonna have a coloring for this bag so this coloring might be like C might be like D is red and F is blue and G is red terrible Mouse drawing here okay so this is C and for every X and every C we want to have one entry in our table notice that you know bag in this picture can contain up to three vertices so the number of coloring see is like three to the three and in general if the bags have like K vertices in them the number of colorings see they you have to worry about is 3 to the K so this is definitely ain't gonna be an algorithm that's exponential in the tree width the bag size but that's okay we're always thinking of that as a constant okay so for every bag and for every coloring of the vertices in the bag you want to fill this table with true if this coloring can be extended to a valid partial covering of the original graph over here involving all the descendant vertices so if you look at X's descendants you see it's also got e in it and it's also got H in it so we have to ask ourselves is there a coloring of the original graph G this is G over here which has D being red we can try it out D being red and F being blue and G being read well actually the answer is gonna be false because we've already got a contradiction here but if we hadn't noticed that we'd also have them worry about is it possible to also find a color for e and a color for H okay so this entry would be false now if we change G to be green G in green so then this would become green let me erase it we'd be asking to assign colors to E&F such that you know this coloring with f being blue G being green d being red and enough being something is valid coloring I guess the answer is yes because you could put both of these two red Ian H could both be red and they're not touch to D so that would be fine okay so this is the table we're trying to fill out and uh I don't know if I have this on the slide or not yeah but you see if we have this table filled out then we could know whether or not the graph was 3 colorable because we go to the root bag X and X is descendants include all vertices and so we could just try all little 3-way colorings for the vertices see for the colorings of the roots you know there's three to three of them but like the entry will tell us for each of these choices for a coloring for B D and G in the root can it be extended to a coloring of all the vertices you know descended from this bag which is everything so it's asking if you know the graph can be three color at all with this fixed three coloring for their root bag vertices okay then you can just try all three to the three possible root bag vertices and see if any of them give you a valid if any of these entries in the table are true good so now I'm gonna tell you how a dynamic program can actually fill in all these table entries from the bottom up I should also mention that like you know I'm only describing like how the algorithm can decide whether or not the graph is 3 colorable but it's not hard to augment the algorithm as well so that actually keeps track of like a three coloring well I guess you can always give it an Oracle for deciding 3 coloring you can generally reduce to finding a 3 coloring in polynomial time you have to preserve the tree wits with your reduction okay enough of that muttering oh wait there's a question the question is is it easier to say you can solve 3 coloring or similar problems instead by somehow applying queer cell to it you can't apply core cell for three coloring you may check that the property of being three color is expressible in extended second-order monadic logic or whatever really even I would say like there exists a set cease of vertices cease of read in a set of vertices see some blue and instead of vertices see some green such that first of all they should be disjoint so like for all you it's not the case that you is in the red set and use the green set and so forth and they should cover everything so for all you either using the red set or it's in the green set or it's in the blue set and for all vertices you V like if you V is adjacent and you know they're not both in the same set so all that can be expressed with second-order logic probably if you then apply core cell serum azzam so that will like reassure you that 3 colorability is one of the things that you can solve in linear time on down and treats graphs if you literally plug it into Cora cells theorem you'll get like a running time which is you know like 2 to the 2 to the 2 to the 2 to the 2 to the 2 to the 2 times and something insane like that you know this algorithm I'm gonna tell you now is like actually perfectly reasonable you know running no not much worse than 3 to 3 times n ok good question so let me now tell you how you can fill in these entries in a bottom-up fashion and uh you just go through uh you know indeed the leaves in a bottom-up fashion filling up the table and you have a like a you choose sort of a different thing depending on which of the four types the node is reserved for types leaf intro forget and join so let's see suppose X is a leaf like this a leaf down here and C is supposed to be a coloring for this vertex and yeah they're all true because they can always be extended to a valid three coloring of the graph there's nothing special about that so that's good ok what about an intro vertex like this one here so how do we fill out the S for this person so an intro vertex it looks like this you know the top bag contains the silver two vertices X which is equal to the the child bag why union some vertex V so in this case the little vertex V would be D so how do you fill out an S of X comma C well remember C is a coloring for Oliver to season X well you're gonna be false if the little coloring for vertex V is the same as the color for you for some vertex u that's attached to V in the graph okay otherwise you may or may not be extendable but you basically just have to look down at a your child Y and look at its Val entry and see if the coloring that you have restricted to you know so your coloring C gives you a color for D F and G you have to see if the coloring when restricted to just F and G can be extended to a coloring for all the vertices down here okay so if that's true and if like you know you don't also have like some local and consistency by C's coloring of D then your coloring for a D F and G can be extended to all these vertices okay you shop to use like sort of basic similar reasoning for the other cases too so let's suppose X is a forget node like this one where like you're sort of having gotten to front you know this everything figured out for down here you're moving up you're forgetting age okay well for a set of vertices this bag X and for a coloring for F and G the entry will be true if for this for all ways of extending C to a coloring C prime of all the vertices down here so there's like three choices you have to take your C for this bag X and cuz they're all three ways of extending it to three because there's three colors to a coloring of FG and H you can look down in your table for s of this why with all the three extensions and see if any of them is extendable if like you ever have any trees and if you do then great that a restriction see is extendable to a valid coloring of the vertices in this subtree rooted at X at X okay and finally if you have a joint vertex like let's say the root vertex up here there's also not too hard so if you have this joint vertex X in a coloring see for the vertices in a B D and G then you just have to look to see you hey can this coloring btg be extended to a coloring of the vertices down here and a coloring of the vertices down here and you have to think a little bit about why this is okay basically it's because due to the nature of tree decompositions the only way this piece of the graph sort of the left subtree of the graph interfaces with the right subtree of the tree decomposition is through b b and g okay let me this is a do to the tree decomposition nature let me just take it explain why you look at this left subtree of the root here it's like a B D and G so it's like this piece here if you look at the right sub-tree it's got like B D G and then like H E and C so it's kind of like this piece here and it's like the nature of the tree decomposition that like this right piece this giant circle only interacts with this left piece like a B D and G through B D and G okay so like that's why it's okay to only check hey can I assign my if I take this coloring for bead G and D you know if I can extend it to a coloring for this left piece with a and I can extend that same coloring of B D and G to a coloring of the right piece with like c EF and H then I can get an overall global coloring okay these pieces don't interact and again this is because like the street decomposition property but for every vertex the the bags in which it appears from a connected sub tree okay so I'm not proving it to you here you have to think about that but that's how it goes okay so the overall time of this algorithm is like in general it's like order o twiddle of 3 to the tree widths like the bag size times n okay so just one more slide and it's just like one more fact about tree with and then I'm gonna stop I mentioned before that planarity you know planar graphs could have huge tree width that's true but what's nice is planar graphs kind of almost have small tree width and so in fact a lot of algorithms that work in palm you'll time for bounded tree woods graphs have some kind of extension to planar graph so this is a one route to trying to get algorithms efficient algorithms for problems on planar graphs so just say two theorems of this showing the relationship what does the theorem is dude Epstein it's very simple it says you've a planar graph if it has small diameter than it has comparably small tree width okay so if you have a small diameter a small diameter planar graph then it's got small tree widths and you can do all the algorithms on it and not a replay graph huh small diameter but you might imagine you could hopefully divide a planar graph up and pieces and there's a few uh to this effect by Baker she proved in nineteen ninety-four that if you have any planar graph G and any parameter K that you want and in polynomial time you can partition the edges of G into K sets such that for every possible way of deleting one of those sets the resulting graph has tree width order K so you know you can take K to be 100 and says you can partition G's graphs into 100 pieces and then for every way of deleting one piece one set of edges and retaining the ninety-nine others the resulting graph has tree width you know constant order 100 and it was also subsequently proved that this also works for contraction in place of deletion but one thing that's cool about that is it's a great thing to do is take K to be like 1 over epsilon for a small parameter epsilon and then basically it means you're deleting like an epsilon fraction of the edges whichever a set of edges how do the 1 over epsilon is smallest it constitutes only at most an epsilon fraction the edges and so basically by deleting an epsilon edges you can get to a graph with tree width order 1 over epsilon this is all for planar graphs ok so it means that like if you were willing to tolerate screwing up epsilon fraction of edges you can apply any algorithm you want for a planar graph so for example using this idea you can find and 1 minus epsilon approximate maximum independent set in a planar graph in linear time to to the order 1 over epsilon times n ok I'll have as always after I stop the recording here I'll take your questions\", metadata={'source': 'Vf-UJlBM3hE'}),\n",
       " Document(page_content=\"okay so this is lecture 23 it's gonna be sort of first in a new small unit now that we're done with CSPs and linear programs and so forth we're going to talk about communication complexity and information theory and maybe blending theory but today's we're gonna start with communication complexity so this is a great topic there's two books I can recommend the first one is the super classic book while Kucha Levinson Nissan that's a you know real classic it's been around for a really long time on the other hand there's a really really new book by two experts the new brow and your to do a yeah that you can check out now so communication complexity is a really beautiful really an abstraction and an abstract sort of model for computation that has tremendous use throughout computer science Theory infiltrates into almost all branches of it but I just listed three particular areas that it's used a lot and they're all kind of in algorithms areas I namely the study of data structures the bounds in the size of linear programming relaxations and in streaming algorithm but really this is just three out of many many many possible application areas of communication complexity okay so we actually already talked about communication complexity once in this course in lecture 10 where we briefly showed an application of univariate polynomials in communication complexity to determining the two part of communication complexity of the quality function I'll remind you what the those things mean as we go along in fact let me just set up here the basic model of two-party communication complexity and I show that there are extensions of this model it's a multi-party communication complexity and usually proving theorems and the multi-party setting is much much harder so we're only gonna talk about to cut party communication complexity in this lecture okay so our two parties are as always named Alice and Bob and they were cooperating to jointly solve a task and the task they want to solve is to compute a function f mapping a pair of inputs x and y into an output Z so they both know this function f they're both thinking about it in their minds and usually and I think almost exclusively in this lecture the sets capital X and y will be the set of n bit strings and Z will be the set of bits so Alice and Bob will be trying to compute a function that takes two n bit strings and outputs a single bit and they can make plans beforehand on how they're gonna you know achieve this but when it comes to actually doing this computation they're gonna be physically separated you imagine they're physically separated from one another but they can communicate there was some channel of communication between them and when the story begins Alice gets her own private input X you got a pin out here little X a subset of big X so I almost always think of this as little X is an N bit string and Bob gets his own end bit string and now their goal in life is to try to compute f of little X and little Y and what they can do is send messages to each other so they can communicate back and forth and their final goal is for them to both know the correct value of little F applied to their private inputs X and Y and as the main communication complexity suggests well I didn't write it here maybe I'm missing something on a slide but I think I'm missing a bit of my slide the point is that the only thing that we charge for against Alice and Bob is how much computation they do or so how much communication they do conversely they're not charged at all for how much computation they do they can do you know computationally speaking on their own time as much as you know it's complicated of complicated computations they want we only challenged them for the communication that they perform okay so we can assume sort of without loss of generality that whenever they communicate a bit to each other a message to each other it's one bit long so these yellow arrows represent one bit communications and they need not alternate either as we'll see okay so let's to some examples and the first example is the one that we talked about in lecture 10 where F is the Equality function so Alice and Bob both have their own private n bit string they're trying to determine if these two strings are identical or not and deterministically and I say this only because we're gonna shortly make a distinction between deterministic communication algorithms and randomized communication algorithms but deterministically n plus 1 bits suffice and are also necessary so in fact whenever Alice and Bob have n bit strings as their inputs and plus and the output is a single bits as in this case n plus 1 bits of communication always suffice because there's a very trivial thing they can do Alice can simply tell her entire string X - Bob that's n bits of communication and now Bob knows both little X and little Y he can compute f of little X little Y doesn't matter you know how complicated that competition is so he can get the answer and now he knows the single bit answer and then he just has to send this bit back to Alice okay so that's n plus 1 bits of communication suffice for any function like this mapping 200 strings to a single bit and the point is that it's gonna be not too hard to show that in fact this is necessary for computing the Equality function ok sure mention also that or remind you also that like you know Alice and Bob are really cooperating here there's no issue of like distrust or anything like this or privacy they're really trying to do their best the only thing we're keep worrying about is how many bits they communicate so it looks like nothing too trivial can be done in the case of equality but as we saw in lecture 10 if you admit randomization if you allow randomized communication algorithms then actually there's something dramatically better actually there's a communication algorithm a randomized communication protocol that uses only order log in bits and it gets the correct answer with high probability for every pair of inputs little X and y so I just want to stress that like in this situation little X a little wire not random strings or anything this randomized algorithm that we saw and I'll mention well a different way to do this in this lecture it works literally for any pair of strings X&Y even if you know x and y only differ by a single bit and this Login bit communication probability protocol still detects that they're different you know with high probability okay I have a question the question is what is the motivation for setting up communication as computing a common function as opposed to sending a message for instance yeah so let me first start up by saying that okay this is a good point right so first let me start off by saying that one thing we're assuming here is like a perfect channel there's no noise or anything so we're working an odd in the setting let's say coding theory it's fine we are also not yet working in the setting where maybe Alice has like a random inputs so far we're only considering worst case inputs and so you're right the question is right that like even if like a task is sort of simpler like Alice wants to transmit a string X to Bob this task sort of only becomes maybe interesting ly non-trivial if you imagine a probability distribution over inputs X that Alice might want to transmit as we will in fact do in the next lecture when we talk about information theory but in this lecture where we don't have such a distribution well you know the task of Alice you know sending an n-bit message to Bob I mean there's not much to say about it it requires n bits of communication and you know randomized or not that's all you can say so that's why in this setting you know the more interesting task is that Alice and Bob are sort of jointly trying to compute some function but their inputs are separated okay here's another example you can think about it as I talk the communication complexity but so this is a when the function f is what I call parity sub 2n here Alice and Bob both get an N bit string and they want to compute the overall parity or overall XOR of all n bits of their shared inputs so I wonder if somebody can type into the chat the communication complexity of this task should think about it for a moment - that's right very good someone said it's 2 bits so this is an example where actually parity is quite a non-trivial function for some models of computation as we mentioned at some point for example it's quite hard to compute by constant depth circuits but in communication it's quite easy basically because addition mod 2 is commutative the point is that you know Alice can first privately compute the parity of her n bits Bob can privately compute the parity of his n bits and then they both have reduced down to 1 bit and the only thing they needed to do is compete the parity or X or of these two bits so Alice can send that one bit to Bob he can compute the parity and send it back or they can just exchange their single bits and figure it out uh here's another interesting example that you can also think about us I'm chatting so here you can always think of an N bit string as a subset of the numbers 1 through n that's what I mean here by brackets and and so in this problem which is the median problem Allison Bob let's get subsets of numbers from 1 through N and here we have like main one example where the output the thing they're trying to compute is not just a single bit here they're trying to compute a well basically a login bit quantity namely the median of the union of their two sets and you know let's say the multi set Union and make some suitable definition if this union is of even cardinality so a it's a pretty non-trivial example and what if you can suggest how many bits of communication you think you need to do this task or commercially you know what's an amount of communication that's sufficient to do this task n is one suggestion so yeah this is a task again where you can always basically do it with n or maybe n plus log n because Alice can send her entire sets to Bob which is an N bit string basically R and bits Bob can do all the computations figure out the median and then he can let's say tell the median to Alice that takes login bits what you can do better than that log N squared is some suggestion I have that's exactly right you can do this with order log squared n bits and I won't give a hundred percent of the details on how but the basic idea is to use binary search ok so you can maybe try two binary search for the median and you know to answer you know your binary searching between n possibilities and to answer you know a question of should you know Alice and Bob collectively take a look to the left or the right of like a current candidate for median they basically need to know how many numbers in their set Union a union B are two less they are smaller than this current potential median and here again they can use a little bit of commutativity like you know Alice can count how many numbers in her set a are less than this potential median Bob can count how many numbers and he said B are less than this potential median they can share that information which is like order log n bits and thereby understand if they need to like you know when doing their binary search you know move the new candidate median like halfway to the left in the interval or halfway to the right ok so it's sort of like log in rounds of communication each round costing them order log n bits and this is a nice example that really illustrates the need for like interaction and multiple rounds so some of these examples up here including if you remember the randomized algorithm for quality testing they're basically only involved one-way communication where like Alice sends a bunch of stuff to Bob and Bob does a computation and maybe Bob gives back the one bit answer they both need to know but here's a nice example this media example where they really have to communicate back and forth for quite some time so actually refined models of communication complexity you can also look at trade-offs between like the number of rounds where a round is you know defined by like Alice sending multiple bits or mod Bob sending multiple bits that's the number of switches in between who's talking but we won't get into this refinement today and just like you know worried about the total number of this communicated both ways okay and just to orient you a little since we know that like every hour a problem can be solved you know in this this framework and plus n bits one bit using like n plus one bits of communication we generally think of you know communication protocols that use poly log n bits as being you know efficient kind of like the analog of P for communication Alexa T and protocols that use some polynomial number of bits like I don't know linear bits or square root n bits or n to the point one bits that's being inefficient it's not a perfect dichotomy because you know there are functions between poly log and n to the epsilon but you know most normal problems fit into one of these two categories and so this is the rough guideline you should have in mind that you're generally trying to strive for poly log in its protocols if you can communication protocols okay so let me tell you actually now the most important communication tasks in communication complexity it's the most important one because it's sort of like the three set of communication complexity in that it's like the canonical hard communication task from which almost all communication complexity lower bounds derive by reduction from this one task and it's called um disjointness it's actually slightly unfortunate that they chose this name as I'll explain in a moment but let me say what disjointness is so again it's gonna map n bits you know Alice and Bob get n bit strings and they have to output 1 bits but you think of their strings little x and y again as subsets of the numbers 1 through n so think of Alice gets some subset of the numbers 1 through n I'll get some set subset of the numbers 1 through n and the task is to determine if these two subsets are disjoint you know they have no element in common if you want to think about this a bit more like logically or computer science II Lily you can really think of it like as follows you think these 2 n-bit strings X of x and y and you kind of end them together bitwise seems a little and on the bits and pairs and basically in the set theory world this tells you you know and facts about whether or not this number 1 is in both sets is number 2 in both sets and number 3 in both sets and then you or together these results and once you order together these end results you learn whether the sets are not disjoint if they have something in common and then because it's disjointness you take the knot at the end okay it's really unfortunate because like this not the end is kind of stupid it makes it like sort of not like an np-complete problem it makes it like a KU I'd be complete problem it'd be real great if instead instead of disjointness they had chosen like non disjointness or like the property of having intersection but anyway they picked disjointness that back in the 80s is like their key problem and so we got a stick with it in any case in communication complexity at least for what we're gonna say today there's not really much difference because if you can compute a function f you can going to be the negation of the function f with no additional communication okay so disjoint this is a very important problem in communication complexity and you can always come you know again compute this using n plus 1 bits of communication and for example a trivial way with Alice sending her whole sets to Bob and Bob computing answering giving here the one bit answer back it's also not hard show in the deterministic communication complexity model that n plus 1 bits are required to be almost the deterministic communicate communication complexity model is not super interesting as well let's say later the interesting case is the randomized case and here is it turns out unlike with the Equality function if you even if you allow Alice and Bob to use a randomized communication approach communication protocol they still need to compute or a Campari they still need to communicate a linear number of bits so somehow this is like still maximally hard even with even with randomness okay and so you know this is uh um you know this exemplars a really hard communication problem disjointness this is first proved by um kalyanasundaram edgar in 1992 and the proof of this is like famously pretty difficult so we're certainly not going to prove it in this course but similarly with 3sat right I mean once you know this fact and like take it as a given then many many lower bounds and communication complexity are derived by actually reduction from this fact about disjointness you know if you want to argue that some problem G is hard requires a lot of communication you argue that well if there was some efficient communication protocol that could solve G then by manipulating it a bit you could get an efficient communication protocol for solving disjointness but that's known to not exist ah so here's another go to hard function disjointness is an important one but here's one that's almost as important it's called the inner product mod to function or IP - for rabbity and it also maps to n bit strings 2 n bit strings so one single bit and what single bit is that it's just the inner product in the field F - so it's just x dot y or the we've written it can write it this way as well it's the sum of mod 2 or the XOR I'll be getting these pairwise hands of the two strings a third way to look at it um as we'll use later in this lecture if you remember way back to our lecture on and also some boolean functions this is basically the same as the Fourier character or the for a monomial swearing characters before in mono so you can think of Alice as getting a string X and Bob is getting a subset s and then they want to compute this Chi SX which is basically again you know the XOR of the bits in packs in the subset of positions s now if you think of here of Y is indicating a subset ask the positions then this dot product is really just the XOR of the bits of X in those positions indicated by Y okay so this is are a very important function to study in communication complexity\", metadata={'source': 'mQQ36cDnmR8'}),\n",
       " Document(page_content=\"okay so we want to get into these tasks of um you know understanding upper bounds and lower bounds for communication complexity you might worry a little bit that like the definition I gave of you know communication protocol is a bit wishy-washy and sort of wishy-washy the way I described it but you know in order to really truly analyze and especially for the purposes of proving lower bounds you really need to like very you know precise mathematical model and so that's exactly what I'm going to tell you now they're really the formal definition of a communication protocol pie okay so a communication protocol formally is a tree binary tree with more stuff incorporated into it but here's a picture so the notes the tree are labeled either by Alice or Bob and these are labeling who speaks okay so this fact of the roots here it's labeled Alice indicates that in this communication protocol example I'm giving you Alice will always send the first message now what happens you know when Alice in a protocol when Alice is you know gonna be sending the first message she really gets a string access her private inputs and based on that string she decides what bit to send in the protocol either 0 or 1 - Bob okay and therefore at this node what you really also need is a function that map's the set of possible inputs capital X for Alice into 0 1 the bit that she would send if her message were in that sorry for input were in that set X okay and so now every node will have two arrows coming out of it zero at one labeled zero for what happens in the protocol if Allison zero and for what happened to the protocol of Alice sends Bob you can see in this picture the protocol is such that if Alice's first message it happens to be zero when we follow this path and we get to an a node which means that at that time Alice and Bob will both understand that it's Alice's turn to speak again and there will be up another function here also annotating this tree mapping capital x201 and Alice will use this function to determine the next bit she sentence so she'll again look at her original input little X apply the function from capital X to zero one that's here and then I'll tell her whether to send zero or one okay on the other hand if Alice had sent one in her first message then you know by virtually this tree Alice and Bob would understand that it would be Bob's turn to speak and he would apply a function from Capital y 2 0 1 annotating this node here he's private input little y to tell him whether to communicate 0 or 1 okay and the leaves in this tree which are in these like square boxes are labeled by elements of capital Z labeled by the answer set which you know from now on is basically gonna be 0 or 1 so in this picture the leaves are labeled by 0 1 and that stands for the you know common answer that Alice and Bob agree upon in the communication protocol once they finish sending messages and they are both agree on the answer you know that's put into the leaf okay so once again a communication protocol pi looks like a tree that looks like this a binary tree you should notice labeled a or b the a nodes each have a function from capital X or any bit strings as 0 1 each of the B nodes has a function from capital y 2 0 1 and the leaves are labeled by elements of of capital Z or in other words bits okay so the question I got a question that says what does it mean for a to go to a like the root to the left sub-tree node that means that in this protocol that happens have the property provide it whenever Alice's first message to Bob is 0 they get to this sub protocol which means Alice speaks again so it's a despite how I sort of illustrated on the first slide it's not necessary that the players strictly alternate their messages that like first allison submit then Bob sends a bit Pinal sends a bit then Bob says it could be that Allison's a bit and then Alison's another bit and it can even be that like Alison's a bit and then depending on whether or not she said zero or one it might be Alice's turn to speak next or bobster to speak next so in this street has a it's example as the property that if Alice's first message is zero then Alice will send a second message but if her first message is one then Bob will send the next message she mention that like it's mostly without loss of generality to actually assume that Alice and Bob always strictly alternate sending one bit messages because in the worst case you have some smart protocol where maybe Alice sends several bits in a row well you can just artificially make Bob send like a dummy or a wasteful bit in between each of Alice's bits and therefore if you're not too concerned about the exact precise cost as we won't be then you know if you don't mind it blowing up by a factor of two then you can assume that Alice in bits Bob's uh messages strictly alternate and then you know your tree will be leveled with a nodes B nodes a nodes B nose etc speaking of costs here's the natural definition so give it a little inputs little X and little Y the cost is the just the path length that would be followed in the tree when the input is little X for Alice and little Y for Bob but you know forces a path down the tree perhaps is this one like if you have a little X and little Y you might follow this path down the tree so Alice sends one Bob sends zero and then Alice n zero and at that point they both understand that their mutual answer is one okay and that really should be F of that particular little X little Y if the protocol is correct and the cost of a protocol PI is the you know the maximum depth of this tree and finally the communication complexity of a function f or the deterministic communication complexity so the D here stands for did turistic this little superscript students for a communication complexity the eval East cost PI over a PI that correctly solves f where the players always correctly you know understand at the leaf the true value of f of their input life's little why ah there's a question from the class will there be a tree for every end good question actually so this is a communication is complexity is like a non-uniform model of communication which means that we just generally talk about like you know we have one specific function on a specific number of input bits n you know like checking the problem of checking equality between two length 100 strings and we think of the communication of LexA T of that now it is true that you know really really we generally think of like families of communication problems like the Equality function on more and more bits N or you know the disjointness problem on more and more bits n and then indeed you would have a different tree for every input length and yes it will like stark a complexity like generally you know each of these trees will you know not be like totally different from each other it'll be some typically when you construct them like underlying idea behind them well yeah formally you're gonna have a different tree for every communication problem and particular every different input like that it's a good question ok so I want to now explain to you a little bit more about how you can interpret these communication protocols and a little bit more about what they need to correctly compute function f and in particular I want to tell you about an object a very important object in communication complexity given a computational problem f that you're trying to solve F is my horrible Mouse writing X maps a pair x and y to you Capital Z there's an Associated communication matrix M sub F these rows are indexed by Alice's input set X and whose columns are indexed by Bob's you put set capital y and since we're always thinking like n bit strings really like you should imagine the rows Collins being labeled by all n-bit strings I guess in my little picture n is four so this will be a very large matrix it'll be like a 2 to the N by a 2 to the n matrix and the entry in the row little X and column little Y is just the function value F of little X little Y ok so every communication problem F gives rise to a communication matrix and sub F and really conversely I mean you know every matrix let's say every boolean matrix you can think of it's a communication problem I'm particularly if it has you know to be n rows and 2 to the N columns you know you just put all the answers together in like a big matrix okay so a communication matrix is synonymous with a task that you're trying to solve and now I want to relate this a little bit to a you know hypothetical communication protocol that's supposed to be solving this communication task F or acutally M sub F so let's think about this in this you know a little picture of a communication protocol the route is labeled by a which means that the first thing that happens when Alice you know and Bob get their inputs little X and little Y the first thing that happens is Alice will send a one bit message in particular she'll look at her input little X and depending what it is she'll send other the message one zero or the message one and that's determined by this function inside this node that map's capital X to zero or one and you can really think of that first you know function she applies or the first you know decision she makes about little X causing Eunice another zero one as making a partition in the rows a partition of the rows I should say so in this little picture I've drawn I've got this horizontal line here which is partitioning all the rows all the inputs little X that Alice might have into those where she would send the bit zero and those were she would send a bit one now I should emphasize that for the purposes of illustration I happened to make all the input strings where she would send zero as her first message contiguous and all the input strings where she would happen to send a one is her first input message continuous this absolutely need not be the case it's just like it's it's impossible to illustrate unless you you know use this special case okay so in general this partition could be a partition of the rows into any two subsets but for the purpose of illustration I've drawn it as though it happened to be like two continuous subsets now furthermore you see if you look at this example communication protocol so I've seen before if Alice's input little X's such that the first he would send is a zero then it'll actually be her turn to speak again which means that if the input little X is in these rows here I also actually speak again she'll apply a different function to her input little X and that will partition the these remaining while these top part rows again into two more subsets depending on whether the little X would make her send 0 here or one here in sort of the second round so this second horizontal line here is like you know maybe cutting off among all those strings that cause Alice to send a zero in her first bit communicate presume on her first message you know maybe these ones up here are the ones where she would also send 0 in her second message and these were the subsets here where she would also where she would send one in her second message and actually in this you know a little hypothetical example note that in this case after she sends her bits the communication protocol has ended and they've agreed in this case either that the answer is 0 or in this case if the answer is 1 so in fact it better be the case if this protocol pie is correctly computing F that in fact this matrix here consists entirely sorry this part of the matrix consists entirely of zeros remember the answers aren't like written into the matrix here and so we need if this communication protocol illustrate on the left to be correct we need it to like literally be all zeros up here and we needed to literally be all ones and as part of the matrix on the other hand notice that if Alice's initial input little X which again you can think of it as a row with such that it would cause her to say the bit 1 in the first round of communication then it'll be Bob's turn to speak next and so Bob will look at his input little Y he'll apply this function from Capital y to zero one sitting in this node to it and he'll either send the message 0 or 1 and you can see this also partitions these inputs here into two sets depending on Bob's input little Y so this node here cause apart it causes a partition that looks like this these are the columns and I should say up here actually these are the columns that are gonna cause Bob - let's say communicate zero when it's his first turn to speak assuming that Alice's first bit was one and these are the columns these remaining columns are the ones on which she'd communicate one there's a question not sure if this was already discussed but does anything bad happen if F is uncomputable no it's perfectly fine for her well first of all it doesn't quite make sense for X to be uncomputable because there's a fine detail but it's like a non-uniform model so f is really a function of like a fixed number of inputs in this particular example for input for bits for Bob and for bits for Alice so there's no notion of it really being computable uncomputable because every fixed length the fixed input lengths function is computable and we don't really ever in communication complexity I don't think talk about communication problems where like Alice can get us as we input a string of an arbitrary length and Bob can get a string of an arbitrary length we only like think about input lines that are fixed so simple everything is computable but furthermore we also like don't charge at all for computation so the complexity of Alice and Bob computing these functions that are inside these nodes is not taken into account yeah so it's exactly it to follow up on what was asking the chat is exactly like a family of circuits you know these like you have a family of communication protocols maybe one for each n if you are trying to solve a natural family of communication problems like F sub n parameterised by an input like that right so when Bob speaks here just get it back to the story that again like partitions these remaining inputs into two sets which again can be just like an arbitrary partition of the columns by just I Illustrated it continuously and you see here when Bob says one the protocol ends and they've decided that the correct answer is one so that means this if the protocol is correct this mate sub matrix here better be entirely filled with ones you see on the other hand if Bob sends a zero then they keep going and Alice will send another message which will do another partition and a perhaps will look like this so in particular if this protocol is correctly computing a function f then we know exactly what F is it has to be the function you know is corresponds to zeros up here and ones everywhere here and ones everywhere here and zeroes everywhere here and ones everywhere here okay so each of these final pieces corresponding to like a leaf it's called a combinatorial rectangle so combinatorial rectangle is a subset of a matrix which is kind of rectangular in the sense that it's a cross product it's a set of matrix entries of the form K cross L where K is a subset of the rows and L is a subset of the columns so for example this this it's probably gonna be a bad color this rectangle here is the cross product of these rows k and like these columns well and again I can tell need not be contiguous we put that back this is K this is L but it's supposed to draw you know if they're not contiguous and so you see um at the end well let me go back to this picture at the end oh sorry Munich ation protocol at tree he really divides the whole communication matrix in some like you know yet Mondrian style collection of rectangles that partition the original communication matrix into regions where like the entries are constantly zero are constantly one so to say it more in words like if you have a cbut communication protocol that correctly you know computes some communication function f and let's say it's deterministic then you get a you know you have at most two to the C leaves and so you partition the whole matrix into at most two to the C combinatorial rectangles and each of these combinatorial rectangles is like F monochromatic which means that the communication function f is constantly well it's constant in particular when Capital Z is zero one it's constantly zero one on all those rectangles okay so this is a critical point place so if you have any question about it please do ask is this how we're going to actually prove lower bounds for communication complexity by arguing that you know F is so complicated that there's no partition of the communication matrix into a small number of combinatorial rectangles on each of which you know it's sort of monochromatic where it's like all zeros are all ones okay when we give upper bounds we usually just like say some words that tell us the stories of what Alice and Bob do you could translate it into this picture if you wanted but this is the theorem that we really use to now then exactly what is the communication protocol so that we can prove lower bounds on C so let me give when f is the quality function on two-bit strings so here's the communication income matrix M sub equality on two-bit strings so it's a four by four matrix for being two to the power of two and it's the Roasterie index by Alice is possible in inputs the columns are indexed by Bob possible inputs and as equality so it's actually the identity matrix you have ones in the diagonal that's where the inputs are the same and zeros elsewhere so here if you just think of like the naive protocol or are naive protocol we're basically Alice sends or two-bit string to Bob and he gives back the answer it partitions you know the communication cost is three and partitions I don't I have this here communication cost is three it partitions the communication matrix into two to the three namely eight combinatorial rectangles luckily in this case they're not just combinatorial rectangles there are actually contiguous rectangles actually maybe the naive protocol gives you non contiguous rectangles but I chose a protocol that's it correct and like uses contiguous rectangles and there you know all these different eight multicolored rectangles I've drawn here wait a minute one two three four five six seven eight nine ten wait what is unexpected hold on one two three four five six seven eight nine ten huh okay I've given a wrong example apparently because this eases ten rectangles however I know for a fact that you can't solve the quality communication problem with three bits of communication therefore you've got to be able to do it with eight rectangles so it should be possible to cover this identity matrix with eight rectangles combinatorial rectangles each of which contains only ones or only zeros so where did I go wrong here I think what I should do is let me go back okay here's the communication let me see if I can do it on the fly here is identity matrix I will now try to cover it with eight combinatorial rectangles such that each rectangle is monochromatic think I can do this he is sure blue okay so definitely I'm gonna whoops sorry one second definitely I'm gonna be probably all done in your head is sort of pointless for me to do it but okay I'm definitely gonna use like one rectangle she's really just a one by one rectangle to cover all the ones good so I've used it for I've four left now maybe I need there's another one that's five mm-hmm six this is going poorly so I still have these ones actually there's a further problem which is that you can't actually just plunk down somebody said the rectangles in the same row can be combined it's what we all said aren't ly you almost like flew rectangles one rectangle on the rows good that's good another I'll get that in fact another point is you cannot just plunk down a cover by monochromatic color tutorial rectangles because that does not necessarily actually correspond to a valid communication protocol every communication protocol does induce you know partition into combinatorial rectangles that are monochromatic but not vice versa but yeah somebody says good so actually this blue guy and this yellow guy can actually be put together into one rectangle thank you to whoever said this this is actually this blue guy together with this yellow guy are actually one rectangle it's the rectangle where the K set is just 0 1 and the L set is 0 0 1 0 and 1 1 so it's like a non-contiguous rectangle and similarly this orange thing and this like pale green thing can actually be put together into B 1 rectangle so this is in fact ten rectangles how do i drawn it properly okay good any questions okay thank you Tim and Rishikesh okay oh good so now having really looked at this situation carefully let me show you a brief theorem that says that the deterministic communication complexity of the Equality predicate on n bits must be at least n plus 1 in fact therefore we know it's equal to n plus 1 because n plus 1 is always an upper bound good well I'm just going to show that like it's not possible to decompose the communication matrix into a small number of common soil rectangles each of which is filled with only ones or only zeros ok so as you may be able to see already the communication matrix in general for a general value of n is a 2 by 2 the N by 2 the n matrix has ones on its diagonal so it's the identity matrix and it's easy to see that everyone has to be in its own little one-by-one combinatorial rectangle it's impossible to have like one combinatorial rectangle which contains more than one of these diagonal ones yet is monochromatic like if you try to put this one and this one together in some combinatorial rectangle well then you know if it's going to include this one then it has to be including like the the row set has to include this one and the row set also has to include this one if it's going to include this one but if it's going to include this one then the column set has to include this column I'm just going to include this second one then the column set would also have to include this column but then since it's a rectangle it's a cross product you would also be getting this 0 and this 0 so you wouldn't be monochromatic so you can reason that like you need to devote one combinatorial rectangle just for every entry on the diagonal which is already 2 to the N combinatorial rectangles and I mean to cover that you still to cover the zeroes in fact you need like way more combinatorial rectangle successfully cover the zeros but anyway you surely need at least one more rectangle of course you actually need like way more but for a little suffice to notice that you need at least one more rectangle so overall you need at the very least two to the n plus one combinatorial rectangles and therefore you know the number of common eternal rectangles you get from a CVI protocol is only two to the C so you need two to the C to be at least two to the n plus one which basically means that C needs to be strictly bigger than n so it has to be at least n plus 1 ok so that exactly resolves the communication that eristic communication complexity of the Equality problem there interesting alternative way to prove it there's a very old theorem and communication with plexi Dudamel horn and Schmidt that gives a different way to prove a lower bound on deterministic communication complexity in terms of linear algebra and it says that a lower bound for the deterministic communication complexity is basically the logarithm of the rank of the communication matrix M if you remember the linear algebra concept of rank let's say over the feet the real the real field it's actually the lower bound if you do it carefully is like log of 2 times the rank minus 1 but same difference up to plus or minus like one bit or so so log of the rank of the matrix is always a lower bound and before applying now let me just tell you the proof hints I can leave this as an exercise for you but the hint if you know your linear algebra you should be able to do it it's that the the indicator matrix of a combinatorial rectangle like that has one's exactly in those positions that look like K cross L for some subset of rows and some stead of columns K and some set of columns L that indicator matrix has rank 1 so partitions into like ten combinatorial rectangles somehow have rank 10 at most n well let me not say more but that's the hint for you to finish up on so basically the number of parts and the partition has to be something like the rank of the communication matrix and then the number of bits is the log of that so we can apply this to get some quick lower bounds we can do it again for the equality function where the communication matrix is literally the to them by to the M identity matrix which I hope you all know has a rank maximal rank two to the N so you plug that into this formula and you basically get long two to the N so you basically get n in fact n plus one if you're careful you can also easily use this to the show that the communication complexity of the disjointness problem is also at least n plus 1 it's literally maximal I will prove that in full detail but I'll give you enough of a hint that I hope you can do it the hint is like this well for the disjointness problem on one bit where Alice gets a 1 bit Bob gets 1 bit and they want to determine if a substance these are disjoint or not basically they're always disjoint unless both of them get the bit 1 so the 2 by 2 communication matrix looks like this which does have rank 2 by the way it has maximal rank and then I leave it to you to work out that in general the disjoint the communication matrix for the like n bit disjointness problem is actually like the enfolds Kronecker product or tensor product of this matrix with itself you know / remember what that means and if you know the linear algebra you can use that to infer that the rank of this matrix is again maximal it's 2 to the N and therefore you need maximal communication complexity namely n plus 1 bits of communication great there's a question that says it's strange that we require communication complexity to be an integer good point I often feel this way myself the question goes on to ask what if it were actually a non integer for asymptotically solving a large number of instances the same problem extremely great question absolutely true that like there's this weird granularity about like oh well you're sending bits so like every time you send a bit and you pay like one like it's impossible to have a communication complexity of like 1.3 or something which it does feel a little bit unnatural we appeal to this and that like weird trick from the lower bound for equality where we're like well I mean as soon as the number of combinatorial rectangles is bigger than strictly bigger than 2 to the N it has to be like at least 2 to the n plus 1 you know so we kind of relied on that and it is unnatural like first of all as you say like you know why are you communicating bits like you can communicate tricks or something and maybe then you could have like a communication cost of 1.5 but what you said is even more pressing the question was even more prescient in that it is ultimately useful and we'll talk about this I think a teeny bit in the next lecture on information theory and information complexity to imagine like repeated like the parties having to repeatedly solve like many communication tasks and then like amortize the bits of communication they need per instance of the task and then you can have communication complexity that's not an integer in fact as I I won't talk about I'll mention it now since the question was asked you know I alluded to before one of the more famous and challenging theorems in communication complexity is that the randomized communication complexity of disjointness is still Omega n and the best way to prove values is information theory and you know this disjointness is somehow as I mentioned before it's the big n fold or well with the negation at the front of like and little tasks the little task of computing the end of like x1 and y1 the end of x2 and y2 and so forth but sort of like computing that that compute like n copies of this one bit communication problem and and you can prove this lower bound by somehow arguing that the amortized communication costs of the one bit and problem or the one bits a disjointness problem is not zero it's at least some universal constant you know 0.001 and from that you can somehow infer using information complexity that the overall cost is linear in n so we will not actually see that but that's how the one of the best proofs of this lower bound goes and it does involve sort of non integer communication cost so a great question okay uh just while I'm on the subject I want to mention one of the most famous conjectures and well yeah definitely one of those famous conjectures in communication of Lexi it's almost one of the most famous conjectures in CS theory in fact it's called the log rank conjecture very famous conjecture made by La Voz and Sachs back in 1988 it's simply asked if this is basically also an upper bound if you tolerate polynomial so is it true that the deterministic communication complexity is always upper bounded by poly log the rank of the communication matrix and to be honest this question is mainly of interest for aesthetics sake I think because as I mentioned before and I'll mention again honestly deterministic communication complexity is not the most interesting kind of communication complexity but still is such a you know clean question that it's intrigued people for a very long time interestingly almost all the progress on this problem has been made recently given that this conjecture is you know almost forty years old so the best upper bound is showing that for a long time the best upper bound was just it's at most the rank but now love it showed that it's basically most the communication complexities that most the square root of the rank the best lower bound for a long time was like log of rank to some bizarre number between 1 & 2 but recently mokuba's tony potassium tom watson showed that it's at least basically log squared the ranked and most recently along along these lines a checkup on a Monday and sheriff's showed that the analog of this log rank conjecture for randomized communication complexity sometimes called the approximate log rank conjecture is false there's another question I'm getting flashbacks to quantum computing stuff with these to the N size matrices flying around is there a deep connection / reason why tuna and matrices appear naturally in both places really it's just because you know you know truth tables are of length 2 to the N we just you know communication problem well yeah and just because we're indexing the rows and columns by all possible inputs and n-bit strings I mean matrices naturally arise in communication complexity because you have two parties so they each hold an n-bit string so you get like a you're kind of your problem F instead of being given by a truth table is given by like you know a truth vector if you will is given by a truth matrix it's actually one reason why communication complexity with three or more parties is way more difficult because the analogous input object is like you know maybe like a 2 to the N by 2 the N by 2 to the N box of numbers and then linear algebra is no good once you have more than two dimensions the reason why you have boxes like matrices in quantum is for a different reason I mean it's the nature of quantum that it's it's involves matrices that's all I can say I guess ok let me know\", metadata={'source': 'zFHWmmThdT4'}),\n",
       " Document(page_content=\"okay let me now move on to randomized communication complexity which as I mentioned before is somehow more interesting than deterministic communication complexity so here the players base communication also on coin flips so in addition to just getting their inputs and blindly following some fixed deterministic protocol the protocol can also intuitively involve you know Alice will like you know flips and coins to decide what she does next Bob will flip some coins to decide what he does next I once again I'll stress that we're working at a worst-case complexity model at least for now where it's not that the inputs are random you're always worried about the worst-case communication complexity over all possible inputs x and y it's just that like by using randomness you might be able to Bob and Alice might be able to come to the answer quicker using randomness and they would on a pretty low string x and y then they would without randomness okay so you always require that they communicate at most C bits in the worst case the output the only difference is they only have to output the correct answer with high probability and actually one important modeling question what are we defining randomized communication complexity with more than one person involved like in let's say interactive proofs or in this case communication complexity is whether the random bits are public or private so private is the model that you might first think of where Alice gets her own coin that she can flip to help her decide what to do next same with Bob but like Bob doesn't get to see Alice's coins and vice versa so let's first make this definition with clinical private coins so we use this notation R sub epsilon CCG stands for communication complexity of a task F it's the number of bits needed to communicate on worst case input and worst case coin flip outcomes for a clinical private coins randomized communication protocol that for any input X and y gets the correct answer with probability at least one minus Epsilon so for an example of this I can repeat something that we did in lecture 10 which shows that once you allow randomness in a tolerate a small probability of error you only get this error for you know with small probability for any pair of strings let's say we're gonna talk about the equality product problem I should say so even if x and y only differ on one bit you still only have you know one-third chance of error in this protocol that I'm about to mention or the protocol we already saw uses order log n bits so the last thing we in lecture 10 we saw a protocol involved in Alice chooses a prime and thinks of her input string as a polynomial and sends like an evaluation of the polynomial random evaluation of the polynomial mod a prime to Bob he checks whether his polynomial gives the same thing but actually now that we also started coding theory I can tell you a different way to solve this problem so first before things start Alice and Bob fix in their mind any quote-unquote asymptotically good error correcting code boolean air cracking code with this notation if you remember way back to coding theory this notation means an air cracking code where the messages they intend to send our n bits long the actual encoding czar order n bits the fact that this is just a constant times n is one aspect of being good and the minimum distance of the code is also a fixed constant point one actually this should not say point one should say point one n but the point I mean is that like for every two messages there in coding strings which are of lengths Big O of n differ on a point one infraction or more sorry a point one fraction or more of the coordinates okay and this two indicates that it's a boolean code a binary code so I think of the remind like an error correcting code that has these properties which as I mentioned way back in lecture 11 exist and now they both get a string X&Y and Alex just imagines the encoding of her string in her head encoding of X and Bob imagines the encoding of history encoding of Y in his head and now they again want to check whether the in coatings are the same or not okay if these in coatings are the same then it was the same strings X&Y and if the coatings the original strings were different then the encoding will be different but huh not only whether they'd be different but like but since it's an error correcting code whenever you encode two distinct strings you get two strings at Hamming distance at least 0.1 n so you sort of amplified it so that they disagree not just in maybe one position but at least 10% of all positions and then you're in great shape because Alice can now just pick a few like constantly many random positions in her encoded string send these positions I over to Bob together with the if--but of her encoded string and Bob checks whether they match up with the appropriate bits of his encoded string encoding of Y ok and since you know it'll always match up perfectly if equality is true if x equals y if X differs from Y and the encoding will differ on at least 10% of positions so I don't know if Alice picks like 10 or 20 positions the probability that all accidentally agree is going to be smaller than the target error of 1/3 now notice actually that that's the conclusion of this but notice actually where's the cost come from here this order log and communication complexity came from house having just send the names of the indices okay there are indices into this length order and string encoding string so just I'm gonna send the names of the indices costed order log n bits now in fact imagine a scenario where there was public redness which basically means that Alice and Bob both get to UM refer to a common public random string imagine like these random bits are written in the sky they both get to look at the same random bits then they wouldn't actually need to communicate these indices anymore they could just get those indices out of the random bits in the sky the only thing Alice would really have to communicate is the it's in the you know these in these bit positions in her encoded string and that's cooler than they could even use like a air correcting code with like the best possible minimum distance regardless of how horribly long the encoding were because they're not paying for the encoding length anymore so they could use like the hamming code which if you remember has relative distance like and over to where every two encoded strings that are distinct different half the positions so that's all of which is to just illustrate this fact that in the public coins model the communication cost the randomized communication costs of the quality function on n bits with error one-third or even one-fourth in fact is two which is pretty awesome and this long hamming code story I've told you you can just use it or when you actually compress it down to its essence it becomes this so Allyson sitting here with her input little X Bob is sitting here with his input little Y these are n bit strings they both get to look these random bits that are in the sky and they interpret these random bits as two n bit strings R 1 and R 2 and then Alice computes R 1 dot her string and she computes R 2 dot her string those are two bits she sends those bits to Bob Bob knows R 1 and R 2 and he just cross checks these answers with R 1 dot y and R 1 R 2 dot y if they're the same he says I think our answers our strings are the same and otherwise he thinks that they're different yeah of course if they are the same string Bob will always you know output that they're the same I suppose maybe that this cost should be three because bob has to tell the answer back to Alice so let's make it three and if x and y are different strings then I mean you could use this hamming code story but you can also basically recognize that you know the difference in these two bits are 1 dot X and R 1 dot Y is like R 1 dot X minus y and X minus y is a non-zero string if x and y are distinct so are one dot that is like a random string dot a non zero string which has a 50/50 chance of being one okay so let me not dwell on this too much longer but this means the error probability for this protocol is like 1/2 times 1/2 and in general if you use K bits you can get the error probability to be like 2 to the minus K okay so uh now we've seen actually that it sort of makes a difference if you use public coins or private coins in the private coins model equality costs log n bits of communication in the public poins model it only costs it like 2 or 3 bits so that's funny it's obviously the case that the public coins communication cost is going to be at most the private coins 1 because you know if there are if you have a private coins protocol you can simulate it with public coins just by like Allison will agree to use like the odd-numbered bits in the sky for her random coins and Bob will agree to use like the even-numbered bits in the sky for his coins but what about the reverse I mean this public coins model seems like a little bit wacky it seems like you're cheating maybe a little bit if you use it but in fact it turns out you're not really thanks to something called Newman's theorem and nubes theorem shows that given any public coins communication protocol you can convert it to a private coins one with not much more error plus Delta error using just an additional login bits and another log 1 over Delta bits okay so if you think of like Delta is like a constant it's basically plus order log n bits okay so in fact you know this gap between private and public for the equality problem of order log n bits is maximal and since we don't care that much about order log n bits that's generally considered like an efficient amount of bits it basically means that we don't really care about the distinction between public and private coins so we actually can settle on public coins and feel as though we're not cheating really not cheating up to login bits and our communication protocols uh let me not go over the proof of this in the interest of time or maybe I'll come back to it after the class is over but it's kind of a standard ish D randomization proof using Chernoff bound + Union bound here's the fast version of it on slides ok so I skip that proof but let me therefore just say that in light of this public coins is sort of the de facto standard model for communication randomized communication complexity and now if you have a randomized communication protocol with uh public randomness then it's called like the public randomness that affects the whole protocols like written in the sky is show some sort of once and for all and given like the random bit string written in the sky Alice and Bob are actually deterministic sort of operate deterministically as a function of their input bits and the random bit string the sky or in other words a public coin protocol randomize protocol can be viewed as a probability distribution over deterministic cost protocols so costs see public coin protocol it's just a probability distribution over deterministic protocols and the overall error of the protocol is the maximum over all x and y of the probability that the deterministic protocol when drawn from this probability distribution gives the wrong answer okay so for upper and lower bound purposes you know for randomized communication complexity you know random right protocol can really just be thought of as like a everybody agrees at the beginning they pick some deterministic protocol to use and then they use that and that's chosen at random and the error is the probability that their different protocols give the wrong answers\", metadata={'source': 'LRef5a88uZQ'}),\n",
       " Document(page_content=\"okay so in the interest of time again I'm gonna get this slide for now but it's on the subject of how you could possibly prove a lower bound for randomized communication complexity and you know you need to prove a lower bound on any sort of probability distribution over deterministic communication problems that solve protocols that solve a problem and long story short there's something called oops there's something called the the minimax principle which maybe I'll come back to if I have time but it says that the randomized communication complexity of a problem can be viewed in an alternate way to understand this alternate way we need to look at something called distributional communication in black city which is version of communication complexity where it we do have random inputs you know always I've been stressing like oh and the randomized model it's still like worst case input and we're just randomizing over internal coin flips but a distributional communication complexity we're now going to actually consider probability distributions on inputs so let mu denote a probability distribution on pairs little x and y like total inputs x and y for Alice and Bob and then D superscript mu of F with also an epsilon is the least cost of a deterministic protocol hai which has error at most epsilon under the input distribution okay so now there's a fixed distribution on inputs mu and you're searching for deterministic communication protocols for Alice and Bob and what you're hoping for you know they're gonna have some maximum amount of communication C as always and you're hoping that they have small era where now the error is with respect to choosing the input from this distribution mu okay and so D superscript muse of epsilon is the least number of bits you need to communicate in the best protocol which achieves error at most Epsilon when the inputs are drawn from the distribution mu and why do we bring this up there's this thing consequence of the minimax theorem which says that the randomized communication complexity of a task F is equal to sort of like the worst possible distribution complexity it's equal to what you get if you choose sort of like the quote unquote hardest possible probability distribution on input pairs XY for F and you look at the deterministic cost under that achieving error epsilon under that distribution okay so instead of considering like worst case inputs and random randomize protocols you consider deterministic inputs sorry deterministic protocols and a really hard input distribution and this thing called Yao's minimax principle which follows from the von Neumann minimax theorem which basically follows from LP duality Farkas lemma says these two things are equal and the reason we bring this up is because it's exactly the starting point you need to try to prove lower bounds on randomized communication complexity and we use it because it's much easier to reason about a fixed deterministic protocol should we know it's like a tree and a matrix partition and so forth and it is the reason about random randomized protocols so a consequence of the rouse minimax principle principle is that it let's say you want to show that the randomized complexity communication cost of some function like the disjointness problem is at least C then it's sufficient and also necessary to find some you know to think really hard as the the mathematical prove our about a really hard distribution mu on inputs that you could give to Alice and Bob and then show that for any deterministic protocol that uses fewer than C bits it's gonna have more than epsilon error on this hard distribution mu so whatever your function is like disjointness you have to think of like some challenging randomized way to give two sets to Alice and Bob that it's going to make them hard for them to solve this jointness with small error and just deterministic protocols so the last thing I want to show you in this lecture is an illustration of how this goes but not for disjointness but for the inner product mod two problem I'm going to show that it requires linear communication in fact n over 2 minus 1 bits with a randomized public points protocol with error 1/4 and so we're gonna use the CIO minimax principle and it's Spice's to show the same lower bound for the distributional complexity for our favorite hard distribution and it turns out the smartcard distribution to choose is actually not so smart it's the uniform distribution Alice and Bob just get uniformly random strings that's nice like that's actually the hard case for the inner product mod to inner product mod to communication task so just to recall a few things about this problem we're gonna think about it in this way we're like Alice gets a bit string which is either plus or minus 1 to the N and Bob gets a subset the inputs s and their task is to compute this like Fourier character which is like the product over the reals of the s bits of X ok so Alice gets an extra plus or minus ones Bob gets a subset of the busy positions and their task is to compute the product of the bits of X in these positions which is either plus or minus one it's kind of the parity of the bits in those positions the trouble is Alice knows the string Bob knows the set and a couple of things will rely on if you remember way back to the analysis of boolean functions lecture we have this notion of Fourier coefficients of a function f it's probably bad that I used F here because it's not the communication problem this is like a boolean function having n bit strings to one bit strings this Fourier coefficient is this formula it's expected value of f of x times a is x and we will also going to need to rely on this parcel ball formula which says that for a boolean function f just sum of the squares the Fourier coefficients is 1 ok so the last couple of slides are going to give this proof so we have a hard distribution the uniform distribution and now we want to show that we're going to fix any deterministic C bit communicating protocol and we're gonna think of this protocol as you know converting an input pair X and s for Alice and Bob to the protocol answer the you know the final guess for you know product mod 2 applied to X and s now this protocol may sometimes give the wrong answer but we want to show is that if it's only communicating C bits that in fact there's a good chance it gives the wrong answer on the uniform distribution so this deterministic protocol Pi which communicates C bits as we know what partitions all possible input pairs X and s into 2 to the C the the M F into 2 to the C combinatorial rectangles which I'll call like script R sub I cross script s sub I this script R sub I is a collection of Alice inputs it's the collection of strings script S sub I is a collection of Bob inputs it's a collection of subsets of s a little confusing but it's a collection of subsets and furthermore this protocol its leaves are you know bits I'm just gonna call plus or minus one and so it outputs the same value let's call it Z sub I on every one of these combinatorial rectangles script our eyes cross script si okay so here's a task we want to show is if C if C is small then this protocol has to make a large error or error with high probability or to put in the contrapositive let's say this protocol gets the correct answer on 3/4 of possible inputs then we want to show that it must be computed at communicating a lot of bits okay so if this protocol PI which effectively defines a matrix you know mostly agrees with the communication matrix for inner product mod two under the uniform distribution then the number of combinatorial rectangles or C has to be really large in other words we take the inner product of two communication matrix which spoiler alert actually is the Hadamard matrix from that quantum lecture or the analysis of boolean functions lecture you know not only can you not split it up into a small number of combinatorial rectangles that are all zeros or all ones you can't even really do it in such a way that like each rectangle is like mostly zeros or mostly ones where mostly means like at least 3/4 ok so what remains is a calculation and I know I'm going a little overtime here so if you need to leave I'm not offended but I want to finish this calculation for this video you can catch up with it later if you gotta go just so there's like two slides ok so uh let's work on to this assumption that this C bit communication protocol agrees with the true answer of inner product mod two for at least three-quarters of input pairs X comma s extra Alice Esther Bob when X and s are drawn uniformly that's what this 3/4 is with respect to so we deduce I mean just restating that really that the expectation when X is random and s is random of the protocols output and the true answer it's going to be large so remember I'm using plus or minus 1 notation so this is plus or minus 1 this is plus or minus 1 and when it's getting the right answer you get either plus 1 times plus 1 or minus 1 times minus 1 so you get plus 1 when it's the right answer and when PI's getting the wrong answer you're getting minus 1 well since we're assuming you're getting the right answer with probability at least 3/4 we get 3/4 times plus 1 plus 1/4 times minus 1 and expectation so we get at least 1/2 okay so this is just translating our assumption about the quality of Pi and to like some math and now let's say that this protocol PI we can also think of it as like a function really that map's pair of inputs X and s into like this Z outputs and there's at most 2 to the C combinatorial rectangles so we can write it as like a sum this function is a sum is I goes from 1 to 2 to the C of the indicator that the input pair X s is in this earth combinatorial rectangle times the answer is Zi that it gives either plus or minus 1 on this rectangle now this is a combinatorial rectangle - so to say that X comma s is in RI cross si is to say that X is in R I and s is in Si so this is a small but neat trick we can replace this indicator with the product of two indicators and I'm going to even think of this indicator as a function of X which I'll call one sub R I and I think of this indicator as a function of the input capital S this is subset of bits that I'll have Bob gets namely one sub script si so I'm going to plug that back up here you can get that this line implies this line expectation of a random X and random-ass of this this is just this this you could put parentheses here so think in parentheses is PI XS okay and it's correlation with the inner product mod two on access is at least 1/2 okay now I'll do some like autopilot maneuvers so we just put this inequality to put the half on the left so it's over here it's at Lee at most so bring the Sun to the outside so this sum over i going from 1 to 2 to the sea whoops of Zi times this expectation and now just say look this is e aí is either plus or minus 1 and so I can upper bound you know Zi times this expectation by the absolute value of the expectation and now I can just upper bound this sum of 2 to the C expressions by their maximum so I get that 1/2 is at most 2 to the C the number of comments or rectangles times the maximum possible value of this expression and this expression if you think about it it's known as the discrepancy of the inner product function on this combinatorial rectangle RI cross si under the uniform distribution okay you pick X and s at random you sort of use this indicator to check that they're in the combinatorial rectangle RI cross si and then you look at the inner product mod 2 value here you guys are plus or minus 1 so you're kind of measuring this this our matrix you're measuring sort of the balance of plus or minus ones or the discrepancy of plus or minus ones in this combinatorial rectangle RI cross si okay so what I claim is that each of these terms here in the maximum is at most a 2 to the minus n over 2 so it's really really small which is sort of saying there's like every comet or a rectangle in this inner product mod 2 matrix it's very close to being balanced with plus or minus ones and therefore just putting that together this is at most 2 to the minus n over 2 I got 1/2 is less than or equal to 2 to the C times 2 to the minus n over 2 so taking log base 2 I get minus 1 is less than or equal to C minus n over 2 and that's what I was shooting for that C is at least n over 2 okay minus 1 okay so this is really the last slide the last thing I need to prove is this claim that this discrepancy quantity is super small and we'll use analysis of boolean functions for this so this quantity I'll now remember that inner product mod 2 applied to inputs X and s is nothing more than this free a character it's the product of the s bits of X okay so I'll just move this x2 the inside so we get expectation for s of this indicator that s is in Si times the expectation over X of the indicator of RI times this Fourier monomial and by definition this is like or basically by the most basic formula this is the Fourier coefficient of the indicator function our eye on monomial our set s now I'm gonna use a very basic fact the expectation of a random variable is that most the square root of its squared expectation squared this is Cauchy Schwarz so I'm just going to upper bound this by the square root of the expectation of the square of the inside so I just squared both things on the inside say less than or equal to because this quantity is either 0 or 1 this corner is non-negative so if we just drop this quantity it can only make the quantity go up and we'd be left with the expectation over s of this squared 48 coefficient okay the expectation over s it's really just 2 to the minus n times the sum over s and pulling that out of the square root we get this 2 to the minus and or 2 times the sum of the squares of the Fourier coefficients which is actually this is another little mistake at most one by parts of all it's a mistake because it's a zero one valued function rather than a plus or minus one valued function but uh let me leave it at that since I'm running way over time and I completes the proof of this claim okay so in the end we prove that the randomized communication complexity of the inner product mod two function is basically linear it's basically n over two by showing that under the uniform distribution any deterministic protocol that uses fewer than n over two bits has to have error at least 1/4 and you might ask can we use the same technique to show this lower bound for the disjointness problem but this exact same thing will not work because you can easily show that for the uniform distribution on inputs this just joint this problem for Alice and Bob is very easy because usually a random pair of sets is not disjoint so they can basically just always output no and be right with very high probability if you cannot use any product distribution to get a good lower bound for disjointness it's a theorem it's not easy but it's any product distribution where Alice's input is independent of Bob's input it's not that hard for disjointness it only requires about route and bits of communication and therefore if you want to prove you know this famous theorem about disjointness that for randomized protocols it needs a linear number of bits of communication your heart distribution is actually going to be a non product distribution Alice's input and Bob's input are not going to be independent so you can think about what that kind of distribution might look like but as I mentioned earlier the best proof of this fact actually relies on information theory which will be the topic of our next lecture okay let me stop recording there since I went ridiculously over time so anybody who stuck around thank you sorry for going over time and making you sorry to the other people for making you watch the end of the video I'm gonna stop the recording now well actually yeah let me stop the recording now and then I'll answer some more questions\", metadata={'source': '0vrqCDcxbxs'}),\n",
       " Document(page_content=\"okay uh should we talk about the homework sure can we talk about 10.1 maybe just start off with uh sure yeah so um i guess in this one we were looking at it and we were just like for the first part which was like to show that the us heuristic runs and n raised to big okay we were thinking of just like enumerating all of the um steps and all the for loops and then like looking at how many operations each one takes but we just wanted to be sure maybe that like what we were doing like made sense um so basically what we um had was that each l y would have like n n choose k like each y could have size and choose k um at least uh sorry at most um elements in it and each and then it just takes like big omega like the the sorry the um cardinality of the domain raised to the size of the set to actually test each one so for the first step so so what we're thinking is like each the first step only takes like big uh n raised to big okay and the second step also like takes p n raised to big okay the third step since there are only like a limited number of um elements in l y you can you can like limit the number of times just repeat two and that was like our intuition okay um yeah i don't know i haven't thought about this problem uh in fact since i wrote it many uh years ago got it which i like i like when i don't know like the solutions to the problem maybe you're like oh you're supposed to know the solutions to help me but i like it when i don't remember because then i can just try to solve it just like you yeah awesome um let's see so i'm kind of rereading it let's see the list of all partial solutions on y okay so you're saying this is uh not too many and for all the subsets um consider all the subsets there's a partial solution which has no extension in ly repeat all right aha okay um okay so you have a question about the first part like running in time added order k or you want to talk about that or yeah yeah yeah maybe maybe that was like because it seems like there's just some there's some details that like i'm not maybe like super comfortable or sure about like whether like each one like each step like how long does it actually take to run so i wasn't like 100 sure about that um so like for the first step the intuition my intuition was that um you have to enumerate through every possible um y and there's n choose um like this n choose like k plus one possible y's that are of size k plus one but then you have to do that for like um like n choose i some of them choose i from i equals to one to um k plus one uh true yeah okay well we studied this this kind of stuff in earlier yeah yeah yeah the year yeah yeah so this is just basically just goes in to how big okay that that's fine that was like okay okay yeah and then for each um and then for each like for each of those sets you have to like generate the list of partial solutions and then the intuition there was like you could just like check um like for each like element for each um each variable in y you could like set it to be one of the main elements and just check so that would be um like the cardinality of the domain raised to the power of like the size of the set um because that like make sense as the way to check off like to generate the number the steps it would take to like generate the list of partial solutions well let's see so for each such y there's like end to the order k y's yeah you want to consider all the x's that are a subset of y so how many of those are there oh sorry i was thinking about like still the first step but still the first step oh i see yeah yeah yeah okay so generate the list of all partial partial solutions on y okay good right right yeah so i just wanted to like make sure because i mean my intuition was that like you're basically like if you have some and if you have k elements for instance in y then you would have to take you would have to check like the the um the the cardinality of omega raised to like the number of elements possible like um possible like sets uh was it like instantiations off like that is that so that would be like the work done to generate like each one yeah yeah and and then this can get like and then we're can we assume that like the cardinality of omega is like smaller than um n uh yeah because i mean it says here that omega is considered to be a fixed constant so okay yeah cool cool yeah and that's eventually larger than that constant and everything's big o anyway so right so then this becomes also n and um big okay then sure i mean it's even like you know it's a const to the k so that's like two to the order k but anyway that's also even n to the order of k right exactly and order k squared is still end of order k so yeah that's fine cool right awesome and then and then for the next step we would consider all like subsets of x oh sorry subsets of y so um this was just like 2 to the k because this is just like the total number of subs that's like the power set off like y so it's like if you assume that the y y has like max is max um it's like has cardinality k then it's 2 to the k possible subsets of y that's x and then for each of those subsets you um check if there exists a partial solution so this is again one of those like big omega raised to k thing and that means that this whole step is to send you some constant raise to k i think uh yes that's right i mean like uh for a fixed y yeah all of this yeah for a fixed way from constant to the power of k that's true right okay cool that makes sense and then like the next step was just like eliminating um elements from the l y and we know that like there are only like into their okay possible elements in ly and at each step you you remove at least one so you can you cannot repeat the previous step more than n raised to okay steps correct that is that correct so then like if you if you like then sequence all of these things together you just like multiply them all out and then you get like n to the power some k plus like some other k plus some other k and it all becomes like um like big okay in some sense um yeah cool all right i think i think that was like yeah because there are some like steps that i just like wanted to like make sure we weren't like because because it was like somewhat complex the algorithm like worked through um yeah yeah and then maybe the next part was that like we want to check um that the primal graph has true with the most k so intuitively i understand what this like kind of means it's like if there are more than k elements that are involved in the uh in the constraints satisfaction problem like i was thinking of this as like sat for instance but like if you have more than k elements then your y like would never like be able to capture all like k plus one of them so the way i was thinking about it was like the primal graph will create a clique of size like k plus one at least then if you have more than k um elements and then like your tree with has to be more than k but does that make sense is that like reasoning because intuitively i understand what it's just saying it's just like the argument um how do you show that has like primograph has tree with at most k um i was like thinking of it like through the lens of maybe seeing it as like a clique on the primal graph based on the total number of like interactions that you might end up having between um k points or something like that um i didn't quite get it i mean we're not trying to show that any graph has tree width at most k i guess i guess you're saying that like if it has tree with um at most k then then um the this will give us like the correct answer like it'll it'll be like i think it's satisfiable is like correct not not like us not not it's not a maybe but like if you have a tree with which has higher than k then you may i'll put this algorithm in my output um i think it's satisfiable when it's not satisfiable because like with k plus one elements like involved in some in some um way uh in the constraint satisfaction problem you could like get some you could find a way of like setting those elements where the vcsp like is not satisfied yeah i mean you don't have to prove that though oh i mean it doesn't ask you to prove that if the tree width exceeds k then this algorithm might fail it just asks you to show that if the truth isn't most k the algorithm is correct i see uh what is so i mean so you need to show that uh so what does it mean for the algorithm to be uh like correct it means that like if you know correctness if the instance is satisfiable and uh it outputs you know i think it's satisfiable and if the instance is not satisfiable then it outputs not satisfiable well you don't have to prove this in general you just have to prove this assuming that uh the tree width is at most we want to prove this assuming the true width is almost k you see so i guess i guess the way i could think about the way i think about this then is that like it's a tree with his atmos k then there cannot be more than k variables that i i'm not sure which like definition to use here but i'm like i mean i i don't know whether it makes sense to look at the bag like the bag of the um primograph the back version of the primograph um or the tree decomposition of it or like you know to think about it from uh maybe perspective of like cliques or like planar graphs or something in in the tree or sorry in in the graph yeah well maybe we should figure out exactly what we need to prove and then maybe try an example sure usually good um see if the two prove two things like one and two which one do you think will be easier uh two two that if it's not satisfiable that will output not satisfiable right i mean actually i'm not so sure oh so actually that's not true i i was thinking um that like yeah yeah because because like that might be hard to actually know but if if it is satisfiable then it says it thinks it's satisfiable yeah i think that's easier number like the first one because you can just like because if it is satisfiable then like you will still have something left over after this like algorithm in ly and it will like output think satisfiable um yeah hopefully so uh sometimes it's easier to prove the contrapositive of things so the contrapositive i was hoping for a nice yellow color contra positive doesn't want to give it to me all right well contra positive of outputs not satisfiable problem two is if it outputs um i think it's satisfiable oh and that implies that it is satisfiable wait i thought it was a naught on both sides [Music] um let's see so i mean what okay you know the contrapositive of two i mean the contrapositive of oh this is the positive of two yeah yeah that's right that's right not b implies not a oh sorry sorry i i i forgot that this is i thought it was controversial one right it's controversial yeah that means oh yeah that's the contrapositive two yeah yeah and the contrapositive one is uh if it outputs not satisfiable and indeed it's not satisfiable and i think this one should be the easiest thing there's got one that's right um that's right yeah yeah i think that's that's correct number one should be the easiest thing because if you if you're not able if you're able to find like uh a constraint that isn't satisfied then like it's obviously not satisfied like in that you have to be careful with your language i mean let's say you find a constraint that's not satisfied i guess i mean you know uh constraining only is or isn't satisfied by a specific assignment right um i guess like in this in this way you this will happen only if there's like some empty list l-y so that means that there's um no way of selecting your um select there's no way of like selecting or assigning um assigning values to the to the subset x's such that like it satisfies at least like all the conditions needed to be needed for y to be like um a partial solution yeah hopefully the idea is that like i mean this algorithm is only that if this algorithm outputs like not satisfiable it's because it's really found like some it's like ruled out ever you know any possible solution right i mean it in intuitively it makes sense like but like i guess like the the verbiage is like um it because like you're basically trying every possible subset of y and then you're seeing like for every possible subset whether you can extend that uh whether you can create like a partial solution that like can be extended into like y and if you're not able to find that then then clearly there shouldn't be a um and there shouldn't be like some way of assigning your values of um your values of x such that like x is satisfied and then y is also satisfied or like you know that that that both are like partial both partial solutions are satisfied yeah well in fact i mean uh i feel like maybe not the whole proof but one should be able to like write the proof a lot of the proof like automatically without oh okay at least a little bit of it without like putting any thought into it in other words like so for example what will the first line of the proof one be oh the first line to proof of like the one off the the correctness one thing yeah like i feel like you can just start uh right at least the first sentence the proof oh you see so we want to like show the counter positive and then like you write out the contrapositive and then once you have the quantum positive that's like outputs not satisfiable if uh if it's not it outputs not a symbol if it's not satisfiable um then then probably you'd say wait if the um algorithm output's not satisfiable then l y is empty at the end there exists an empty ly yeah yeah there exists an empty ly otherwise it'll like have some element in it and if there exists an empty ly that means that there is no um there's no partial solution of one um yeah there exists some y where there is no partial solution on that y uh for which what is the thing the i'm not exactly i'm not exactly sure how to uh um say this because there's some like extension of f so x so basically there's some like x for which like there's no way of extending some partial solution on x such that like uh the two maps like basically um wait okay actually i think it'll be easiest to prove the forward direction not the contrapositive um at least for part one it might be easier to prove the contrapositive of two but let's see well if you try to prove the forward direction then we say the instance is satisfiable and then like if it is satisfiable then we know that there must be some y uh with i'll sorry all the y's must have um some element um in l y like del y must not be empty and [Music] yes i mean this will be like maybe the last lines i mean the first line will be like you know suppose instance is satisfiable right and then like the last line will be so the algorithm ends with all l y's non-empty right right and therefore outputs i think you know it's satisfiable so that's good we have the first and last sentences no sure the second sentence or the second to last sentence [Music] i guess it's easier to [Music] let me just work on the second one for now so suppose the instance is satisfiable then there must exist some um there must suggest some like assignment exactly for some partial assignment well there should exist some full assignment and then you can like reduce that partial assignment i guess yeah so there exists a satisfying assignment perhaps we should do an example um just an example with k being uh one okay so it's a tree then if it has it you know so k equals one so like the tree width is one so okay it's a tree or a forest and um let's say the uh the csp is three coloring that's my favorite simple csp so uh we're given a graph right and it's not that the graph that's okay it's like the instance of a three coloring is a graph but it's not necessarily that the graph right that's true with that it's like the primal graph right the csp the csp has incense at most uh is true with the most one now we can see if those are the same thing or not um so if this is the instance graph i just drew this kind of randomly right um then uh oh it was a good choice for graph to draw anyway um if this is an instance graph so then what will the primal graph look like it would be well i guess it is the same as the incident upon reflection right because you have like one vertex for each yeah vertices here and then you put an edge if there's an edge so okay it's the same that's lucky right uh okay so maybe we should just run the algorithm now right so okay for each y of cardinality one or two we need to generate the list of all partial solutions on y okay so let's do let's call these vertices i don't know one two three four 5 6. so let's do the ones with size 1 first so there's y is just the set one so what are all the partial solutions here i guess you can just do like all of them rgb yes yeah i guess so like there's basically i mean to draw them pictorially there's like one is red and then like one is green do i sell green yeah green like one is blue that's good i guess it'll be similar for all the other y's right two up to six okay now we're gonna do the y's of size two right so this this this is like l y i guess is this list right that's l that's fine yeah all right so two so one example is okay one two so we need all uh valid assignments for one and two how many of those are there gonna be we should be teachers too right [Music] yeah like well to draw a green green yeah like one two you have like red green and you have like red blue and then red well that's it green blue oh we have like the reverse of all these green blue green red okay there's a line oh wait it's just i guess that makes sense yeah because this is not um so it'd be like permeate not it wouldn't be like invariant to permutation so yeah like green red and then blue green and blue red it's a little bit long to draw but okay so that's that yeah that one has size six and then we also have like you know some y's that look like maybe like one three up here that's here's one and one three so how many how big is the the list for this one like you can have every possible color yeah exactly it's nine uh okay um so let's see so then what's the next thing that happens reach such y go through all the subsets whoa if there's a partial assignment there's a partial solution in its list which has no extension in like its parent list and delete it and all its extensions from all the lists so let's see we're going through the subsets right well i guess you know if x equals y then you're gonna i mean as long as the list is not empty it's not really an extension so we kind of only need to worry about like when x is like a strict subset of y right yeah so we might go through this one okay and it has two associated subsets you need to look at x equals one and x equals two um and their lists look like okay so for x equals one like lx kind of looks like like this yeah okay all right we have like one colored red is there anything in here where one is colored red well yes there's two things in there where one is two red right it looks like it's all good and so it looks like it won't delete anything right so it looks like probably it'll just nothing will happen and then there'll be no change to the list and it'll output satisfiable right uh at least i think so oh is there yeah because i think there's like always was some way to color um one where there will be something in the in the um in the y y subset yeah this is not the greatest example for two reasons first of all like i guess actually this graph does not have tree with one yeah yeah although it's still the algorithms still work right um but uh i guess another reason is like okay okay if the graph has tree width one then it has to be a tree right i guess like trees are pretty much like always too colorable so they're always three colorable so like it's not a very interesting situations we somehow like need to like get into like a potentially interesting situation we need like an example of like a csp that could be like not satisfiable on a tree so i mean wouldn't it be like something like if this is wouldn't an interesting case be like a serious parallel graph where you're like you'd be doing um tree with right where you're doing it with like what k equals to one and then you can show that there's some like problem with that or something well i mean i think the trickiest part is like trying to show that um [Music] if the algorithm doesn't like find any problem and outputs i think it's satisfiable that it really is satisfiable all right so like to sort of stress test that you'd be like interested in like a case where it's like maybe not satisfiable right right right right um for example yeah let me see if i can get like a good example in my mind um [Music] just give me a moment here [Music] um i'm thinking i could use that or something like not pre-coloring maybe okay maybe what we should try is let's say the csp yeah it's just a little tricky to construct instances of sat let's say the csp is two coloring okay and k equals uh one but let's say that the graph that we tried on doesn't actually have tree width one right so there's a chance that the algorithm might get the wrong answer right and i can actually give an instance where it does get the wrong answer let's say the graph is this this five cycle okay uh so this is not too colorable if you try to go like red green red green red you get stuck here right this is true with two right yeah this is true with two it's not a tree so it doesn't have tree with one but it is like a series um yeah so actually what we should maybe do is what would be like constructive maybe is like um do this algorithm with k being one yeah and see that like it doesn't work and then maybe you do the algorithm with k equals two and see that it does work right it's like the correct answer here is like not satisfiable right right right right exactly so if you do like k equals one then it might work and sorry if you do k equals to one then it shouldn't work it's possible that it works though right it's possible that it works but i think it won't because like it'll be a little bit similar to what we just saw so um here when k is one there are still again two kinds of i mean there's the y's of cardinality one and they'll all just have like size two like you know l y will look like you know just red and [Music] green yeah and then you'll have some y's of size two and there might be two different kinds so like there's like when you have an edge of like y is in this case like one comma two then l y will have just two possibilities it'll have like red green and green red red green and green red right um oh and there's also when there's like no edge and then this will have size four because it'll just have all four possibilities right red green green red red red and green green right and then um well i guess okay go ahead one uh i was thinking that like then if you take the x's then you probably are gonna end up with um some like some some x where it's not actually like where you'll end up with an empty set for the wait is there is there a way you could end up with an empty set in this though i don't think with k equals one because like basically it'll just go through all the singletons right and be like okay like you know it'll say to itself okay it'll try you know maybe uh yeah x is like one and it'll be like okay like it's either red or green but like for each of these like you know the vertex one being red it is consistent with like you know something in the other list right i think like when case yes like it won't detect yeah in fact that it's not too colorable right because it'll go through everything and it'll be like everything is fine like for every x you can like you can find some extension and because of that like it'll say um yeah it's pro it's possibly satisfiable so which is wrong right when it's not actually right yeah that makes sense so and if you run yeah i was gonna say so now maybe we should see it going right when k is two so the difference when k is two uh is like now you'll start considering like y's of size up to three right yeah right in case two then you'll consider y's of size up to three right and it's okay like initially like you know one such case will be they're all connected yeah like why that might be the most interesting case and then like l y will again have size two only yeah red green look like red red and green red green green red green right yeah if it's like one two three you'll get like these two possibilities yep and i think finally something will happen here and i'll i'll say why yeah or maybe you'll notice at least it finally get an instance of like something getting deleted so if you have an x that has size two so so some like subset of this with size two where it's like the two end points like one and three exactly yeah then you can't um you you can't like have the extension their site that's right so like if this was one two three you know it'll say like because initially the list for this x one three has like size four initially right right then and like in this checking stage you'll be like okay like here is one of the things that we had in l one three we had like red red right and then it'll be like oh but there's nothing here that has like red red right so this will like get struck out of like l one three right towards the exactly the beginning yeah yeah yeah that makes so it's kind of like the algorithm kind of realized that like it's not possible like red red yeah so let's see is it gonna like make some further deductions like this um [Music] i mean it can be red red it can't be green green either but those are like those are right so it's gonna get to like for example it'll get to like l one three that will just be oh i think i think i think it'll end up being zero right because it'll be red red and green green will not work but if you set red green then like that won't work with the l y when y has cardinality three um say it again so so so l one three will be that but then when you go through the second round like that's not going to be consistent with the y cardinal when cardinality of y is three right because their remaining options are one is red and three is green but that actually wait a minute sorry i turned her up i screwed it up didn't i like it's like oh wait a minute wait wait this was wrong the thing that rules out is like this right uh it rules out this gets ruled out initially for like 1-3 because it's like oh there's no there's no version of that up here right right right right right right that's right that's right so yeah i was saying it kind of wrong so like rather it realizes that like one three have to either be red red or green green right okay that makes sense um oh okay so now i think i know well let's see so then like l i want to claim that l 3 5 will similarly will just be look like this as well right that makes sense that makes sense and then as a consequence 1 will be empty then after that uh not well i think what will happen is then like okay so it'll be like back here and it'll like you know go through this and it'll chop down some of those like like l one three will get chopped down to smaller size and l three five will get chopped down a smaller size right but then like okay then i think it's gonna like loop again and now i wanna consider y to be one three five sorry one yeah one three five one okay so initially like here's like one three five and initially this will have like a few things in it like you know maybe they'll have like red and then i see green okay and then this could either be red or green right uh yeah it could either be red or green because like as far as the algorithm knows it's like well i don't know there's no problem it's pretty myopic so like doesn't three is not connected to anything so it's like it could be red or green as far as i know but everything in like l of one three five will definitely have one and five different ah that makes sense and that's inconsistent with the what we what we just like is it though no because one five one five are connected and if you take the subset one five um then we know that like what one five has to be zero or that you're sorry it has to be like have different um values right uh i'm just looking here so okay the actual rule is if there's a partial solution in lx that has no extension in ly then delete f and all its extensions from all the lists so i think what's going to happen is it'll how it'll look at 135 and it'll be like okay now i'm also going to say this is a superset of like 1 3 which is some x and it's going to look into like l one three and you'll be like okay oh and l one three like i see that one and three have to be both red or both green um so then [Music] those actually do both have extensions okay so maybe you're right maybe we should focus on l15 yeah one five uh okay so uh one five well if one five initially had just green green or sorry i had green red or red green right right green those are both those are both consistent with l135 uh one three i guess so i guess the problem here is like in one one three requires them both to be red so so the inconsistency here is that like through three like one and three need to be the same color and three and five need to be the same color so transitivity one and five need to be the same color but like that's like not work like i mean mathematically that's how i think i'm thinking yeah that's the reasoning but the question is like how does this algorithm actually notice it yeah yeah yeah yeah i think we just have to like think for the right way here um let's see so let's see when it's processing one three five it seems like a good idea it's going to go to like one five and it's going through all the things in one five right there's a partial solution in the list which has no extension then delete it so let's see one five will have these two things and both of these do have extensions yeah so nothing seems to happen but there's some inconsistency with this and uh l one five one three three five together like kind of like limit what one three five can be like i'm not sure like how to like say that in extensions but like if one five is like red and green or green and red and one three is only red red and three five is only red red then like you can't like extend all of that into like what we have for why but i think it'll take like two loops off the x's or something or like two loops or step two to like figure that out maybe like where but i'm not exactly sure how it would do that like would it like look for it it would have it it's eliminated something from the one three set and the 5-3 set but uh but now it's still like consistent right because like 1-3 can still be red red um yeah it feels like in fact we've kind of effectively reduced it's kind of like we effectively okay reduced the graph to like just one three and five and like this is like a like a simpler case we could have started within you know i start with this like five cycle which is not too colorable but even like a three cycle is not too colorable right right right um and so like this should also right this should this should also fail but like this one is tricky because this one will fail right away because if it's just the three cycle then here when you do the list of all partial solutions on like the three vertices you'll just be like oh like it's not too colorful so you're like right fail right away so it's got i mean we can't okay so we have to see like why does it fail still on the five cycle right um pretty sure it does but let's see um so what what else does anything else get eliminated we can think about like okay maybe we need to maybe we need to think about other vertices maybe uh one i guess like one three four wait why one three four well no i just thought of i don't know maybe looking at it like one three four i i was thinking one two three because one two three kind of kind of um limits like because it connects one to three then as well yeah we kind of did one two three didn't we though right but we haven't like like if you consider one three five and one two one two three one and five are connected in one of them and like one two three are connected in the other one so like that kind of imposes some constraint on one and three what one two three imposes some constraint on one and three and then one and then like that like will limit what like l13 can be i it's hard to say actually because i don't i forget like how exactly the extensions will end up working out like this is just mathematically true using like logic and sensitivity but in extension form what i'm not sure what the algorithm actually ends up doing though yeah i mean one has to be a little bit careful because you know as you say like it's quite easy for us like you know as humans to be like oh like i detect a contradiction but we kind of yeah the point is that like we have to show that this simple-minded algorithm even detects a contradiction right right right um okay so yeah so like maybe [Music] okay so one two three will look like this and then therefore like one and three will look like this and i think you know two and four will look also like this and so forth i don't know maybe let's just look at like one three four hypothetically okay so that's like pictorially it's like this one three four so like initially like this could be you know red or green and but these two you know had to be like red green or green red right red green or green red so at the beginning like uh i guess this had size four because you know there are two choices for this they could be anything and then there's only two ways to make these right but now i guess okay so i guess at some stage like the list for one three gets down to red red and green green right and so at some point there like it'll come back to one three four and say like oh it's got this subset one three i see and this the list for one three is just red red and green green so that'll like knock two of these down out it'll go down to two and it'll realize that like it can only be like green green red or this is kind of messy and hard to describe but like or red red green that's right so it like made some kind of progress there yeah one three four okay um [Music] right so maybe now uh oh but actually wait a minute now we're done right because uh then it'll consider like x like one four okay and this and if you look back at like one four that's a path of length two like in the first round similar to like one three it'll realize that one four can only be like red red or green green right yeah that's right yeah at some point you'll get that like l one four is like red red or green green right right right but then when it does like l one three four together with l4 it'll be like wait a minute like like red red has nothing consistent here and green green has nothing consistent here i see so it removes both of them then and then you end up with this set having nothing yeah wait let me try that let's try to like do that deduction again so like i'm like you know l135 this is like sort of shorthand at the beginning it kind of detects that like you know color of one has to be different from color of yeah three from three right and then i mean among other things then from looking at like one three four together with this oh so that okay yeah okay so this implied that like maybe another way to say it is like l one three is like red red or green green right and then looking at one three four kind of realizes that uh this has got to be either like red red kind of skip red green or green kind of skip green red right also knows from the first step that one for for the same reason right red or green green right similarly this got to be okay this got to be red red or green green and then when it processes these two like it'll get an empty set right okay that makes sense yeah the thing that this reminds me of most i don't know if this is you know as i said i haven't like thought about this problem in a long time so i like i don't even know so i'm just like suggesting things um the thing that this reminds me of most is like the helicopters or the cops and the robbers right like you know this thing had tree with two right um which kind of meant that like uh two well how many cops did you need to catch a rubber you need 32 oh wait no was it the three-way 32 oh you needed three right yeah yeah yeah you can tell i guess let's see so like two cops cannot catch a robber here because like let's say the robber like is sitting here and then like a cop lands here and a cop lands here the robber's fine but then like if this cop comes out in the air to try to catch the rubber the robber can just run wherever right but like three cops were sufficient to catch the robber right and um i feel like i feel like this was a little bit like somehow three cash cops catching a robber it sounds weird but like right right uh like somehow we studied one three we somehow studied like one two three and like made a deduction about one and three it kind of reminds me of like i don't know this cop two going up in the air and then like sorry sorry one three four and like this kind of reminded me of yeah forcing the rubber this way maybe that's not the best way to look at it um i mean i'm sure there is some equivalence there since yeah yeah yeah i mean i guess like the the the cop strategy is also the same as like this like bag decomposition right that makes sense um yeah i guess that makes sense so so in some sense knowing the tree with kind of like allows us to catch the robber or like figure out how to that that like using these like all possible permutations by enumerating every possible permutation you'll eventually like figure out the inconsistency but like if you have too many like possible combinations like if you're in the case where you're in a five cycle but you're only allowing three with like you're only allowing white to have like size one then you can't you can't like narrow down that inconsistency in some sense yeah now when it comes to actually proving it i mean my guess is that my guess is that we want to use the contrapositive so that if it outputs that it thinks it's satisfiable then it really is satisfiable okay and again you can think about like okay what is the structure or outline of this proof gonna look like it's gonna have to start by saying like okay suppose the algorithm outputs you know thinks satisfiable and then that'll be the first sentence of the proof and then like the last sentence of the proof is like therefore the algorithm is or the instance is satisfiable yeah right and like this this line has to be like you know the second last line so we found a status like you know we found like a globally satisfying assignment yeah and we can also go to the second line of the proof the second line of the proof is suppose the algorithm well the first line is suppose the algorithm outputs thinks it's satisfiable so that means what does that mean it means it like got to the end or rather it got to a situation but i'm not scrolling properly it got to a situation where like nothing happened like no nothing additionally happened here and um right none of the lists were empty right like even if things are removed it's not enough removed to be completely there's all there's there's going to be some no all the lists have some element in them exactly so like at this point we'll say since algorithm outputs things satisfiable like we know you know um for all you know y's of sub cardinality most k plus one we have some you know non-empty list l y you know so it has like you know it has at least one partial assignment in it right and we also like kind of know like what else do we have we also have um i think one should think about this in the context because another thing we know is like we know the the primal graph has free wit tree with okay so it has like a tree decomposition you know with bags of cardinality at most k plus one which might match up with this k plus one somehow and the other thing we know is that like the algorithm finished so like you know the algorithm couldn't okay i'm putting this like a little bit informally but like couldn't detect any contradictions yeah with these you know lys yeah and like so now here's like where the mystery and the magic happens but like at the end of it so it's kind of like we've got like these like sort of locally consistent solutions right you can only appear at like groups of k plus one variables everything looks fine and we have to somehow argue that um we can patch them all together into like a globally satisfying assignment right right and that that that would involve some like notion of appreciating that like using the tree become not treat decomposition but using like the fact that like we can actually um we we have some like tree with k so that like if you look locally that's sufficient in some sense to like know the global setting because there's no way to like if or it'll be some contradiction or something like that where like if you had more than like like if you if you only have like k possible k plus one possible elements in a bag then you can't like find a contradiction for like then this algorithm would find like the contradiction or something like that yeah exactly i mean i think what will happen is you'll say like okay we have all these partial assignments expect to be like some not too hard way to try gluing them together into a global assignment and then the main work will be to argue that this global assignment really satisfies all the constraints right and i think you'll want to say that like well if it failed to satisfy some constraint then the algorithm would have detected something like detecting the problem and yeah you'll have to use the tree decomposition and somewhere in that reasoning that makes sense but that seems reasonable i i mean that that sounds about right yeah i guess that's how it'll go cool yeah that that's really useful thank you yeah i mean i haven't done this type of stuff before so it's like very interesting to see how like they tackle it it reminds me a little bit of um in machine learning when you have like interactions between variables and you're trying to like capture interactions and then like the more interactions you have and the more complex you're set up would be like harder the problem is and the higher the sample complexity you need so i it it would be interesting to see if like there's some relation between like the pro if like you have like a problem where all of your variables are very highly inter interrelated then like the problem itself is harder and as a consequence like you need like more more data or like your algorithm needs to be more powerful or you need to have like more compute but if you have like a really simple like linear classification where everything is independent then like this like reduces to some like you know poly algorithm or something like that yeah like it reminds me of like there's these like you know phase networks or like markov networks we're like yeah tree like it really uses the fact that like these facts are like oh if it's like a tree then something is really cool and that like if you condition on one vertex and like everything all of it's like sub trees become like independent and things are really easy and then like you can have like more complicated ones where they have these like markov blankets where like if you like exactly like this small piece of like one or two or three vertices and everything else becomes disconnected that's really kind of exactly what's going on with the the tree with here and yeah yeah yeah it's really interesting i guess because like i mean even like in research where you're thinking about complexity of your data set and how hard it is like sample complexity currently if you have if you don't use like if your data is inherently complex or some like inherent complexity in it maybe there's some interesting way of like capturing that complexity through like tree with or something and that could like give us indication for like how much sample complexity we need for like a downstream task or something that which is interesting i mean i i guess like current current formulations kind of ignore the problem itself but like that that's been shown to not work pretty well so like if you can like try and identify tree with kind of problem with tree with or something of this sort in the problem set up it might give us some indication into like how how hard the problem is in some sense yeah i expect you know it is a strong assumption having small tree with but i expect like you know from like many machine learning problems they'll have the property that if somehow the data is coming from like a model like some kind of graphical model and the graphical model has small tree width then like some algorithms will either work correctly or they'll work much more efficiently yeah yeah very cool yeah it's very interesting i i mean it's a very interesting like take which i haven't seen before cool cool uh okay uh thanks for coming let's let's wrap it up and uh good luck with the rest of homework thanks thank you so much\", metadata={'source': 'U_WCq-IxrcQ'}),\n",
       " Document(page_content=\"all right great so this is lecture 24 this is gonna be about information theory so information theory has a classic textbook by Coburn Thomas but there's more modern stuff out there which we'll talk about a little bit and it includes a long list works by Braverman recently more time all right okay so today's topic information theory there's lots we can talk about I'm gonna talk about some of the basics just definitions about entropy and mutual information and then we'll see one application information complexity that's an application to communication complexity so I'm doing an application so that it connects with previous lecture okay so let's start out with entropy so all this topic information theory was developed almost in one shot by Claude Shannon in 1948 in particular he gave this definition of entropy which we're gonna describe now so let's start with an example let's a say that X is a discrete random variable takes on finitely many values so in this example here X takes on the values red green blue and yellow and the probabilities are as shown up here in this example half quarter eighth and eight so first I'll tell you what the entropy of this random variable is intuitively so the entropy is usually denoted H of X it's a number so say with the random variable and one way to capture it intuitively is to say it's the amount of randomness in X in bits another intuitive way to describe it is it's the minimum number of bits needed to generate a draw from X on average if you're using the best scheme so imagine that you need it to like write a computer program that like output draws from this random variable X with these specifications the only thing you're allowed to do is like flip a fair coin how many coins are gonna have to flip on average with the like most efficient program this is what we're asking ourselves so one scheme you can do for this particular X is first flip a coin a you know comes up heads you can output red because red is supposed to come up with probability half comes up tails now you can flip another coin and if that one comes up heads you can output green so then green will you know correctly B output with probability 1/4 and so forth you can flip one more coin to decide between blue and yellow and now we're gonna compute you know the expected number of coin flips your program use to generate this random variable X well with probably half you used one coin flip it's probably a quarter you finished in two and well with the remaining probability you finished in three so if you add this all up an expectation you get on average 14 eights or 1.75 coin flips okay so the entropy of this random variable X is gonna be exactly 1.75 another way to describe it is it's the minimum number of bits you need to store a draw from X on average using some kind of compression scheme I show you many ways like you know there's not a lot of difference between communication and generation and compression so you could use that scheme that we talked about to generate X and just record the coin flips you use and use this as sort of like a code by which encode a draw from X so for example if you're trying to draw you know code Charles from X you could say all right read I'm going to write down heads green all right down tails heads blue lb tails tails heads yellows tails tails tails and you know in some cases you're using one bit some pieces using two bits in some cases you're using three bits to record this variables outcome and this is a nice example the scheme here is what's called a prefix-free code which means if you're drawing several instances of this R in variable X and getting a sequence like red blue red green green you can just write down these encodings with heads and tails with no additional punctuation in between them and you can recover any sequence so like tails heads tails tails heads heads heads if you look back at this chart at me is first you flips let me get my little pen out first you flipped green the first draw of heads was green and then the second draw was blue so you wrote the ho tails tails heads and and you flipped an H got uh so you know did flip an H but you got a red as he recorded with heads you got another red we recorded with heads oh you know another way to think about it intuitively is that imagine you know your friend took a draw from this random variable X and got like either red green blue or yellow and then you had to like play 20 questions with your friend to determine you know what their outcome was red green blue and yellow and you had to only ask yes/no questions and it would be like sort of the minimum number of questions on average you would need to correctly determine what their outcome was there's a question do you always mean the question from the audience is do you mean there's always a prefix-free encoding which is the best well actually there is always a prefix-free code again is the best and in this specific case as well mention in a second I chose an example which has a very special property namely all the probabilities are powers of you know inverse powers of two in that case this is uh the analogous thing to what I did for a general random variable leads to a prefix code which is the best when you don't have this powers of two thing gets more complicated and as somebody else pointed out in the questions the the best thing you can do is something called Huffman codes which we won't actually get into in this lecture this is also a particularly trivial example because like if you think about this yes/no 20 questions version of it then like the optimal strategy is kind of a very silly strategy where like the first question should be like well is it red if it is then great if it's not then your next best question is what's a green and then your next best question if not is you know was it blue but you can have a more complicated scenario with a more complicated random variable so as I mentioned yeah so all these you know intuitive definitions of entropy that I've given so far actually exactly you know correct if X does satisfy this special property that all the probabilities are inverse powers of 2 like 2 to the minus K for some natural number K then all those into of definitions are exactly correct and this would be the formula gave me good you just imagine for example the sort of example we saw with the heads and tails you imagine the expected number of coin flips your program would need to generate a draw from random variable X you could have go through each possible outcome red green blue and yellow those are the little X's here and you know there's some probability that I've denoted P sub Capital X of little live X so capital X means the random variable little X means like an outcome like red green blue yellow and if there's the probability that you would be trying to generate that outcome and then the number of you know coin flips you needed was like you know the log base 2 of the reciprocal riots so like blue correspondent 2 like I don't know tails tails heads or something corresponding to 3 flips which is like log base 2 of the reciprocal of 1 hangs so probably you've seen this formula before at some point this is the definition of the entropy of a random variable X it's given by this formula and okay we get imagine applying it to maybe a different discrete random variable X like this one where the probabilities are not like all like you know negative integer powers of 2 so like you know X is a but probably I have to be with probability 1/3 C with probability 1/6 now if you imagine like trying to write a computer program that you know generated draws from this distribution X or like you tried to you know communicate a draw from this distribution to your friend by sending a few bits as possible well things start out well with this probability of 1/2 you know it's probably a half you can you know communicate or store one bit but then with probability 1/3 you know you would want to try to send or store or use like log base 2 3 bits according to the formula and that doesn't really make sense because that's not an integer so you don't have like a perfect correspondence between the intuition and this formula in case that you know these these things these probabilities have the property property that log base two of the reciprocal is not an integer well you know you just go with it I mean you just accept it and so this is the formula that gets used for any random variable X and then it doesn't have like an exact perfect correspondence with these intuitions that I've mentioned before but they're good intuitions in fact we'll see in a moment how like you can make them even more precise these intuitions so just on the topic of the this coding a situation like if you want to store like a bunch of draws from this random variable as bits you know one thing you can do is just take the ceiling of all these quantities to make them in integer then it's okay so like you could have a scheme for storing like say draws from X where you'll use heads to store or even a tails heads to store B and tails tails heads to store C and then it's a bit wasteful because you're not using the sequence tails tails tails for anything but that's life and this is a pretty good way to you know compress a bunch of draws from a random variable into bits this naive way of exactly defined it here but I sort of sketched it is called the Shannon or Shannon Fano code and it's pretty good scheme as mentioned by somebody else in the chat there's actually a better scheme that uses fewer bits on average called the Huffman code is sort of the optimal scheme in general but you know this scheme where you sort of just like pay up to one extra bit per draw due to the fact that you know these logarithms are not necessarily an integer is not such a bad scheme so in general how many bits do you use on average if you try to store your you know draws from your random variable or communicate draws from your random variable in this way well sort of the same formula except you have this like ceiling in here and this ceiling is that most what you get if you take the actual you know real number and add +1 to it so if you look at this expression here and like you break it up into its two parts the first part when you expand out gives you the actual you know entropy term this thing here and then you get plus the sum over X of P of X times 1 so that's just plus 1 okay so overall this simple method of encoding or generating or storing draws from a random variable X uses a number of bits which is that most the entropy plus 1 okay so as always if you have questions do you type them into the chat question is is it hard to determine if a given encoding scheme is uniquely decodable whether has that well these schemes were mentioned like I've liked this property that's different from the proper the situation when we're talking about coding theory well the difference is that the number of bits he used to encode a message or a draw is not always the same it's like a variable length encoding and there if you're not gonna use punctuation then you need your but you will do want a variable length encoding your you need your scheme to have this prefix-free property where no code word is a prefix of another code word not of course is easy to check if you have a whole list of code words and if your code has that property then it can be uniquely unlike a sequence of encoding x' can be uniquely determined it can you know be turned back into there's a unique way to break it up to get code words yeah the question is non prefix-free codes can be uniquely decodable yeah probably possibly I'm not a hard % sure to be honest ok Oh all right so this is the formula and let's uh if you haven't seen it too carefully before let's go over some basic facts about it so okay it's again it's a number that's associated with a random variable that somehow measures like how many bits of information a draw from it carries and so here's some basic facts I'm gonna ask you to fill in these blanks in a moment so think about those let's say you ran a variable X can take on one of our most capital n values then it's a fact that this entropy is a at most log base two of capital N and it's always non-negative as well so it's at least zero okay so we can think about when these two inequalities are sharp so feel free to type into the chat the situation what random variables X have zero entropy and which random variables have log in bits of entropy where n is the bound on the number of outcomes yeah people of set it correctly that's right so the first case a random barrel has zero entropy if it's constant if it's not really variable if it's always the same value or it has like a probability of one of some Malcolm and has maximal entropy log base two am an if and only if it's uniform on the hand possible outcomes hm probability 1 over N and that's true whether or not N is a power of two or not good so sort of the least variable and the most variable random variable and there's a lot of when you're proving things about like entropy and other elements of information theory there's like a lot of like little exercises that involve like writing down expressions like logarithms in them and mainly using the fact that like the logarithm function is convex or like the logarithm of a reciprocal function is concave and using you know Jensen's inequality and things so I won't prove it I'll only do like maybe one of these proofs but like the proof of this fact the second back is an exercise for you using concavity okay so let's go back to this slight annoyance where we saw that like let's say we have a random variable X and it doesn't have this you know pleasant property that all the outcomes have a probability that's like 2 to the minus K for some integer K we saw that like if we want to you know compress a draw from this random variable X we could you know invents like a simple scheme where the number of bits commute or like stored on average or communicated on average if we're communicating it was that most the entropy plus 1 and the post ones kind of annoying and but there is a way to get rid of it and to kind of get around this fact that you know I mean ultimately a bit number of bits has to be an integer and that's the thing about the idea of amortization or about like what happens if you're gonna rip repeatedly draw from this random variable and store like many copies of it or repeatedly get draws from this random variable and communicated and to think about like how many bits on average you have to communicate so let me go into some more detail about that so let's start with what's like a sort of simple fact let's say that X and y are independent random variables so this independence is going to be important for what I'm about to say so you can view the pair X comma Y as itself as a single random variable ok you just say oh stick x and y together and call like that one object also a random variable and it's often or at least in information theory often just denoted by concatenation as though it were a string XY and when x and y are independent it's not hard to check from the formulas that the entropy of the joint random variable X Y is just the entropy of X plus the entropy of Y and here's the intuitive proof of it you know if you take the viewpoint that like ok entropy is the number of you know random bits you would need if you're writing a computer program to generate a draw from the random variable in question well I mean on one hand if you want to generate a draw from this random variable X together with Y like one thing you could do is take you know the best most efficient program for generating a draw from hex and the most efficient program for generating a draw from Y and like just run both of them and now you've got to draw from like x and y independently and you know the total number of bits you used on average was the sum of the number you needed for X in that arena for Y so in some sense that's like an upper bound that would show that or that would give a sketch of Y we have the upper bound here and that's for the lower bound just basically I mean if x and y are independent random variables like there's no savings that are possible there's nothing you can really do except separately try to generate X and separately try to generate Y so this is a two improvement like this is the prime proof intuitions look like it you very very far in information theory um ultimately you know you have to write down proper mathematical proofs which look more like this so as I said although I'm going to show you this one proof and then I'll just rely on intuitions for mostly other facts that we talked about because the most cases like these proofs are kind of like an exercise in like manipulations and they followed the intuition so we can you know write it on the definition since we're doing you know a first example here ok so by definition the entropy of the joint variable X and y is well you sum over all possible outcomes for this random variable XY which consists of pairs little X and little Y and you get the probability of seeing this pair little X little Y times log of 1 over the probability of this little X and little Y and now we crucially use the fact that we're assuming the random variables x and y are independent in this story which means that well it means that this equality is true that like the probability of seeing some little x and some little Y is just the product of the probabilities the probability of seeing little x times the probability of seeing little Y we all can also break up the sum into a double sum because the range of possible values for this joint random variable X Y is just the range of X together at the range of Y and then we use the property of the log function that log of the product of two things is the sum of the logs so we kind of made the same replacement about the probability of XY being the product of the probabilities here in this expression and then we separated that into a sum and then in this next line I kind of asked you to imagine like you know expanding or distributing across this sum and you'll get a term that looks like the sum over X and the sum over Y of px py and then log 1 over P X so if you put all the X stuff together you get the entropy the expression for the entropy of X but you also have this sum over Y where you're just summing the probabilities but luckily the sum of the probabilities of Y's is 1 okay I'd like you see this term and similarly this second logarithm expression gives you the entropy of Y times the sum of the probabilities that's for X and that's 1 so you get this H of Y ok so that is the formal proof of that equality ok and you can iterate this and so corollary you get that let's say you have n independent random variables then the entropy of that single random variable that you get by like just treating them all together is the sum of the entropies in a particular if you have like one random variable called X and you make like n independent draws from it and thereby get like sort of copies x1 through xn independent copies of this random variable X then the entropy of like all those draws together it's just n times the entropy of like a single copy okay so let's remember that fact because it relates to this amortization that we were talking about like imagining you know instead of storing or communicating or generating one copy of X like let's imagine storing or communicating or generating like in a large number n of independent copies of X so okay so let's say you were trying to store or communicates in drawers from some random variable X well one thing you could do is you could say okay I know like a scheme for communicating like one copy of a draw of X and it used like entropy of X plus 1 in the worst case many bits so that if I need n copies I could just do it n times that it's the total number of bits I would get is n times the entropy plus n just ok actually but it is a little bit wasteful so as an example like let's say X is a uniformly random trip it's zero with probably a third one was probably a third and two with probably a third so the entropy of that random variable is 1.6 bits log base 2 of 3 you can check and so this scheme that I mentioned here you know basically uses 2.6 bits on average per trip you know be like I mean code like zero with heads and one with tails heads and two with tails tails or something like that and so overall you'd be using 2.6 n bits on average to send entrants you can see that's kind of a you know a waste because really if you have n draws from this random variable X it's like you're getting like a random number written into within turn read it so it's like basically a random number between 1 and 3 to the N or 0 and 3 to the n minus 1 you should really be able to encode it with a number of bits that's like basically the log base 2 of that which should give you this log base 2 3 per bid like 1 point 6 n bits and so you see using this corollary up here you can do something different at least in principle you could you could instead say wait a minute like let's just treat this X 1 through X n as like one single random variable whose entropy is 1.6 times n by this corollary and now we could have a scheme for generating or communicating or storing a draw from this like n fold random variable and the number of bits we use would be the entropy n times the 1.6 the base entropy just plus 1 ok so instead of plus n we got like plus 1 if we kind of batch to these draws together into groups of N and that's kind of cool because that if you you know divided this quantity by n to sort of see the number of bits per batch that you are per draw I should say that you used it's like exactly the entropy plus now sort of plus 1 you know plus 1 over N which is tiny it's like going to 0 as it's a little of 1 as n goes to infinity so by using a like a different scheme where you like group your draws together into big batches and encode them using some kind of binary coding scheme like this Shannon Fano code you get to a situation where like the number of bits you communicate per draw of X like in the long run is you know asymptotically close to the entropy H of X so this kind of shows that like H of X is sort of exactly equal to like the amortized cost of transmitting one draw from the random variable X if you're allowed to amortize over like a you know a large number of independent draws okay so this box I guess is what I just said\", metadata={'source': 'b6x4AmjdvvY'}),\n",
       " Document(page_content=\"okay so let's move on to talk about another topic event information theory mutual information and once we define this it'll lead us into the third topic for this lecture which is connecting everything with communication complexity uh okay so let's say x and y now are random variables but they're not necessarily independent there possibly dependent and so now let's say I ask you okay write me a computer program that you know uses random bits and it it's supposed to generate draws from this joint random variable x and y taken together well then you know the number of random bits needed to generate this pair is sort of ideally or in the amortized long-run entropy of XY house we saw the last thing we talked about was that if x and y are independent then this is just H of X plus h of Y there's no way you can sort of get savings I mean they're independent so you basically have to spend your bits to generate X and spend bits to generate Y and that's all you can do but if they're not independent random variables you may possibly be able to get savings beyond this H of X plus h of Y and this savings that you can get or sort of the most savings you can get is exactly this quantity called the mutual information between x and y so this again a number it's denoted I of X and y with for some reason a semicolon instead of a comma between x and y it's a number associated with a pair of random variables x and y which you imagine are definitely not necessarily independent ok what is it it's H of X plus h of Y which is like how many bits you would have to a computer program would have to spend to generate a pair the pair X Y if X and y were independent - the actual like optimal number of bits you need to generate X and y together so sort of the savings you get by the fact that x and y are dependent so let me I mean it's best to really not focus too much on these formulas involving logarithms which is sort of suggested before but anyway if you write down the form you know you see that it's got this expression you know the sum over all possible outcomes little X and little Y and the probability of little X little Y this piece gives you an eventually leads to the H of X term this piece leads to the HM wide term and this piece which is subtracted leads to this term and then by properties of logarithms you can turn it into this expression here and the main reason for writing this down is like this expression and you can use to formally prove that the mutual information is non-negative okay so if you think about it it's kind of clear that it should be non-negative because you know it's supposed to represent a savings which is like a non-negative quantity okay so let me give an example so here I have the complete sort of truth table but like probability table for two random variables x and y which are not independent and X can take on the value is heart 0 or 1 well I can take on the values heart a B C or D and it's like this you know it's probably a half they're both hearts with probability 1 over 16 x is 0 and Y is a and so forth okay these numbers add up to 1 okay so let's uh compute everything just to get like a feel for these numbers and quantities so let's first think about like the entropy of X like just imagine like you did a draw from these things but like through the outcome of Y in the garbage and only thought about like the the random variable X so basically you would see this situation here the random variable X is heart with probability 1/2 and zero is probably a quarter and it's 1 was probably a quarter okay so you have a write available that's probably bilities are like a half quarter quarter so it's entropy is like 1/2 times 1 plus 1/4 times 2 that's the log base 2 of reciprocal of 1/4 plus another quarter times 2 so that all comes out to one and a half so H X has like sort of one and a half bits of information or if you only had to generate X it would take you and a half bits on average alright because actually it's quite easy flip a coin if it comes up heads you'd be like Max's heart otherwise you flip one more coin to decide if head X should be 0 or 1 so half the time you're flipping one coin half the time you're flipping two coins so on average you flip one and a half points conversely if you look at this random variable Y and just ignore X you know it's heart with probability 1/2 and it's a with probability 1/8 because these are the two outcomes where it's a in fact it's a B C and D always probability 1/8 so that's entropy is you know the simple calculation it's - okay so if you just needed to generate Y would take you to coin flips on average okay so therefore the sum of the entropies is 3 and 1/2 but if I said you know write me a computer program that generated x and y together according to this distribution and try to minimize the number of random bits you need you could do it with less than three and a half so in fact right it's just like sort of the entropy formula applied to this column sorry for the terrible circling but a random variable that's probabilities are 1/2 16 16 16 16 16 so it's like 1/2 times 1 plus 8 copies of 1/16 times 4 which is 2 and 1/2 okay therefore finally the total infer the story the mutual information is 3 and 1/2 minus 2 and 1/2 which is 1 okay let's check that this makes sense this is supposed to represent like the number of shared bits or sort of the number of like you know the amount of savings you can get by virtue of the fact that you're generating x and y together as opposed to like independently and it's pretty clear right in fact the way you would generate this random variable is like you know first you flip a coin if it came out heads you'd be like ok both of them are hearts otherwise if it comes up tails then if you like pure at the rest of this table here you actually see that they're independent like once you've decided that they're not both hearts okay and so like X you know you generate independently from Y you flip a coin and for Y you flip two coins to tell if it's a B C or D okay so in other words it's sort of like the number of shared bits in your like computer program for generating X&Y together that you first spend to reduce to the situation on where the remaining calculations or the remaining generation is done independently so does this make sense okay so we're gonna study this mutual information into little bits to get a bit more of a feel for it um so let's see how big or small I can be so this is also as I mentioned non-negative and I'll ask you again to fill in the blank here when is the mutual information zero you can kind of tell based on the name even what property must X&Y have for the mutual information to be zero independent that's right yeah we saw before if they're independent then the entropy of the joint random variable XY is the sum of the entropies so the mutual information will be zero okay so upper bound you could say to yourself well the initial information is H of X plus h of Y minus the entropy of x and y together so that trapeze are positive so that most an upper bound is H of X plus h of Y that's true and I wonder if someone in the chat can tell me when this is equality is achieved this one requires or thoughts I'll take a nice sip of water yes one person said when x and y are constants that is correct this is actually not a great upper bound it's true and sometimes it is achieved but the only time it's achieved it's like both x and y are constant random variables and all the entropies are zero it's a very trivial case and I think you know you'll understand this better when I just give the better upper bound which is the better upper bound is an upper bound for the mutual information between x and y is whatever is the minimum of the entropy of X and the entropy of Y and the idea here is remember like you know this mutual information is supposed to represent like the savings that you can get from the fact that you're generating x and y together as opposed to like independent copies of x and y and the most you can save is everything so I mean if you know the entropy of X is 3 and the entropy of Y is 10 and now you're trying to generate them together ok you're gonna need that most 13 bits depending on whether or not if you need 13 if they're independent but if they're not independent like at the at the very least you have to generate Y if you're gonna generate X and y together and generating why it takes entropy of Y which in my little sorry example here was 10 so the most you could ever save is 3 which is the entropy of X and of course let me ask you again you know under what circumstance I should have reversed my example but anyway under what circumstance let's say that H of Y is the minimum Y has less entropy than X when is it the case that the mutual information between x and y is equal to the whole entropy of Y it's like asking you when you're generating X&Y together as opposed to independent copies what is the amount of savings you can achieve entirely like sort of all the entropy of why yes somebody has said in the comments correctly when uh well somebody wrote I think he wrote it a little bit backwards let me said X is the F of Y but it may be the other way around this when y is a determined by X okay so if Y is determined by X then you're in a situation where you can like generate X which cost you like entropy of X bits and then you don't need to spend any more bits to generate Y because it's determined by X so you in total spend entropy of X rather than spending entropy of X plus the entropy of Y so like the amount you saved with entropy of Y props that'll be more clear with an example so let's do an example let's say X is a random n bit string and Y is like the XOR of all the bits in X so these are two non independent I mean dependent random variables X is an N bit string Y is the XOR the bits so X is an N bit string so it's uniformly distributed the entropy is N and Y itself if you only squinted and saw Y and ignored story on X then Y just looks like a random bit so it has one bit of entropy but um if you were to generate X and y together you can sort of see that the best thing you can do is just or you know the best thing you can do is generate X and then just compute deterministically the value of y so you use you spend n bits to do that and so you're savings over the sum of these two things you spent n out of possible n plus 1 so your savings is 1 ok it kind of makes sense I hope to say that the mutual information between this x and y is one full bit it's like the the entropy and y uh okay so this was their example and now I want to look like a slightly less trivial example where why is not completely just a function of X let me just set up a situation where you know X is not a function of Y and Y is not a function of X but they know they're not independent they have some dependence and I'll dish you in a kind of like a simple way like let me just say now Y is two bits it's a two bit string and the first bit of Y is the XOR of all the bits of X and the second bit of Y is just like another independent random bit okay so just something meaningful about this example just some simple one I randomly cooked up so you can talk about like generating the pair x and y together but let's take the I mean communication point of views let's say like you doesn't matter which analogy we talked about mention you're gonna like try to make a draw for you get a draw from x and y you want to communicate the draw to someone using as few bits as possible I mean an idea is you could first communicate the common part I've drawn like a little bend diagram here to try to illustrate this like sort of the you know spend enough bits for the the joint amount of information between them which is one bit in this case and that having communicated the sort of the common part you know a number of bits that's equal to the mutual information then you kind of communicate or spend in generation terms like the deficit of H of X that you need to make up X and you also communicate the deficit so I should have drawn like this the deficit from H of Y that you need to communicate why okay if you remember back to that example with the hearts like they're the mutual information was one bit you can imagine first communicating the single bit that tells you are they both harder knots and then you've got reduced the case where they're if they're not they're sort of independent of each other and you had to like generate the rest for X and the rest for y so I say all this to motivate another definition so there's sort of the almost the last definition or simple definition the term we're defining here is the conditional entropy of Y given X and it's sort of like intuitively the green shaded part of this Venn diagram region or this piece here sort of the information component of Y that excludes the mutual information so as a formula and as a formal definition it's again a number it's the entropy of Y but minus the mutual information between x and y and as we saw from last time mutual information it's always at most the minimum of H of X and H of Y so in particular it's at most H of Y which implies that this quantity is non-negative there's also the symmetrical definition the conditional entropy of X given Y but let me just focus on this one right so this Venn diagrams are an information theory are helpful they can when you have more than two random variables they can be harmful but when they have two random variables are generally helpful right so here's this definition again the conditional entropy of Y given X and we can just expand out this formula I mean just to get another formula the mutual information of x and y is this the sum of the entropies minus the joint entropy so we plug that in we get a different formula for the conditional entropy of Y given X it's the entropy of the joint random variable X Y minus the entropy of X we're like in the Venn diagram terms right it's sort of like the total amount of information content in this blob that's like H of X Y and then you take away of X the circle okay you get the green shaded region and if you rearrange this inequality with this you get like a expression which is called the chain rule in information theory which is that the entropy of a pair of random variables x and y it's like the entropy of X plus the entropy of Y conditioned on X okay so you'll see that get used quite a lot in information theory ah and I just wanted to write like yet another I mean I'm just writing all possible rearrangements in this formula so like another rearrangement of this formula is this and it's giving like sort of a inwards description of the mutual information so the mutual information between x and y is also the entropy of Y minus the entropy of Y conditioned on X all right so let's like sort of taking like this circle here which is the entropy of Y and subtracting out the green part and getting the overlap part and you can kind of read this H of Y entropy of Y minus entropy of Y given X you can kind of read it in like English language terms these are all like you know intuitive crushes to help you reason about quantities that you'll eventually reason about by arithmetic and formulas with logs and things you can read a nice like the amount of information gained about Y knowing X so you know it's like if a person like knows the outcome of X it's like you know somebody draws x and y and then LS like comes to know the outcome for x how much does she learn about the outcome for y what's interesting about this is like this formula is a symmetric I mean it x and y play a different role even though we know the mutual information is a totally symmetric concept remember it's entropy of X plus entropy of Y minus the joint entropy so it means you know we can exchange x and y the left-hand side doesn't change but the right-hand side does change and so this is another formula for the mutual information and it sort of says in you know according to these definitions the amount of info gained about X knowing y is the same as the amount of info about Y knowing X so that's maybe not the most I mean depending on how you look at it that might or might not seem intuitive but it is true under this way of looking at things okay so I just put these formulas up here and the men diagram again and I want to say actually this entropy of Y given X is in my opinion kind of bad notation or maybe it is good notation but it's like misleading in at first because it looks like it's somehow the entropy of a random variable Y conditioned on X but it's actually an expectation and I'm gonna leave this to you as an exercise again it's just like writing down the formulas but this conditional entropy of Y given ax it's really an expectation of something it's an expectation over you draw like an outcome little X for X and then having gotten a outcome little X for X like Hart or 0 or 1 you consider the random variable which is y but conditioned on capital X the random variable taking this value little X so like if little X's Hart you know you think of like oh now let's imagine the random variable Y conditioned on X being Hart which is actually a simple random variable it's just constantly Hart I don't if you remember back my example with the hearts and zeros and ones and ABCD so in that case the entropy of that random variable is zero but some other time you know X might be one little X might be one and then the random variable Y conditioned on little X might just be uniform on ABCD and that has entropy too so even though like this expression starts with an age it's really like an expectation over the X outcomes of the entropy of a conditioned random variable okay so exactly need later about something to watch out for okay so uh the last thing I want to tell you is this is really like sort of a last definition and pure basic information three things and we're not gonna really work with it but I need to make it here so we can talk easily about another concept it's just a conditional mutual information of x and y given Z so now you have three random variables involved and has like sort of the natural definitions so it's like what is the mutual information so you match an x y&z are all intertangled like they may all depend on one another and you sort of imagine like what is the on average like once you draw and learn or condition on a value for a Z now what is the mutual information between x and y so I'll do an example in a moment but I'd level of formulas it's like it's the same definition as the mutual information between x and y except you stick a condition on Z everywhere so now that we know how to define the conditional entropy we can you know plug it into the definition for mutual information and get conditional mutual information and again just like with the conditional entropy this is really an expectation it's again like you draw an outcome little Z for capital Z and then you consider like okay how does that change the random variables x and y X&Y may still be dependent but the distributions might sort of change now that you've conditioned on capital Z being little Z you measure the mutual information of those random variables and then you like to take the expectation or the average over the the draw from Z so let me do an example that hopefully will clarify and again I'm gonna ask you to fill in these blanks here so this is the definition of communal mutual information and let me give an example here where we have three random variables and it's a classic scenario in probability x and y are independent random bits and z is their parody or their xor or their sum mod two this is a classical example of like three random variables random bits in fact where they're not collect of Lee independent but any pair of them is independent and these are pairwise independent bits so right so what is the mutual information between x and y does anybody want to say in the chat zero very good that's right so if you just ignore Z in your life just you can draw x and y and z all together but there's throw Z in the garbage then x and y are just independent random bits and we now have two random variables the mutual information is zero there's no savings that you can get yeah zero we're on the other hand what is the mutual information between x and y conditioned on Z someone can say one very good that's right because basically in fact no matter the outcome of Z if Z is either 0 or 1 once you condition on that x and y go from being independent to being highly dependent right so I mean if you condition on let's say this X are being zero then that's exactly telling you that X plus y mató is zero so x equals y so now x equals y and the mutual information between two equal random variables that are just a random bit is is 1 and conversely if Z happens to be 1 then you totally know that x and y are opposite random bits and again the mutual information between two definitely opposite uniformly random bits is 1 ok so uh that's that's true that's a fact and it might be considered counterintuitive but that's life that like conditioning on a random variable can make the mutual information go up but it's sort of true and this also implies that there's like no I've mentioned before that once you have three random variables these like Venn diagrams of information like can't really make sense because you know imagine the only thing you knew about in life where x and y and there are independent random bits so that's zero mutual information and so you'd be like all right well I'll draw that Venn diagram here's X and they have no mutual information so they're non overlapping Venn diagrams there's y on the other hand if you think about just let's say X and Z and isolation and forget about Y then X and Z are also independent random bits so you'd be like oh the X and Z have no mutual information so I better draw like an empty Venn diagram between X and Z and similarly an empty Venn diagram between y and z um right but there is uh I mean this would sort of the picture somehow suggests that they're all like independent and there's like no mutual information at all but you know condition on Z there is mutual information between x and y so there's no way to like capture it with a Venn diagram so that's life I mean yeah the Venn diagrams are just an intuitive aid when you have two random variables ok great so if you have any questions do chat them somebody says this is a one-time pad how is it a one-time pad it's kind of like a few are you saying if like your message X is like one bit and you're like oh I'm gonna encrypt this message by X or ring it with like a random one-time pad Y maybe so and then so Z looks totally random and it's independent of X but like yeah if you know why yeah I guess so I see what you're saying like it'd be more like a yeah more like a one-time pad if we like just how to Z here if we wrote Z here it's all the same and we condition on Y here and then be like saying like oh yeah like you know there's no mutual information between the message X and like the encoding Z when Y is a random one-time pad but yeah like the mutual information between X and Z is perfect if you condition on like knowing that one-time pad Y this conditioning only increased mutual information no it cannot increase it or decrease it so yeah it can go either way so a quick example I couldn't think of for its decreasing mutual information yeah well like imagine I don't know if you right since I've worked with these slides before I like remember that hard example very well but I think in that case with x and y where x and y you know they're either both hard was probably a half or else they were like independent like x is a random bid I'm like why I was a random letter between a and D let's see the mutual information between x and y there was one but if you let Z be the the let's say indicator random variable that x and y are both heart then condition on Z whether you know they are both heart or not both heart they become independent independent are constant so I mean that would mean their conditional mutual information is zero even though the original mutual information was one I think I got that example right if I made a mistake let me know\", metadata={'source': '7D8_-S6d5RU'}),\n",
       " Document(page_content=\"oh that's all I want to say about mutual information and now we're going to take a bit of a left turn from the point of view of very classical information theory but you know this is a CS theory - okay so I want to now connect this to topic and CS theory namely the one we talked about in the last lecture communication complexity so there was like a great Renaissance and like using like information theory to make big strides in communication complexity about ten years ago so I want to tell you about a little bit of that on now ten to fifteen years ago five to five to 15 years ago okay okay so today so far I mean if you think about communication we sort of thought about implicitly like one-way communication when you have a probability distribution on input so like when we were even defining entropy um yeah well you're we redefining entropy we imagine that like maybe Alice gets a random variable X and she's now gonna try to communicate its value to Bob using bits and she's gonna try to communicate it using as few bits as she can on average and if you remember the last time when we're talking about communication complexity you know we're talking about randomized communication complexity because it's kind of an interesting form of communication complexity well quantum is interesting too but we'll stick with randomized communication complexity and the first model of randomized communication plexi the one we actually care about most is we're like it's worst case model over inputs what Alice and Bob are allowed to use randomization would like try to minimize the amount of bits they communicate but we saw like the the main way you tried to prove lower bounds on that is by looking at distributional communication complexity anyway distributional communication and complexity is interesting in and of itself so distributional communication complexity of Alice and Bob and they're trying to compute some function a for you know do some tasks with their inputs F and we do imagine that their inputs x and y are random from some distributions I'm not necessarily independent but we imagine x and y get a random possibly dependent inputs and then once they have random inputs you can actually it's without loss of generality to only worry about Alice and Bob using deterministic protocols so we're gonna talk about it we talk about deterministic protocols in the context where you have random inputs and this is like a little uh SAT analogy not like SAT like satisfiability problem but like the tests in high school it's like you know it like a colon here and like a colon here and like a double colon here so in the simple one-way communication a problem with distributions on the inputs you know we talk about average communication cost and we kind of related this number entropy and we saw that you know there's some sort of a one-way relationship the average communication cost actually I never mentioned this but it has to be at least the entropy and it's at most an entropy plus one we sort of saw that they were in fact the same if you were allowed to like amortize and yeah so now you can ask for a similar sort of story where instead of just one way communication and like one input you have communication complexity like two-way back-and-forth communication and they are sort of like the optimal cost for solving like a problem or F or computing a function f under the assumption of like using are the inputs coming from centers distribution mu and making air Upsilon was some number this is like the distributional communication complexity of F under distribution mu and it turns out you know the right information type con struct number two for two-way communication is this quantity called information complexity so that's what I want to tell you about in the last twenty minutes so let's first remember about a little bit about this distributional communication complexity so recall the set up you know we have Alice and Bob and they want to compute some function f of their mutual inputs little X and little Y and they're gonna use a communication protocol pi which is you know some system whereby they understand who will communicate which bits next and how they'll send communication bits based on their inputs little X and little Y and PI does not necessarily have to always compute the function we're gonna have error when we have randomized communication complexity can we also imagine we have this mu which is a Joint Distribution on inputs 4 by x and y and we'll say that the error of protocol pi under distribution mu it's just the probability that alice and bob come to the wrong answer so this pi of XY is sort of the final answer they agree on and it's supposed to be f of x and y but it might not be in the probability of these failing to be the same thing under mu is the error probably the distributional communication complexity of function f under distribution mu with error rate epsilon oops was a mistake is the least communication cost of a deterministic protocol PI that achieves error at most Epsilon okay so that's a recap from last time so let's put that in a box and now I'll tell you this thing information complexity which is ultimately gonna be like a number associated with F and mu and epsilon that's kind of like the information theoretic part of this problem okay so the information complexity of a protocol PI under a distribution mu is this okay so the sum of two terms that are kind of symmetrical looking the first term is the mutual information of the random variable pi and the random variable Y conditioned on X so this is using that last most complicated definition the conditional mutual information so x and y are bob analysis well Alice involves inputs and you know I'm kind of abusing notation a little bit here pi is you know it's deterministic protocol but here what I really mean by it in this expression is I want to think of it as a random variable but it doesn't take on numbers it takes unlike transcripts as its values and so high represents imagine you like drew the inputs for Alice and Bob x and y little X and little Y and then you like gave them to Alice and Bob and let them talk according to this communication protocol PI they would output like a stream of messages back and forth and the whole transcript of what they output not their inputs but just the transfer and what the output is the random variable pi okay and so this quantity conditional mutual information of PI and Y conditioned on X it has an intuitive meaning and the intuitive meaning is the amount of information the conversation teaches Alice about Bob's inputs so Alice remember knows X alex gets X and so for her point of view she sees this random variable pi which is partly determined by X and it's partly determined by Y and she sees PI but she doesn't know Bob's input Y and so this mutual information conditioned on what she knows is Huff's quantifying this sort of bits of information Alice learned about Bob's input at the end of the conversation okay and similarly the second term is the amount of information that Bob learned about Alice's input so the amount of information in bits that Bob learned about Alice's input at the end of the conversation okay and so the information complexity of Pi under this distribution is the sum of these two information amounts and I'll explain a little bit intuition more intuition on that in a second but let me just say now that it's a sort of abuse of notation too but the information complexity of the problem or function f under distribution mia with air Epsilon it's sort of like the imagine like Bob and Alice still want to solve the same task as before but instead of trying to minimize the communication the number of bits they send back and forth they instead try to minimize this informational quantity they try to minimize the sum of these two information amounts so the protocol which you look at the protocol which minimizes this amount of information and the amount of information that it creates among all protocols that achieve error at most epsilon is called the information complexity of f-100 distribution mu and one thing I should have said before is like it's um sort of the case is much in the same way that like well I think I have it here in the next bit this is intuitive fact in much the same way that like any successful protocol for transferring or transmitting you know a draw from X uses at least entropy of X bits similarly any communication protocol which achieves error epsilon on distribution mu must send a number of bits in total which is at least this information cost information complexity and the reason is like you know I mean this is the number of bits that Alice learns about Bob's inputs from at the end of the conversation and like she already knows the whole transcript and she knows her own input X so the only way she gets information about Bob's Y is through like the bits that Bob sent to her and the only way a Bob gets information about Alice's input X is through the bits that Alice sent to Bob so the total communication which is the sum of these two things bits Allison to Bob and bits bobs into Alice intuitively has to be at least information complexity of the protocol and that's an not hard fact to proof I should also mention a funny mathematical thing you cannot formally write min here you actually have to write infimum if you remember that is because it can actually it can definitely be the case that for a fixed problem or function f the fixed distribution mu and a fixed error term epsilon there's no one deterministic protocol that like sort of solves the problem with air at most epsilon but uses the least amount of or conveys the least amount of information it could be that like you have a series of protocols at all you know convey less and less and less and less information because you're not putting any upper bound on how many actual bits of communication they have so you could like imagine like Bob and Alice who are extremely guarded and like they very slowly exchanged and like actual bits of information that are bits of communication that are not too revealing about information and so that's why at a math level you have to take this infimum the least lower bound up greatest lower bound okay oh right so you can compare this as I said to this much simpler fact that if you want to you know Alice wants to one-way communicate a draw from X to Bob you know she needs to communicate at least entropy of X bits you might start thinking about the converse like you know is a reverse true like you know can you always communicate not much more than this information complexity number of bits maybe if you allow amortization well we'll get to that so this information the plexi it's like a code a new concept it was only really first defined in this paper from 2004 by Barrios FJ Ram Kumar and Siva Kumar and they proved in this paper one beautiful theorem about it which is that it's perfectly additive and this is like the analog of the fact that like the entropy of a bunch of independent random variables is the sum of the entropies of those random variables so it's a more complicated fact it says okay let's say you have some function f that you're trying to compute and some target error epsilon and some distribution mu so it's like one problem now imagine that like you know Alice and Bob were like hey you're gonna have to solve FM x and you're gonna get n independent inputs like so you know x1 and y1 will be drawn from you so they're not independent but they already independent from x2 y2 and from xn y n so this mew to the amount of o times n it means make n draws from the Joint Distribution on inputs x and y mu and this F with like the superscript power n means they trying to solve all these communication problems and figure out f of X I Y I simultaneously with air at most epsilon for all the coordinates so they get all the inputs like Alice gets x1 through xn Bob gets I also gets x1 through xn Bob gets y1 through yn and have to figure out all the pairwise answers with error at most epsilon you can again ask the how many bits of communication do they need and how much information do they need and it's a funny thing that like actually you might think like well they're independent instances what can they do except like around the protocol you know communication protocol end times but they can do better than that is similar to what we saw with communication right I mean if you have to transmit one trip from Alice to Bob okay then you know there's a bit of waste and the best thing you can do involves you know two point six bits on average but if you know you're gonna be solving this problem end times like transmitting end copies then you can like look at all the end tasks you have somehow collate them together and get some savings and that can also be true in this context of two-way communication complexity problems but just like with entropy like if you go to this refined measure that doesn't you know it doesn't have to be an integer that really mentioned it's like the real number amount of information and they showed that it's like perfectly additive like that the amount of information you need to that you know Alice needs to learn about Bob's input plus Bonnie's to learn about Alice's and foot to solve this problem F on like n independent instances is really n times the amount you need for a single instance so that's cool [Music] so in particular since we have this intuitive fact that the communication cost is at least the information cost tells me that like the communication costs for solving n independent instances of like function f on n no pairs X I Y I with err epsilon on each pair is at least n times the information complexity and so this could be a route potentially to proving you know lower bounds on randomized communication complexity which in general can be quite tough so let me save fact up here uh and I should say that like now I'm gonna start telling you things that's sort of a bit of a high-level and I will not be like 100% precise and I think it would be precise I think I won't tell anything that's wrong but like I won't I'm gonna hide some details so uh remember last time you know we talked about sort of the three side of the communication complexity world the disjointness problem was sort of this really hard problem you know X&Y are strings maybe representing subsets of the numbers 1 through n and Alice and Bob have to tell if they're sub sets are disjoint or not or if you think of them as strings and you like stack them on top of each other they want to tell if there's any coordinate where they both have a 1 they want the opposite to that answer so really like you know they do like a sort of a pairwise and of they're two strings and then they want to see if there are any coordinates between 1 and n where that pairwise and does of one and then because it's disjointness you take the negation but never mind and we saw that you know i told you it's a true but very difficult to prove theorem that even if Alice and Bob can be randomized the communication complexity needed for this is linear and n they can't do you do much better than just Alice sends a whole string to Bob but a good way to prove it to try to prove it as I suggested last time is through this information theory and you see it's kind of similar so um it's kind of like Alice and Bob have to solve n copies of like a teeny tiny problem of computing the end between like a bit of Alice and a bit of Bob it's not the same because like in this notation really they have to compute n bits they have to compute all the N answers all the N hands and in destroying this you really only need to compute the or of the answer is you don't actually compute all the answers but this is where I'm saying I'm going a little bit vague I mean this problem is pretty similar to like solving n independent instances potentially of a very simple problem F equals the you know one bit and function and so uh if you want to use this then you also need to have a probability distribution on inputs for disjointness that's a product distribution but it's like a product distribution across pairs okay so you might ask yourself okay if the n-bits let me just put this up so alice is eventually going to get a string and Bob is eventually gonna get a string and in this mindset you're really interested in like okay we're gonna have some probability distribution on their mutual first coordinates and some probability distribution of their mutual second coordinates and we're gonna use the same distribution for every pair of coordinates but what distribution should we put on those pairs of coordinates to make it a challenge for Alice and Bob so if we're trying to prove it like a cool lower bound we want to try to think of a distribution that's gonna make it hard as possible for Alice and Bob and I'm gonna ask you in a moment to think about that so well that moment I guess is sort of now so if you're trying to design a probability distribution on pairs of bits mu so you have to come up with four numbers the probability of putting plug in 0 0 0 1 1 0 & 1 1 ah do you want to make a tough one Alice and Bob for the point of view of them trying to decide if their sets are disjoint or if their strings have like a position where they have a 1 in common what kind of choice form you might you may notice by the way that this way of doing things gives strings for x and x and y for alice and bob that are not independent the strings are not independent the pairs of coordinates are independent so one suggestion you could make Mubi independent that would be like saying like okay I'm gonna do 1/4 1/4 1/4 1/4 and that actually corresponds to like X&Y just being independent uniform strings I guess at least intuitively this does not make it too hard for Allison Bob because if Alice and Bob both get you know completely random strings but you know 99% of the time we're in fact like uh 1 minus 2 to the minus end of the time the answer is gonna be no the strings are not disjoint you know because though like a you have 1/4 chance of having a column that's 1 1 so like probably you're gonna have some column that's 1 1 so this is not gonna make it too hard on Alice and Bob they can use the deterministic protocol that just always says no and like they'll achieve an error that's like 2 to the minus 4 to the minus maybe 3/4 to the end or fail to be exponentially small somebody else suggested half 0 0 1/2 okay that's interesting to hear let me get this one out let's think about it half 0 0 1/2 actually this is not so good because basically anytime you have this eventually the point I want to make maybe yeah somebody else suggested this to you let's try the other thing let's try 0 half half half 0 so what about this is this gonna make it hard for Allison Bob well yeah this one is not exactly gonna be hard because actually if Allison Bob knows it like this then actually no that'll never be disjoint um so that's not so good they can just always happily answer yes okay we have another suggestion of 0 1 1 & 1 over N normalized yeah I like it 0 let's think about this one so the suggestion here is let's put like 1 over N here and that's actually a pretty good idea and 1 & 1 here or like maybe whatever 1 minus 1 over 2n basically 1 so a good thing about that is with this being one of our n basically you know there's like a gonna be a constant chance that you'll have one or more one one so that the answer is sort of roughly equally likely to zero or one which is good at making it hard on Allison Bob this is reasonable it actually doesn't hurt and may help to put some probability on here as well but the main thing I want to get out of this discussion is yeah like the main thing is just to make sure you put like almost zero probability on one one although not literally zero probability so the main answer I want to take away from this is you know lots of news could be fine but like me alone one should be very close to zero and here is another cool theorem this one is hard but true um and it says the following it's kind of transforms this theorem into something that's like a exclusively about like basically what happens if you do combining not with you got to compute all the answers but you've got a computer or of the answers they show that the the the communication complexity for a disjointness when you use this end fold copy of mu distribution is at least M times basically the information complexity of computing the and function under with zero error under your distribution mu for any mute has exactly zero probability on one one she's a bit funny but this is the theorem that you can prove and so for example you can take this distribution that's like one-third one-third one-third zero and then it's funny you just have to ask yourself okay like a question about like you know one bit communication problem or information theory problem if Alice and Bob both get a bit they have to have error zero so they need a protocol that correctly determines 100 percent of time if their bits are both true if they satisfy and and now the story is one-third of the times are getting zero zero one third of the time they're getting one zero one third of the time they're getting zero one and they're never actually getting one one but they Stephanie have a protocol that would be correct if they were getting one one you know what's the least information and they can communicate in some sense what's the least amount that Alice needs to learn about Bob's input plus Bob learning about Alice's input to decide the end function this is actually even though it's a problem about like one bit it's actually surprisingly challenging but with a lot of effort you can figure out the answer it turns out to be one over three lon - which is about point four eight and once you know this theorem you plug in in and you get that there exists some epsilon greater than zero and this distribution the n fold product of this one-third one-third one-third zero distribution such that under this distribution any protocol for solving disjointness with error epsilon needs at least point four eight and bits of communication and then by y'alls principle you get that a randomized communication to pluck C of disjoint this is at least point four eight and that's how you can prove it it's actually 0.48 to something that's like slightly better to take a number it's like slightly different than one third but close enough okay so that's sort of story time about this and let me just end with a little bit more story time what one more slide or two or more slides this is an old slide we saw with beautiful theorem number one about the exact additivity of information cost and how you can use that to like get a lower bound for communication bits of communication in terms of information complexity and uh if you just take this inequality and divided by n we get this quantity and we do that because we're thinking about amortization and you know how much does it you need to communicate on average if you have to do n copies of some compute F task so I'll put that all up here and so this is true for every N and so in particular it's true in the limit as n goes to infinity and I say that because this quantity here is really what you might define to be the amortized communication cost under distribution miu know like you know if you're getting inputs from distribution mu you're trying to Alison Bob were trying to communicate to solve F and they're gonna get like a really large number of copies of this task and independent copies you can look how much bits they communicate divided by N and say like okay that's the amortized communication cost we know that it's at least the information cost or you know just doing this one time and so you might ask about the other direction and let's beautiful theorem number two is proved sort of independently by my var and Gupta yeah from the information theory community and Braverman around from the CS theory community and they show that they're equal so it's exactly like with the entropy capturing like you know average case one-way communication the average case you know two-way communication cost of solving a problem f1 amortized out is exactly this information complexity quantity and I'll just end by saying one word about the proof you see what you have to do to prove the upper bound is you need to actually exhibit like a communication protocol which really takes advantage of amortization and takes like maybe a protocol that has like a lot of communication but you know not so much information being conveyed and compressing it into like a short communication and they do this and so like their theorem kind of shows that any conversation that even though it takes a long time that conveys very little information may be compressed to a short one but just really I mean that's like a dust that's deep for me if you think about it like this a metaphor uh yeah for maybe these lecturers or like just general conversing with people like a lot of a lot of words sometimes and you want to compress it down to the just the right amount of information okay so that's it um I shall see you again on Tuesday all stop the recording now but I'll stick around to answer questions\", metadata={'source': 'Tujzr9DOI1I'}),\n",
       " Document(page_content=\"okay so today's lecture is lecture 25 and it's gonna be another lecture in which Alice is sending something to Bob and there's gonna be some encoding involved but be a little bit different because we're discussing cryptography so of course you know you could teach an entire course on cryptography and you know professor people Doyle does that I cannot cover hardly anything in one lecture but I'll do my best to tell you some of the basic tools of cryptography and one reason I want to do that is you know often when you may be secret buggery for the first time there's a lot of discussion of you know RSA and some particular number theory things but actually there's a rich theory of cryptography that Ness does not necessarily have anything to do with number theory and so forth and so I'll try to give you a glimpse of that today there are many many textbooks and courses and so forth about a theoretical computation with cryptography I just want to highlight this one textbook that I like it's by Raphael poss and I'll be Charlotte it's called a course in cryptography okay so etymology expert someone you know cryptography really means hidden writing and that phrase motivates you know one of those basic problems attacked by cryptography namely how you know party Alice who has a message M that they want to send can and send it to a receiver Bob using some kind of a encryption scheme but can attempt you know yeah process it can attempt to a foil an eavesdropper with or an adversary with ill intent so we have a third character here in the standard Alice Bob set up Eve who uh listens in on the communication between Alice and Bob at least and it's trying to you know learn something about the message here I guess eavesdropper actually has an A in it but this is the closed one can do with human names okay so the set of miss Alice has a message M sometimes called in I'll give you the cryptography terminology it's called the plaintext imagine it's there's a bit string and then it gets encoded by some encoding map producing the another let's say bit string C which is called the ciphertext and we want that Bob is able to decode the cipher text and get the message back but we're concerned about Eve learning something about the message secret message so one thing one has to do in the cell you know study cryptography is understand what does it meant by security and this is something we'll we'll talk about how to formalize you know at a high level you want somehow that Eve should not get any information about what the message is M just by her ability to eavesdrop and learn the message of the encrypted message C um and it's unclear how exactly you might do this or arrange for this so one possibility is you know maybe you could make this decryption our than a secret that only Bob knows what you know this runs into the kinds of problems one always has and you know cryptography and secrecy you know I mean the secrets can fall into the wrong hands and you know you might learn that thing that decryption algorithm or I mean people might get suspicious that they're like backdoors or things built into the decryption algorithm so we don't like the solution and basic tenets in most of you know computational cryptography theory is that all the algorithms involved should be public knowledge to everyone so given that you know if decrypt the decryption algorithm in the encryption algorithm are publicly known algorithms you know you could just say well what's to stop you from running the decryption algorithm on the ciphertext but the way we get around that is to still have some secret things but these will be secret inputs to the publicly understood algorithms so I'll explain more about it how this set up a work in subsequent slides so first one talk about the so called symmetric key model or the shared key model ske for encryption of messages and this is a more limited that maybe the public key encryption model you know based on RSA or so forth that you might have heard about if you've heard a little bit of popular discussion of a computational cryptography so in the secret uh sorry the symmetric key model we still have Alice who has a message she wants to send it to Bob and Eve is an eavesdropper but there will be some algorithm called Jen which generates secret keys and secret keys are also you can think of them as bit strings but we'll denote them by two letters SK and we're imagining this is a symmetric or shared key set up where the secret key is generated by this generation algorithm and it's given to Alice and Bob but not to Eve okay and this will be the secret I think puts that will help them you know solve this computation problem or cryptography problem so now the we mentioned that the encryption algorithm takes two inputs one is the message and the other is the secret key okay and so let's see the ciphertext is a function of that and now Bob who also knows the secret key in this story can decrypt the ciphertext and get back the message and hopefully as a function of the ciphertext and the secret key so in order for this to make sense this generation secret key generation algorithm must have a key property and that's the property of being randomized so this is a an essential aspect of all you know cryptography theory the use of randomized algorithms or generating keys and running protocols point being I mean it sounds silly to say but I mean if the key generation algorithm is deterministic and it's well known you know so it's always outputting the same secret key then you know even around herself and get the secret key and then she's in the exact same position as Bob but if the generation algorithm is randomized then you know an output so I'm gonna n bit strings or something man you know you can run it herself but that doesn't help her to know the secret key that Alice and Bob agreed on so I know we must start to address the question of what security of such a scheme can mean and as always I mean feel free to interrupt even audio with audio for questions but you can also type them into the chat and there's lots of possibilities you might say that okay like maybe we try to formalize that Eve shouldn't learn anything about the message and some information theoretic way or maybe we could try to formalize that the ciphertext looks random from Eve's point of view but to cut the story short a key idea that's been developed in the course of you know the computer science theory of cryptography is the base security around the notion of Simula ability which is to say that you know anything Eve can sort of learn from the ciphertext she should be able to simulate living herself even without Alice and Bob sending these messages so let's make a definition here and this will be a definition for concerning symmetric key encryption schemes and the term we're defining is perfectly secure and let me take a brief moment before I read the definition to emphasize but this we're defining a technical term here just like we always do in mathematics and the term is called perfectly secure which is a very suggestive name but it's merely a mathematical definition and it might not necessarily comport with like an intuitive human level meaning of like what is perfectly secure so I mean you might imagine okay well what about in the scenario of Eve can I don't know listen in on the secret key generation or you know what if there's some like timing attacks and you can do and so forth that maybe this is not perfectly secure certainly all these things have to always be considered when you're studying cryptography but you know even though special phrase we're just making a mathematical definition here called perfectly secure which you can argue about whether it comports with your notion of reality okay so we say that a ske a symmetric key encryption scheme which consists of these three algorithms key generation encoding and decoding is perfectly secure if for every pair of messages m0 and m1 that Alice might want to send and for every potential ciphertext strings see that she might send we have an equality b2 probably between two probabilities and these probabilities are when you you know do Alice's basically alice's role with message m0 and Alice's role with message s1 so you draw the secret key from the generation algorithm and you encode the message 0 from with a secret key we look at the probability that this results in the ciphertext C and that should be the same for all pairs of messages and all ciphertext C so basically what it's saying is there's one probability distribution over cipher texts perhaps the uniform distribution perhaps something else but your scheme should have the property that for any message you're encoding when you encode it and when you do that you have to get a randomize you get a random string because the generation the key generation is random when you encoded you get that probability distribution on ciphertexts and this is a reasonable scheme because I mean it means from Eve's point of view no matter what the message is she sees a ciphertext whose probability distribution is the same you can imagine the uniform distribution in case so no matter what the message is she sees a random string then you know that's a reasonable notion of the communication being secure this also called Shannon security because you know our hero Claude Shannon also invented this notion back in the early days in the 40s okay and there's a well-known solution to this problem perhaps you've seen it before if you have feel free to type it into the chat OTP yes one-time pad in fact we discussed it yes or not yesterday but in the last lecture the one-time pad achieves perfect security and it's a simple solution if you haven't seen it before so here it is give it a parameter n ah you're gonna have this scheme where am the missed messages and see the ciphertext and also SK the secret keys are all n bit strings and it's pretty simple the key generation algorithm just chooses a random and bit string and then to for Alice to encode her message M with the secret key she just X ORS them together it was okay and for Bob to decode the message he takes in ciphertext and also in code X or is it bitwise with the secret key which she knows and you know X soaring twice cancels things out so this correctly recovers you know the message from Bob's point of view and furthermore let's try two okay so that's a check mark it's sort of like a correct scheme Bob gets to correct a decryption with 100% chance and let's check that it satisfies this definition of being perfectly secure well in fact it does and the reason is under the scheme for every n-bit message and every end bit ciphertext the probability over the choice of the secret key but the encryption of the message will equal that ciphertext well it's 2 to the minus n because this event occurs if and only if the secret key is equal to the XOR of the message in the ciphertext this fixed M and C and you know some fixed strings the probability that a uniformly random string is equal to that string is 2 to the minus n so the one-time pad has the property that from like let's say each point of view you know if Alice encodes one message of her choice would no matter what it is when she encodes it with this one-time pad sends it to Bob what Eve sees is a uniformly random string so she learns nothing about the message okay so great you might say all right problem salt so we are done came over we got it well sort of yeah I mean this scheme does achieve perfect security but we might want more because there are some issues with this one-time pad scheme what issue with it is it has some sort of inefficiency to it particular the secret key here is of length a number of bits equal to the length of the message so our secret keys are longer than in fact equal to in length to the length of the message this is kind of a drag but it's it's not hard to show that this property is necessary for any scheme which achieves perfect secret security and because this property is kind of a drag we're gonna relax our notion of perfect security in a moment but it's kind of a drag because you know I mean you know if you want to send like you know as some piece of 10x you need to store this gigantic equally long random string and you know these string these secret keys are hard to keep around they require secret agreement and perhaps another major problem is that this scheme is terrible if you use it more than one time it's the phrase one time the name and probably all know or seen the reason if you wide so bad to reuse the secret key I mean if Alice sends two messages m0 and m1 then under this scheme she sends m0 and m1 then they both get encrypted by X or ring with the same secret key and so the two cipher texts c0 and c1 have the property that c0 X or c1 it's my terrible Mouse writing is the same as m0 XOR M one that's pretty bad because the eve can compute C 0 XOR C one herself and she gets the XOR of the two messages that Alice sent and you know if you get the XOR of two you know English language messages you know a good crypt analyst can probably figure out the two messages so it's it's terrible to reuse this scheme so there's some problems with the one time that's you know what different cryptographic schemes are developed for to try to get around these problems so a goal that you might have is to you know come up with a scheme where you have a particularly short secret key maybe not too long you know some thousand bits or ten thousand bits but that lets you encrypt lots of messages you know multiple rounds of messages Alice can you know do several messages their total length can be much longer than ten thousand or a thousand bits that would be cool well that's we see we're gonna have to relax our definition of perfect security if we want to achieve this um question asks what Jen deck Inc is not perfect what if it's not perfect does it mean you can infer the message well if you have a scheme which is not does not satisfy this definition of perfect security that it means that like in short different messages that Alice might send might induce different probability distributions on ciphertext and therefore this will give Eve some non-trivial better than trivial chance of guessing what the message is so we're gonna exactly sort of talk about that like what if maybe you know perfect security about these probability distributions on cipher texts are not the same but are four different messages but are very similar oh now there's an issue here which is that if you're trying to model Eve as an attacker there's no way we can I mean an attack that she can always try is to just like try all possible secret keys that Alice and Bob might be using you know decrypt with each one and see if you know that leads to a message that looks plausible so there's no way around that and that sort of suggests that um well it suggests that we want to take computational complexity into account so I mean maybe we might choose not to worry about that too much because you know if there's too to the you know secret keys or likes a thousand bits so they're two to the 1000 possible secret keys I think it's reasonable to assume that like while we're not concerned about an attack where Eve tries all possible to of the 1000 keys\", metadata={'source': 'Ptl7pjDmQjk'}),\n",
       " Document(page_content=\"jeez I think it's reasonable to assume that like while we're not concerned about an attack where Eve tries all possible to the 1000 keys so the main assumption made in cryptography theory is this that your adversary or Eve is and this is some cryptography terminology PPT well that just stands for probabilistic polynomial-time algorithm so the main assumption is that you even though the adversary is working against you you still model them as though they're a polynomial time algorithm if I can model all the participants as algorithms we're pretty queueing model the adversaries is being polynomial time and this is some kind of limitation on what they can do and generally this is it so usually the only thing you assume is that their upon no time algorithm you know you allow them to be probabilistic that seems fine you might ask polynomial time in what parameter and generally when you have an encryption scheme what you really have is like a family of encryption schemes indexed by what's called a security parameter N and you know the users of the schemes can make n larger and larger according to their tastes you know you might think of this roughly is like the length of the secret key that's involved in bits and so you know the the bigger you make em you know the more inefficient it is you know you can might need to store thousand bit keys you might need to store 10,000 bit keys or 1 million bit keys but you know you increase these in order to get some better presume at security and this is the parameter that we Ramin when we were talking about Eve being polynomial time ok indeed as I said you we usually model um all the honest parties is also being probabilistic polynomial-time algorithms and there's one more detail which does come up in cryptography I don't want to dwell on it too much but generally in cryptography theory you allow the adversaries to be non-uniform models of computation we talked about this way back and like lecture 3 or something what this means basically you imagine they can be circuits or in other words that they can have a different algorithm for different values of N and in turn what that means is you it basically means they can pre-compute any amount of information based only on n the like security perimeter so if you decided okay we're gonna have keys that are of length 6,000 65536 then you know you allow the adversary to have a special you know hacking scheme that is based on you know 65,536 okay but that's not such an important point for this this lecture finally I want to use this opportunity to make one more definition so probabilistic in other words that our algorithms are adversarial algorithms can be probabilistic we have to start talking about error probabilities because you know okay your polynomial time maybe you could not try all possible 2 to the 1000 many secret keys but you can guess a secret key and uh that's a possibility but of course you know you can only guess a 1000 bit secret key correctly was probably 2 to the minus 1000 which we consider you know an acceptable risk in other words it occurs with negligible probability and this is a convenient definition in cryptography theory it's a piece of notation so any GL negligible of n basically literally means this big no notation 1 over N to the little Omega of 1 this is with respect to n going to infinity I see another question I'll answer that in a moment so maybe you're not quite useless little Omega of 1 notation so this is a function that goes to zero faster than any fixed polynomial so f of an is that most negligible in n means that for any fixed C even if you multiply F of n by N to the C the function still goes to zero as n goes to infinity so like a big but still negligible function would be something like 1 over N to the log N or particularly like a general function would be like 2 to the minus n question was asked can you give an example of how knowing the length of the key can be useful uh yeah it's not such a big deal this is sometimes made you know just for technical convenience and arguments but um you know you might you know if you know this length of the key is like 65,536 and maybe there's like a number theoretic protocol you know maybe you allow the adversary to know something like what is the first prime number that has 65,536 bits so that's something that like you can't necessarily compute in poly and 65000 time but we allow out the algorithm to know this fact or you know it's gonna be you know starting to work with you know the integers mod to ^ 65536 or some prime around there you know we may allow the adversary to know some you know generator of the cyclic group of multiplication of this prime or something so there are some things related to the the security parameter length that might make sense for the adversary to pre-compute okay so with these definitions in hand we can now start talking about a new notion of security based on computational indistinguishability and we'll have to actually build up some more terminology in order to get there so uh I mentioned that this notion of perfect security is saying that like for any two messages m0 and m1 that Alice might send when you encode them by the C encode them using the secret key and the secret key is remember a random string random variable perfect security says that least to probability low probability distributions that you get on ciphertext are exactly the same and we can weaken perfect security by something like well there would be literally exactly the same probability distribution as long as there are two probability distributions which cannot be really distinguished by a PPT probabilistic polynomial-time adversary so let's make some definitions that will help us encapsulate this so the first definition we're making is the definition of an ensemble which is just as some crypto terminology for a simple concept basically a sequence of binary string valued random variables so you know generally we'll be talking a lot about random variables which are strings you know they'll have notation like X or Y or SK and we usually write a subset subscripts to indicate how many bits in the string so you know a random variable a bit string random variable is you know general like this but usually there's some you know a succession of security parameters N and so we actually have a sequence of random strings and a sequence of random strings is called the ensemble and we assume that like you know the enth string has some fixed length which need not actually be n mitts but it should be like some M of n bits where M of n is a polynomial okay so if you have xn is like a random string some kind a random string of length N squared you have such a distribution for each end that's an ensemble okay and then the notation is like parentheses X n ok so now here's the key definition it's a definition for when two ensembles xn and yn are computationally indistinguishable and basically it's saying that a ppt algorithm cannot tell them apart except you know with something like negligible advantage so you should imagine a game where you have some pbt algorithm a I think of it like an adversary that's trying to yes if you're feeding it strings from drawn from xn or string this distributed as yn and you know has to make a binary guess it outputs like a bits if it thinks it's getting xn strings or yn strings and you can look at the probability that the algorithm when fed a string X n outputs let's say 1 it outputs 1 or 0 is the probability it outputs one when you give it a yn string these are two numbers and if they're both extremely close if they're like you know 0.6 and 0.6 0 0 0 0 0 0 1 that's basically saying that yeah the adversary a cannot really tell the difference between the random variables X and the random variables Y ok so we say that two ensembles xn and yn are computationally indistinguishable and the notation is this curly equal sign with a c above it if this holds for all polynomial time algorithms a this difference in probabilities is at most some negligible function of n ok this difference is called the advantage a has on ensembles xn and yn the advantage in distinguishing ok I hope that makes sense if you have any questions please ask and so this is a small factor this is an equivalence relation it's not hard to see the you know the xn is computationally indistinguishable xn I mean algorithm can't get any advantage there at 0 and it's symmetric the key point is that it has this triangle inequality a sort of property that if xn is computationally indistinguishable from yn and yn is computationally indistinguishable from Zn and X and z are also computationally indistinguishable in fact there's a stronger version of this this is sort of saying you know if you have three ensembles and these two are indistinguishable turn out to people the algorithms at least you are then so are the outside two you can even extend this to like 10 or 100 or even polynomial a many sequences and this fact is like a workhorse fact in probability theory and it goes by the name hybrid argument or hybrid lemma and it says let's say you have quite a few ensembles of random strings X 1 through X T and T can even depend on n so but it should be at most polynomial in n so you might have like N squared different ensembles for example and suppose you know that any two like quote unquote adjacent ones like X I and X I plus 1 are computationally indistinguishable then even the first one X 1 and the last one XT are also computationally indistinguishable let's just see the proof of this so here's the proof written out so we want to show under this assumption that the adjacent ones are indistinguishable but the endpoint ones are indistinguishable so sort of we need to show is if a is any PPT algorithm then a cannot achieve more than a negligible advantage in distinguishing the x1 ensemble from the xt ensemble so this is what we're looking at here aids advantage at distinguishing the x1 ensemble and the xn on saw xt ensemble and these are numbers is the difference of two numbers we can use the ordinary triangle inequality and telescoping to like say that two numbers differ by at most you know the sum of the difference is between you know these T intermediary numbers so this is triangle inequality plus telescoping I guess and so now our assumption is that the adjacent ones xixi plus one are computationally indistinguishable so that this quantity is a function of n is at most some negligible function and we only have T of them and we are assuming T is at most a polynomial in n so we have a polynomial in n and a fixed polynomial on n like N squared times a negligible function and we're just saying well that's still a negligible function this is a property and like a convenience of this terminology negligible function you know if some like exponentially small quantity you multiply it by N squared it's still you know exponentially small or now replace those words with negligible it just means you go to zero faster than any polynomial and you're in good shape okay so that's a key tool that's used a lot in cryptography let me tell you one more basic fact about computational indistinguishability which will only be important let's say x and y are computationally indistinguishable ensembles and b is a fixed algorithm even a probabilistic algorithm and you might imagine that you be takes us input a string and outputs a string and now you might say like let me consider be applied to xn you know the output string you get which still random string if you take a random string X and plug it into B and run B and let B output something that's a new ensemble B of X and B of Y is a new ensemble and the fact is that um these ones are still computationally indistinguishable so computational indistinguishability is like preserved under applying the same algorithm to both ensembles and well I was gonna ask you to think about why that's true but let me just put up a sketch let's assume for a contradiction that they're not indistinguishable B of X and B of Y which means there's some algorithm a which distinguishes these two Essam ensembles with non-negligible advantage you plug in B of X to a then you plug in B of Y to a and a has like a noticeable non negligible difference in how putting probably a probability difference in outputting one well then a combined with be like first run be then run a is a distinguish sure that gets non-negligible advantage and define distinguishing xn and y am okay and that you know contradicts the assumption that xn and yn are computationally indistinguishable so relying on the fact here that you know the composition of two problem polynomial time algorithms is another polynomial time\", metadata={'source': 'TNS6K5oYt1g'}),\n",
       " Document(page_content=\"okay so once we have this definition of computational indistinguishability we can define a computational cryptographic object called cryptographic pseudo-random generator so we've gotten a little bit away from encryption decryption schemes but we'll get back there in a moment because it's now it's a convenient time to talk about this notion of a cryptographic pseudo-random generator in fact you know one of the games in cryptography is to invent you know not just you know encryption schemes for sending messages but other cryptographic schemes for doing all sorts of cool things like computing on encrypted messages and digitally signing messages and steganography and all sorts of other like fun stuff and pseudo-random generator you know one example of a fun stuff that you might want to develop now we talked already about pseudo-random generators in this course I got a got another question I'll get to it in one second in the context of D randomization and it's the same definition but with like different parameters and a different emphasis on parameters so cryptographic we've seen around generator that's different emphasis and a different sort of parameter range than the ones you use for do u randomization let me go back to this question it was asked could a compose with B be not probabilistic polynomial-time though like B cleverly generates the exponentially rare input that takes a super long time perhaps it depends on what exactly you mean by P P is meant by PPT but generally you know we set up the definitions of that like oh there was a amendment that says oh I think that was a PPT yeah generally we set up the definitions so that you know this composition is holds like a composition from opponent I'll time algorithms upon real time everyone this is another polynomial time algorithm for randomized algorithms yeah you have to be a bit careful because you might be saying oh are we talking about the expected running time or the max running time or what are we talking about for complexity theory reasons the best definition is that you have a fixed running time like you know ten and cubed where you're randomized algorithm always a hundred percent of the time runs it at most that time ten n cubed no matter what the input is of length n but you know it's still a lot to make random choices that's the general definition of this most use definition of probabilistic polynomial-time and then it's clear that composition preserves this ok good so ah so you can see up here we need one more piece of notation that's popular in cryptography if I just write you in that means the simple ensemble where the nth string u n is just a uniformly random string of length n ok so a cryptographic PRG is a pseudo-random generator it suppose to be a deterministic and also technically uniform but a deterministic polynomial on time computable function G that takes a string of length N and outputs the string of length L of N and elephant should be longer than n it's supposed to be you know expanding seeds or short strings to longer strings and we say it's a cryptographic PRG if it falls all polynomial time adversaries with exponential fooling probability so we could really use the old definition with these parameter settings to say another way is to say that okay if you give G a uniformly random string of length and it produces a some kind of random string of length L MN and that string should be computationally indistinguishable from a uniformly random string of length element so these should look the same to all polynomial time observers up to like a negligible advantage okay it's a basic assumption or slash conjecture in cryptography that cryptographic pure G's exists and particularly the ones exist where the length is just n plus 1 so they just add one bit this is a basic assumption that they exist and we should say that first of all it's a reasonable assumption because there are some simple number theoretic candidates where you know based on you know presume Partners of factoring or other tasks like discrete log I'm not gonna list them because I actually don't want to talk really about number theory in this lecture but there's some simple number theory candidates that make people believe that these objects do exist but it's certainly proving that's a candidate PRG really has this property you know involves proving that there's no polynomial time algorithm that solves a certain task and you know we're very bad at that right we cannot prove that P does not equal NP so we don't really have a like a lot of hope to prove that a certain candidate PRG really as a PRG we can only do it based on assumptions and it's actually quite easy to show that assuming cryptographic PR jeez exists implies that you know NP is not easy that there are hard problems that NP is not contained in BPPV probabilistic polynomial-time in fact it also implies that P does not equal NP okay so this assumption that you know there are cryptographic pseudorandom generators which is like a basic tool you need to get off the ground and studying crypto theory it's a stronger assumption than P equals NP does not equal NP but you know we make it and move on of course there's a lot of work towards you know understanding this assumption I may be trying to weaken it we'll talk about that later so make sure there's even a more fundamental difference here I mean way in which it's stronger in the sense that to say that P does not equal NP is really about worst-case problems you know it's like saying that there's no polynomial time algorithm for sat in sort of intuitive saying there's no polynomial time algorithm for sat in the worst case you know for every Sat algorithm there's some input that it gets probably Meal Time sad algorithm there's some inputs that it gets wrong general in cryptography you need much more though to get started you need these sort of average case assumptions that are about like well how algorithms you're assuming that algorithms even fail to succeed also known as fail almost failed even um let's say with high probability on a random inputs and so this we don't really know it's a stronger thing to say you know there's a way to randomly generates at instances that are gonna be hard for all polynomial time algorithms let's quite a bit stronger than saying just you know for any polynomial time algorithm there will exist some input where it gets the wrong answer okay but let's like take this as a assumption or for now that there are cryptographic pseudo-random generators that stretch a random string by one bits and they give you back an N plus 1 bit string that still sort of looks random for any polynomial time adversary it's a fact it's not too hard to prove I'll sketch it sort of that if you make this basic assumption then you can actually deduce a stronger assumption that for all constant C like 10 you can also have a cryptographic PRG that stretches n bit strings to n to the 10 bit strings length strings okay this is sort of the general thing that happens in crypto is here you try to make a weak assumption that okay some weak object exists and then you know using smarts and things try to use it as a building block to design a stronger cryptographic object that's what's going on here and I I won't prove it but it's the proof is not too hard what I can give you is the construction I just can't prove the in this amount of time the fact that it satisfies the definition of a cryptographic PRG but the construction is this sort of by a picture so imagine you have this picture here gee it's supposed to represent like an algorithm or like even a circuit but like say an algorithm that takes in n bits and outputs n plus 1 bits it's the pseudo-random generator and now we want to build a pseudo-random generator that takes in n bits and outputs like a lot of it like into the ten bits or something so how it's gonna work is you're gonna take in your n bits X you're gonna pass it through your stretch by one pseudo-random generator that gives you an n plus 1 bit string that's like this thing here and you're gonna take the very last bit the n plus first bit and like output it that's this picture here and you're left with n bits that's these here and then you're just gonna feed them back into the pseudo-random generator and get like another n plus 1 bits you're gonna take the last bit again and output it and then you have you take your remaining n bits and feed them again through the pseudo-random generator get n plus 1 bits output one bed take your next other end bits feed them through the pseudo-random generator and you just do this like as much as you want like you do this n to the ten times and that gives you an overall algorithm that involves running G and to the ten times it's polynomial time it spits out n to the ten bits and you have to prove and you in fact you exactly use the hybrid algorithm to prove this it's to prove that you know the end of the ten outputs by this up its output by this look indistinguishable from end of the ten truly random bits to any ppt adversary assuming the original seed was a uniformly random string you have to prove that just using the assumption that like one step of the generator that goes from n to n plus one bits you know outputs a string that looks totally random to a ppt adversary I hope you like at the intuition for why it works I mean basically an adversary looking at this picture can't tell the difference between like all these things output from G being uniformly random you have to take care of because you know in even in the first step we're in the second step what you're putting into G is not a truly random string but it's merely a string that is indistinguishable from truly random to a polynomial-time observer okay great so that's a pseudo-random generators I want to bring them up and now we can actually get back to a symmetric key or a shared key encryption schemes and I can make sort of a definition for weakening from perfect security to computational security for that so cast your mind back again to Alice trying to send a message to encrypt the message send it to Bob and Eve is eavesdropping so we say like a such a scheme is computationally secure and I've added another term here a single message for reasons we'll get to if 22 and bit messages m0 and m1 we have that when you choose a random secret key according to the scheme and you make the encryption of m0 versus you make the encryption of m1 in both cases you get a random cipher text and instead of demanding that these have identical probability distributions we just demand that they're computationally indistinguishable and here's a theorem that's about achieving this and it assumes that you have cryptographic prgs this is another example of thinking you know one cryptographic primitive a PRG easier to construct another primitive symmetric key encryption schemes so let's say you have a cryptographic PRG that stretches n bits to element bits and as we saw before you can just assuming you can stretch n to n plus 1 you can convert stretching n to n to the ten then you can build a computationally secure SK encryption scheme for messages of length L of n like n to the ten using secret keys that are only n bits long so the secret key generation scheme just is a simple one you just draw a uniformly random string that's the secret key and random bits but now using this you can encode you know end to the 10 bit messages and how do you do it it's basically you do the one-time pad but instead of using a truly random one-time pad you use the output of a pseudo-random generator so Alice when she's encrypting she applies the sooner atom generator to the N bit secret key gets back into the 10 bits and she's like great now I can XOR them with my hand to the 10 bit message and Bob knows the secret key you can also generate the pseudo-random one-time pad XOR it with a C and get back the message and the proof that this is computationally secure is you know straightforward in fact the proof so let's prove it so let so we need to show this computationally secure so we need to like fix any two messages m0 and m1 and argue that this ensemble the cipher text for m0 is computationally indistinguishable from the cipher text for m1 so being point is for any message m and for a random secret key if you look at the ensemble the encoding of m by the secret key okay it looks like the in samba where you XOR M with the output of the pseudonym generator and I claim that by a the fact that G is a super graphic PRG this is computationally sorry I said it wrong simply by the fact that the PRG g is an efficient algorithm i claim that this is indistinguishable from okay I should say elleven okay it's indistinguishable from sorry because Jesus is pseudo-random generator I've sort of watched this explanation we know that G applied to the secret key is indistinguishable from a truly uniform string of length element so the PRG property shows that these two boxed things are identically indistinguishable ensembles now I claim that the whole parenthesized things where you take the box things and XOR them with em these are also computationally indistinguishable and this follows from our little mo from before that said if you have to computationally indistinguishable ensembles and you apply the same deterministic algorithm or same algorithm to both of them you preserve computational indistinguishability and that's what we're doing here we're applying the little algorithm that XOR is the string with the message okay so the PRG property gives us this and now it's the fact that for any fixed string M if you XOR with a uniformly random string that's exactly equal in distribution it's 100% indistinguishable from a truly random string okay so that shows that like every encoding is computationally indistinguishable from a truly random string so all pairs of messages have indistinguishable encodings I hope that was clear if you have any questions do you ask okay so let's review this is kind of cool we're happy because now we have a scheme where the length of the cheese can be much less than am you know key lengths can be n and you can encode messages of length and the 10 or n to the 20 or anything like that that's nice still very bad to use it more than once though this is still like a one-time sort of thing because you still this property that you know what's the secret key is fixed the suitor hundred generators deterministic so the sort of long pad that you get out of it is fixed once the secret key is fixed and so if you encode multiple messages you're gonna encode them with the same pad and so you'll still have the property that like you know Eve can take two ciphertext XOR them together and she'll get the XOR of to plaintext messages and that's very risky for crypt analysis purposes now you might say oh here's a cool trick for getting around that whenever Alice you know he's gonna send a message to Bob she takes her message that she's gonna send and she appends to it a new freshly generated secret key and she encodes the message plus the secret key sends that to Bob and that's fine and now a Bob gets the message he can decrypt it he gets the message and he gets like a new secret key that they can agree on and so now they can you know do their next message with the new secret key and so we don't have this one-time issue ah that's true it's a cool trick and you can use it but still not like the awesomest thing in the world because it's kind of stateful like you know Bob like misses one message then he's totally out of luck for getting them rest of the messages so yeah it's not ideal it's better than nothing ah let me end by saying that uh I mentioned that this kind of security is called single message computational security I'm gonna get into a more detail but there is a better security notion for encryption schemes it's called this indistinguishable under CPA chosen plaintext attacks I want to find it formally but it's kind of the gold standard security desire for cryptograph Schemes it basically means not just you know if you're gonna send one message it should look the ciphertext should look indistinguishable to an adversary you want to support like the ability to send encrypt polynomially many messages and security parameter and you also want to allow a very powerful adversary who can sort of request Alice give the encryption of some messages so like the adversary says like Alice why don't you or she's maybe Trix Alice into like encrypting a bunch of messages and maybe in that way tries to learn something about the secret key but then finally you measure like okay for me the true messages that Alice wants to send you know we measure whether Eve can get any an advantage in distinguishing them okay so I didn't really explain it formally or even maybe that well but it's a it's I mentioned because it's sort of the more general gold standard of what you want for a an encryption scheme and let me just tell you I mean now we get into more sophisticated results in cryptography theory which I won't even try to sketch but it's known that you can get a symmetric key encryption scheme with this very good you know sort of gold standard notion of security indistinguishable against chosen plaintext attack provided oh not provided that PR jeez exists but provided this other crypto object called cryptographic pseudo-random function generators exist which is a well a stronger looking object but I'll just tell you another fat this is like another theorem in cryptography that we will not prove well it's a good theorem it shows one of these construction things that like just starting from a pseudo-random generator you can construct pseudo-random function generator and therefore you can construct symmetric key encryption schemes that have this create security property so again this is like just telling you about this story of like you know you know main tasks that go on in cryptograph cryptography theory you're starting from a weak assumption about pseudo-random generators you show you get the stronger things around function generators and you can get symmetric key encryption with like these great properties that's sort of the name of the game so I've told you how you go from pseudo-random generators to at least single message secure symmetric encryption schemes and I told you you can fact get you know the NCPA one from PRS and PRS you get from PR g's there's like a most famous theorem in this area called the hill theorem from 1999 sense for the authors surnames host out in palo alto 11 and Looby and they showed that you can get this object pseudo-random generator just assuming you have this weaker looking object called one wave function owf stands for one wave function and this is basically sort of the end of the line going you know leftward in the sense this is sort of like the weakest this is basically like the weakest object that people are willing to assume exists and people are willing to see what exists but luckily like all these chains are known about how if you just have a one-way function you can get all sorts of cool stuff so let me quickly give the definition a one-way function is a function that map's strings to strings perhaps you've even heard it before it should be an efficient to compete function and deterministic and it should basically have the property that although it's efficient to compute it it's very hard to invert it so basically you want to say that for every ppt adversary a if you pick a random string X apply F to get you know the output f of X and now you ask the adversary okay here's f of X you just ignore this one to the end basically you just have to also tell the algorithm what n is here's f of X you don't have to find X but just please tell me any y that has the same F value as f of X so the a gets this f of X is just don't find some y where f of Y is the same thing as f of X and we assume that F has this property it's hard to invert that no ppt algorithm can do this with better than negligible probability okay so this is the definition of a one-way function there's a question if they're weaker objects is it possible slush easier to generate PR G's and when we functions from a pseudo-random function generator yeah so generally this direction is always sort of easy and almost like from the definition so in fact all these are equality equivalence is in the sense that assuming one exists you infer that the other exists but like this is always the this is the direction of difficulty and like going from here to here it's almost like by definition uh I should say I said this was the end of the line but I can go you one further there's even something called a weak one-way function where you just a weaken this instead of saying a negligible chance you allow them to just not be able to do it perfectly you know they can't do it with probability at most 1 minus 1 over N to the 100 or something but there's even a construction that goes from week one wave function to one wave function and I can even tell you to it to you because it's kind of easy let's assume that F is a weak one wave function so it's easy to compute it it's like slightly hard to compute preimages for it well you build a new function G which takes M strings and G on these M strings x1 through XM is just the string gotten by you know attaching together F of x1 through F of xn and so now the task it's easy to compute G because it's easy to compute F but to invert G you're giving like a now you're giving like a bunch of strings and you can set m to be n to the C plus 1 where this is the same C here the task is now you have to like sort of simultaneously find preimages for all of these things and so if it's slightly hard to find one pre-image then it's gonna become super hard to find simultaneously preimages for a bunch of so I want something kind of fast but maybe you can imagine how this construction and even proof goes okay but this hill theorem is actually very difficult is on this why that's sort of a famous theorem I can even tell you on the subject of one-way functions a candidate one-way function it's not people as most cryptographers like series cryptographers most favorite candidate because a it's kind of intuition tanned B not so convinced of its security but on the other hand nobody knows how to break it and the candidate one wavefunction is the inputs are n strings of length N and you think of these strings as numbers mod 2 to the N plus s a subset of the numbers 1 through n so overall you have N squared plus n bits an input and F's output you just output the same strings and together with the sum of some of the strings namely the ones in the set s and you do it mod two to the N and so this is really easy to compute but going backwards if I give you it's very much like the subset sum problem like I give you a bunch of n bit numbers and I give you some number which is achieved as like the sum of some kind of subset of them and the idea is that perhaps when the numbers are random and the ones that you secretly added up are also chosen randomly it could be hard for an algorithm to figure out what that set is so this might be a one-way function and so it's you know why you might believe that when my functions exist and once you believe that then you get all this other stuff as well thanks to previous work of cryptographers so you know you get a lot of crypto primitives out of one-way functions thank to a lot of work of cryptographers like you know your symmetric key encryption you know digital signatures all sorts of other things but not everything at least it's not known how to get all the things you would want and one thing that it's not known how to get is public key encryption which is that thing that you know RSA solves so we have this very minimal assumption one-way functions a lot of stuff but it's not known whether it gets you public key encryption and there's a very famous paper in the theory of cryptography and average case complexity by Paul out so it's called the personal view of average-case complexity but it's from 99 he famously introduced his so-called five worlds of like what might be possible about average case complexity hardness and easiness and you know one of the world's is like the world where P equals NP and like everything is easy and there are no hard problems and there's no cryptography but two of them are called mini crypt in crypto mania and mini crypt is the world where one-way functions exist so you get like a lot of cool crypto primitives but not all the ones we know in crypto mania is like another world where public key encryption exists and you know we don't know which world-weary in but we actually kind of believe we're in this world because we do have some schemes for doing public key cryptography like RSA but these are two distinctions that need to be kept in mind so I mentioned that because it's a pretty famous paper in complexity and\", metadata={'source': 'c4SrYqN6ItY'}),\n",
       " Document(page_content=\"so what is this public key encryption let me tell you about it again it's like symmetric key encryption you have a generator function an encryption and decryption function and a security parameter and and you can have Allison Bob but now it's a more complicated story there's another question I'll get to in a second there's no complicated wrinkle to the story which is public encryption goes like this and it's exactly like RSA if you know RSA so Bob when he wants to you know start participating he runs the generation scheme and tells the generation function you know the security parameters and which might be a thousand or something and he gets two keys one is called the secret key and one it's called the public key PK and SK and you know the public key he publishes you know on his home page some like you know thousand bit string or something and he keeps the secret key key secret and now Alice if she wants to send a secret message to Bob she uses an encryption function that takes as input the message and also Bob's public key and that produces the ciphertext and Bob has the ability to decrypt her message here her ciphertext C using his secret key okay so this is kind of cooler because it does not involve Alice and Bob like having the ability to get together in advance and secretly agree on a secret key that they privately store forever now you just need everybody to privately do their own thing and hold their secret key secret but they don't have to do this collaboratively okay let me get back to a question that was asked the question is how does quantum fit into these worlds ah good question I mean it's uh it's fine in the sense that question let me bring it up so what these world means depend loops on what is your definition of efficient computation like P equals NP he's like you know it really means NP can be solved efficiently so you know employs whether you take that to mean and P equals P or NP equals B P P you know whatever I'll just call them the same thing that world was called algorithmic ax and so now you can say like well perhaps I wanted to make my definition of an efficient computation quantum efficient computation and in that world when you make that decision then you might become less convinced that we're living in the world of crypto mania because for example RSA security relies on the fact that it's hard to factor you're basically a that relies on the fact that it's hard to efficiently factorize integers but we now Oakland some computers can efficiently factorize integers with Shor's algorithm and so that might lead you to believe if you accept quantum computation to you think oh maybe we're not living in crypto mania unless we come up with some different way to achieve public key encryption that does not rely on hardness of factoring and this very popular area of study post onto cryptography and as much later we in fact do have candidates for public key encryption that are not broken by any known crypt quantum algorithms so maybe we still believe that we live in crypto convenient despite the fact that RSA is broken with quantum computers ok so this was a public key encryption and there's a notion for public key encryption of one bit message security which is exactly like one message security but it's even weaker and only your first two messages that are one bit long and it says basically when you do this scheme and you you know the public key and the secret key whether alice is planning on encrypting the message to the single bit message zero or the single bit message one anybody that sees the public key of Bob together with the encryption of zero they cannot tell the difference if they're ppt from the public key of Bob together with the encryption of one so that's the definition of security it's like inability to tell the difference between encryption of zero and encryption of one even knowing the public key as Eve certainly does and a theorem which is actually not that hard is if you're given a one bit secure public key encryption scheme you can construct from it you know a public key encryption scheme with this like much better you know multiple message chosen plaintext attack security property so it's actually nouns the theorem that like if you want to get like awesome public key encryption you just have to well quote unquote just have to get like one bit secure public key encryption but just like good you know it gives you something easier to aim for so one might ask okay well I sort of told you that even assuming that like cryptographic prgs exist which we know is a consequence of one-way functions existing we don't know how to get public key encryption the best thing we know is we know that like if trapdoor one way permutations exist then you can construct public key encryption from them and I'm going to find trapdoor one way permutation but it's like one way permutation is like a one-way function which is also a permutation it Maps you know in bit strings and bit strings in a by directive way this is really basically not much of a big difference between not and one-way functions it's practically the same thing but trapdoor you know means as it's like extra gadget property we're like mmm it's easy to compute hard to invert but there's a way to generate like a trapdoor secret that makes it easy to invert if you know the trapdoor and long story short cryptographers are not super happy with this assumption because like we don't know great candidates and we don't really know great candidates that are not just like direct public key encryption schemes so like you can get public key encryption by RSA and then to assume that it's secure you basically assume that RSA is secure I mean it's related to these security of factoring that's not actually the same thing or for you can also get it from this like computational diffie-hellman assumptions just some other assumption it's like number theoretical but basically it's sort of designed to give you public key encryption so somehow people are not that satisfied with this state of affairs here I mean in practice RSA seems believable and it gives us public key encryption but sort of only gives us that and it is broken by quantum well I would like to now tell you well another deficiency let me mention another not efficiency but a fact these are all assumptions about sir our problems being hard on average so it's not just that part of the worst case but even you know when you choose a random input all but a negligible fraction of the inputs are hard I mean looks like it's hard to factor to the product of two random primes for example that's like an assumption related to our essays assumption but now I want to tell you about a cool development from 15 years ago due to Oded regev which was recently awarded the girl prize and it led to great developments in cryptography theory and his paper from 2005 did two things one it introduced a new assumption called and I'll explain this the lwe assumption all of you V stands for learning with errors but it's some assumption so it's an assumption that a certain problem is hard okay it has something to do with well I'll explain in a moment but one thing you showed is that if you have if you make this assumption then you can get public key encryption people like okay fine I mean we've seen that before you make the assumption that some problem is hard and you get public key encryption great and you know this problem did seem to be hard but why is this better than other why is this better than just using RSA well one thing which is what excited people the most was he also showed another theorem that this assumption lwe which is also of the form you know a certain problem is hard for random inputs he proved it assuming some other problem is hard in the worst case and that was really exciting to people this first time it seemed like this worst case to hardness a a verge case reduction so there's some problem about lattices this is a geometric object I'm not going to talk about it more but like lattices are like you know regular arrangements of points in space you know like the cubic lattice or triangular lattice honeycomb lattice but extended high dimensions and there's like tasks associated to it like find the shortest vector and a given lattice and so forth and some problem which had been studied before certainly by a lot of algorithm assists in fact had been studied by Lovaas in the context of trying to come up with well enough time implementations of the ellipsoid algorithm we talked about this at the very end of the LP lecture about how you need some like number theory results about like multi-dimensional greatest common devisor algorithms that actually is related to geometric lattices and these people had studied these lattice problems anyway he showed this cool result which is if you assume this problem is merely worst-case hard like there's no you know efficient algorithm that solves this problem in the worst case then he deduced the average case hardness of this lwe problem that it's hard even for random inputs and from that constructed public key encryption so these two things together were pretty great and make people like a lot happier about I don't know the theoretical foundations for a let's say public key encryption and this is indeed a hard problem seemingly de-facto the best known algorithm for it runs basically an exponential time two to the N over log n people have thought about it so it's not like we just haven't known anything but this weird problem but also some treating those it's there's also good evidence that it's not an np-hard problem which is very intriguing it's like one of these intermediate problems where doesn't seem to be involved meal time but also doesn't seem to be np-hard which is intriguing that at this worst case to average case results are not hold for an np-hard problem another kind of amazing thing it's almost like a weird coincidence when I tell it to you I put a little asterisk here this result that LW is true assuming this gap SVP problem this lattice problem is hard for efficient algorithms he needed to assume that it's hard not just for efficient classical ogron's but efficient quantum algorithms so if you believe that the best quantum algorithm for this gap SVP problem for solving in the worst case you know still take exponential time and lwe holes and you get public key encryption it's kind of amazing that also sort of blew people's minds should say that was some paper of peickert sort of relax this to get it like a variation on the parameters which is not as good a variation on the parameters but assuming it you only need to assume that like class Loggins cannot solve some lattice problem okay so in the last few slides I want to tell you just a little bit about this lwe assumption because it's pretty cool and worth knowing about so here it is all on this one slide so there's a lot of parameters and like there's one security parameter N and there's all like two or three other parameters like Q and alpha and you can choose these parameters according to your taste and it makes the assumption different in slightly way different ways I'm just gonna tell you like one reasonable way to fix all the parameters in parenthesis and don't get too hung up on the parameters just try to get the gist of the problem so given n the lwe learning within our problems involves fixing a Q which is gonna be like you're gonna be working with numbers mod Q and you should be polynomial in n ideally and typically a prime that's around N squared alpha is some parameter we'll see which is like 1 over poly n typically like something a little bit like 1 over root n and Chi is an error distribution and it's an error distribution it's a probability distribution on numbers mod Q and it's like a a Gaussian distribution on numbers mod Q which is a little bit weird because you think of gaussians is distributed stations on real numbers but you know this axis here is like the numbers mod Q and Chi is sort of like a discretized Gaussian so formally to get chi you draw a regular old Gaussian a real number Gaussian with mean 0 and standard deviation alpha Q so remember alpha is like a small fraction you know the numbers mod Q range between you know 0 and Q or minus Q over 2 and Q over 2 and you choose a random real Gaussian you know with so deviation a small fraction of Q and then you just round it off to an integer and that's Chi this means rounded off to an integer and now uh having fixed all these parameters there's some kind of like algorithmic game or some algorithmic task and how does it go you know somebody chooses a secret string well a secret vector s which is like a list of n numbers mod Q it's chosen uniformly and now the algorithms playing this game gets to ask for some information about ass it gets to ask for these quote-unquote noisy linear equations about s so the algorithm goes to say like I don't want the secret vector s is but please give me one low easy linear equation about it ok and when it asks makes that request what happens is coefficients a1 through a n are chosen uniformly integers mod Q and then the person who knows the secret you know calculates a 1 s 1 plus a 2 s 2 plus Delta a n SN and that's some number that's some number but they don't just tell the girl 'm who's playing the game the sort of the secret holder doesn't just tell the algorithm the number b they tell them the number but with some error e and this error is drawn from chi so they compute the actual dot product of a with s but then they add in this extra error to make it like a noisy linear equation and then they tell back the algorithm the coefficients a and this noisy version of b times some like noisy linear equation the other one can say something interesting and then it can request another equation noisy linear equation another noisy linear equation but the algorithm is polynomial time so can I ask Fred most polynomially many know it's equations like this and of course if there is no error then it could just ask for like in equations and then would be solving a system of linear equations and n unknowns over a field and so great it would just recover the secret but kalsye elimination or solving equations is very sensitive to noise especially like in the world of like modulo a crime as it turns out and so it's not clear what to do if you're getting these like noisy right-hand sides and finally the lwe assumption is that like when you play this game somebody holds on to the secret s and gives you back these noisy linear equations no polynomial time no PPP algorithm can you know exactly figure out s with high probability okay so that's the assumption it's you know related to things like decoding random codes and learning you know parodies with error so I'll answer a question in a second you know it didn't come out of the blue so people could have some reasonable intuition that it was hard and therefore a believable assumption and then we have this other theorem that reggae approved that show that this problem is this assumption is true assuming the worst-case hardness of some lattice problem the question is are the a eyes are drawn only once no they're drawn every time like the algorithm though you've like an algorithm a PP app or PPC algorithm interacting with like some person that knows a secret and the algorithm can always say like please give me a linear equation about your secret s and at that time the A's are some A's are drawn randomly and an error is drawn randomly and the algorithm learns the A's together with the true value of like a dot s but Plus this error okay and so when the algorithm asks for another equation fresh A's are drawn and a fresh error is drawn and every time I ask for a question it gets a fresh a and fresh Air's okay ah so that's learning with errors and what I can now tell you is I can actually literally tell you how reggae have built a 1-bit a public key encryption scheme which is secure assuming the lwe assumption so I won't prove that it's secure assuming lwe and but I'll tell you what the scheme is at least and then also remember I also told you if you have a one bit encryption scheme you can it's known already how to bootstrap it into like a multi round indistinguishable gets chosen plaintext attack scheme so here's the scheme think of it as like you know an alternative to RSA it allows you to send one bit messages so when you're generating your republican secret key you know with security parameter n the secret key is like one of these s's okay it's a random vector of length n mod Q remember Q's like N squared or something so it's like and I don't know to log 2 n log n bits and the public key that you generate is you yourself generate and like noisy em noisy linear equations about s of this type and M should again be set something like a little bit bigger than n like n log n so you you pick you know M different vectors a and you know s you compute like AI dot s but you don't publish the exact answered be I you published B plus an error and that's it so you give away this like information like and noisy linear equations about your secret key s you make that public but like the lwe assumption intuitively tells you well even given this nobody's gonna be able to guess your secret key except with non-negligible probability so that's good but you know the task is not about guessing a secret key it's awesomes about encryption and you know guessing messages so what's the encryption scheme now let's say Alice comes along and she wants to send a message to Bob so she looked at Bob's public key which is like these A's and these you know error filled B's and she wants to encrypt a bit zero what does she do she chooses a random subset of the A's the a vectors no so we have like maybe this is s it's like unknown to Alice with all these rows these are the A's you have an above them and those rows are published and you know they're associated to some answers these are the bees that have noise and she just adds up the a vectors and also the B answers for this random subset she chose and she this is the subset a ciphertext the sum of the A's and the sum of the B's all mod Q and uh when she wants to encrypt one she does basically the same thing except she takes the final sum be some and she adds key over to mutt Q which is basically like the maximal noise she almost like sort of negates it and the idea for why the secure is like you know an adversary he's like so bad at like understanding what s is just doing these A's and B's that this is kind of like sort of like a new equation a new noisy fact that like a some dot s is basically the same as a besom it's like a new noisy equation that like Alice could generate herself but adversaries are so bad under saying them that they can't tell if you generated it like kind of correctly or you put in like maximal noise on the answer but Bob who knows the secret key who knows s can with high probability decrypt so what Bob does is he takes the secret S which he knows and sorry this should say well I can say B here it's either be sum or it's be sum plus Q over to this B and you just computes this and he's like if it came out really close to zero then he's like she was trying to encrypt zero and if it came out pretty close to Q over two then he said she was trying to encrypt Q over two and you know whichever is closest to he'll uh you know guess based on and it won't be exactly equal to them right because like the thing he published had some noise in the right hand sides so the only thing I'll say is why this algorithm is correct in the sense that Bob can decode and if you know the correctness follows from the fact that like suppose that he didn't put any errors into the public key which is a bad idea because then people can actually learn s but if he didn't put any errors and then the decryption scheme would be always a hundred percent correct you know either always get zero or always get exactly Q over two now since there are errors uh the decryption will fail if the sum of the errors he put into his public key or at least the ones that got put into S into the encryption have magnitude that's bigger than like Q over four that's like an amount which will be enough to like screw up your ability to distinguish between 0 and Q over two but now you can just check like you have the sum of m errors each one is like a Gaussian basically it's like a rounded off Gaussian with standard deviation root M times the standard deviation of a single error single I or standard deviation was alpha Q and root M is comes from the fact that you're summing m errors and now you're just plugging the parameters the suggested parameter so M was suggested to be n log n alpha was suggested to be like this so you know basically it's chosen such that even with you know n log n equations the total amount of error is like a Gaussian with standard deviation is still a small fraction of Q like 1 over poly log Q and then the probability that like a Gaussian with standard deviation like a small fraction of Q would get as large as Q over 4 he's exponentially small in the square of the number of standard deviations those like e to the minus theta of log cubed n which is negligible ok so I'm out of time but if you want to stick around for the last couple of slides please do otherwise it's fine the proof of security is not actually that hard assuming lwe buts I don't have time to show it but that's the scheme and so let's actually my last slide um let me tell you this is like a new approach to cryptography basing it on LW e or these lattice face problems it's quite different from all the previous stuff that maybe you've learned about before or then you heard of that was invented in like the 60s and 70s that are like based on number theory and it's got some advantages so one the cool thing about you know Ray gives theorem is that he showed his assumption is true based on the worst-case assumption a worst case hardness assumption about some problem which hadn't really been seen before another thing that's cool is that over the last 15 years people have found cool cryptographic primitives that they wanted but they do know how to get based on like lwe assumptions that they do not know how to get by other assumptions like RSA or like hardness of factoring so for example a famous one that came out I don't know ten years ago called fully homomorphic encryption it's like a cool cryptographic primitive generative a lot of excitement people know how to construct it assuming the lwe assumption and they know how to construct it if you just like to say assume you know factoring random integers is hard finally uh it's also not broken by quantum computing so if you believe that quantum computing is around the corner then factoring integers is not hard and so RSA is broken but people have thought about it a lot you know Peter shor himself has attempted to you know crack these lattice problems using quantum computing and it's over the years you know fifteen years it's not been done uh what about disadvantages ah well we said or at least for a while that the number theory based constructions were like more efficient here like you know you might have to worry about like oh if you want this amount of security like how many bits does like the secret keys and public keys need to be how fast are the protocols for like encryption and decryption and for a while you know the numbers you know is always polynomial this polynomial that for lwe but you know practice you know if you're gonna be running you know one of these schemes like every time somebody types HTTP in the browser then you want them to be like hyper efficient and number theory ones were more efficient um they used to be but there's been a lot of theoretical work over the years and now the number three ones a lot of space encryption a lot of space crypto algorithms are just as efficient as the number three ones and therefore there are no disadvantages one might say to a lot of space cryptography so forget everything you know about RSA and just go with a lot of space cryptography and all your future endeavors okay so let me end the recording there but I shall stick around to answer your questions\", metadata={'source': 'QcVns57MTxg'}),\n",
       " Document(page_content=\"all right cool i think we're set hello everybody nice to see you um yeah only only two problems to deal with on this week so we can talk about some homework or anything else you got on your mind about the class um yeah what's on your mind did you solve everything i guess can we look at number one number one yes sure thing this is i assume it's not too bad but um yeah i guess i haven't gotten too far yet um yeah i've been trying to do it like algebraically like how you i guess hinted in class that a lot of the proofs go um but it just gets kind of messy um like i had some ideas for how i'm expanding it algebraically but i can't get the uh inequalities that way yeah this one a maybe easier if you know some inequalities and definitions which i did not tell you in class and therefore it's like therefore harder if uh you're trying to prove everything like literally from first principles which and i guess some sense you are um well let's see i guess let's see first of all maybe we should think about it intuitively because this sometimes is helpful yeah i was trying to think about it intuitively as well but um so let's see it's a little bit about this like you might imagine that like okay okay x and y are discrete random variables so um [Music] not necessarily independent so we see something showing up in the problem we have like h of x this is like you know roughly speaking the number of bits you need to generate a draw from x right on average and similarly for h of y and now i guess we kind of have this like kind of computer code that's sort of like okay uh to define z so like you know like do a z kind of goes like this like c gets like a random bit and then if c is one then kind of uh like let's say little z colon equals i might say draw y i call it do a z or whatever i'll call it drawsey else if you know c is zero we'll do z equals draw x and then we'll look return little z and so if we're interested in like the entropy of z it's kind of like saying like okay if i run this computer code like how many um times does it call rand bit on average um well i guess one thing you can say is this need not be this okay so this is one way to make computer code that does a draw from z this might not be like the best way like the most efficient way but this is like one way to do it um yeah i guess thinking about when the inequality will be tight then or just i thought about that as you were writing this code like um i guess one direction might just be if they're independent right because then you really do have to flip that random bit yeah that's very intuitively uh correct as you say in the sense that like imagine x and y are like independent so like you know you have this code that can draw from y you have this code that can draw from x and like you might imagine that like in principle there could be like you know some code that like you know it's like called draw x and y and like you know it returns a pair and like potentially if x and y are not independent there might be like some most efficient code for this that like i don't know does something smart um but you know the idea is that if x and y are independent then like well you know this thing may as well just be like one call to draw y and one called to draw x and it just returns both of them uh intuitively and then you you get the feeling in that case like okay if you had to if that were the situation like if you had to make code for like draw z you kind of say yourself well what am i going to do other than this like code it seems like it should be the best um because there's no way like you can get i don't know any savings this is the idea when x and y are independent now like i guess like normally i guess you know one thing to think about is like um the way you can prove an upper bound on entropy like if you have some random variable z and you want to show that h of z is like almost something you could do this by like you know in some sense like giving some code for drawing like z and showing you know it uses at most you know t bits you know or t calls to rand bits on average and then that would kind of imply that the entropy is at most t of course you gotta be a little bit careful about this because like you know in some sense the entropy is not literally the number of average number of i mean this will prove it but like it's not literally the number of uh bits used on average like maybe this is only exactly right when the probabilities all have denominators that are like powers of two but um i guess this is at least one intuition and i think this code kind of matches up syncs up with like this intuition in the sense that this code's like always using like one bit for c and then again intuitively like half the time it's using however many bits you need to do x which is this and half the time however many bits you need to do why which is this so this is like one of those things that like comes up a lot in like you know information theory where like there's a very strong intuition behind some uh you know expressions or inequalities then you have to take a little bit of care to actually prove it i guess for the intuition of the other side of the inequality like this might be kind of obvious but i guess i can just say that uh if they're the same random variable i guess then you don't have to flip right and so yeah for example um indeed like in a specific case just as you say like if x and y happen to be have the property that like you know x is literally y always so you know draw x and y would just be like you know you could say uh you know x equals you know draw from x and then like y equals x and then return x y then indeed as you say like you don't have to flip the coin like you can just uh return uh one of them like you can just you know call draw x basically and this will have the right distribution um so that's uh in particular like in this very special case you know h of x will equal h of y and so this will just be h of x um yeah so those are some [Music] intuitions for when this will be tight now can you also think of an intuition for why this lower bound is always true we saw like we just talked about intuition for like what's the case when it will be tight but like what's an intuition for why this lower bound will always be true i mean i guess just it z depends on x and y right which depend on this c which gets flipped with one half each in each k or probably one half in each case and then so i don't know that's like the intuition for why it's the average of the entropies i guess i don't know yeah um [Music] you know i guess like an in intuition for like entropy lower bounds you know like h of z is at least you kind of like saying like you know again like always like on average you know to communicate or store or draw from restore a draw from or you know create a draw from z you like any algorithm or any method like needs needs at least you bits um renovates and uh i guess in some sense like you know it is true that like half the time like you're gonna have to be like communicating or drawing or storing x and like half the time you're going to be doing y and there could be some savings if you were required to draw like x and y if you you know you could generate x and y together um if they're not independent i guess there's some intuition that like well be that as it may like half the time you have to at least generate x and like half the time you have to at least generate y so it's some kind of lower bound so i mean i guess okay i mean this is a little bit evading the final details of like okay how do you actually prove these things but um this is a good start to have some intuition now in terms of how to prove it maybe that's another matter um i don't know does anybody have any suggestions [Music] we tried to use the probability technique but we found that was a little bit complicated so we decided that it would be it was easier to just stay with the entropy kind of the forms of the entropy and like um conditional entropy kind of stuff that that was in lecture yeah you can try to i think attack this using like these objects we talked about like conditional entropy and like mutual information and things like this yeah um [Music] yeah i don't know let's i mean maybe we should try to prove one of these inequalities [Music] let's see which one is more enjoyable to look at the right one okay um [Music] yeah i guess for the right one we had this intuition right yeah um [Music] so yeah you can try to go ahead uh we're maybe thinking you could use the um forget what if this is the chain rule but like h of z is equal to h of z given c plus the mutual entropy of z and c i was thinking like did we do that in class i guess we did you're saying like yeah of z is what now h of z given c plus mutual information of z and c [Music] yeah let's see if we agree that this makes sense um yeah i mean i guess uh this is how it's defined i mean this is the amount of like savings you can get when generating z and c together and you know this is how much it costs you to generate c it's like how much it costs you to generate z like if you sort of already generated c on average so okay again these are like maybe uh intuitive quantities but okay we also have these as definitions basically right let's see and hmm so we might want to like match this up yeah i suppose we should remember like what is this quantity h of z given c expectation yeah this is really an expectation over like sort of little c drawn from capital c of kind of the entropy of the random variable like z condition on c being little c it's kind of funny notation but um so i mean you know since c is a coin flip it's really i guess just uh half entropy of like okay well i'm writing it slowly z conditioned on c being one uh plus a half entropy of z conditioned on c being zero okay but this is i guess x and this is y right i guess that's good and then i suppose that's getting us part of the way there then we just have to ideally show that this is at most one right this should be doable though right i mean if you remember well a couple of things but for example you know as we talked about in class you know this is like the savings you can achieve when generating z and c together as opposed to separately but you know the most you can ever save is everything and um you know everything for generating c is is one big one and we know that mutual information is positive from um gen students inequality of result right yeah or i suppose i stated this as well so yeah yeah i guess one can put these things together from things that i stated in class most of which i didn't prove but well if you want you can like write down proofs of them yourself but you can i suppose take it also for granted the things that i stated right um we're in good shape though because then we're pretty much done right this also helps the left-hand side yeah exactly basically that like mutual information yeah so maybe one can uh well if you're feeling noble like also write some i guess i was gonna say like write some some proofs of like things like this but you know it's you might want to do it for fun just to like see what's going on but um i guess it's more or less does it yeah so so we try to do this using statements but it was really tough actually it was really bad right yeah i mean not not bad it was just like hard because like we don't know what the domains are of probability like like the ranges that the x can take and the y can take are not necessarily um related like if x was like um a b c d and um and and so and and y was like one two three like it wasn't clear how you could like do it like if you do it this way then everything just like works out but if you take the other one it's hard to like figure out like the the intersection of the ranges of the um values in some sense yeah i see you're saying like it's a little bit of a mess well maybe let's just try it and see like how it goes and goes wrong or like okay so x has some range maybe r sub x and y has some range r's of y so i guess like the range of z is the union of these things yeah it's a bit funny though like these are just like abstract sets it does seem like a little bit weird because like they could have some intersection or hey exactly although it shouldn't somehow really matter like you should sort of be able to pretend that they're disjoint but like i guess it's not immediately clear from the the definitions um maybe not even okay uh yeah probably not okay but you could just be like okay i'm going to try to do this like really blindly so z has this range so you know like the entropy of z it's like the sum over all little z and like the range for z which is this thing right um right of like probability that z equals z times you know log of 1 over this probability i do see it's like starts to be a bit of a mess though because okay what is the probability that z equals a particular z yeah it's sort of annoying well i guess it's it's half the probability that like x equals that's not a probability probably that x equals little z plus a half probability that y equals z it's a bit funny though because you're like oh if little z is like not in the range of x yeah exactly relying on the fact that this is zero if little z is not in the range of x right see if there's maybe a little bit like worry about type checking yeah um but maybe it's okay i mean okay we could then like plug this in and be like all right that's sum over little z of half probability that x equals z plus probability of y equals z [Music] then it looks doesn't look too great because we have like this uh yeah exactly this you know sum but well one can persist see what happens um because okay like i you know there's no moves to try here other than like breaking this into two sums so all right we have like half sum over a z probability that x equals at oops little z [Music] times the log of this kind of annoying expression all right it's not very clear how to split that though because like logs are nice and in um when you're multiplication not not so nice when you have addition yeah that's true so i guess the the game here is going to be like you can artificially think of like x as taking values you can like think of x as taking values i can't write today taking values in like the range of z like it's just like okay they might have like a bunch of values probabilities that are zero if you know it's like some element major z that x doesn't take on but you could say that's okay and then okay then this expression i mean whenever i like see something that looks like this i get the temptation to write it as an expectation expectation over z [Music] uh drawn from x i mean if you just you know formally convert this to like some probabilities it'll look like this so the rotation of z drawn from x of this expression log base 2 of this kind of gross thing and then i guess like here the probabilities are gonna be yeah this is where it was like tough if you didn't know like what the probabilities of y or here because it depends on the range right off yeah i guess this is although this is also the point where like you know you can take the hint that like in um information theory like there's only so many tricks in one heck the only trick that we haven't yet used is basically jensen's inequality or the log is like a concave function so right right i always just remember like this okay log somehow looks like this is like the plot of log so like if you take you know the average of its values at like two points here and here the average of the value is it's terrible drawing the average value is like maybe this green point like that's below this orange point so okay so it's concave so that means like the average of some values is smaller than a function applied to the average so this will be at most log base 2 expectation z drawn from x of one over oh yeah that's not going to look so great but we'll see we'll keep going i guess um i wonder if okay yeah we'll keep going but like yeah like probability that x little z plus probability under y of little z over two that doesn't really look like a dream quantity though because you know this thing i think there's the half constant um in front of everything right yeah yeah yeah there's a half out of here yeah i mean maybe it's okay you could say that okay this is let's just keep going like on autopilot until we get stuck like log base 2. we're going to turn this back into a sum sum over z probability that x equals z is 2 over look so great x equals z uh plus probability that y equals z you know your dream here is that like somehow this would like nicely some nice algebra would happen right it doesn't look super promising we have some expression that looks like you know 2a over a plus b this is a and this is b um one can also see what are we trying to do here i guess we're trying to show that this is at most the average entropy is plus one at least so far since the move that we made was this [Music] i i was like drawing inspiration from the like maybe what we were guessing like you would have to introduce some form of mutual entropy here so initial information here um but it's not like like from from the from the completely abstract way of like looking at it through conditional entropies there would have to be something like introduction of like and communicable information but it wasn't clear how we would introduce that in some sense yeah i mean i guess that's a good point in the sense that like okay if you look at like this proof sketch that we looked at it was like an equality so nothing had happened and then we had this inequality here but this was at most and really i guess as you're saying like the mutual information we kind of use that the mutual information between a and b is at most the entropy of b right and one way to know that is to recognize that this is the entropy of b minus the entropy of um either b given a or a given b right i suppose it should be yeah yeah well i need a case and then you can recognize like oh but entropy is not negative right i'm not saying that this is almost this so like in the end like the arithmetic will come down to or like the inequality will become because the entropy of some expression you'll get like maybe the entropy of some expression somewhere and be like oh this is non-negative which um is ah i suppose this is just because why is entropy not negative entropy of like a is like sum over a probability that a equals a i mean it's times the log of 1 over this probability this is like an average so like it's the average of some right non-negative numbers so it has to be non-negative so actually that means that like actually you know this is kind of like retconning the whole proof but like it kind of means that like to prove this upper bound we didn't use like uh anything that involved like jensen's inequality or complexity we only use that entropy is like non-negative right so actually it kind of tells you that like this is not the right move because um and i think i even kind of see that now like uh yeah because this looks kind of bad maybe you wanted to actually use yeah but anyway yeah so it must be that instead like you somehow um you should be able to just like manipulate manipulate manipulate this until you get um like a half plus a half the entropy of x plus a half the entropy of y plus one minus something that's like manifestly non-negative yeah yeah exactly so some some like if we rewrote like the conditional um the conditional entropy kind of equation in terms of probabilities i think it would work out but it wasn't like obvious to do that when you start off because if you just use the probability stuff i feel like you get stuck somewhere here yeah i mean you have to be a lot smarter i guess it goes to show that like like it's like a nice feature of like information theory that like if you had to prove it by like manipulating these formulas like it would seem to require like a lot of smarts and insight or like good guessing or something but like somehow if you've like named all of these quantities and have like natural intuitive meanings for all of them like it really guides you on like how to do the manipulation yeah exactly yeah yeah yeah i mean i suppose like the thing to do actually like when we got to this point is i mean we kind of know in the back of our heads we need like entropy of x to show up and like here we have something half the interval of x actually we have something that's like moderately close to being entropy of x right you can probably just like you know forcibly get entropy of x in here so like here we have like sum over x probability oh actually it's z but sum over z probability under x of z times like log of something some some weird thing involving stuff involving z and whoa sorry i lost my screen here uh one second okay um and you know you can be like well i really want to have entropy here so i'll just like add and subtract something in order to put it there so you can like add the entropy of x and subtract the entropy of x and i guess when you so that's doing nothing but like when you subtract the entropy of x yeah you'll get minus sum over z probability x z log one over probability of x z right and then you're like okay it's looking not so bad because you know logs have these properties that like you add two logs and it like multiplies so like you'll get something from probably x of z and then maybe something that's like not terrible like you'll get um log of well probability x z times whatever this stuff involving z was right and i guess it was this if you multiply by this well it's still not looking that great actually but i think it gets closer to what the mutual information kind of thing is because the but like i think i think you have to use this with the probability of y thing and put them all together and then you'll you'll like have an aha moment and figure out that this is actually the same as the mutual entropy or something like that yeah in fact like i think if probably you'll like oh this like you have this like two in the numerator here and you'll probably be like oh this is actually one over a half and so like i kind of see that like the entropy of like a probability distribution that's like a half half is coming in right but i mean i guess like once you finally do all this what it'll eventually amount to will exactly be like just writing down all the proofs of this like simultaneously and like yeah exactly yeah it was just an interesting kind of thing to like realize that like because the my intuition would be to go to probabilities first but here probabilities actually lead you down like a very tricky path yeah exactly that's very true yeah so it's really nice to have this operational meaning of these quantities to get the right reasoning it's funny like looking at this you might come to understand how someone could even come up with the definitions of like mutual information and conditional entropy in the first place yeah that's a very good point like if whatever reason like you only knew the definition of entropy and you like wanted to prove this you almost like are kind of forced not forced but you were like you could see how you would invent like you know mutual information and uh conditional entropy and things yeah that's a good observation well should we talk about the other one did she'll solve the other one sure yeah this is a little trickier i think but um yeah maybe someone who said somebody else outside yeah yeah i think you can like show that one individual bit uh the probability of it being brought the word for it but like the probability of a times uh like one individual bit from the output generator is negligible and n close to the uniform distribution and then you can use that to like inductively build out that the final output is negligible and close to the uniform distribution yeah what's the story here i like to draw this picture this one's like output um yeah i forget what i called this length uh i guess it's like uh oh i called it n to the 10. okay well uh yes this whole thing is like g prime and maybe there's like n to the 10 output bits here so i mean i guess the way one tries to break that i mean one okay what do you have to prove you want to show the g prime g is like a good prg implies g prime is a good prg and i guess you will always prove this by proving the contrapositive so like if g prime is a bad prg implies that g is a bad prg um i guess you'll do this right because these are really like for all statements like say that g is a good prg is like say for all adversaries this adversary fails and once you do the contrapositive like this will be like a there exist statement like there exists like an adversary that can like break your prg and there's like an adversary breaking g prime now we have to deduce that there's like an adversary breaking g so you know this is more in the spirit of like designing algorithms you know we have some algorithm that it's g prime now we have to use it some kind of subroutine or something for an algorithm breaking g so you know it's much easier for us to like prove existence of things and to show for all type statements so yeah you might imagine like uh yeah i wonder if we can imagine i'm wondering if you can imagine like sort of a specific case here or like i don't know so what does it mean that like you know we have some adversary breaking g prime well let's call this adversary a maybe we should call it a prime but i am a a so it can like basically what is what's the deal a can like look at the output of a draw from g prime and like notice that it's not the uniform distribution so it can like look at all these bits and say like oh that's that's not the uniform distribution now there's the trouble is like you know there's many ways and that can happen and like what do we have to deduce there's like some other algorithm i can like notice that g is like a bad pog i can look at all these bits and be able to notice that like they're not uniform so for example maybe we could like do like a special case um maybe maybe the adversary a has the property well maybe like this bit this first bit is always equal to the last bit um that would be like a way in which uh g prime would be like broken i mean if like the first bit of these end of the ten bits was always equal to the last bit you'd be like this is not uniform distribution like that's only supposed to happen fifty percent of the time and it's happening 100 of the time um so what could one do like imagine you know it's like some crypto challenge out there and you know somebody will give you like a you know big prize if you can find some adversary that breaks g and you kind of notice that like oh if if you know if you do this construction if you take like n bits you plug them through g plug the you know the first this through g and then plug this through g and you collect all these bits then like this bit is always equal to this bit um how would you build an algorithm that like directly broke g i guess this is an example of like one thing we need to do like eventually we have to take like any method of breaking g prime and turn it into a method for breaking g but let's just assume it's a is like a simple method happens to work how to do it um the algorithm could just output the value in the last bit ignore all the other bits interesting let's see so let's say okay so this is your algorithm so like let's say n is um 50. maybe twenty twenty ten i just wanna like write something on the the screen here let's say uh okay let's say n is ten i don't know if this is gonna be a good idea so then you know you get a draw like you get like a draw of 11 bits and they look like you know one one one zero zero one zero one one zero zero one how many says one two three four five six seven eight nine ten eleven okay it looks like this you have to guess like hey did this is this just like 11 random bits or did somebody get this by like plugging 10 random bits into like g 10 uniformly random bits into g and it's fat out this and kind of the only thing we know is that like if you do this whole construction like the last this bit will always equal this bit so what was your suggestion uh the adversary could output the bit which always equals that last bit so wait oh the like you know let's say the code breaker or the prg breaker is you the person trying to win that prize just output this you're saying yeah so i guess the in order for you to win the prize like it must be the case that this bit is not 50 50 balance because like if it were truly if this were truly random 11 bits and this would just be like you know 50 50 coin flip but you're kind of postulating that it has to not be a 50 50 coin flip right so sure [Music] um it was the setup that the very last generators output bit um is always the same um no like maybe we should even imagine okay maybe to make things hopefully this will make things simpler let's imagine that this is not n to the 10 but it's two so uh let me try to draw this picture again then so imagine there's like some you know you know nist puts out this like you know standard like generator g that like stretches uh you know 10 bits to 11 bits okay these numbers are obviously too small but like somehow like nobody should be able to like if you see if you know if you plug 10 random bits into g and like look at the 11 bits that come out it should be like hard it doesn't really make sense if they're like 10 and 11 but like it should be hard to um to notice uh that these 11 bits are not uniformly random but now like let's suppose you're like really enterprising and you notice the following bizarre feature that like if you take this g and you put in um you know any bits here like x1 through xn okay what comes out what comes out is like y one y two dot dot y ten so i should say ten and then y eleven comes out and you notice that like okay if you do this and then you feed this back into g what comes out like z1 z2 z 10 z11 you notice that like weirdly like y 11 always equals z11 maybe we should like maybe show the speed this should always be like um ten thousand it should be like ten thousand one there's a little bit more uh should also use like large numbers that don't themselves involve zeros and ones because then it's like it's confusing like maybe this should be like stretches seven thousand okay never mind um i'll go back to 10 and 11. okay so uh yeah so you know you want to like you know get some like you know g breaking prize but you didn't quite break g like you don't obviously have an algorithm that looks at 11 bits and says like aha i can tell this is the output of g and not just like 11 random bits you have this like weirder thing where like you happen to notice this like fatal flaw seem potentially fatal flaw in like jesus system in the sense that if you like kind of do this weird thing with g like y 11 always equals z11 so uh yeah now what this is like this is some like somehow a security flaw in like g but like how do we exploit the security flaw i guess maybe something like you feed y one through y ten into g as part of the adversary and then if y eleven equals z eleven your output one or what is like y one through y ten so you have to actually i mean to really break it like how do you how do you oh maybe i understand what you're saying so to break it you have to write an algorithm like my crack where like i'm trying to crack this g it's gonna get i maybe yeah as you might want to call it like y one through y eleven and like you're trying to figure out like is this you know eleven random bits or is it like is it like g applied to like 10 random bits and you know we got to kind of guess so how what is this code going to be oh okay so you take y 1 through y 10 as a c like the al the adversary takes y 1 to 1 by 10 is the c to g and then if the output of that uh generate that generator is like z1 through z11 yeah 011 equals y11 your output one yeah very good yeah so you're saying i mean i think this is going to work so you're saying let z1 through z11 be the result of applying g to y1 through y10 and then you're saying like if you know z11 equals um y11 then you'll like return you know one that like means like i think this is this is you know pseudorandom like it was it was this case you know else maybe you'll return zero meaning like basically you're like i don't know it could be a fluke or like fine maybe you think it's like random but um all right good so let's see there are two things to think about so what do you actually what you actually need to prove is that like need to show that like probability that like my cracking algorithm when given you know 11 uniform bits outputs one is quite different from like it's not approximately equal to the probability that like my cracking algorithm will return one when it's given like g of 10 like random bits so one of the things that you think about i mean first thing to think about is like what is this value um letting you know [Music] it's going to be half yeah why because there's one half probability that you get a one or a zero at z11 um or it's half plus minus negligible then oh wait yeah what is it what is it exactly somebody else well you can also pipe out but like i hope everybody is thinking about this question i think it's exactly a half but why why do i think that well the probability of the first bit being one or zero is a half uh which first sorry like the y 11 equivalent yeah um but then the then you feed oh but then you feed y1 through y10 into the generator to get z11 yeah um and y1 through y10 is going to be this uniformly at random uh true so then z11 well are we assuming the generators of uh prng here or cryptographic uh no i guess i mean we kind of don't know whether it is or not i mean we're all the whole goal of this proof is actually to show that it isn't a good prg but right i mean i claim we can prove this about this no matter what maybe like what z11 is like z11 is like p uh probably one um like the probability that z11 is one is going to be like p um and probably at zero it's gonna be like one minus p maybe the condition based off of the weather uh something like that where if it's oh so then if the first bit is one then you have a probability p of the matching up um and that happens it's probably one half so one half p and then if the first pit is zero you have probably one minus p another bit being one minus p that happens probability half so then the p's cancel and you just get probability a half yeah i think i know what you just said so like i think you said the following we don't know too much about g but you're saying let's just focus on like what's the chance that z11 will be one when we plug in these ten random bits so like you said let p be like the probability that like z z11 equals one you know when y1 through y10 are uniformly random which they are in this like story we're thinking about and then you were like okay so we return 1 if z11 equals y11 so you might say like what is the probability that z11 equals y11 well first thing you need to ask is like are are z11 and y11 dependent or independent um i guess they're independent right because they're independent because like z11 was part of the z but like that came from just plugging g into like y 1 through y 10. um but you know we're hypothesizing here that y 1 through y 11 are like 11 independent random bits and so y 11 is independent of z 11. so then you're right like the probability they're equal is well there's like a p chance that z11 is one and like a half chance y 11 is one there's like a one minus p chance that z11 is zero and like a half chance that y11 is zero and this adds up to a half i might also just say like a little bit more casually like you know z11 is something it's either zero or one we don't really know a lot about it because you're not assuming too much about g but anyway whatever z11 is like y11 is independent of it so like the probability that a random coin flip equals like some fixed thing is 50 50. so okay so good so we believe that this quantity is a half so now we have to hope that like this quantity which is i don't know like i need a better color here this purple quantity is like not around a half at least it's like non-negligibly different from a half so okay what is the probability of this or yeah what is this quantity one let's see uh yeah yeah i mean that's that was like our whole assumption our whole game that like indeed like whenever you plug well wait a minute is that so um isn't it like yeah i guess so like if we call these x these 10 random bits we were kind of our you know the security flaw that we noticed if you take like any 10 bits x and you plug them into g that's what's happening here you plug them into g and you get y 1 through y11 that kind of goes up here and then you like save y11 and you plug y1 through y10 into g again which is what's happening here and you get z1 through z11 then yeah i like the security flaw that we're hypothesizing that z11 is always equal to y11 and so you're right this is exactly one and then we're like hey we broke it like we have this this uh my cracking algorithm has a property that you know if you give it um truly 11 random bits there's a 50 chance it outputs one but if you give it like output of the random generator there's 100 chance it outputs one so it's really great at noticing the difference yeah this is way simpler than i was trying to prove it directly and basically using i've proven that every single bit that's outputted is um an adversary can't see if it's like uh more than that should be different than the uniform distribution now the concatenation of all of those bits um is negligibly close to the uniform distribution that's definitely more yeah well i mean i think it may be yeah um it wasn't too it's not it's not i don't think it's too bad though because um you have a pretty short argument for like every one bit being close to the uniform distribution and then the concatenation can then be induction where you're only adding on like a constant times a constant times negligible and n like uh distance from the uniform distribution with every single concatenation yeah you have to be a little bit careful though because it's not like a very traditional notion of distance like where this uh you know computational and distinguishability is referring to like a notion of disturbance that's like a little bit non-traditional distance that's not traditional from a statistics point of view in the sense that like it's about the ability of you know polynomial time algorithms to distinguish the two strings right so yeah using the definition yeah like the proof would use the definition um non-distinguishability but yeah so for example let's see um [Music] yeah so for example maybe we'll do like one more test like okay maybe this is a message yeah can i ask like a high level kind of thing yeah go for it how how would we um so we're trying to show is that the that like this like output of g prime is um good in some sense right it's like close to it it's like indistinguishable by a um by a poly time algorithm to uh to be able to distinguish it from some uniform how does like what was the like the sketch of like what we were doing like how that's like being able to break g relate to like the final thing like what was the sequence of um steps in the proof that we were trying to show again uh yeah it's kind of like okay maybe i don't know if this will help but it's kind of like over here so this is what we're like asked to show originally if g is a good pseudo-random generator you know being vague here but um then g prime is a good pseudo-random generator and the contrapositive of that is that if like g prime is a bad certain random generator then g is a bad pseudo random generator cool and what is the definition of g prime is like a bad pseudo-random generator it means there exists some adversary that can break g prime right right yeah we have to conclude that there's like some adversary breaking g i see and and we're saying that like if g prime is a bad pseudo random generator like we gave an example for that being a bad pseudorun generator but like but like in general like all we need to do is assume that there exists some breaking g prime some adversary that can break g prime right like like the thing that we just did that was showing that you could come up with something like that potentially if you have a bad pseudorandom generator is that correct yeah so i mean what is the definition of being a good pseudo-random generator it's like saying you know g prime is good implies well it's by definition like for all you know adversaries a you know a fails to tell the difference between you know truly random bits and you know output of g so therefore if you just negate this g prime is like bad it's like there exists some like adversary a such that a like can like sort of tell the difference between like the random and output of g prime right so like when we're showing you know g prime is bad implies g is bad you know g prime being bad is like oh there exist some adversary that does some stuff and we need to show that g is bad so we need to show there exists some other adversary that does some thing involving g so yeah so basically we're we're saying that there exists some adversary so so we're taking as a postulate there exists some anniversary that can break g right prime oh vector break oh sorry sorry i i think i got it wrong okay yeah that makes sense um yeah so we're saying that there's a matter of certainly they can make g prime and now we want to see how we could use that to break g okay that makes sense yeah all right sorry i think i was just a little confused don't know where it is thanks so actually just there's one like more way to think about it let's let me just like set up and think about for one moment like one more example so suppose like say you know you're the person like playing around with g prime and say you noticed this the following about g prime oh sorry i have to get going but um yeah no worries yeah sure no problem uh i'll just like take two more minutes here so you notice the following thing about g prime like you noticed like okay um you know if you put in um [Music] like random bits here like you outputted this and call this a and then you fed these things back into g and you call this b and you fed this back into oops my drawing is going sloppier and sloppier like c and you fed this into uh g again and you call this like d like maybe uh maybe you notice like weirdly like 95 of the time and that's 95 of the time is respect to these random bits like i don't know a equals like the and of c and d and you're like that's weird like if i were you know if if these bits were truly random then like this would happen like um 50 of the time but like if i do this bizarre construction like g prime like it's actually happening 95 of this time well you would say to yourself man this is like gotta be like some sort of weird security flaw in g like g if g was like you know totally random function like it's supposed to be like this equation would be happening like uh 50 of the time but it's happening 95 of the time so it is like some kind of security flaw in g but now like you need to like how would you like explicitly demonstrate like a direct security flaw in g like not to say like oh if you know somebody gave you um again like imagine g maps like 10 bits just for clarity to 11 bits you had some like you know you want to like get your like my breaking algorithm that like gets in 11 bits y1 through y11 and it's supposed to guess like hey are these just like 11 random bits or are they actually the output of g on 10 random bits like what code could you write here or like exploiting this security well explaining this like weird security flaw if you will that would just like directly guess whether these were like 11 random bits or they were the output of g wouldn't this just be like kind of the same code as what was pretty much just just except with like the equals to with the second thing you just run the y1 through y11 maybe like four times through g and then you check to see if like or not three times and then you check like if a y11 is equal to like c and d or something like that yes but now we might see if it's like a little bit trickier well you're saying like what you're going to do is you're going to say like i'm going to do like i'll save y11 this is what i'm guessing you said i'll save y11 yeah and then yeah i mean i'll basically do this right like i'll let you know i'll do g of like y one through like y ten call the last bit and b and the rest z and then i'll do like g of z and call the last bit c and the rest w and then i'll do like g of w and call the last bit d and then like you know i'll like return you know whether or not like a equals like c and d or something right exactly so [Music] here's where it gets difficult i guess we would want to do the same thing here we want to show this probability is quite different from this probability and i guess we know this thing right this is like not the 95 that i was talking about what about this thing this is like the probability if i what if i take this code and i just give you 11 truly random bits here then you're like okay this will be a random bit actually i guess this is going to work so maybe maybe let me change this uh like crack to b equals c and d because i think that will make it essentially a little bit more interesting so you're like okay this will be a truly random bit but i guess anyway we're not using it um then we call g on y1 through y10 and we get back b now these were truly random but now b actually it's not necessarily random i mean g is like the last bit of this pseudo-random generator applied to 10 random bits so if b were truly random then you'd be maybe in good shape because you're like oh this is like a 50 50 bit so maybe this is like 50 50 but it's actually a bit more subtle because you don't know if this is like 50 50 or not yeah it seems like you've shifted the whole thing a little bit right like like if you're taking the output off um if you're if you're giving in y1 through y11 that's essentially given one round of g but if you were given likes if you have a truly uniform thing then it's basically giving x in some sense so basically the outputs you get here are like a b c d but like the output should be given um like if it was truly uniform would be like some something like that's pre that's like x11 and then a b and c instead in some sense so you're comparing like b is equal to equals c and d to something where it's like x 11. i mean there's no x 11 but theoretically an x 11 is equal to equal to like b and c in some sense right is that correct yeah i didn't fully give you saying but i think so actually to make it like even simpler like let's even you know i kept keep changing this security flaw let me just say let's change it to like b equals c let's say this part is not even happening um right then we don't really need this and we don't really need this um [Music] yeah so i guess the point is like it's no longer obvious that like i mean before we were like relying on like okay if y1 through y11 is truly random like the probability this you know my break returns one was a half but that's no longer so obvious like um so now kind of unreadable i guess um because we don't really know i mean let me let me just rewrite it like what we're doing is you know my break on like y1 through y11 just to be a bit more clear we're saying like um kind of z comma a is g of y 1 through y 10. and then um oh wait maybe you know maybe we're calling this b and then you know w comma c is like g of uh z and then we're returning whether or not b equals c right but if you know if these are around uniformly random then definitely these are uniformly random but you know now you know if you ask like what's the probability the b is one or zero well we called it you know before love called it like p for like the probability yeah um one but then we luckily we're kind of comparing it against a uniformly random bit but now we're not we're comparing against like an even more kind of convoluted bit like the c that we get out of doing this so you may say like who's to say that like why should this be a half like maybe by some like weird miracle it's like also 95 percent that's true so i guess like you know i better write about that but i guess the point is that like this i mean the final conclusion is actually this breaking algorithm might not do the job it might not be the thing that works but you'll be able to like develop like maybe one or two or three different like breaking algorithms like my break one my break two my break three and you'll be able to argue that like one of them has to work i see and maybe a little hint for this is like you you might be like sad if like b is you might be sad if like b is not like a 50 50 bit because i don't know that's going to like make it difficult to analyze this but suppose b isn't a 50 50 bit well then you've kind of discovered a different security flaw in g like apparently if you take 11 random bits throw away the last one plug them into g the last bit is not 50 50. well that would be like a different security break or security flaw in g and be like a different method of showing that g is broken cool so like either you could directly exploit that one or if that wouldn't work you know it doesn't break g then you've kind of gained some new information that this last bit is 50 50 and then well you have to start doing some more reasoning but at least these are some of the ideas that you'll you'll need cool yeah that's interesting so it seems like you have to like create like this like sequence of things like if if one is not sufficient to like prove something then you know that like like that that g is like broken in some way if g is broken in some way you have to kind of like narrow down which way it's broken in some sense yeah it's a little subtle i mean um i think if you like run through like this exact example you'll probably get the general idea i mean it's kind of a way to solve it like you know maybe solve this exact example and then if you still don't get it like maybe make up a slightly more complicated example and try to solve it but um i think like one general phenomenon as well is that you know if there's like five bits down here if your method of if your weird method of breaking g prime involves like five methods five output bits down here and they'll be like maybe like five different potential breaking algorithms and like at least one of them will have to be breaking ge i see so it's like if there's like some way that incorporates like how many ever possible like outputs you have then you have to like you have to be you can basically break it down into all of the possible ways that it could fail with all of them so it's like you get into the 10 like possible outputs yeah then potentially there might be like if it's like excessively complex which means that you have to like figure out some combination of all all n to the ten of them you can break it down into smaller and smaller breaking algorithms potentially yeah but like another feature to remember is like oh you kind of see like this picture is somehow irrelevant um you know one thing that's like working on your side is like in order to counter is it like a break you just have to make an algorithm that like slightly non-negligibly can tell the difference between uniformly random and like the output of the generator so even if you have like you know end of the 10 different methods and you're like well maybe like on average one of these methods can break do a good job of breaking things the idea is like that's still okay because like 1 over n to the 10 is like not negligible so like it's sufficiently large that it you know it's like counts by definition as like a good a good break uh i see that any sense cool yeah okay cool i guess i better wrap it up um but yeah uh if you have any more questions you can leave in a piazza and otherwise i'll see you tomorrow\", metadata={'source': '9dxXH5D8_Po'}),\n",
       " Document(page_content=\"one famous hardest assumption pitas not equal NP but there's more to life than just that so I want to tell you about some more opportunities you have I think we know it's super hard in complexity theory to prove any impossibility results for algorithms I mean we basically cannot do it at all I mean the one thing that we're great at as researchers is developing algorithms and the one thing we're bad at is proving our rules do not exist efficient algorithms for problems you don't exist so that's why we have assumptions so that we can make progress on the subject of negative results so uh let's get into it when you want to prove hardness results for a certain computational problem way it works this is usually a combination of two things you got a hardness assumption and a reduction and reductions are just algorithms that's kind of nice I mean it reduces in a way the task of proving hardness do you know making let's say one or a limited number of assumptions and then designing algorithms which is the thing that we're good at so in as I've you know mention probably before in this class a lot of complexity theory is actually just algorithm design let's take a look at this here's maybe the most famous or classic kind of example we may take as an assumption that you cannot solve the 3sat problem in polynomial time and as you all learn in an introductory si s Theory course you can deduce from this that let's say the Hamiltonian path problem can also not be solved in polynomial time this is basically just you know the NP completeness of Hamiltonian path but this deduction is by virtue of a reduction algorithm so just to remind you this reduction algorithm are that you designed to prove this result you know it takes us input a 3-cnf formula Phi and its output is a graph G so that's the algorithm and then the correctness property that you prove about the algorithm looks like this you prove that if Phi is the satisfiable 3-cnf formula then the GE output by your reduction has a Hamiltonian path and if Phi is unsatisfiable and G has no Hamiltonian path she use the lighter instead of what the highlighter the laser pointer right so this uh you know of course tells you that a few could solve a Hamiltonian path problem in polynomial time then by composing that algorithm with the reduction you would be able to solve 3sat in polynomial time if you're assuming it's impossible okay so that's an illustration of you know how you can start with a base assumption by designing a reduction algorithm get a new hardness results and in fact even this base assumption can self be thought of as the consequence of a reduction of this type namely you can take the weaker assumption that P does not equal NP which is to say that all the problems in NP there's at least one of them that's not solvable in polynomial time and the reduction that allows you to prove that this fiscal problem 3sat is not in polynomial time is this cook-levin theorem which itself is also just an algorithm for basically transforming a Turing machine into a circuit and thence to a 3sat 3-cnf formula okay so as I said the sort of the gold standard hardness assumption in CS theory since we cannot prove anything we have to do something is you know the assumption that P does not equal NP which as you all know thanks toach 11 theorem is equivalent to the assumption that 3sat doesn't have a polynomial time algorithm no this assumption has been around for I don't know 50 years and you know it's the baseline assumption that we're sort of always willing to make in order to get started and ideally you know if we you know can't prove anything well we can assume one thing and then try to deduce all the other facts about hardness about impossibility of designing efficient algorithms based on this one single assumption that would be the best thing we can do and sort of the thing that you're suggested is the thing that gets done in complexity theory in introductory courses but there are some downsides to just taking this one assumption about three side or about you know and P does not equal int does not equal P as your one and only assumption one is it's an assumption about worst-case hardness so it basically says you know for every candidate polynomial time algorithm there's at least one instance that it gets wrong but you know this is why it's called worst-case you know there may only be like one instance or like a some very small number of instance is a kinetic candidate algorithm could get wrong and what would be better is um if you could have like large classes of instances that were hard and in particular if you had like an efficient randomized algorithm for generating hard instances they might say like why do you say better like are we happier you know when we can solve problems why would we want hard problems well we sort of talked about this last time it's better for cryptography I mean given that there are seemingly are hard problems for efficient algorithms out there in the world we may as well put them to good use and base you know cool cryptographic primitives on them but for cryptography you want typically or you often need more than this worst-case hardness you need for cryptographic applications the ability to like efficiently generate hard puzzles like efficiently generate hard seeming instances of tasks so that's one downside to like just this P does not equal NP assumption and the other downside is it's a little bit vague on like how hard let's say this 3sat problem is I mean okay it conjectures that it's not in polynomial time I mean it is in 2 to the N time but you know it doesn't this conjecture myself does not say much about how hard 3sat is so you might be like ok it's not an opponent of time but could it be in n to the login time or it could it be in 2 to the Routan time or could it be in 1.3 to the end time in fact it is in 1.3 1 to the end time that's uh result of hurdle from 2011 and so you see that already something kind of interesting is going on I mean if you care about small values of n you know 1028 30 then you know these things can make a difference so these are the kinds of things I won't talk about in this lecture like maybe making different assumptions that allow us to get richer hardness results more precise hardness results and hardness results that are better for applications okay\", metadata={'source': 'bYfQrITGyUs'}),\n",
       " Document(page_content=\"let me just briefly remind you of like you know this story of like applications and hardness and worst-case hardness from last time when we talked about cryptography which is sort of exactly on this subject so I won't dwell on it too much but remember there a one canonical assumption that you could make is this assumption that one-way functions exist this is stronger than P does not equal NP and if you make that assumption then by reductions you can get to this world of mini crypt where a lot of cryptographic primitives exist like symmetric key encryption digital signatures and so forth we also talked that by like making another minimal assumptions such as this very nice one called hardness of the learning with errors problem you can get to crypto mania this is a world where are lots and lots of cryptographic things exist like public key encryption fully homomorphic encryption all sorts of cool stuff and one nice thing we saw there is you can actually it's now that you can make an assumption about the worst-case hardness of some problem involving lattices which implies this lwe so it's kind of like a nice example of making like a more minimal assumption and still getting lots of cool effectively hardness results which you can then transform into cryptographic algorithms elissa fear of cryptography before we leave it i want to manage and one more cool assumption which is extremely interesting and there's some people have a lot of a lot of current research in cryptography it's the assumption that something called indistinguishability obfuscation or IO exists and I really won't say much more about that except it's well it's to say that there's some cryptographic procedure that kind of obfuscates programs and has the property that like if you give it to programs with equal functionality their occupations will be computationally hard to distinguish well let me not get more into it but like if you make that assumption and you assume that you know basically on P does not equal P or NP is not contained in DP P then this has been the subject of research over the last five years you get to a world it's like even better than crypto mania that has been dubbed obvious Autopia where you can get even more cool stuff that we didn't know how to get with just let's say learning with errors like deniable encryption and non-interactive key exchange without trusted setup blah blah blah all sorts of crypto stuff well we downside with it is you know one always tries to make minimal assumptions or assumptions that you know we assume that a problem is hard to derive consequences you better also put in the effort to try to disprove your assumption to try to find algorithms for it and this is currently a highly questionable assumption so it's now not it holds if you have I mean the best thing we know is that it holds if you have three things if lwe is true which is we're very happy with that assumption and if there's like soon around a cryptographic pseudo-random generators with a certain locality property which I personally think is kind of fine although it's considered less standard and then you also need a third cryptographic assumption unlike the existence of K linear maps for K greater than equal to three and this is considered I don't say dubious but like I don't think there are a lot of people that definitely believe that this assumption is true so extremely exciting times actually to figure out like if this assumption is valid or not because if it is oh boy you have so much great cryptography okay so let me move on from that and actually this does give me a nice lead-in to the next thing I want to talk about so if you remember learning with errors what was this problem it was assumption about the hardest hardness of solving a certain task the task being sort of solving noisy linear equations mod a large number q so you know we know if you solve you can solve a sibling your system modulo prime power or prime no problem using Gaussian elimination but if there's a little bit of error in the right hand sides then it seems to make it very hard what I want to leave out now another assumption called LP n which stands for learning parity is with noise is actually an older assumption than lwe a little bit motivated lwe which is like the mod 2 version of learning with errors so it's a bit easier to stay tuned and understand although it doesn't have such rich consequences as it turns out as compared with learning with errors so let's talk about what this learning parodies with noise problem / assumption okay so it's all here on one slide so the learning parody with noise problem with parameters N and also a constant epsilon which is noise rate that's between 0 and 1/2 it goes like this you know some person nature or some person chooses a secret string s of n bits uniformly at random and then the if sometimes the algorithms task is to learn what this secret and bit string is but you know the algorithm cannot just say what is the N bit string what it gets to do is it can request whenever it wants a noisy linear equation about s ok it's quite similar to learning living with errors and what it you know says ok please tell me a noisy linear equation about s it gets back something that looks like this so this is a linear equation and I didn't write it but it's considered mod 2 and so it looks like a linear equation on variables x equals some number B which is 0 or 1 mod 2 okay and you know the secret holder when is gets a request for a noise in the new equation picks all these coefficients a 1 through a n uniformly at random those are just bits 0 or 1 and the thing is it does not and it gives also back B but it does not give exactly the true answer the true answer would be like a 1 s 1 plus a 2 s 2 plus a n plus SN it gives that back with probability 1 minus Epsilon and with probability epsilon it flips the answer ok so basically you know the learning algorithm is trying to figure out s gets back an equation that's like a correct equation with probability 1 minus Epsilon and it's like a wrong equation with probability Epsilon remember these a's are zeros and ones so it's like a sum of a subset of the bits on the left-hand side okay in the algorithms task is the output s you know with high probability let's say and this origin has been thought about for a while in a variety of contexts learning theory coding theory and people couldn't solve it I'm particularly like if you know epsilon is some constant but even a small one like point zero zero zero one people did not know any polynomial time algorithm for solving this problem and so that's the LPM assumption that there is no polynomial time algorithm for it so again the sum order to see that all these assumptions I'm going to be talking about today are like strictly stronger than P does not equal NP so they're making stronger assumptions but hopefully from using them you can gain stronger consequences there's a question is there any insight to be gained by saying that any decision problem is an instance of IO for instance any 3-cnf that is unsatisfiable is the same as the constant false but obfuscated this is observation can be used to show that like if you get if you have if I'm not mistaken that if you have indistinguishability obfuscation and NP does not equal P and one-way functions exist but let me think a little bit more about what you've said one aspect of indistinguishability out infestation one reason I didn't get into it too much besides all the reasons of time is the definition was kind of subtle [Music] let me try to think about it just address your question online because the offline because the definition is a little bit subtle but for example the definition doesn't really require that you'd like don't learn something about the original program by looking at the obfuscated version of the program it just implies that you know two programs with the same functionality when you want escaping and the same complexity when you obfuscate them they cannot be distinguished at the same circuit size so one example of an inefficient algorithm that achieves this indistinguishability obfuscation is given a circuit you find the lexicographically first other circuit that has the same size and the same input-output functionality so this does the i/o task but it's not efficient okay let me get back to learning parodies with noise now and we'll talk about that iö later oh there's another question is there any known slow algorithm for LP n yes I will get to that on just a moment so let me just say what is this LP an assumption good for ah it's not hard to show that you can get one-way functions from it so that's nice and you get to this mini crypts and you in fact I was very it gives you very efficient symmetric key encryption and it's sort of debatable whether you can get public key encryption from this assumption like we know you can from learning with errors if you're willing to still believe the assumption when epsilon is very small like 1 over root n then elect no vich in 2003 showed that you couldn't get public key encryption but people don't know how they feel about this like extremely low error rates but makes the problem certainly potentially a lot easier so somewhere like between like mini crypt and you know learning with errors so to answer the question fastest known our limb was developed right here at CMU in 2003 by Alfred Blum Adam Kalai and sermon and it's a very beautiful algorithm actually sometimes called several Gaussian elimination and it runs in time two to the N over log n which looks um not so impressive because you know it is true that was like 2 to the N time you can solve the problem this is like saving a one of our log in in the exponent but it's the fastest algorithm we know and for cryptographic person purposes is actually also interesting to look at a different a slightly different version of the question so this algorithm uses about this much computation time and it requests this many two to the N over log n many noisy linear equations for cryptography purposes sometimes you want to ask a slightly subtler question what if like I insist that you can only ask for poly and many noisy linear equations which is actually sufficient for you to solve the problem in to to the end time information serially then what's the fastest algorithm you can do and there it's known you can do 2 to the n over log log in time so really just barely faster than a trivial but this one you can do with even like a near linear number of samples so this is all for constant epsilon so you know people have been think about this for a while and these are the best algorithm so you might argue that this is a pretty safe assumption you know maybe you can base your cryptography on it as well\", metadata={'source': '6MbuyAIZ2_w'}),\n",
       " Document(page_content=\"I also want to mention an alternate version of it or a variation of it because this will lead us into another important hardness assumption which is the sparse ltn problem so in the sparse LP n problem you fix a very small constant K like maybe three or five and now when the learning algorithm asks for a noise equation you don't pick you know the the secret holder does not pick all a 1 through a and uniformly at random which would have the property that about half of the A's would be one and half the A's would be zero instead they just picked exactly K AI is to be 1 okay so basically you know if K is 3 and they pick three numbers between a 1 and n and they give you a noisy equation let's call those numbers i1 i2 i3 those are chosen randomly between 1 and n and then you learn you know you get an equation that looks like x i1 + x i2 + x i3 equals B mod 2 and you're given the correct right-hand side with probability 1 minus epsilon and the wrong right hand side was probability Epsilon so this definitely makes your task easier because you're getting like a lot more intuitively you're getting a lot more information about the secret string with each noisy equation and in fact let me make that even clearer let's say you're a polynomial time algorithm and we let you get em noisy equations that look like this these sparse equations first of all imagine a meze like noticeably bigger than n to the K so then you're in mind for this discussion fixed K to be 3 and say you're calling real time algorithm and you're allowed to take like way more than n cubed algorithm AGGA cubes equations like n to the 4th equations well there's only n to the K or so many possible left-hand sides like n choose K left-hand sides and choose 3 left-hand side so if you're a lot to take you know way more than n cubed samples you'll get like you'll see the same left-hand side like many many times like you'll see like x1 plus x2 plus x3 equals 1 or 0 like lots and lots of times and you'll be able to figure out what is the correct right hand side because you know like 1 minus Epsilon of the time when you see X 1 plus X 2 plus X 3 equals something you know you'll see the same value for that something be it zero or one and only epsilon at the time when you see the opposite value so you'll basically be able to get rid of the noise yourself and get back to the case of like non noisy equations with high probability and then you can solve them with Gaussian elimination so this shows that you can you can solve the problem if you're given sufficiently large polynomial number of equations like n to the K in fact this one is not as easy to see but it's known that you can even do this if you're allowed to take like n to the K over two samples okay this is you know not as easy this uses a semi definite programming algorithm um kind of based on work of Fagin Ofek and aqua mom so there's that but this is like the best thing that's known and so for example you know if you could fix K to be three and you might say well I'm not going to give you I'm not gonna let you take n to the 1.5 equation it's noisy equations I'm only gonna give you Big O of n equations then it could be hard so this is a possible assumption that if you're a polynomial time algorithm and I'm only giving you like order n equations which is information theoretically enough for you to solve the problem in exponential time as it turns out you can assume that there's no polynomial time algorithm that can solve this task even when K is 3 and in fact uh this test could actually even be way harder for example we don't know how to solve this task in time better than 2 to the N over log n like 2 to the little o of n over log N and uh we don't even know how to solve this problem if like you don't even have to find s you just have to sort of tell the difference like am I really getting like you know ninety-nine point nine nine percent accurate equations about a fixed string s or is you know the person just feeding me pure junk or by pure junk I mean imagine you take epsilon to be a half so that means like the right-hand side is just randomized so it's like you get completely meaningless equations so it's like actually an easier task could just tell the difference between are you getting completely meaningless equations are you actually getting like 99.99% accurate equations even this is not known to be doable and this is pretty cool because it's like this is an example of like a problem where it's really easy like in three lines of computer code you can generate hard seaming puzzles like you just write code that like picks a random string s it picks M equals order n three tuples of indices and you know it writes down the Associated true equation then it flips these right hand side with probably 1% and it produces a bunch of like noisy equations and then you know you give it off to an algorithm and say here's like you know 100 n noisy equations about a random string s each one involves only three variables in the left hand side please try to find s nobody knows how to do this in less less than 2 to the N over login time and you know it's like a five line piece of code for generating like a cool puzzle in fact a cool puzzle where you the code generator or the algorithm that generates this puzzle it knows the secret it picks the secret s itself so that's a very useful primitive and if this will let me transition to talking about worst case hard problems for a while so in fact the worst case version of this farce LP n with K equals three he's actually provably known to be hard well as always in this world we don't know how to prove anything as hard so when I say like provably hard I mean assuming our most basic fifty-year-old assumption and then we always take P does not equal NP so let me transition to talking about that but feel free to add any questions in the chat if you have them right so what's this worst-case version the theorem a problem so in 1999 host approved the following quite famous theorem it's about the NP hardness of a certain CSP task and the CSP is like the 3x or CSP and it's exactly this sort of thing that we've been talking about like there n unknown you know values x1 through xn and you're supposed to assign the value 0 1 mod 2 and each constraint is like a linear equation mod 2 that involves exactly 3 of them that's exactly what we were talking about on the last hand side last time but before we talked about you know picking the solution at random and like the right hand side would be like correct but with some noise and here's just a worst case problem and you're trying to find the solution and the assignment to the X is that satisfies as many of the equations as you can and hostess theorem is the following even if I give you an instance basically it's hard to tell the difference between like a 99% satisfiable Sol system and a merely 51% satisfiable system or like even if I give you a system where there is like a solution that satisfies 99% of the equations it's hard to find a solution satisfying 51% of the equations and that's it'd be hard in the worst case so it's sort of you know definitely hard assuming P does not equal NP and let me just remind you what that looks like I mean host on theorem can be viewed I mean if you put together the entire proof of hosted serum like maybe it's 200 pages but well 100 at the pages it's a very long and P completeness reduction and B hardness reduction like ultimately he creates a polynomial time reduction R takes his input like a CNF 3-cnf formula Phi and it outputs you know a system of equations I and he proves a zero in that like okay this reduction has the property that a Phi is satisfiable then there will be a solution to these equations that satisfies 99% of them and if I is unsatisfiable then every solution satisfies at most you know 51 percent of them just to remind you why there's you know two quantities like ninety nine percent and fifty one percent are relevant you know if you're given a system where a hundred percent of the equations can be satisfied by some assignment then there's a simple polynomial time algorithm that finds such a solution that's solving a system with satisfiable equations it's Gaussian elimination on the other hand there's always like a super simple algorithm that will satisfy 50% of the equations particular you can just look at the right-hand sides and if there's more ones than zeroes and use the assignment that sets all the variables to one and then whenever you have a right-hand side of one you satisfy the assignment and if there's more zeros and ones set all the variables to zero and you satisfy every equation with a zero on the right-hand side so one of these is at least 50% of the equations okay so that's how stars theorem and as I mentioned you know if you really dig into this reduction it's extremely long I mean a built on I don't know something like eight years worth of effort and complexity theory so it really it divides into like several steps like there's a reduction from 3sat to the problem of like slight inapproximability for three SATs we talked about this in lecture twenty if you want to go back and check that out and this first reduction is called the PCP theorem it's a famous theorem and in fact this we'll talk about it in the final lecture and then there's like another famous theorem called the parallel repetition theorem proved by Roz which gets you to hardness for a problem called label cover which we'll talk about later in this class and then Howe starts contribution was this last piece involves like a gadget reduction involves some very cool analysis of wooly functions and so this is you know one of the longer NP hardness results you'll ever hear about okay so you know even though there's like a tremendously long story from three side reducing all the way to post Don's theorem you know for like a practitioner if you're just a person that you know cares about understanding future problems it's really great to just know this theorem is a black box I mean forget how it's proven but just remember that this problem is np-hard because it's a great starting point for further reductions I mean you put that in your pocket this problem is np-hard and you put you know NP hardness reductions on top of it and this is a turns out to be like the great problem for deducing further results about hardness of approximately solving optimization problems for example here's just like three out of a very well a bunch out of a very huge number of results that are known of this flavor so starting from host less theorem and adding NP hardness reductions on top of it you can get the best-known and B hardness of approximation factors for max cut for two sat for the traveling salesperson problem in the metric case it's known that it's and be hard to approximate it to 1 plus 1 over 122 factor she's not too impressive but it's the best thing that's known you can see the best-known np-hardness approximation for a dense case subgraph non-uniform sparse the Scott approximation version is the graph isomorphism problem yeah so this it's it's sort of not really like an assumption because we have proven it well it's it's equivalent to the assumption that P does not equal NP but it's good to mentally think of it this way is like okay this is like a black box I can put in my pocket and try to start driving future hardness results from and just to connect it up to the thing we were talking about before this farce LPM assumption what's cool is the sparse LP an assumption so the host lines theorem of this NP hardness theorem it's like a worst-case result it just shows that you know for every algorithm probably one that purports to be a blue you know tell the difference between 99% and 51% satisfiable instances of this 3x rcsb it's gonna get it wrong on at least one instance this far self-cam assumption gives you like an efficient way like a really efficient simple way to generate hard seaming instances of this problem that like seemingly foil all the you know polynomial time algorithms that we know because it generates you know 1 minus epsilon satisfiable instances of this 3x or CSP where we don't know any polynomial time algorithm that can you know even satisfy 51% of the equations so therefore actually you know what you can do if you like is you can use like the sparse LP n method to randomly generate hard instances of 3x or and then you can feed those instances through these polynomial time reductions and thereby I get polynomial time algorithms that efficiently generate random seemingly hard instances of you know TSP and graph isomorphism and all these other problems now a downside is these are not especially natural instances that you'll get out but you know if your friend has like you know an algorithm that they claim is real good at solving you know approximately solving max cut problems or TSP like in some ways is our best known way to test them spy like you know generating sparse opium instances and passing them through these poly Tom reductions okay now I want to tell you about a related topic quite related which is instead of like random 3x or CSPs we talk about like the more famous CSP three-set now let's talk to you about what's up with 3sat what if you want to try this you know do the same story with 3sat like if I wanted to say okay we know three sounds and be hard but like please write me a computer program that efficiently generates a hard seeming instance of 3sat what would you do well one thing you can do is just like choose a purely random instance and so let me tell you about that so let's say you fix some number of variables and that you're gonna have and you still decide how many clauses you have to have like each Clause is gonna be a random it's gonna be the or of a random collection of three variables or maybe I'll randomly negate them as well to get literals but how many clauses will you choose well let's let this be a parameter let's let C be a parameter and say that you use C times n clauses so before we get to the question of like well how computationally hard will it be to solve this random 3-cnf for satisfiability let's first estimate a simpler or a more primary question which is um well is this instance gonna be satisfiable or unsatisfiable or is it gonna be 50/50 or what like what will the right answer be so ah there's a fact that for this problem there's a so called sharp threshold behavior with respect to the sort of the number of clauses you choose in particular there is a certain constant alpha 3 which numerically is about 4.2 667 but which has like a formal definition it's like it's a very painful formal definition it's like some smallest root of something involving a differential equation and that takes half a page to write down but there is some special number alpha 3 such that the following is true if your clause density C is bigger than alpha 3 then when you choose a random three set formula with you know and variables and C times and clauses it's gonna be unsatisfied probability okay of course the you know the more closes you have the more constrained you know the instance the more likely it is to be unsatisfiable and conversely if C is less than this magic number then with high probability your instance will be satisfiable okay there's a little asterisk here I put it here because this is not a theorem but it is definitely true people incredibly enough in the field of statistical physics I've been like thinking about like random instances of csps for a really long time they have an incredibly thorough understanding of them which is only very hard to analyze mathematically and only lately have been mathematicians been able to make progress on it but so they know this fact to be true and it is true and has recently sort of recently been proven to be true like in a 100 page paper but only for sufficiently large values of three by which I mean for random KSAT there's some number alpha K which comes out of some weird differential equation such that this theorem is true but only for sufficiently large K but it's surely also true for K being three okay so ah that's just the sort of story unlike okay if you okay fix seem to be something and now you generate random 3sat instances like this this sort of will tell you are they gonna be unsatisfiable or they're gonna be satisfiable and it turns on it see as big or small and you know if you're gonna be generating satisfiable instances then the appropriate puzzle to give your friend or hey an algorithm is try to find a satisfying assignment and there are actually reasonably good algorithms at this you know it's easier the fewer constraints there are so the smaller C is but for 3sat like we do know efficient satisfying algorithms that with high probability find satisfying assignments if C is like as large as 3.5 which is not as large as 4.2 but okay it's something on the other hand if you choose like a big value of C so that the random 3:7 sense is likely to be unsatisfiable then the appropriate puzzle is to ask the algorithm to find a certificate of unsatisfiability or perhaps it should write down an LP relaxation for like the max 3sat problem on this instance and it should solve the LP and you know maybe the LP value will be 0.99 and that will be a certificate that it's actually unsatisfiable this seems to be a lot harder so it's known that if you make a super constrained instance where C is at least like root n so we have n variables and n to the 1.5 clauses then there are efficient algorithms known that will find certificates of unsatisfiability and interestingly enough this uses spectral graph theory these algorithms well this is the best thing that's known so if I give you n variables random 3sat instance with n variables and n to the 1.1 clauses even though it's going to be unsatisfied with very high probability we don't know any efficient algorithm that will find a certificate for that clock and so one can take this as potentially a hardness assumption because people have been working on statins for a long time so it's sort of reasonably believable if nobody can solve some problem about random 3sat that it might actually be genuinely hard then you could try to use it for cryptography purposes or some other purposes so this assumption I was codified by refi guy around 2000 and it's called phagocytosis or Fargas are 3sat hypothesis and it says um just receiving any large constant like 100 or 1000 if I choose a random 3sat instance with n variables and C times n clauses there's no polynomial time algorithm that with high probability finds certificates of unsatisfiability okay so focus hypothesis is up there and we just tell you about it this one of my favorite hardest hypotheses by the way it's a great one to keep in mind I mean if you can't prove your problem it's hard assuming P designs on P try to assume faggus hypothesis so it's actually quite similar to learning parries with noise i'm what have a chance to say much more about the connections but they're quite deep connections and it's basically like a little bit stronger than the learning parodies but noise assumption one thing about it is it's know to apply many strong in approximability results that we already know how to prove assuming P does not equal NP but in an easy way so there are many consequences of like host odds theorem about hardness of Max 3x or that are very well proving them is a little bit hard and like you have to rely on host does theorem which has this like 100 page proof but if you assume focus are set are 3sat hypothesis then you can prove a lot of these theorems like in like two paragraphs so that's cool you know the conjecture only says that polynomial time algorithms can't do the job but you might conjecture it's possible to conjecture that even any sub exponential time algorithm must also fail we don't know any like tooth of little o of n time algorithm that solves this task either and it's very powerful algorithmic framework that we talked about in lecture which one do you maybe the sum of squares SDP hierarchy you can actually prove that it cannot you know solve Phi goes three set random tree set problem in anything less than like two to the big Omega of n time that's pretty good evidence and you can generalize this also to like random case at where we know that if I give you random case at instance it was with like n variables and n to the K over 2 random clauses it's know that you can refute satisfiability like output a certificate of in satisfiability with high probability in that case but if you make them a bit smaller like in to the point one k1 might still hypothesis that no poly-time algorithm can solve random case that in this case and this stronger assumption is also known to have like cool consequences especially for hardness of learning so Danielli and cello schwartz showed that under this assumption you can prove that it's hard to learn DNF formulas pack learn them which was pretty notable open problem they also show that it's hard to learn how spaces like linear classifiers with agnostic noise and a hard noise model and so forth so these are examples of like hardness results about learning that we don't not approve just assuming P does not equal NP but we do know how to prove now if you make Phi gus' hypothesis\", metadata={'source': 'Ehk0IRIFTck'}),\n",
       " Document(page_content=\"so now I want to transition into talking more about hardness of sat like KSAT but not for random instances just in the worst case so I want to explore a little bit or tell you a little bit about what is known about solving KSAT are not random instances but potentially any instance so okay let's say you have a case a door CNF side like you're given a CNF formula and there's actually two parameters there and the number of variables and M the number of clauses but basically n is the most important parameter and you should mentally assume that M the number of clauses is polynomial n' and the number of variables well there's definitely the brute-force algorithm that runs in time basically two to the n - 2 then x poly M you just enumerate all possible 2 to the interest assignments and for each one check if it satisfies the given CNF formula so can you do better uh well let's first talk about 3sat I put a lot of information up on this slide but let's go through it slowly so let's first talk about the three side as I mentioned before you can do better than 2 to the N time for 3sat in fact you can do you know solve 3sat instances in 1.3 1 to the power of n time which is interesting and this algorithm is kind of complicated but there is none other than that gets 1 point 3 3 3 to the power of n time which is only very marginally worse and the algorithm is so simple I can like fully state it to you here and not only do I state the algorithm here the proof is actually not too hard I mean can do it in half an hour or so depending on your level of sophistication that's called the walks out algorithm do discerning from 1999 and here's how it goes you give me two three CNF formula pick a random assignment and check to see if it's a viable maybe it is well the chance of that could be like 2 to the minus n but ok if it's not satisfiable but when you do is just take any clause that's unsatisfied so clause is an or of three literals and you know this current assignment must be making all three of the literals false because you know clause and or is satisfied if at least one literal is made true so uh just take any of the variables appearing in that clause and flip its value let's truth assignment and that'll actually make that clause now satisfied it might make some other clauses satisfied and it might make some other clauses unsatisfied but that's it just take any closets unsatisfied flip a random variable in it and um do this step a bunch of time to do this step like order end times and like as you flip variables you know find on set clause pick a variable in it flip it find it on Cyclops pick a variable in a flip it do that like order n times and maybe you'll find a satisfying assignment if so great if not but you kinda need to save yourself you know I picked like a bad starting random assignment let me just go all the way back to step one find like a totally new start with a totally new random assignment and then do this like local search kind of thing for like order n steps and you do that routine like 4/3 to the end times and yeah the analysis shows that this algorithm you know if the instance is unsatisfiable and we'll never find a satisfying assignment so it'll eventually give up and the analysis shows that if this if you given any satisfiable 3-cnf then with high probability you'll find a satisfying assignment with this algorithm after like oh till the 4/3 the n steps in fact there's a nice generalization of this algorithm to the case of KSAT whoops case that where the running time becomes like 2 minus 2 over K to the power of n so it's 1 point 5 to the end for 4 set and it's 1 point 6 to the N for five shot and so forth so it's large it gets close to 2 to the N time which is trivial but for every fixed K it's better than 2 to the N time okay so Oh 3sat we know is solvable in better than 2 to the N time it's even solvable in 1.3 to the end time uh but you know can you solve it a lot better can you solve it in 2 to the root end time maybe well we don't know any algorithm like that we don't know any algorithm that solves it in 2 to the end to the 0.99 time and ok after a lot of work people couldn't do it and so in Palo Alto in fitori in 1999 made a mess up ssin called the eth exponential time hypothesis that simply says you know 3sat takes at least two to the point zero zero zero one time to end time or marketly you know there exist some universal constant delta such that 3sat needs time to to the delta n for all sufficiently large ok so this is like a direct strengthening of P does not equal NP right Peterson like when P is equivalent to saying that 3 set cannot be solved by any polynomial time algorithm this algorithms of the ETH are saying three set cannot even be solved by any 2 to the little o of n time algorithm okay so there's ETH and again what's cool about it is you know you're making a stronger assumption than the P does not equal NP but if you make it you can derive some stronger conclusions that we don't know how to conclude merely assuming P does not equal NP for example let me give you an interesting example back in the 70s like they're really into what are the consequences of P does not equal NP and there's a whole theory of NP completeness developed and like many many many theorem problems were proved to be np-complete and one such np-complete problem shown to be and be complete in 1976 is the planar Hamiltonian path problem the version of Hamiltonian path where like the graph is planar so it's like Hamiltonian path but it's easier because like it's definitely gonna be on a planar graph what they showed it's still in P hard ok and how do you do that well they came up with some smart reduction from 3sat so polynomial time and it outputs a planar graph and you know if the three sound instance is satisfiable the planar graph has a Hamiltonian path if the three side instances unsatisfiable the planar graph doesn't have a Hamiltonian path now I'm gonna ask you to fill in a blank here in a moment there reduction has the following property on input of size n the reduction for producing graph runs in cubic time and it produces a graph whose size like number of vertices and edges is N squared so under the assumption of ETH you can conclude that planar Hamiltonian path requires at least a certain amount of time on size capital n inputs and I'm asking you what is that certain amount of time you know that 3 the SOT requires this much time and you know this reduction how much time does planar Hamiltonian path require yes I got the correct answer in the chat thank you very much it's 2 to the Omega square root capital n time and just to talk you through it you know imagine conversely that you could solve planar Hamiltonian paths in better than 2 to the square root Capital n time imagine you could solve it this is my terrible drawing in 2 to the little o of root n then you would contradict ETH you would solve 3sat in better than exponential time how give it a three side instance stage one you would run this reduction the fact that this is n cube time is no problem at all it's hardly relevant because you're shooting to solve the 3sat instance in like 2 to the little o of n times n cube time is nothing special but you do get a planar graph of size little N squared and so then your hypothetical planar Hamiltonian path algorithm runs in time that's better than 2 to the root Capital n so it's running in time 2 to the little o of N and that contradicts ETH so you see what was actually quite important here is the how efficient your reduction was and this is why I like this sport of proving harness results is actually really about algorithm trying to find really efficient production algorithms and what really mattered here I'll switch the running time of the algorithm about how much the size blow up was okay and what's also interesting is it's a fact that you can solve planar Hamiltonian paths in time to to the order route so this result is kind of tight um in general uh each H is good for showing that you know your favorite problem your favorite np-complete problem requires a large exponential amount of time so we saw a planar Hamiltonian path that show that required to to the root and time and for mostly classic MP complete problems that you learn about and like a first course on NP completeness you should like you know three coloring is np-hard and Hamiltonian path an independent set all these classic reductions tend to have only linear size blow-up that's why I chose this like weird one for my example planar Hamiltonian paths and that means that for all these I mean have to take my word for it you have to inspect these reductions but it's true and it means that if you assume ETA H you conclude that all of these kind of classics like three coloring and independent set also require sort of full exponential time to the Omega of n there's a question that says so under ETH there cannot be a linear size blow-up reduction to planar Hamiltonian path absolutely correct so like if you assume ETH then there does not exist a algorithm for Hamiltonian a reduction from 3sat to Hamiltonian path that takes instances of size and two instances of size n to the 1.9 and that's kind of cool actually because we'll get to this in a moment if you think about that that itself is a non-existence result for algorithms okay sort of like a weird problem the problem of reducing 3sat to have planar Hamiltonian path but it shows that it cannot be done in a very efficient way like with size blow up into the 1.9 in particular it cannot be done in time n to the 1.9 because if you only have n to the 1.9 time you only have time to write out a planar graph of size n to the 1.9 so as we'll see in a moment these kinds of assumptions can also be used to show hardness result in P yeah let me also mention that like uh it's maybe I should mention earlier but like it's a common misconception among some people that all np-hard problems should require 2 to the Omega n time that's not true example planar Hamiltonian path can be done in 2 to the root n time and it's empty hard to decide if a graph has a independent set of size n to the point O one that's true but you can solve that problem in time like basically exponential in n to the point O one by trying all subsets of size n to the 0.01 so ETH basically shows that if you assume in no polynomial time there's sorry no np-hard problem can have a running time that's better than 2 to the n to the epsilon so you can't have like an end to the log n time algorithm for an NP complete problem assuming ETH but there's nothing wrong with a 2 to the N to the point oh one time algorithm there's a question I think regarding this discussion of potential algorithms for reducing 3sat to planar Hamiltonian paths the question is what could there be an exponential time reduction yes maybe so you do have to be I said this this past week to the reduction usually doesn't play a big role but you know if this reduction took to to the end time then you know you wouldn't be able to make any of these conclusions because then the reduction time would swamp you know your your algorithm time for 3sat in fact there is a 2 to the N time reduction the reduction can just like solve 3sat itself into the end time and then basing based on whether the instance was satisfiable or unsatisfiable you'll output some trivial constant size planar graph which either does or does not have a Hamiltonian path so let me just uh say a few known facts eth and my trying to get here is you should you know put eth in your pocket too and if you ever trying to show some problem it's hard to solve in a certain amount of time maybe consider starting from eth so there is an algorithm for vertex cover if you want to decide if a graph has a vertex cover of size k most naive algorithm would run in time like n to the K what a smart algorithm exists that runs in time like 1.3 to the K times polygon but it's known using a th that you cannot get a sub exponential in K times Polyana or them so the K clique problem there's definitely an end to that order K time algorithm you check all vertex set sets of size K and see if they're cleek unlike with vertex cover you can't seemingly do better than that and ETH shows that there's no algorithm that's like even 2 to the 2 to the K times and to the square root K that's impossible under ETH that's a theorem I mean it's not like obvious or anything or for one more example when we studied tree width we saw that there are a lot of results like this like if you want to have max cut you know they're PR problems you can't do it but on bounded tree with graphs you can do it in polynomial time and these things these algorithms run in something like 2 to the tree width times polygon and under ETH you can show that you cannot get like a sub-exponential entry width times polygon so it really kind of lays that out like what is possible or not by efficient algorithms ok so let me show you a variation on this story so I mentioned that for K sad which is a harder problem there was this walks out algorithm that runs in time like 2 minus 2 over K to the end time some number that slightly less than 2 and gets more and more slightly as K gets bigger to the power of n and the fastest nomogram is a bit better than this it's kind of crazy it runs in time to to a certain constant times n where that constant is very close to 1 but it's 1 minus PI squared over 6 K Sosuke gets bigger because this also goes to the to to the end time but for every fixed K it's like a little bit better than to to the end time and there's another assumption called strong ETH or Seth made later by a pullout sofa tyrion Zayn which says that you know this is the best thing so it says that for any number Delta there is some large enough K such that case that requires time to to the one minus Delta end so you know it's basically saying as K gets bigger and bigger you really need an amount of time which is converging to two to the end the brute-force algorithm um we also comment it's not obvious actually that strong ETH is stronger than ETH but it is a theorem that each strong ETH implies ETH so it is well named it is stronger and um you know how if I said here yeah a slight leave variant statement is basically morally the same as strong ETH it's just to say that like the CNF set problem CNF that problem where you're not given any it's not case sad there's no a priori upper bound and the Clausewitz it's just you're given a CNF is it satisfiable or not the set is basically equivalent to saying that that problem requires time two to the N times poly in the number of clauses what is the formal definition and its main uses this topic that we touched on a little bit earlier which is called fine-grained complexity which is like a kind of a new field of algorithms in complexity that's been developed over the last five or ten years particularly by a couple of people that are CMU graduates Virginia Abbas Alaska Williams and Ryan Williams some other people it's awesome of all sometimes called hardness with within P so this is about showing that it's for problems that can be solved in polynomial time you really want to drill down until like well what's the fastest polynomial for solving them and that's a very important question because of course in real life there's a tremendous difference between like a quadratic time algorithm for a problem in a linear or quasi linear time algorithm for a problem quasi linear time algorithms are usually very practical quadratic time algorithms are usually basically unusable so it's very important but you know an assumption like P does not equal NP has no power to tell you anything about that because you know P versus NP you totally gloss over all polynomial time factors so you can hope to drill down into these things but this is a very fine assumption and you can actually use it to prove cool things along these lines so let me tell you an example uh fact about reduction this is due to backers and indict from 2015 I'll just tell you a fact about it and then ask you to fill in the blank at the end so they came up with a reduction algorithm for Kate from KSAT to a very famous problem called edit distance the famous string problem um it's given two strings you know what's the shortest what's the Edit distance between them what's the least number of like insertions deletions and replacements you need to do to the string X to get string Y it's a very important problem in bio and other things and it can be solved in polynomial time it's not MPR it can be solved in polynomial time using dynamic programming but here's what they showed they show there is a reduction from case ad that takes an instance of n variables and it outputs two strings whose length is like two to the N over two which is a bit crazy if you've never thought about these kinds of reductions before strictly speaking it's 2 to the N over 2 times polygon but basically 2 to the N over 2 so it outputs these enormously long strings incidentally it's running time is also something like 2 to the N over 2 times poly n that's like basically linear time in quasi linear time in its output size so Texas you know 3 okay so I mean since outputs to enormous strings and the theorem is that if Phi is satisfiable then the Edit distance between these two strings will be some number K you know that the reduction can also output but if Phi is unsatisfiable then the Edit distance between these two strings will be at most k minus 1 and so this shows that in some sense if you can solve edit distance you can solve k set and sick the set assumption is about you know how long it takes you to solve case I so you can actually deduce something about how long it takes you to solve edit distance so yeah no I'm asking you want to type into the chat if you like from this what can you conclude about how long it takes to solve the Edit distance problem on length capital n strings and we got the correct answer and squared yeah well basically N squared so and to the any number less than 2 so why is that well again suppose on length and strings you had some cool algorithm that solved the Edit distance problem it computed that its distance between the two strings in capital n to the 1.9 time well also say you'd become instantly famous in the field of algorithms because getting such an algorithm has been open for well at least 40 years anyway suppose you had such an algorithm then you could solve case at in time like slightly better than 2 to the N 2 to the Len how while you take your case audience since you run this algorithm not only takes 2 to the 0.5 in time so that's no problem for your overall plan budget you get these two strings of length capital N equals to 2 to the little n over 2 now you run your hypothetical capital into the 1.9 time algorithm for edit distance and so that lets you tell which case you're in you over here for edit distance and therefore it lets you tell if I was satisfiable or not and so you solved case odd and your running time is like this quantity ^ 1.9 which is you know a little bit less than 2 to the N it's two to the you know 0.95 and so it contradicts F so this is cool it tells you like I mean if you assume Seth which is a very strong assumption but if you assume Seth then you get that you know and that distance cannot be solved in some quadratic time and uh trying to solve at a distance in some quadratic time is a problem that's been intensively studied for like over 40 years the fastest offering was like N squared over log squared n so this was like an awesome dollop in algorithms theory over the last five years I mean erases the question well how much do we believe Seth it's a very strong assumption and um I know some experts who are like I believe ETH but I don't believe strong ETH but I believe strongly ETH why not I don't know we don't know any algorithms that contradict it seems pretty hard another way to think about it is like if you could just get an end to the 1.9 time algorithm for edit distance you can translate that into an amazing algorithm for a case I'd I mean you would break Seth okay so a few more facts like this about Seth there are many many consequences of sets that are like this I'll just say real quickly it's an ancient algorithm for finding the diameter of a graph in order M times and time basically run Dijkstra n times and Seth implies that you cannot get M times n to the 1 minus epsilon so if like M is proportional to n you have like a sparse graph you can find the diameter and quadratic time this shows you cannot get in some plug attic time similarly if you want to solve all pairs max flow in a graph you can do it in M times N squared time Seth implies that you cannot do it at M times n to the 2 minus epsilon time you know though there's like a was it called pseudo polynomial time algorithm for subset sum so if you have M integers and your target T is not too large you can actually solve some sense some efficiently so subset sum is only hard when you're talking about like a target number which itself is like in digits long but if the target number is like like poly n so it's only log in digits long actually by using a dynamic programming algorithm you can solve stuff that saman like n log n time what Seth sort of shows this is optimal so assuming Seth is like another theorem of a boot at all no you cannot do it at T to the 1 minus epsilon times anything sub exponential in n ok so some other example of getting very precise results about algorithmic running time under you know one assumption set\", metadata={'source': 'dVQrj52JhyA'}),\n",
       " Document(page_content=\"okay so the last portion of this lecture I want to talk about NP hardness of approximation and some of the stuff I talked about a little bit in lecture 20 about I'll recap it somewhat so here's like another like SAT style syllogism for you um 3sat is - NP hardness as this problem label cover is - in approximability okay in the sense that like 3sat is like the you know a problem that everybody likes to start from when you prove things are and be hard and similarly if you want to prove things are not just and be hard but and be hard to approximate to some factor like the Erb problem that everybody starts from is called label cover what is label cover it's the kind of CSP and actually there's a parameter Q which is the size of the domain so remember domains CSP is you know they have some variables that you're assigned trying to assign values to from some domain and then label cover parentheses Q the domain is just the integers 1 through Q and you're given some constraints and label can cover all the constraints are binary meaning they operated some two variables so then it's good to draw the picture of the could CSP as a graph where you have a vertex for each variable in a edge for each constraint so in the label cover problem you're given a bipartite graph in fact you can even assume it's regular on both sides so it's a very enjoyable graph it's a bipartite graph and each edge represents a constraints and it has like a pretty liberal kinds of constraints so every can schedule a constraint written on it and it's just given by like a truth table kind of it's given by a function PI some UV like you have one of these pies for every written on every edge and the PI is just a function from once your Q to one through Q here just give it as a table or think of Q as a constant I don't know 10 or something so pi of UV is just a list of 10 numbers in that case so as always you're trying to find an assignment of domain elements numbers 1 through Q to all of vertices you in you and B and you want them to satisfy as many constraints as possible and what is the constraint the constraint is like is this kind of it's called projection to constrain from the capital u side to the capital V side and the meaning of this pie is whatever value you give to let's say this vertex like 6 that's a value between 1 and Q pi is a function so it Maps 6 to something maybe 3 and this constraint says you have to give this guy the value 3 right that's what you should give ok so the constraint on edge little you little me is that whatever you assign to you if you pass it through the pie that's written on the UV edge that's what you're supposed to assign to me it's a little hard to get the gist a bit it's some kind of generalized graph coloring problem but this is the problem that's sort of like the great starting point for all hardness of approximation results so here's the theorem about it it's actually a bus stop - on that like bus - are that in the proof of host a theorem that took us from like 3sat hardness PCP theorem God is too slight in approximability for three sad and Roz's theorem which is called parallel repetition theorem got us the hardness of label cover that's this piece here and then host I put some Fourier analysis on top of that to get this hardness of 3x or approximation but here's a rasa's theorem from 1994 about the hardness of label cover and this you know is like unconditional it's like a proven theorem well it's conditioned on P does not equal NP so it's about a problem being np-hard and it's saying the following for every small number Delta there's a sufficiently large Q which is you know polynomial in 1 over Delta such that this label cover problem is not only hard to solve exactly it's even hard to remotely like optimally solve so this notation if you recall Delta comma 1 approximate means basically I give you a label cover instance with domain size Q and even promise you that there's a perfect assignment that satisfies 100% of the constraints it's np-hard for an algorithm to find a solution that satisfies a delta fraction of the constraints so you can set Delta to be you know 0.1% and it says any algorithm that can find up an assignment satisfying point 1% of the constraints you know perfectly satisfiable label cover in sense can be used to solve sad in polynomial time so that's Roz's theorem rewritten up there and as I said like this is like the starting point even more so than host odds theorem is the starting point for many many optimal inapproximability results so this is Hostos theorem we already talked about it but it's a reduction from Roz's theorem that 3x or it's hard to 1/2 plus epsilon comma 1 minus epsilon approximately close that also showed an even better result for max 3sat it's better because this thing is 1 we talked about this I think in lecture 20 so it's known that even if I give you a perfectly satisfiable 3sat instance it's np-hard to find an assignment that satisfies more than 7/8 of the clauses and that's tight because there is an efficient algorithm that satisfies 7/8 of the clauses and there's also an extremely strong hardness results for max independent sets starting from hardness of label cover it says I give you a graph where there's an independent set of size n to the point 99 if P does not equal NP then there's no polynomial time algorithm that can find an independent set of size n to the point O one so there's an enormous independence sense like almost all the graph well it's n to the point 99 vertices you can't even find an independent set of size n to the point O 1 and by the way this problem is np-hard but it can be stalled in time basically 2 to the n to the epsilon 2 to the N to the point O 1 so that's an example like I've seen before of an np-hard problem that can be solved in 2 to the N to the point O one time there's one outside of rasa's theorem which is related to the blow-up size of the reduction and the problem is of you said like Delta to be you know some constant then Roz's theorem you know takes a 3-sided sense and produces a label cover in sense of size n to some constant so the blob is really eats like it's exponential kind of in in Delta so if you said you don't also to be 1% then you know the blow-up here in mind this thing might be n to the 1 million size and that's not so great because then if you want to say like oh now I'm going to use a ETH to conclude you know that it takes a really long time to solve some of these problems not just that they can't be done in polynomial time which I get out of P does not equal NP but I want to use ETH to show that they take a really long time so that's so great like you can only conclude that because the palm the blow-up is like into some huge polynomial you can only conclude that let's say 0.51 comma point nine nine approximating max 3x or requires time to to the end to the like some really small constant which is still exponential but on the own hand you know if this is like time for for any value and that you've ever encounter in real life like and it's smaller than 2 to the power of 100 then this is at most 4 okay there B goes in there but you get my message but I've got some good news for you which is that uh Ono had some good news for you but I accidentally deleted the slide ok let me tell you the good news in words so I'll put a smile on your face because later with Moskovitz I won't write it up with a ton of Moskovitz this is like 2010 they got a reduction that for any constant Delta this was like n to the 1 plus little of 1 okay so for Delta even as small as like I think 1 over log log n the reduction in had almost linear blow-up and therefore actually you can conclude that this requires time 2 to the n to the 1 minus little o of 1 and so basically two to the end time ok so that's great ok so last thing I want to do is contrast this with the unique games problem which I also mentioned in lecture 20 and I'll just tell you a little bit more about it so the unique games conjecture made by coda in 2002 was really motivated by this roz serum you know this Roskam gave this like really strong hardness of approximation result for this label cover problem and was used to prove many many many other optimal inapproximability results but not everything you wanted so there's some problems we couldn't show her and be hard based on label cover and coat notice that if you made like one small tweak to label cover and assume that didn't change the fact that is np-hard then you could get all sorts of additional cool and B hardness results but didn't know how to prove it so it was made up conjecture and it's about label cover but all the PI's are now required to be by diction's so we talked about 34 we call this CSP like by action parentheses Q so like in the bipartite graph picture like not only does the you said when you choose a candidate assignment for this for text does it force what I mean you're supposed to give to the neighbor vertex also vice versa so that makes the task certainly a lot easier and in fact if I give you a hundred percent satisfiable instance it's easy to find that satisfying assignment what if I only give you a ninety nine satisfied percent satisfiable instance it's not so clear how do solve it well and this is a conjecture that for all Delta if you make you big enough the domain size big enough then even on instances where it's possible to satisfy you know one minus Delta fraction of the constraints its n be hard to satisfy a delta function okay and again many more optimal in approximately results were able to be deduced assuming this unique games conjecture for example we saw that there's this semi definite programming algorithm for max cuts that in polynomial time always finds a cut that's within a factor of 0.8 7/8 of the maximum cuts just proved by Gomez and Williamson and it was shown that if you assume this unique games conjecture then this is tight you cannot get 0.87 eighth plus epsilon we're just kind of funny because point eight seven eight is like a strange number it's actually this presumably irrational or transcendental number the solution of some trigonometric equation in fact raghavendran 2009 proved an amazing generalization of this he basically showed that assuming the unique games conjecture we know the optimal approximation and algorithm efficient approximation on them for every CSP we should take any CSP of air DK and consider this degree k SOS semi-different programming based algorithm then basically whatever approximation or certification it achieves it's np-hard to do better than that by Epsilon and so sort of saying you know that this is the best polynomial time algorithm this SOS algorithm for approximating CSPs but only assuming this unique games conjecture do we know that so that's a super amazing result if unique games conjecture is true it really close the book on the theory of approximating approximately solving csps in polynomial time but let me just end this lecture by asking you know well is it true is it false like indistinguishability obfuscation it's one of these hardness hypotheses that it's kind of more controversial or people are not sure whether they should believe it or not things like ETH I think most people believe faggus hypothesis I think most people believe even as Seth but we'll see we just briefly mention that it's known that this is equivalent to getting half of the constraints correct on instances that are 1 minus stuff is satisfiable so uh here's some evidence that it could be true I mentioned this before just two years ago code min zurafa approved a variation of the unique games conjecture called the 2 to 2 version where instead of by Chechens PI you allow 2 to 2 maps which makes the CSP more complicated and therefore harder to potentially solve and they showed that you do have basically Delta versus 1 minus Delta approximation hardness for this and as a consequence you get Delta versus 1/2 approximation hardness for ujc okay so unique games conjecture is a cool into saying that if I give you a 99% satisfiable instance and be hard to get half they show that if I give you a half satisfiable incidence it's hard to get 1% well they don't have maybe the UGC is false so in 2010 it was shown by Aurora at all that you can solve the task of getting a half of the constraints correct on one of my stuff to satisfiable incense in time that's like to to the end to the Delta to the one-third it's pretty funny but for any fixed constant Delta or for like you know really small constant Delta this is like a really small sub exponential quantity like to to the end to like something tiny so that's kind of close to being polynomial time and as we saw before like if you assume ETA age which is less controversial this is sort of like the easiest an np-hard problem could be like under ETH you can solve NP hard problems in time 2 to the n to the epsilon but not better than that so we kind of know that you can solve the unity games problem and time to to the end of the epsilon but it's still possible that it's np-hard uh so we really don't know and let me just say that you know there's a third strange thing here that I sometimes like to think about which is that maybe it's not clearly important whether it's np-hard or not it seems to be de-facto easy so the unique games conjecture has an extremely strange property that even though it's conjectured to be np-hard we don't know any hard instances we have no way of writing a computer program to generate unique games instances maybe at random where we don't also know a polynomial time algorithm for solving those instances this is quite in contrast to like you know the max 3 X 3 X or CSB and like we don't know any explicit family of instances which are not already solved by this polynomial time SOS degree 4 algorithm so it's a problem where like it's conjecture to be hard but like we don't even know how to find any hard instances so maybe in practice it's easy ok so that's a bit of a philosophical note to end on I guess I shall end the recording here but as always I'll stick around to answer any questions and otherwise I'll see you at the last lecture on Thursday which will be about a sketch of the proof of the PCP theorem\", metadata={'source': 'pp-oaeOuGDY'}),\n",
       " Document(page_content=\"okay so this is the last lecture on lecture of the course lecture 27 on PCP theorem so I try to sketch to you this famous theorem in CS theory okay so as always I'll just start talking if you have any questions feel free to type them into the chat or interrupt so this is just a sketch in one lecture I'm gonna try to tell you a little bit about how the proof goes a million years ago together with Professor of encode guru Swami I co-taught a course about the PCP theorem and you can see the lecture notes for that if you google the information which is currently on the screen okay so what is the PCP theorem we mentioned a couple times before in this class but we'll really get into it today uh first of all what does PCP stand for it stands for probabilistically checkable proofs and it's a real gem in computer science theory it's you know considered one of the most famous theorems in the field I was originally proven in the early 90s and sort of a succession of works by Fayed goldwasser Lovaas Safra sega d aurora sudan aurora luna 20 sudan sega d and i was pointed at you know difficult proof and like the concatenation all these papers you know 150 pages or something is considered quite challenging but then everybody was real excited in 2005 I remember sitting in my apartment in Seattle when I like checked the internet and ECCC and I saw this paper there's like oh I got a drop everything to read this paper by a region or where she gave a new proof of the BCP theorem that was like way Shorter's like 25 pages or so which is really an totally different proof what's interesting and part of the reason i want to present it you know it's the final lecture is that it's really only shorter assuming you have you know CS theory in your in your pocket so what people liked about it is you know if you kind of have like a baseline you know amount of knowledge about computer science theory all these tools in your toolkit then her proof was quite sure and not too bad but it really combines a lot of stuff I mean we're gonna see some spectral graph theories of boolean functions error correcting codes expanders csps it's all going to come together in a glorious synthesis and we're gonna prove them the PCP theorem using all these tools so it's kind of like a nice way to I wanna recap and mention a number of things we talked about in this course okay so what is the statement of PCP theorem there's a slightly different equivalent statement so I'm gonna give you one that's a bit different from the one that I mentioned in previous lectures but I'll then tell you why they're all really the same theorem so the PCP theorem is an NP hardness results for a specific problem were and it says the following says there's an absolute universal constant epsilon naught greater than zero and so think of it in your mind is like one percent or something but really just a fixed constant such that the following problem is np-hard okay this is the problem you're given as input I'm three coloring problem but think of it like a CSP like a maximization problem so I called it max three coloring problem G this is my weird handwriting fonts for G hope you can get used to that so you're given a three coloring instance G is np-hard to distinguish between two cases first case is where the opt is one which you know in our CSP notation and this means there is a perfect assignment I II the graph is 3 colorable you can three call it as vertices such that all edges are by chromatic versus the case where the opt is at most 1 minus epsilon not so think of that as like 0.99 or 99% meaning that normally is the graph not 3 colorable but any three coloring of the vertices will make at least 1% of the edges by chromatic you only get 99% of the edges at most I should've said monochromatic there you'll only get 99% of the edges at most by chromatic like you're supposed to do in in coloring problems okay right so as I said this thought is you know the maximum over all three colors the vertices of the fraction of my chromatic edges but it's gonna be convenient for us to look at like the difference of this quantity from one okay so I'm gonna do some non-standard notation here for any CSP at all script G not necessarily three coloring but think of three coloring for now I'm gonna write um you know badness of G for one minus the Ox okay so this is sort of like the the least error you can occur incur at least loss you can occur by an assignment so in the context of three coloring or max three coloring if I give you a graph G what is its badness it's just you know the minimum over all three coloring to the vertices of the fraction of monochromatic edges okay so these are the the violated edges in the three coloring CSP okay I'm just changing the rotation up here saying that um you know what's hard to distinguish and be hard to distinguish under the PCP theorem is the case where you given a graph where the badness is zero meaning there is a perfect three coloring versus the badness is at least this you know epsilon zero like 1% meaning every three colors vertices has to monochrome Lee color an epsilon zero fraction or more edges okay and you know the proof that I'm gonna sketch for you actually yields like a very pathetic constant like it'll I don't know I'm not even work through the numbers but you know might give you something like 10 to the minus 20 which is horrible but the point is it's like a universal constant independent of n the size of the graph in case you're curious the best known epsilon not that is a proved these days is any number less than one 17th or think of it like five percent okay so 0 versus 5% is now known to me to be hard that's a result proven by Austrian 10 writes and yours truly and one match of the summer in 2012 okay but let's worried about the constant too much but just think of it as a fixed constant in your head okay so it's an NP hardness result okay so it's just like those classic NP hardness results you know you prove this in undergrad just much more complicated and so it's a polynomial time reduction from an NP hard problem let's say the most canonical ones circuits at and so it takes the circuits on instance which is a circuit C and outputs a 3-coloring instance G so converts circuits to graphs and it has the following theorem I mean this is the key aspect of it but a to cases if C is satisfiable then the graph that's output has a perfect three color you notice C is unsatisfiable then every three coloring makes at least an epsilon not fraction of edges monochromatic okay so badness is at least epsilon zero when C is unsatisfiable okay this is in contrast to like you know the classic and B hardness result for three coloring is normally understood which would have look exactly the same except that this would just say the graph is not 3 colorable but here is saying no is it not 3 colorable like it's very not 3 colorable any three coloring you know violates at least epsilon zero fraction of the three coloring constraints okay therefore if you could tell the difference between these two kinds of graphs you could solve circuits ahead and P would equal NP okay so there's the statement there and now let me try to answer this question why is this called PCP theorem like what it this is you know PCP stands for probabilistically checkable proofs and what does this have to do with proofs or probabilistic checking or what so I won't dwell on this too much but let me try to tell you a story that illustrates why this result is super cool and has something to do with probabilistically checkable proof so if you look on it from a certain angle so ok let's assume we prove this theorem and so we know this cool polynomial time reduction from circuit set to 300 and now let's consider a particular circuit see let's consider a circuit see which checks if it's input is a proof of the Riemann hypothesis or some you know famous open problem in mathematics okay so you know you know there's work these days on formalizing mathematical proofs by computers of the Machine checkable and there's lots of systems for doing this and you can easily cook up you know a like a circuit that checks if the input string is an encoding of a proof of the Riemann hypothesis in you know some standards a proof system okay and that's nice because you know okay proof could be any likes but you might say I'm only interested in Procera at most 1 billion bits long so you might have like 1 billion bit inputs so great so now let's say you're the the editors of a journal you know a fancy math journal and you know perhaps somebody comes along and says hey I've proven the Riemann hypothesis I would like to submit to your journal well uh you could say you know please coat up your proof in you know a machine checkable format and then you know I'll pass it through this circuit and check your your proof that's reasonable right extraordinary proofs require extraordinary claims so our planes require extraordinary proof so you know we can make the mathematician work a little bit harder by encoding the proofs formally but you can do an even cooler thing that's kind of a funnier thing which is you can say you know what uh don't give me a bit string that satisfies the circuits see pls instead give me three coloring a perfect three coloring of the graph G that you get out of running this reduction okay so I mean you can publish the circuit see everybody you can publish the circuit G - everybody's Bruce by Pauling all time reduction and it's actually not made clear in this theorem but it's also true but there's a polynomial time algorithm that in this case where C is satisfiable if you have a string X that satisfies see there's a polynomial time algorithm that translates it into a perfect three coloring of G but part of the proof is actually easy so you can say hey mathematician who claimed to have proven the Riemann hypothesis don't give me a good proof that I can fina to see instead work a little bit harder run your polynomial time reduction and give me a three coloring of this graph G okay it doesn't look like especially interesting because you know then the journal couldn't be like all right I will now take this 3-coloring you've given me and you know check if it's a perfect three coloring of G but you know a lazy Journal reviewer can do a cooler thing and this is where like the name probabilistically checkable proof can come in they can spot shed it get in an extremely remarkable way you see uh if the mathematician is correct and they had a perfect proof of the Riemann hypothesis and this graph will be perfectly 3 colorable and like the three coloring they sent to you would you know make all the edges might monochromatic or by chromatic but if the mathematician had a mistake a proof unlike a wrong proof the Riemann hypothesis even if it you know is different how to Hamming distance one bit from like a correct proof even if it was like you know slightly wrong in the slightest possible way the graph would have the property that well you know if the sorry if the remote hypothesis is I suppose uh okay if actually add a little bit on to this but will have the property that when you run this reduction every wrong proof will actually make them on a chromatic like an epsilon not fraction of edges so like you know think of that it's like five percent of the edges and now what a reviewer can do is randomly pick a constant number of edges from this graph G let's say a hundred over epsilon knots or you know two thousand if epsilon naught is five percent and just look at the four thousand different endpoints colors and just check that each of these four thousand or two thousand pairs are distinct and if they're all the states then the reviewer can be confident that except with probability like e to the minus one hundred the proof is correct so you get this amazing spot checkable proof where you just have to look at you know four thousand symbols red green or blue from the the proof and you can be highly confident whether or not the mathematician like had a correct proof that satisfied the circuit so that's why it's called probabilistically checkable proof that said I mean that was actually how he's originally conceived but people eventually realize is equivalent to this much more mundane thing the np-hardness of sort of approximately solving satisfiable csps like 3 colorability i'll there's a question are such circuits easy to construct yeah well this is getting a bit more into like you know mathematics formalization but you know if you have like a like a mathematics a verification system like you know these ones that exist like egg though or lean or whatever a then they have like a small set of rules and so you just need like a circuit that like first checks that like the proof is trying to prove the Riemann hypothesis so like the you know the statement of the theorem of the proof is the Riemann hypothesis and then it just has to check there like each of the deductions in the proof like follows from likeness like short set of valid deduction rules in the proof system so in principle yeah it's it's no problem to construct these things it's actually these days I mean all these constants I'm gonna show you our horrible you'll see like some numbers like 10 to the minus 20 and things are much worse but actually people are using PC peas these days for cryptographic applications that are actually practical so it's a pretty cool stuff\", metadata={'source': '1wNIvo7jwzc'}),\n",
       " Document(page_content=\"okay so that's all I want to say though about this proof checking mentality and we'll get back to just thinking about CSPs so one thing I want to say is that it's not important that the CSP here that we're talking about is 3-coloring let's just like I chose it for convenience but you could have any other CS non-trivial CSP here and get like an essentially a similar result so every time I say the PCP theorem in previous lectures I actually stated it for like three sad or max three sad instead of max 3-coloring but it's equivalent and I'll show you one direction of the equivalence or schedule one direction of the equivalence let's say you want the exact same theorem but like you wanted to be talking about max 3sat here instead so to conclude that it's hard to distinguish between the case of a perfectly satisfiable 3-cnf and the 3-cnf we're like the best assignment satisfies that most 99% of the clauses well you can do is first get this PCP theorem about three coloring and then we know from you know our undergraduate days our textbooks that there's like a you know many one polynomial time reduction mapping 3 colorable 3 colorability 2 3sat let's just by virtue of the fact these are both np-complete problems but uh okay so this takes a graph and outputs like a 3-cnf phi but what you can also observe if you take a look at the reduction in the textbook is that it's a particular kind of reduction like a gadget reduction which basically means like given the graph G you make a 3-cnf or like for every vertex in G you like kind of encode it by like one or two or three variables in your Phi then for every edge or every constraint in the graph you kind of convert that edge to like three or six or eight or something you know 3-cnf clauses in such a way that you really like there's like a one-to-one correspondence between assignments and colorings and like you know the constraint that the two colors on an edge are different and like the satisfiability of like these eighths gadget clauses you introduce weight being that like on one hand if you have like a perfectly 3 colorable graph this reduction produces a perfectly satisfiable 3-cnf formula but and i leave this as an exercise you can conclude that if the if the graph is not just not 3 colorable but has badness at least some Epsilon zero and the badness of Phi will be I don't like at least epsilon zero over eight or ten or twenty or over some constant okay the point is like you just observe from the reduction that like if you had some assignment to Phi you could translate it back to like a coloring for G and this coloring for G you know if you're in the case where G is not satisfiable you know has to violate at least an epsilon zero fraction of the edges and then for each violated edge it had you can infer that at least one of the Associated 3-cnf clauses would also have to be unsatisfied by your the C line but you started with so basically for every unsatisfied edge and G you get like one out of eight or one out of four ten or something unsatisfied clauses and Phi okay so this epsilon zero goes down by a constant but you know you can pass from one constant to another okay basically this holds it sort of formally holds by the dichotomy theorem for any CSP that's and be hard there's a gadget reduction between any two CS peas that are and be hard and therefore this kind of translation holes just this constant epsilon zero might change okay but you know for simplicity we're gonna formally stick with three coloring in this lecture I also want to mention that it's there's nothing special about starting with circuits at here this is just an NP completeness result or happy hardness results so you could have started with any I mean the PCP theorem would be equivalent if you put any np-complete or np-hard problem on the left hand side here instead of circuits that just because you know it's happy hardness results it doesn't matter what np-complete problem you start with a particular I'll make a funny observation which is one np-complete problem we know is 3 colorability but like the classic version of just trying to distinguish three color ball versus on three non three color ball so you have a three set here but you know as I said you can also put three coloring which would be kind of amusing in fact that's what we'll do this is like the final form of the PCP theorem that will prove will show that there's a well sketch that there's a polynomial time reduction from 3 colorability to 3 colorability like from graphs to graphs but with this sort of property that it's sort of um badness amplifying so you know if you take a graph G that's 3 colorable the pass it through the reduction you scan another 3 colorable graph but you take a graph that's just not 3 colorable but maybe it's almost 3 colorable you get a graph that's sort of far from 3 colorable ok so this is the inherent nature of the PCP theorem ok or to just summarize the notation and it looks like this ok and now I'm just gonna make one small observation that like if you have a graph G with n vertices and edges to say that the badness is at least 0 is equivalent to say that it's at least 1 over N that's very trivial comment it's just that like if you have a graph that's not 3 colorable it means that every three calling the vertices must make monochrome at least one edge which is a 1 over m fraction of all edges ok so this is really if you phrase it in this way it's really like a badness amplification theorem so uh in returners proof you know she used the term gap instead of badness is the slightly state or thing and she called her her proof like a proof by gap amplification and this is how universe proof goes and how I'll sketch it to you I know like when I reach uh you know sometimes gives a talk about this result cheese is this analogy you see the reduction has to transform a graph into another graph in polynomial time such that if like it's slightly not 3 colorable it becomes highly not to be colored well but if it's is 3 colorable it stays 3 colorable and so the poor polynomial time reduction is just falling real time it cannot tell what case the graph is initial graph is in whether it's 3 colorable not it has to do some operation to it such that you know if if the graph is not 3 colorable it really spreads out like the badness so her analogy is like you know you like you're given like a piece of toast it has like a teeny spot of jam on it potentially or maybe doesn't like the jam is like the badness maybe there's no badness but maybe that's like a teeny tiny spot of jam on it and you're not allowed to look at the toast we have to do is like some operation which will spread out the jam fit exists so the operation is to just like somehow in real life you know take their knife and like just rub it all around the toast and then wherever like the badness the tiny bit of badness is it gets like spread out but of course if there's no badness at all then you know rubbing rubbing your knife all around the toast doesn't do anything I don't if that analogy helps I probably only gave like a poor version of like a race version of this analogy but I liked it so uh her proof goes not by trying to prove this in one shot but by sort of the slowly gently method of building up to it in stages so the main thing she proved and what the main thing that we'll try to sketch is a badness amplification theorem that looks like the following so this is a new theorem which is going to imply the PCP theorem it says there are universal constants epsilon zero and some big constant K and a polynomial time reduction R this R will not be the final R but some polynomial time reduction are not Maps 3-coloring instances two three coloring instances so it takes a graph G and outputs a graph G Prime with three properties so property one is that the size blow up is just a constant factor this fixed constant K and when I say size here I really mean like the number of bits it takes to write down the instance which is basically like that you know the number of edges and vertices in the graph so this reduction well you know take a graph and produce a new graph that has more vertices and edges but everything will be within like a fixed constant factor to it'll have you know the classic property of these reductions just that if the graph G is perfectly 3 colorable then the output graph G prime is also perfectly 3 colorable but the main property well together with the first property is this that this reduction R increases the badness by a factor of 2 well you can't literally say it always increases the badness by a factor of at least two because you know the badness could you know can ever be at most one in fact for 3-coloring the badness can never be more than 1/3 because there's always a coloring of the vertices that falsifies or violates at most one third of the edges has a random coloring will have this property and expectation so you know if the badness is already high then you cannot like make it go up by a factor of two but it says as long as the badness is lower than this epsilon not this fixed constant like 1% or whatever this reduction are like at least doubles the badness okay that's not inconsistent with bullet point two by the way because you know if badness of G is zero it stays zero so this only increases the badness by a factor of two whereas you know the PCP theorem we want to get it up from one of our M two epsilon zero a constant you can probably guess how this is going to go though the key aspect of it though is it's really strong on this it only blows up the size of the instant by an absolute constant factor so perhaps you can let me know in the chat like how you would take this badness amplification theorem and use it to conclude the PCP theorem repeat the application yes how many times should you repeat it keep an eye on the chat window here log very good so this botanist implication theorem implies the PCP theorem because you know to get this final polynomial time reduction giving your graph G you just do this reduction are that doubles the badness like log M times okay then you you know do it log n times and the badness either stays zero the whole time in the case one or in case two it starts out at one around but after doubling it log M times it gets up to this universal constant basically this point you know at which it stops doubling and what's great is here it's very critical that the size blow up is only like linear the size blow up is this constant factor K to the power of log n well gam you know the number of edges that most of the number of vertices are squared so like log m and log N or the same up to a factor of two so the overall size blow up is a constant the power of order log n which is poly n which is great so you all the time you're applying this you know sub reduction are everything is staying polynomial size all the way to the end so definitely a cool strategy which is quite different from the original strategy for proving the PCP theorem which involved like I think it involved like two recursive steps instead of like a log n recursive steps great so the goal for the rest of this lecture is to sketch this proof of this badness amplification theorem okay so if there any questions uh please do let me know so ah the diverse method for proving this is also really cool she does not do it I just in one shot one big reduction are but her reaction are is the combination of four steps one degree reduce to expand her eyes three power four mini PCP so I'm going to talk to you about all of these steps but before algorithms reduction algorithms that you put them all together they achieve this badness amplification algorithm are and some of these steps are more important than other steps so these first two steps are kind of straightforward a straightforward I mean I'll sketch the proof of like them they won't be that straightforward but they're kind of known already or like well these are standard of sort of ideas by the time dinner approve or theorem uh the main step is this power step this was like our main innovation this is actually the step that really does this badness amplification by a factor of two this power step is the main innovations very cool and um you know it only works to increase the badness by a factor of two if you know your underlying instance has been expander eyes so that's like the whole point of steps one and two is to sort of turn your CSP into it like an expander CSP and then this power step will work but this powering step screws up some other parameters so increases the badness five factor of two but as we'll see shortly like it's sort of ruined several other aspects of the reduction and then this like mini PCP step like fixes some of them and this mini PCP step was also sort of partly innovation in Donora proof as well it's partly an innovation partly kind of already now so also cool but like this is the most cool bit and what we'll spend the most time talking about and I shouldn't mention that okay this this overall are is reduction from three coloring instances to three coloring instances but uh as you do these steps one through four in the intermediate stages you will no longer have a three coloring CSP you will have different kinds of CSPs these will actually still be binary CSPs meaning that each constraint will all involve exactly two variables and therefore will always be good to think of these CS pieces like graphs esps we're like the variables are vertices and the edges are constraints it's just that the constraints might not just be like the standard coloring constraints like you know the two endpoints should get different colors that's the coloring constraint they can have much different it'll have different constraints and in fact the even the domain will get larger as we go along well throughout these steps are particularly the powering step will change the domain kind of drastically meaning that you won't be assigning just red green blue to the B of vertices you'll be signing like you know from a bigger label set to the vertices and you'll have like okay two constraints binary constraints in your CSV so that'll happen sort of in the intermediate stages of one two three four but you know at the end of four and at the beginning of one you have a three coloring CSP okay so now I'm gonna give you the overview of like what the point of each of the four steps is and but I won't say how to do this step and then we'll see that if we can implement each of these steps we'll get the badness implication theorem and hence the PCP theorem and then I'll tell you some details about each of the four steps okay so the first one is degree reduce and this takes the CSP a 3-coloring CSP g and produces another CSP g prime that actually will still have domain size three it'll still have like red green and blue as its labels but I'll have both not equals constraints like in three coloring and also equals constraints which is different from three common so the main point of this part of the reduction as you might guess from the name is to reduce the degree of the underlying graph so the sort of starting graph here G could have any degree it doesn't have to be regular enough some vertices of degree n because of some person true down it could have some vertices in the breeze 3d whatever the point of this degree reduction will be to clean things up so that the underlying resulting graph G Prime that should say becomes nine regular I don't know where the word regular it's okay so at the end of this every vertex will have degree 9eo participated exactly nine constraints it's nothing overly special about the the number nine just some fixed small constant is the point but the way I decided to describe it it was gonna be nine hmm yeah this funny without the word regular anyway so that would be the effect now unfortunately this oh there's a question I'll get to in a second unfortunately this reduction that gets things down to degree nine right nine regular graphs will have some side effects I will have to tell her and fix later let me check out the question it says do all know improves incur a log and blow up in the size of the output graph not sure why you say oh do you mean for the final uh blow up in the PCP theorem so with this version of the PCP theorem the final blow up is like a huge polynomial because you know multiply by this factor K which might be like a million it's actually much bigger than that log n times so the blow up is like n to log a million and to a huge constant we mentioned last time that like mush mitts and Roz have like have a better version where the blow up is quasi linear it takes instances of size and produces instances of size n to the one plus the low of one mm I think the smallest known log little of one factor actually I'm not sure what it is I think it's Polly log in but it's a major problem actually whether you can have like a linear size blob in the PCP theorem that's unknown good so let me uh I hope that answers the question let me get to the side effects so okay one side effect of this degree reduction is the size of G prime is bigger than the size of G by a constant factor k1 that's totally fine we're planning on blowing up size by constant factor so that's no biggie this reduction does not change actually the size of the domain but it does add equality constraints as I mentioned so this new CSP G prime has both you know edges labeled not equals and edges labeled equals so you're trying to make some edges my chromatic and some edges monochromatic um it has the property that preserves perfect satisfiability so G is perfectly 3 colorable then this new G prime will also be perfectly satisfiable and all the steps all these reduction steps are going to have this property so I'm going to basically stop mentioning it you can assume all the reduction steps have this property and the final property is that the badness of G prime will always be at least the badness of G divided by some potentially big constant C 1 now this looks funny because it looks like the badness could also actually go down by a constant factor like imagine C 1 is 100 and it's true this can this operation can make the badness go down by a factor of 100 and the whole point of capital R is supposed to be make the badness go up by a factor of two so what's gonna happen and you'll see this over the next few slides is that you know step one and step two and step four actually make the badness go down by a constant factor potentially so that really hurts us but the point is that the powering step is going to make the badness go up by a big constant factor that we can tune in such a way but the overall badness change will be increasing by a factor of two so basically we're gonna tolerate this badness because we're gonna fix it this badness drop because we're gonna fix it in step three okay so that's degree reduce next step expander eyes so expander our eyes it also takes a CSP this new kind of CSP with red green and blue is the labels with some equality and non equality constraints and the point of expander eyes is to output like a new CSP G prime which will be a binary CSP but the sort of underlying graph the point is it'll be an expander graph and again it'll be you know regular for some fixed small constant which in my version of the story is seventeen but again don't worry too much about this so the point is after step one you got down to like a regular graph like an eight or nine regular graph or whatever um but it might not be an expander and after step two it'll be an expander graph and like in order to expand rise you need to have a constant degree garage so that's why you did the degree reductions just remind you what does an expander graph it's usually defined in terms of eigen values being an expander so you know it means that the take the graph G prime and it's normalized adjacency matrix K as we've called it you know the maximum eigen value is always one and you're an expander if your second largest eigen value is bounded away from one by some constant it'll gonna be you know bounded away from warmed by some mystery constant like point zero zero zero one but somehow solute constant another definition of expanders that's equivalent as you might remember is that like every smaller set of vertices has a lot of edges at the graph touching you so the graph has large minimum conductance so that's the main effect there it's setting you up so you can do this power step and it has some side effects that are the same as the side effects as in one so the size goes up by a constant factor and the badness actually goes down by a constant factor okay let's expander ice now the main step the powering step and you'll notice that I've actually stuck in like some parameter here T so this is a user selected parameter the reduction can choose it and will eventually be an absolute constant but T is sort of the amount by which the powering operation increases the badness so you're gonna take T large enough a large enough constant to overcome all the badness losses in steps one two and four right so what is the main effect of this powering reduction it's to increase the badness by a factor of T okay it actually it's you know it's like kind of funny it doesn't recreate increases by exactly T it increases it by a factor of T divided by some other universal constants III which I don't know again might be like 100 or something but you know you can always overcome that fixed C 3 by making T a larger constant and also as before right like I mean the boundness the badness is always at most 1 so this cannot be literally true it's only true the badness is already smaller than this fixed constant epsilon 0 and by this way this epsilon 0 will basically be 1 over T where T is whatever constant you choose you know so it's preventing you from you know getting more than a badness which is a fixed constant less than 1 so that's the main effect and that's the main power to increase this badness and it's a called power touch because it's like a powerful step but it's sort of like a graph powering it's sort of like you square the graph or it's even more like you raise the graph to the power of T so like the edges are like length T paths sort of in the input graph but we'll get to that so this one has some side effects too and some kind of worst side effects so it increases the size of the instance by a factor of 2 to the order T but tea is gonna be a constant so 2 to the order teen is just a constant so that's fine we don't mind a constant size increase um you've been more notably uh the label said in our you know CSV used to be of size three just red green and blue but the CSP output by this powering operation will have a pretty each I Norma's label set uh I use a Omega as always for the the domain or the label set of a CSB you'll have size that's approximately three to the power of 18 to the power of T so that's big but remember T is a constant so it's just a constant so that's fine and normally well like the the labels sets you know the thing that you're trying to assign to the vertices in the output CSBG prime be very large but the constraints which constrains to variables be quite elaborate I mean well they're just fixed constraints on you know two variables and label sets of this giant kinetics eyes but they're kind of complicated okay so that's powering and the last step is mini PCP and the main thing aspect of that is till I get you back from this crazy CSP toy with huge domain and weird constraints back to the 3-coloring season CSP sucks the point the main effect is to get the domain size and G Prime back down to three and back down to three coloring constraints and you won't even like kind of see I mean I'll talk about this a little later like why this step is called mini PCP you see we're starting from like a CSP where like these binary constraints but there's super complicated constraints like these things that the you know these constraints that are being checked on the labels are very complicated so complicated you might you need to encode them by like a big circuit and we're kind of replacing these by just the simple three coloring constraints and that's kind of like what the PCP theorem does right it replaces this complicated circuit sad thing with like a 3/3 coloring CSP well we'll come back to this okay this one has some side effects too ah this is like the awesomest side effect the size goes up by a constant factor of something like 2 to the 2 to the 2 to the order T so merely triply exponential in T still constant I guess this is a good time to remind you that the name of this course is CS theory toolkit and I should exercise that you know the point of this proof is just to prove the theorem show that all these constants exist it's like ridiculously inefficient you know you can make this merely doubly exponential with like not too hard of a trick but as I mentioned before they're actually not through dinner well actually yes through dinner is proof and some other coding theory based ideas from the original proof they're like very practically efficient versions of the PCP theorem these days what you know the point of this lecture is just to prove the results in theory so great to the two to the two to the order T still a constant and as in steps 1 & 2 this step has the feature that it makes the badness go down by some universal constant factor or C 4 but we've seen that before in steps 1 & 2 so that's the description of all four pieces of the reduction are that achieves by no simplification this factor to badness blow up while only blowing up the size by a constant factor and so putting all these four steps together I'm you know notated here like you know it takes a three calling instance G and outputs a new three calling instance G quadruple Prime after passing it through these four steps and what are the overall effects well we go from a three coloring instance to three calling instance great the size of the final instance is dominated by the last step so it's like at most 2 to the 2 to the 2 to the order T times the input size but we haven't selected the parameter T yet we still get to and what about the badness well if you put all the badness statements together every one of the steps loses a universal constant factor in the badness C 1 C 2 C 3 C 4 but step three the power you step gains a factor of T the to choose to yourself so you can just take T to be like C 1 times C 2 times C's 3 times C 4 times 2 to get out this factor of 2 and you accomplished the theorem so T is just a constant happily and the size blow up is also a constant and all the reductions are polynomial time and the instance size stays polynomial even after all like the log in application applications of the status amplification theorem okay so this is a great time to stop for questions because what I'm gonna do in the next well in the last half of this lecture is just kind of give you a sketch of how each of the four steps of the proof works\", metadata={'source': 'd3Kgy_y0m7g'}),\n",
       " Document(page_content=\"great so step one degree reduce so this actually one we're all I'm gonna go into moves or a lot of the detail because what kind of use some of the flavor of how these reductions work some aspects of them so let's do it let's do the rear adieu steps remember main effect is that it's sort of supposed to take a general 3-coloring incidence G and produce a new 3-coloring instance G Prime which where the graph is nine regular and these are the sort of allowed side effects that we're gonna tolerate so what's the idea uh it's kind of similar to the ideas that are used in constructing expander graphs in iterative fashions like we had talked about this on one homework this line graph method of Spielman and a lecture on expander as I talked about the zigzag product kind of similar so the idea is you go for every vertex in the input graph G which could house any old degree D and different vertices could have different degrees and you could have replaced that vertex by a cloud of vertices and particularly replace it by an explicit expander graph on D vertices that's like eight regular so actually although expanders is not in the name of this step you also need expanders here okay and we're gonna rely on the fact that you know as we saw from the expanders lecture there are polynomial time deterministic algorithms that can give you like a constant degree expander graph on any number of vertices actually the bit about any number of vertices takes a little bit of effort but let's assume it so I the reason I actually chose eight through here is like as simple as possible like explicit expander is this one we mentioned due to Margulis and gabber Galil that's like extremely concrete that's like you have the grid of vertices and you connect up each vertex to eight others in a simple way so you can use that construction for example she also mention that like you can get rid of all these of expanders here in this proof by replacing them with random graphs the only trouble of doing that is you'll get a randomized reduction instead of a deterministic reduction so your conclusion will not be that you know distinguishing three color ball from far from 3 colorable is hard assuming P does not equal NP your conclusion that assuming you know B P B does not equal NP that's not so bad what are they go ahead and get like a classic and B hardness reduction deterministic using expander graphs I see a question here what if there's a vertex of degree at most eight in the original graph yeah we can let these expander graphs have multiple parallel edges so it's possible to have like an eight regular graph one fewer than eight vertices and you know such graphs will actually not be good expanders um you know but we really I'm not gonna get into this level of detail but we really only need like it to be a good expander for like sufficiently large D and if it's like a poor expander for like constant D like T equals eight or equals ten the proof will be okay but let me not get into that level of detail there's like other like like slight annoyances like you know this Margulis Gabbard Lee ol expander really only gives you an expander on a number of vertices which is a perfect square so you have to do some hacks to get around this but I'll be ignore those so let me show the picture here so for the degree reduce step so here's your graph G you know the different vertices can have different degrees I didn't make the degrees too high here for pictures sake but it's gonna be make it a little bit weird but just use your imagination to maybe pretend these are quite high degree vertices so this vertex you might have D neighbors and you're gonna do this replacement for every vertex in the graph and so the new graph G prime is kind of drawn an ugly fashion down here looks like this you see that like every vertex I kind of blew it up into like a sub graph and each of these sub graphs is an expander graph one of these eight regular expanders and if you have D neighbors back here in G you're gonna be vertices over in this blown-up graph G Prime so in this little picture you had five neighbors so like I had like five vertices here and then I plunk down inside here and eight regular expander on five vertices well that's a little bit funny but imagine five is much larger if it really does have five vertices then you'll put in you can put in any old graph that's eight regular even though it's not an expander wendy is so small um you do that for all vertices in you and that's your final output graph G Prime this is most of the construction and so one thing to notice is that like G Prime has two different kinds of edges uh has like it kind of has like edges that correspond to the old edges which I call the white edges so like this edge here corresponds to this edge here and so this is supposed to be a CSV so they're supposed to have constraints not just edges and for the white edges I'm gonna retain the classic three coloring constraint that like an endpoint should get different colors but we also these like intra-cloud edges coming from the expanders which I call the yellow edges and I'm going to put equality constraints on these so that's the definition of the reduction produces a new CSP with some edges like the old white edges have non equals constraints the new yellow edges have equality constraints and let's see like so the new graph is nine regular because like each of these vertices here is connected to eight inside edges and one like old edge that's why it's non regular nine regular and uh yeah the new domain size is still three you're still like million vertices by red green and blue but we have some equality constraints now let's think about the size now um you see for every vertex here that had D edges you kind of replaced it with a graph on D vertices that was like eight regular so this graph here has like four D edges so you kind of replace like de edges by like four D edges or something like that so the number of edges goes up by a factor of I don't know five or something I don't do the calculations exactly right maybe it's nine or maybe it's two and a half or something maybe five anyway the point is it goes up by a constant factor so that's good and what's the idea behind this reduction what's the point of it and why does it satisfy this simple property well the point is this supposed to be that like okay imagine this graph is 3 colorable G so there's a perfect coloring the Simon red-green-blue to the vertices such that every edge is by chromatic then there will be a perfect assignment to G Prime right the what you do is for each cloud like this cloud of vertices corresponding to the original vertex you like in the good perfect coloring for G if you got red and what you're supposed to do is put red on all five of these vertices here okay and if this was green and the perfect design for G just what's what green on all three of these vertices here and you see that'll be a perfect assignment right for G prime because all the the vertices in one cloud get the same label so all these yellow intra-cloud edges have their equality constraints satisfied and on the other hand these old white edges are also satisfied because these white edges are sort of exactly corresponding to edges back in G and the assignment you're using perfectly satisfied G so it's also going to satisfy these white edges in G prime so that's this property here so this is the main thing we have to worry about like why does the badness only go down by at most a constant factor so how come a I mean maybe I mean maybe you know you could have the badness go way down like there is no particularly good three coloring here like every three coloring for G you know screwed up 10% of the edges but now suddenly there's like an amazing assignment for G prime that barely screws up any edges well basically the ideas that shouldn't happen so like roughly speaking you can kind of look at G prime and consider assignments and if these assignments like mostly give the same color to like most clouds like for each most clouds they an assignment pretty much uses the same color for all of them then that will kind of correspond to like a coloring back for G and therefore like most of the white edges will get screwed up on the other hand if you don't use an assignment that mostly uses the same color for like within each cloud then you'll mess up a lot of the yellow intra-cloud edges that's the idea of the proof we're gonna write the proof on slides it's gonna be kind of messy and I'll go a little bit quick because I don't want dwell on this too much but that's the idea so let me messily kind of sketch that in a bit more detail we're trying to show this thing down here so let a prime be the best assignment for G prime okay and so it by definition satisfies or violates a badness of G prime traction of the constraints so these are the white and the yellow constraints some epsilon prime get violated by the bestest I'm in a prime and what I want to kind of do is decode a prime to an assignment for G that we can reason about and a coloring for G that we can reason about and the natural way to do this is as follows I'm going to construct an assignment a for G by going through each vertex like you and what I'll do is I'll look over the cloud for you which is over here and a prime is assigning colors red green and blue to the vertices in this cloud and I'm just going to take the plurality color whichever one of red green or blue gets used the most by a prime in the cloud and I'm going to make that the assignment for you back in G so in this way from a prime and a Sun for G prime I've got an assignment a for G but um by definition a violates at least whatever a is it violates at least a badness of G fraction of constraints in G so I'd like monochromatic Li colors at least the let me call this epsilon epsilon fraction of edges in G so this a that I've gotten from a prime kind of messes up a reasonable fraction of these original edges in G and I want to conclude that a prime must be messing up a lot of edges as well in G prime may be the same fraction epsilon divided by some universal constant C c1 so here's the idea take an edge in like this original graph like maybe this edge here UV that this assignment a I've got violates I'm sorry I called it u prime take it a white edge u prime B prime that's violated back in the original graph a ok so this edge exists back in the original graph a and let's think about some possibilities what's going on with what a prime gives to you Prime and that a prime gives 2v well if a prime gives to you prime the same thing that a is giving to you which is to say if a prime as assignment to you prime is like the plurality assignment for this cloud and that plurality thing also happens for V Prime's cloud then you know a Prime's assignment to you prime is the same as A's assignment to you and same for V Prime but UV u prime V prime is a violated edge for a so then you know u prime V prime is also a violated edge for a prime and G prime so this violin age for a translates into a violated edge for a prime in G Prime when these two things occur on the other hand what if these things don't occur well then for at least one of them whether u primer of B Prime let's say u prime it means that a Prime's assignment to Yoona Prime is actually a minority assignment with respect to u Prime's cloud and so what that means is since overall a violates an epsilon fraction of edges original edges you can apply this sort of deduction to each such edge and conclude that either a constant fraction of the white edges over an a prime get violate it's like half epsilon of the AI just get white edges get violated or for the other or a constant times epsilon fraction of edges and hence vertices by regularity in G prime kind of our minority labelled by G Prime and what we want to deduce in this case is that like if a prime basically has creates like a lot of minority vertices vertices u prime that or assigned the minority label for their cloud then we want to argue that a lot of these yellow intra-cloud edges which have equality constraints by the way get violated now actually that might not happen in certain you might worry that that might not happen because it could be that for this cloud it has like a lot of minority assigned labels but they're all kind of like hanging together in like a small set where most of the cloud edges are interior to that and then they're like kind of all equal to each other this little like minority side set and maybe they don't have a lot of edges to them my majority or plurality vertices you don't get a lot of constraints because violated constraints but here's why we've used expanders this cannot happen in an expander so this cloud is an expander which basically means that every set of at most one-third of vertices has a lot of edges coming out of it it has high conductance that's the nature of an expander so whatever the plurality assigned vertices are they have a lot of edges coming out of them to the whatever the minority assigned where does these are they have a lot of edges going out to the plurality assigned vertices and therefore it is true that a lot of these equality constraints get violated so that uses the expansion property okay so and you lose like a constant factor because that's the nature and expander that has constant minimum conductance okay so that's the idea and if you don't follow all the details then well you can review them later but I want to show most of the details for this one okay so that's actually the end of degree reduction and we're gonna move on to expander eyes this one's gonna be a lot shorter this is a step lower we have a regular CSP now and nine regular CSP but we want to make an expander like globally an expander right now it's like made of these little expander clouds but we want to look like global you'd be an expander and if the original white graph was not an expander then this new degree reduced graph won't be an expander either but it's not gonna be too hard so our main effect is we want to make it a 17 regular expander and these are the side effects and here's the idea we have this graph now unlike some n vertices and has its like nine regular we're just going to take like any old eight regular and vertex graph called script II and just stick it on top just it's the dumbest possible thing just stick it on top and now we've stuck all these new edges on top this eight rate you'll expand ur on top we have to put constraints on these edges and we'll put like the most liberal possible constraints these constraints are just always satisfied it's like free be like no matter what assignment you put to the end points of this edge satisfied so okay let's see what's going on with these side effects so and also the main effect so the first thing is well the main thing to check is that the resulting graph is an expander so we took like an arbitrary nine regular graph and just slapped on top of it an expander that's eight regular it's okay that makes a seventeen regular graph that's fine and I claim that if you take any graph and stick an expander on top assuming they have about the same number of edges which they do because one's eight regular and one's nine regular then it's still an expander maybe the expansion gets worse by a constant factor and the point is e has some being an expander has some you know positive minimum conductance every set of vertices touches at least a point one fraction of D time the degree times the size of the vertex scent so does a lot of out edges so every vertex said in e plus G he also has like a lot of out edges because it at least has ease out edges you know in G all the edges could stay inside but you know G has only slightly more edges than e because G is nine regular a is eight regular so the convex sums can go down by a factor of this like basically half maybe a little bit worse and then if you like the expander or sorry the eigenvalue definition of expanders then you can conclude still that this is a expander with the eigenvalue definition because of chiggers inequality that says minimum conductance and second-largest eigenvalue of K prime gap are equivalent up to 1 is a constant if the other is a constant okay so that's how G prime becomes an expander you basically just double the number of edges slightly less so the constant size goes up by only a constant factor Yuliya let's just check these side effects you know these new constraints we added are always satisfied so like if you have a perfect assignment for the input G still have a perfect assignment for G prime you can just use it and the badness can only go down by a constant factor because you know really you've just added like roughly the same number of constraints as you had before that are freely satisfied but any assignment that violated you know 1% of the constraints in G will also violate it basically half times 1% of the constraints in G prime because okay G prime just has these sort of dummy you know half of half of the constraints are like G prime dummy constraints okay so that's it for expander eyes now I'm going to leave power and step three the powering step and come back to at the end because that's the most interesting part so I'm going to skip to step four and this is the part that I'm gonna sketch the most I'm really gonna just say the highest level words about this even though it's like a non-trivial aspect of the the theorem it's like maybe one third of the content is in this part so what was this mini PCP step the mini PCP step is if you recall at the end of the powering step you have this um oh there's a question what do you mean by stacking a graph I literally mean like you have a vertex set you had some original graph I had like n vertices in with some eight regular graph and now like just take another sorry was nine regular take an eight regular expander' on the same number of vertices in and just um you know call the vertices here 1 through n call the one that first sees there once or end and then just identify the vertices so you know and now every vertex has like its nine edges from the original graph and like the eight edges from the expander that use flap on top or if you think about anything in terms of adjacency matrices literally sum the two adjacency matrices okay hold that answer the question coming back to this mini PCP business the output of the powering step was a CSP that was kind of crazy I had this enormous domain size you were signing the vertices not red green and blue but some gigantic label set and had some complicated albiet binary constraints and the goal of this step is to get back to the three coloring constraint constraint satisfaction problem well okay let's take a look matchin this is this white stuff here is like just a little teeny piece of the input CSBG so remember these vertices in G are supposed to be labeled from this gigantic set W exponential in T so if you think of like writing writing down those assignments with bit strings you'll need like exponential and T many bits just to write down an assignment here remember to use a concept oh that's okay and the constraint on this edge you know it says something about how you're supposed to label these two vertices from this gigantic set domain you may be so complicated that you should sort of think about it is being expressed by circuit which takes in two to the order T bits for this label and to the aura T bits for this label and output zero or one depending on whether the constraint is satisfied now from this point of view uh as I mentioned before what you can try to do is rien khao this constraint with a PCP it's kind of a recursive idea nor the PCP theorem as we originally are trying to prove it sort of reduces circuits at to three coloring in such a way that like if you're like not satisfied the resulting three coloring instance is sort of highly not satisfiable there's some details here like it's not quite as easy as this and you actually need a slightly different object not a PCP but a PCP P probabilistic lis checkable proof of proximity that's more or less the same so I can only give you the high level ideas here but why is this not like circular argument it's not a circular argument because in the PCP theorem itself which is the thing we're trying to prove you want a polynomial time reduction that takes a circuit and in polynomial time outputs like a graph that you know has some relationship to the circuit but here we don't mind about efficiency at all because the size of the circuit and the number of bits here is a constant - to the order T so even if you have like a non efficient like exponential size or doubly exponential size PCP it's fine in fact this we're going to use a double exponential size PCP which is why we got this crazy blow-up here now there's an ax funny question here which is that normally in you know the world like this you know you're trying for a pong real-time construction and you know it's trivial to get an exponential construction or W exponential construction but it's actually not trivial at all here in fact it's not obvious that how to even do this it's not obvious how to get PCPs of any size because uh has to be like one construction of like three coding instance from a circuit which is kind of oblivious to whether the circuit is satisfiable or not like the reduction cannot like first decide if the circuit is satisfiable and do one of two things it's just be one transformation from circuit to possibly W exponential sized three cholera instance but which has the two properties about if it's satisfiable it's 3 colorable if it's not satisfiable it's far from 3 colorable so even though this looks like a much weaker task it's not obvious how to do it but prior to dinner's paper it was basically known how to do this and I can only give you like a little bit of ideas here but there's a not too hard construction of doubly exponential size pcp peas or these even these PCP Peas but I can't tell you about more but it uses to agree it's air cracking codes and for analysis of boolean functions so when I talked before you know since boolean functions many years ago like I gave this proof in like one lecture and let me just say the main step is the Fourier analysis of boolean functions and particularly the thing you need is almost identical to something that was on your homework homework 3 number 5 your Nobel Prize in Economics that part where you gave a free analysis of boolean functions based proof of arrows impossibility theorem remember the theorem about voting like if you have a three candidate election you know any voting scheme that like does all the three pairwise comparisons and tries to like figure out if a beats B or B B to C and C beats a and picks a winner arrows theorem is not like the only way you can do that is by having a dictatorial scheme so you prove that using for analysis of boolean functions and that's basically it's hard to explain why but this basically what you need for this mini PCP exactly that and somehow the idea is that like randomly having everybody vote and checking to see if there's a condor say winner like a beets B and that there's like one clear winner that beats the other two candidates it's kind of like a pcp for an election function being a dictatorship so some kind of task that only looks at the election function outcome on three different strings and it has one property there's always a Condorcet winner if F is a dictatorship and use for analysis to show that if you're close to always having a Condorcet winner then you must be close to being a dictatorship so it's kind of amazing but this like robust proof of Eros theorem that you get from for analysis of boolean functions it's kind of like the most important ingredient in this construction of the mini PCB so it's perhaps hard\", metadata={'source': 'ZB-8_HpAm3A'}),\n",
       " Document(page_content=\"okay so there's only one step left the key step the powering step which has this parameter T this is the part that increases the badness in a 3-coloring instance okay well it transforms to a different kind of CSV so the main effect of this powering step is to take a three CSP with certain amount of badness and increase its badness by a factor of T well it's you get a factor of T divided by some universal constant and the side effect will it'll blow up the size by a constant to the order T you can also give you this like kind of complicated CSP with a gigantic I'll be a constant size label set and still complicated binary constraints so let's see how this goes and then we'll be done with everything so here's the idea you take this three CSP G which has may have been made an expander and that's crucial for powering and the output CSP G prime is going to have the same vertex set it's just gonna have like many more edges and as I mentioned the powering operation is kind of like teeth powering the adjacency matrix so the edges aka constraints in the new CSP G prime will correspond basically to length t walks or length t paths in the input graph g you can basically think of g by the way as a three coloring instance it actually has some equality constraints in addition to the non equality constraints but basically like three klein and will i even do it like the lazy version of walks where you can also do a self-loop and so for every vertex in G & G prime although it had degree 17 and G it's gonna have a degree which is something like something like 18 to the T and G prime because it's gonna have like a constraint for every walk of length up to T and they'll be like you know 18 choices at each step because it's a 17 regular graph and you can also do a sub self loop and the new to new domain is gonna look like sort of strings over the symbols red green and blue of length like roughly 18 to the T like the number of these possible pads so that's why the domain size is this like three to the 18 to the T okay so that's the story in the size the domain and like basically now I want to tell you what the constraints are and why they achieve this main effect and that's gonna be everything okay so G prime same vertex set is G but like the edges flash constraints in G prime are gonna correspond to these like length T walks in the original graph and the domain is gonna be like these really long strings of like red green blue and I should also say that like I'm now gonna start to get quite hazy on a certain point these paths I said their length T but they're really like length around T and try not to stress out too much about like what does it exactly mean like around T it's like a bit of a point where you have to do some like tricks which I don't want to get into but just let them be like hazily what length exactly T but in the neighborhood of T okay so what is the meaning of this new domain size so in the G prime CSP you have some like some vertex let's call it u prime and think of a label for u prime I don't think of a string so much it's just like if like you primes label is supposed to represent an opinion a quote-unquote opinion on what use I guess you and you are probably in the same year I'm not sure I use different letters on what use neighbors a distance at most T should be labeled in the old graph okay so a label and G prime for u consists of like a bunch of labels of each label being red green blue for like use distance T neighborhood in the original graph let me try to draw a picture so let's say this is like a piece of the original graph G and let's say T is 3 for extreme simplicity so here's a path of length 3 in the graph and so this will become like an edge in G prime okay and what's I didn't say what the constraints are yet but I'm talking about the labels now so the point is that like the labelings instead of labeling the vertices by red green and blue you're now gonna relay boleyn by like these very complicated looking things like a label for a in omega prime is like an assignment of red green or blue to all of s neighbors at distance at most three so it might look like hey should be red you should be green C should be green you should be green he should be red why should be blue G should be blue and you know probably there's like more edges out here as well but I just drew a piece of it okay and in the CSP G prime G will also supposed to be this vertex here will also supposed to get like labels that look like a red green blue assignment for all of the distance at most T neighbors of G okay and I call these opinions and we'll use that terminology because that's how you think of them okay so this is what the new graph is it's got these like length t paths as its new edges this is what the new label set is it's like these big opinion sets what are the constraints well the constraints are actually the most natural thing so like this AG is now an edge and G Prime we're supposed to put a constraint on it about how these two kind of labels from Omega Prime should be given and what you basically do is check all the things I mean check everything that would be logical to check so the constraint checks a lot of things first of all it like you know a and G have some vertices that are potentially have some vertices at distance you know at most T from each of them so there's some vertices like let's say f that are like opined about by both a and G little G should have these little G is the key vertex but anyway so for example a is opinion on F is that it should be blue little G's opinion on F is that should be a green and this is contradictory so this assignment this example assignment would falsify or like would violate this constraint because it would fail this check but that's not all the constraint between a and G in this new ESP does some more for each edge UV in the original graph like let's say Eve F which is within distance T of both a and G if like a is close enough to e that it has an opinion on it and G is close enough to F that has an opinion on it then you check that a's opinion about e and G's opinion about F satisfy this EF edge back in G which is either a non equality constraint or an equality constraint so this is the definition of like these constraints actually you really only need these ones you don't actually need these ones but you may as well put them in okay so it remains to show that I've now defined for your G Prime and it remains to show that this powering step works that like the badness and G prime is bigger than the badness in G by a factor of T and I wrote like Omega of T to denote like T divided by the universal constant and again provided the badness is smaller than an absolute constant and I switched to putting show and quotes here because like I've glossed over enough stuff now that I'm really just sketching not showing and so the argument of similar to the argument we saw for degree reduce you take an assignment a prime for G prime and you say that like well I can get from an assignment a for G not assignment a for G has to be violating a badness fraction of edges and I'll deduce that a prime is satisfying like T times badness fraction of edges in in G prime so I like more slides sketching this and then we'll be totally done so this is what we got a show and here's the start of the proof don't panic ah so let a prime be the best assignment for G prime and so a prime is like for every vertex W a prime consists of like a bunch of opinions for like the red green and blue for the neighbors of W or the distance T neighbors of W so let's write a prime of you sub b4 w's opinion for these color so that's just red green or blue and now we're going to define an assignment a for the original free coloring type instance G a of a vertex is to figure out what a value for a vertex is you basically look at all the paths of length about T starting at V going to some vert different vertices W and each of these vertices W has an assignment a prime and since there are distance T like this a prime of W has an opinion about what V should be so like all of these like distance T or more or less neighbors have like an opinion about what quote-unquote opinion coming from a prime about what red green or blue value I should get can you take the plurality opinion okay so that gives us an assignment a for G now like a little bit of bookkeeping T is this distance parameter let epsilon 0 be 1 over T and let epsilon denote basically denote the badness of G whatever that happens to be except that if epsilon is if badness is already bigger than epsilon 0 then instead let epsilon denote epsilon 0 this is a minor point ok so we know by definition of badness that a this construction a that we got violates at least a badness fraction of edges back in G and a particular that badness is bigger than epsilon so it violates at least an epsilon fraction of constraints back in G so just pick out any fraction as set of exactly epsilon fraction of edges from G that are violated by a you know it's gonna be at least epsilon so just fix some set of fractional size epsilon and call that set F so this is a set of edges back in G violated by a and it's an epsilon fraction of edges and finally like before like for each of these edges we want to kind of show that it induces like kind of like tea time tea mmm bad path constraints and G Prime so this is our final goal here will do on think the last slide so this is our definition of the assignment for a and we have this fixed set of excellent fraction of edges back in G that are violated by a now consider the following sort of thought experiment pick a random edge from G that's in this violated set F I don't think of a sustained for fail ok this edge is also in in G prime and now form a random path of length around T by doing like a walk of length around T from you and let's say it ends at a and also ran a walk from B of length around T let's say is ends at B now this walk this random path itself full of length like about 2 T but like please gloss over this fact please like don't worry about the fact that like well this path is length T this path has length T but I'm also gonna try to claim that this whole path is length T they're all around T like don't don't worry about that part too much so this is like kind of a thought experiment for every edge UV and F imagine sort of extending it to like a length around T path in G prime now this is also the procedure that's used to define a you see to get A's assignment to well let's look at B to get us a is assignment to V by definition we kind of take a all the paths or like a random path from V of length around T and somewhere let's say B and then look back at Bea's opinion on what we should get red green or blue so sort of by definition of this plural and assignment when you like kind of take a random path of length around T starting from B you get somewhere B and you look back what a prime of B says you should assign to V by definition of plurality like there's at least a 1/3 chance that that opinion will be the plurality vote so what I'm saying here is there's like a one-third chance in this thought experiment that a prime of Bees opinion about V will be the same as the plurality a of V and similarly for you and when that happens and this happens with a 1/9 chance over this choice of random 2 paths or this random path from A to B a prime violates the a B constraints that exists in G prime so we have this constraint it's a path of length around T so we have a constraint between a and B and G Prime and one thing that is checking is that the assignment a prime that a the assignment a prime to a that its opinion on you and a Prime's assignment to B's opinion about B have the property that they satisfy this edge but if they agree with the assignment a and you V is an edge that a violates then a prime is violating this A to B path constraint and that happens for like 1/9 of all these pads so thus and I put this in quotes because I'm really getting a bit sketchy here but this is exactly how the proof goes with a little bit of care the overall badness the fraction of path constraints violated by this best assignment a prime for G prime is at least 1/9 the fraction of length T or length T or so paths in the graph that pass through this set F of epsilon fraction of constraints violated by a and this looks pretty good all we need to show is that this is at least you know T times Epsilon it feels good because um you know you have these paths of length T that have our on T edges on them and intuitively an epsilon fraction of all the edges are these like you know violated edges from F which is and so like you know you get a violation in G prime by a prime whatever a one of these random paths like passes through like the set F 50 edges these paths and an epsilon fraction the edges are bad so you kind of expect T times Epsilon chance of a bad path in G prime I'll say that in more detail at the bottom but the only thing that could go wrong here is that you know this set of violated edges F they could be all like cornered up in like one piece of the graph and maybe most paths in G evade this set of violated edges F well this cannot happen finally because it's an expander graph which is the whole point of expander izing so the picture here the high-level picture is you have these edges in F they constitute an epsilon fraction of G these are the yellow edges and like you pick a random path of length about T and now since you made G and expander the edges and this random path kind of look like they're distributed uniformly like this is the key property of expander graphs and like spectral bounds and if you do like a random walk like pretty quickly you get to the stationary distribution in like a typical edge that you traversed looks just like a random edge in the graph okay so you need the expander mixing lemma to actually formalize this but the point is you know if you just now because it's an expander you can sort of pretend that each edge in this random lengthy path is like a uniformly random edge and therefore it has an excellent chance of passing through one of these violations F and so the probability of a random path which is like a random constraint in G prime missing F completely is like 1 minus epsilon to the power of T which you know by our original lecture 2 or whatever about approximations we know 1 minus epsilon to the T is basically the same as 1 minus T times Epsilon well provided you know epsilon is smaller than one of our T but it is by this set up so this says that the probability of random path mrs. F is like 1 minus T Epsilon so the probability a random path hits f is like T Epsilon and like a we know that like if you hit a random edge you sort of have like a one night buzzer if you are in a path hits F it has like a 1/9 chance that it's endpoints are violated get assignments that violate the constraint being put on this path okay so you get this divided by 1/9 and that completes the proof sketch so sorry for going quite a bit over time but I'm done this completes the proof sketch of the PCP theorem it also completes the course so that's the end that's our last lecture of cs3 toolkit and yeah thanks a lot for sticking through it it was quite a memorable semester usually like you know in the last semester or the last lecture of like a course oh I like to you know bring in like doughnuts or like croissants or whatever but I cannot do that because you're all at home in a lockdown so the next time I see you it's it's on me so next time I see you in the fall or whenever remind me of this and we'll go down the tossa and get like a chocolate person or something and we can reminisce about see a theory toolkit okay so as always I will stop there and about stick around to answer any questions you might have\", metadata={'source': 'bw7OQN7TubI'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_theory_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Music] thank you hi my name is Greg durat I'm really excited to introduce you to my course natural language processing I'm an assistant professor in the department of computer science here at UT Austin and I've been working in the field of natural language processing for about 12 years now and it's been a really exciting 12 years and we've seen a progression all the way from linear models uh things like classifiers up through the latest and greatest systems like Chachi BT so if you've played around with Chachi PT you know that it can do things like this we can ask a question like how many years after the start of the Revolutionary War did George Washington step down as president and the system will do its computation and it will produce this answer that has to look up these two dates and take the difference between them and figure out this number of years so Chachi BT is a large neural network language model but what's a language model what kind of neural network is being used here what can it be used for how is it trained these are all kinds of questions that you might have that are going to get answered in this course so the goal is to start all the way from ml fundamentals like classification and then build up to the tools that we need to understand systems like Chachi BT we're going to cover all of the methods that we need here and we're also going to understand the applications that modern NLP research is used for and we're going to give you the tools to understand how to take these systems and apply them to real world problems so when you're faced with something down the road you know what to use the lectures for this course are going to walk you through a lot of these different modeling details and show you how these are actually built from the ground up here's an example of what one of the lectures is going to look like so their formula involves q k and V which are computed based on the kind of input embeddings like we've been doing multiplying by these parameter matrices WQ WK and WV and we take this soft Max of Q times K transpose so that we've all seen and then there's one extra little step that they do here which is they divide and rescale everything by square root of DK which is the basically Vector Dimension that all of these uh inner products are happening in it basically just has the effect of making the softmax less peaked it's not too important and then they multiply by V here in order to get the output so they're taking their uh attention Matrix a and multiplying that by V and so what they're getting is the output is a kind of weighted combination of V in addition to the lectures we also have a number of programming assignments which are designed to give you hands-on experience you're going to be actually building these models basically from scratch in pi torch so you're going to understand all the pieces from the ground up and you're also going to understand the data and how important that is in these processes so a big emphasis of the course is going to be thinking about data you can look at the syllabus in the course website for more details I'm really excited to teach this course and I hope to have you as a student thank you\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_course_transcripts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"welcome to 15 751 I suppose we prefer this title TCS toolkit my name is Ryan O'Donnell you can call me Ryan and his lecture one is the only lecturer which has slides which is nice because you can see we could not control the lights correctly and it's very bright on the screen but that'll be good when we're using the blackboard which we'll be doing in all subsequent lectures well be you know proving Sterling's formula and stuff on the blackboard but today in this lecture its signature luxury let it lecture so I'm just gonna like tell you some stuff about the course hopefully you can read the screen ok so today's lecture will be divided into three parts part one and I'll tell you just some stuff about the course and then oops part two I'll tell you how to TCS and in part three we'll talk about street-fighting mathematics so it'll be a little bit math at the end of this lecture okay so let's dive right in I'll start by telling you about the course and you know what we're gonna do in the the next 15 weeks so the title of the course is TCS toolkit so we can just figure out what it's about by going through the two words in the title first one is TCS that stands for theoretical computer science naturally there's lots of topics that are at the intersection of computer science and theory that we will not be talking about in this course so for the purpose of this course CCS means algorithms and computational complexity so as I said there's lots of other things that's theoretical and mathy and computer science like logic and PL and verification and so forth but we won't be talking about those things there's actually a nomenclature division between theory a and theory be so algorithms in combination complexity is theory a and the other stuff is Theory B and there's no like ranking or anything it just came from the fact that these two really thick books called handbook of theoretical computer science volume a which is about this stuff and handbook of theoretical computer science vol B which is about the other stuff and we'll be doing the a stuff another way to put it if you're kind of be in the know if you're already a little bit sophisticated is this the stock Fox topics if you're not in the now the two main conferences in which all sort of the top research in this field gets published are called stock symposium on theory of computation and Fox foundations of computer science and so somehow these two conferences that haven't once a year are used as a shorthand for the set of topics that we'll be talking about great so that's TCS and toolkit well that's a little bit self-explanatory so it's toolkit refers the fact that I want to give you an intro to some of the tools and topics especially mathematical tools and topics that arise in TCS research so in fact this course is gonna be maybe like 50% math topics and 50% like computer science topics and you know if you had 28 additional lectures as I do to talk about math and computer science topics relevant for TCS research then you know you could choose some subset of you know a hundred possible topics to talk about and so the ones that I'm choosing are inevitably biased by my own interest so we're probably gonna get you know more I don't know probability and complexity theory then you would get from some other lecturer but that's how it's gonna be and another thing I want to emphasize about this course is there's a bit of a warning it's more reps than depth and actually not all students are you know like this fact some people would prefer a course that you know goes into more depth about fewer topics and this course some extent goes into less depth about more topics that's the way I've designed it and as I said it's a bit of warning if that's not your taste well be prepared for it if you're officially a CMU undergrad it's not completely dissimilar form like the course 251 but for graduate students which is I guess a little nod to that in the the number of the course 751 okay so when I spring this course well back you know what I was really thinking about is the following you know if you're getting into research in TCS or if you're reading a CCS paper or if you're attending a TCS talk there's some level of background knowledge that usually the speaker or writer sort of assumes the audience knows you know they kind of assume you know usually what does the Chernoff bound or like what is the linear program and you know you may not necessarily know that you might not necessary have learned that as an undergraduate and so I was trying to you know scoop up a bunch of such topics that are considered sort of background knowledge all those sort of things that mathematical things especially that you may need to know to try to follow a TCS talk or read a TCS paper and Alfred who this class is for well on one hand it could be for anybody that's interested in doing TCS research again when it's designing the course like the Platonic ideal of like a student I had in mind was maybe like a first-year graduate student that's either going to intending to do CCS research or at least has an interest in TCS and TCS research so it's court who initially the course was designed for and I should finally add that it's a very matthew course so the mathematical maturity is important so I mean a lot of its gonna be on the board you know definition theorem proof and you're gonna be required to write you know proofs in your homework so that's a little bit of either a warning or an enticement if like me you really love math okay so I wanted to flush up here a little bit of the syllabus roughly speaking this is a bit tentative and maybe not exactly precise but roughly speaking the topics of the course kind of fit into like seven units each of which will have between three and five lectures so this is kind of the approximate syllabus for the remaining 28 lectures so we'll talk about asymptotics and probability talk about Fourier transforms in some applications we'll talk about some elementary algebra it's applications spectral graph theory the straight satisfaction programs and linear programming and SDP hierarchies information and learning theory and some hardness computational hardness so these are the topics that you can look forward to and as I said there's many other topics I could have chosen but unfortunately we only have so many so many weeks in so many lectures this is what we'll hear about and the first thing I will tell you a little bit about is of course the logistics for the course I actually I'm not going to spend too much time talking about this out loud because I'd rather spend the time talking about non nuts-and-bolts things the main thing I have to tell you is this website which you must go to so there's a website called dinner oh this is developed by two professors here at CMU and it's kind of going to be the combination of the webpage for the course and the the Piazza or the Bolton Baltimore for the course canvas whatever you want to call it it's all rolled into a dinner oh so maybe you've used it before maybe you haven't but it's essential that all of you go to it and find everything there so feel free to do that on your phone or your computer while I'm rambling up here this where you'll find like all the announcements like you know there's a correction to the problem set or has changed the due dates so you definitely want to watch out for that that's where you can ask questions about homework or about the lecture so you can find the course policy in details and where you get the homework and so forth so make sure you're enrolled in that speaking which as I said I will not you know read the entire syllabus and course policy details here but it's definitely obligatory that you do that yourself I don't know they'll take you to ten minutes or twenty minutes or seven minutes or something but please definitely go online to dinner I'll find it and read all details so you know everything there is to know about the course policy and actually ask for homework it's a little bit of a warning we're gonna have six homeworks and the first one's gonna be a little bit short and just for your information it'll be due and you know ten days or something it'll come out on Thursday so it's just to get you ready for action in this course right so as for grades this is how it's going to break down the main component of your grade is going to be from homework there's going to be basically six homeworks but the first one is a little bit short so there are each worth 12% and that adds up to 68 percent there's also going to be a written project which I'll tell you about later in the course the due date for that is like April 3rd or something and for the beginning of the course just focus on doing the homeworks and I'll tell you about how to start doing that project asynchronously later basically the short story is that you'll be writing some a page document about like another topic such as I might have lectured about but didn't actually after those are turned in you'll also be writing a short review of two of your classmates projects your project will not be graded by your classroom classmates but you know will grade the reviews and these will also be taken into account when we raise your projects this is partly done to just have you give you the right mindset about who your audience is you're writing this project for your classmates and another component of that class is seminar attendance you can find more details about that again in the course policies but in short you'll be required to go to three theory talks from our regular weekly seminar series the ACL seminar theory launch seminar and theory seminar that counts for some of your grade and class participation is the other 3% this is a 12 unit course which means you're supposed to send 12 hours a week on it so this is in my mind how I thought to imagine you would break it down your mileage may vary you can see your main effort I think will be on doing the homework there's a lot of long homework in this course so maybe a couple hours every other day and the other ones don't have like or not for the whole time so maybe these are amortized and of course will be in lecture for 3 hours per week I think this adds up to 12 so I think there are more details online by any questions right now yeah yes all homeworks must be written up in my tech I'll mention that working actually for the first a a bunch of questions I'll ask you your name I may ask you your name even if I know your name because I want to disguise the fact when I don't you know your name even though I should know your name so what's your name Jennifer right I didn't know that Jeff was in my class last semester okay\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_theory_transcripts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8036.622448979592"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(t.page_content) for t in nlp_course_transcripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22071.5871559633"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(t.page_content) for t in cs_theory_transcripts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1024\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=embedding_model, dimensions=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=500,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_chunks = text_splitter.split_documents(nlp_course_transcripts)\n",
    "cs_theory_chunks = text_splitter.split_documents(cs_theory_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_embeddings = np.array(embeddings.embed_documents([chunk.page_content for chunk in nlp_chunks]))\n",
    "cs_embeddings = np.array(embeddings.embed_documents([chunk.page_content for chunk in cs_theory_chunks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01747688, -0.00596334,  0.0323505 , ..., -0.0323505 ,\n",
       "         0.0387855 ,  0.00395606],\n",
       "       [ 0.00384601,  0.02023843, -0.00394154, ..., -0.02280008,\n",
       "         0.00653503,  0.00516221],\n",
       "       [-0.01835858,  0.02080639,  0.02186528, ..., -0.01348359,\n",
       "         0.05363181, -0.00238249],\n",
       "       ...,\n",
       "       [ 0.04120168,  0.03338476, -0.01041442, ..., -0.04100626,\n",
       "         0.0281572 , -0.01683895],\n",
       "       [ 0.04499371,  0.03227453,  0.05284871, ..., -0.05215852,\n",
       "         0.01587433, -0.06099951],\n",
       "       [ 0.04200813,  0.04408563,  0.05706996, ..., -0.04909605,\n",
       "         0.02888632, -0.0479962 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(537, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = np.concatenate([nlp_embeddings, cs_embeddings], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((537, 1024), (1621, 1024), (2158, 1024))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_embeddings.shape, cs_embeddings.shape, all_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "all_vectors_tsne = tsne.fit_transform(all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(all_vectors_tsne[:len(nlp_embeddings), 0], all_vectors_tsne[:len(nlp_embeddings), 1], label=\"NLP Course\")\n",
    "plt.scatter(all_vectors_tsne[len(nlp_embeddings):, 0], all_vectors_tsne[len(nlp_embeddings):, 1], label=\"CS Theory\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 output_size):\n",
    "        \n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        # print(out)\n",
    "        # print(out.shape)\n",
    "\n",
    "        return out\n",
    "    \n",
    "input_size = embedding_size\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "classifier = Classifier(input_size, 100, 2)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "X = torch.tensor(all_vectors, dtype=torch.float32)\n",
    "y = torch.tensor([0]*len(nlp_embeddings) + [1]*len(cs_embeddings), dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1726])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=1024, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:00, 159.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.686926543712616\n",
      "Epoch 1 Loss: 0.6775022149085999\n",
      "Epoch 2 Loss: 0.6688109636306763\n",
      "Epoch 3 Loss: 0.660183310508728\n",
      "Epoch 4 Loss: 0.6511101126670837\n",
      "Epoch 5 Loss: 0.6415942311286926\n",
      "Epoch 6 Loss: 0.6317020058631897\n",
      "Epoch 7 Loss: 0.6215692162513733\n",
      "Epoch 8 Loss: 0.6113398671150208\n",
      "Epoch 9 Loss: 0.6010866761207581\n",
      "Epoch 10 Loss: 0.5908772349357605\n",
      "Epoch 11 Loss: 0.580787718296051\n",
      "Epoch 12 Loss: 0.5708993673324585\n",
      "Epoch 13 Loss: 0.5612822771072388\n",
      "Epoch 14 Loss: 0.5519810318946838\n",
      "Epoch 15 Loss: 0.5430375337600708\n",
      "Epoch 16 Loss: 0.5344750881195068\n",
      "Epoch 17 Loss: 0.5262957215309143\n",
      "Epoch 18 Loss: 0.518460214138031\n",
      "Epoch 19 Loss: 0.5109170079231262\n",
      "Epoch 20 Loss: 0.5036114454269409\n",
      "Epoch 21 Loss: 0.49649176001548767\n",
      "Epoch 22 Loss: 0.48951125144958496\n",
      "Epoch 23 Loss: 0.4826275408267975\n",
      "Epoch 24 Loss: 0.4758051037788391\n",
      "Epoch 25 Loss: 0.4690207540988922\n",
      "Epoch 26 Loss: 0.4622635245323181\n",
      "Epoch 27 Loss: 0.4555366635322571\n",
      "Epoch 28 Loss: 0.44885948300361633\n",
      "Epoch 29 Loss: 0.44226494431495667\n",
      "Epoch 30 Loss: 0.4357984662055969\n",
      "Epoch 31 Loss: 0.42951008677482605\n",
      "Epoch 32 Loss: 0.4234471023082733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:00<00:00, 163.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Loss: 0.4176523983478546\n",
      "Epoch 34 Loss: 0.41215407848358154\n",
      "Epoch 35 Loss: 0.40696585178375244\n",
      "Epoch 36 Loss: 0.4020874798297882\n",
      "Epoch 37 Loss: 0.39750584959983826\n",
      "Epoch 38 Loss: 0.393202006816864\n",
      "Epoch 39 Loss: 0.38915395736694336\n",
      "Epoch 40 Loss: 0.385336697101593\n",
      "Epoch 41 Loss: 0.3817296624183655\n",
      "Epoch 42 Loss: 0.3783169388771057\n",
      "Epoch 43 Loss: 0.37509047985076904\n",
      "Epoch 44 Loss: 0.3720460534095764\n",
      "Epoch 45 Loss: 0.36917784810066223\n",
      "Epoch 46 Loss: 0.36648011207580566\n",
      "Epoch 47 Loss: 0.3639465272426605\n",
      "Epoch 48 Loss: 0.3615703582763672\n",
      "Epoch 49 Loss: 0.35934314131736755\n",
      "Epoch 50 Loss: 0.3572556972503662\n",
      "Epoch 51 Loss: 0.3552982807159424\n",
      "Epoch 52 Loss: 0.3534606695175171\n",
      "Epoch 53 Loss: 0.3517334461212158\n",
      "Epoch 54 Loss: 0.3501072824001312\n",
      "Epoch 55 Loss: 0.34857410192489624\n",
      "Epoch 56 Loss: 0.3471270203590393\n",
      "Epoch 57 Loss: 0.34576016664505005\n",
      "Epoch 58 Loss: 0.3444681763648987\n",
      "Epoch 59 Loss: 0.34324660897254944\n",
      "Epoch 60 Loss: 0.3420915901660919\n",
      "Epoch 61 Loss: 0.34099963307380676\n",
      "Epoch 62 Loss: 0.33996739983558655\n",
      "Epoch 63 Loss: 0.33899152278900146\n",
      "Epoch 64 Loss: 0.3380686342716217\n",
      "Epoch 65 Loss: 0.33719581365585327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [00:00<00:00, 139.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Loss: 0.33636990189552307\n",
      "Epoch 67 Loss: 0.3355875611305237\n",
      "Epoch 68 Loss: 0.3348458707332611\n",
      "Epoch 69 Loss: 0.33414193987846375\n",
      "Epoch 70 Loss: 0.3334730863571167\n",
      "Epoch 71 Loss: 0.33283695578575134\n",
      "Epoch 72 Loss: 0.33223146200180054\n",
      "Epoch 73 Loss: 0.3316546082496643\n",
      "Epoch 74 Loss: 0.33110466599464417\n",
      "Epoch 75 Loss: 0.33058005571365356\n",
      "Epoch 76 Loss: 0.3300792872905731\n",
      "Epoch 77 Loss: 0.3296010494232178\n",
      "Epoch 78 Loss: 0.32914409041404724\n",
      "Epoch 79 Loss: 0.32870733737945557\n",
      "Epoch 80 Loss: 0.32828953862190247\n",
      "Epoch 81 Loss: 0.32788971066474915\n",
      "Epoch 82 Loss: 0.3275068700313568\n",
      "Epoch 83 Loss: 0.3271399736404419\n",
      "Epoch 84 Loss: 0.3267882168292999\n",
      "Epoch 85 Loss: 0.3264507055282593\n",
      "Epoch 86 Loss: 0.3261265456676483\n",
      "Epoch 87 Loss: 0.3258151113986969\n",
      "Epoch 88 Loss: 0.32551565766334534\n",
      "Epoch 89 Loss: 0.3252274692058563\n",
      "Epoch 90 Loss: 0.32495003938674927\n",
      "Epoch 91 Loss: 0.3246828317642212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 153.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Loss: 0.32442522048950195\n",
      "Epoch 93 Loss: 0.3241768181324005\n",
      "Epoch 94 Loss: 0.3239371180534363\n",
      "Epoch 95 Loss: 0.3237057626247406\n",
      "Epoch 96 Loss: 0.3234822750091553\n",
      "Epoch 97 Loss: 0.3232663869857788\n",
      "Epoch 98 Loss: 0.323057621717453\n",
      "Epoch 99 Loss: 0.32285577058792114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classifier(X_train)\n",
    "    # loss = criterion(outputs.squeeze(1), y_train.to(torch.float32))\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9930555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "outputs_test = classifier(X_test)\n",
    "_, predicted = torch.max(outputs_test, 1)\n",
    "\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       110\n",
      "           1       0.99      1.00      1.00       322\n",
      "\n",
      "    accuracy                           0.99       432\n",
      "   macro avg       1.00      0.99      0.99       432\n",
      "weighted avg       0.99      0.99      0.99       432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(p):\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "all_outputs = classifier(X)\n",
    "entropy = np.array([shannon_entropy(x) for x in all_outputs.detach().numpy()])\n",
    "all_chunks = np.array([x.page_content for x in (nlp_chunks + cs_theory_chunks)])\n",
    "\n",
    "df = pd.DataFrame({\"entropy\": entropy, \n",
    "                   \"text\": all_chunks, \n",
    "                   \"label\": y.numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entropy</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>high_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.997796</td>\n",
       "      <td>okay you don't have to suppose that that's jus...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.983612</td>\n",
       "      <td>kind of element here is that when you have the...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.974104</td>\n",
       "      <td>of commute the maxes past all the terms that d...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>0.966993</td>\n",
       "      <td>essays things like that to what extent is this...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.943698</td>\n",
       "      <td>w minus one of the last two items in it and we...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.942415</td>\n",
       "      <td>and all the other scores are going to be minus...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.934243</td>\n",
       "      <td>in details and where you get the homework and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.930708</td>\n",
       "      <td>comes from and if we plot this for different v...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.895031</td>\n",
       "      <td>right and then we're gonna have to have this e...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.883362</td>\n",
       "      <td>just not showing them here so as a good little...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.882746</td>\n",
       "      <td>of vectors a priori but we are taking this fix...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>0.882419</td>\n",
       "      <td>that seems reasonable i i mean that that sound...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.856571</td>\n",
       "      <td>you know it maps things into the range from mi...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.852169</td>\n",
       "      <td>and so let's say our data is the following rem...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.834222</td>\n",
       "      <td>just the maximum likelihood estimate of the pa...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.829519</td>\n",
       "      <td>associated with this well we have to look at t...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0.819251</td>\n",
       "      <td>those are turned in you'll also be writing a s...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.818722</td>\n",
       "      <td>the gray here is the user and the blue here is...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>0.805822</td>\n",
       "      <td>coding will be up because there are actually a...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>0.803176</td>\n",
       "      <td>or somehow racist symbol we'll just imagine th...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       entropy                                               text  label  \\\n",
       "84    0.997796  okay you don't have to suppose that that's jus...      0   \n",
       "398   0.983612  kind of element here is that when you have the...      0   \n",
       "266   0.974104  of commute the maxes past all the terms that d...      0   \n",
       "535   0.966993  essays things like that to what extent is this...      0   \n",
       "317   0.943698  w minus one of the last two items in it and we...      0   \n",
       "296   0.942415  and all the other scores are going to be minus...      0   \n",
       "542   0.934243  in details and where you get the homework and ...      1   \n",
       "41    0.930708  comes from and if we plot this for different v...      0   \n",
       "268   0.895031  right and then we're gonna have to have this e...      0   \n",
       "318   0.883362  just not showing them here so as a good little...      0   \n",
       "121   0.882746  of vectors a priori but we are taking this fix...      0   \n",
       "1917  0.882419  that seems reasonable i i mean that that sound...      1   \n",
       "83    0.856571  you know it maps things into the range from mi...      0   \n",
       "259   0.852169  and so let's say our data is the following rem...      0   \n",
       "151   0.834222  just the maximum likelihood estimate of the pa...      0   \n",
       "297   0.829519  associated with this well we have to look at t...      0   \n",
       "543   0.819251  those are turned in you'll also be writing a s...      1   \n",
       "440   0.818722  the gray here is the user and the blue here is...      0   \n",
       "1226  0.805822  coding will be up because there are actually a...      1   \n",
       "1119  0.803176  or somehow racist symbol we'll just imagine th...      1   \n",
       "\n",
       "      high_entropy  \n",
       "84            True  \n",
       "398           True  \n",
       "266           True  \n",
       "535           True  \n",
       "317           True  \n",
       "296           True  \n",
       "542           True  \n",
       "41            True  \n",
       "268           True  \n",
       "318           True  \n",
       "121           True  \n",
       "1917          True  \n",
       "83            True  \n",
       "259           True  \n",
       "151           True  \n",
       "297           True  \n",
       "543           True  \n",
       "440           True  \n",
       "1226          True  \n",
       "1119          True  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"entropy\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O texto de maior entropia, pertence às aulas de NLP, e explica sobre a tangente hiperbólica. Talvez por trazer muitos cálculos e nomes de variáveis, o modelo tenha se confundido com as aulas de Teoria da Computação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"okay you don't have to suppose that that's just true tan h of 1 we're going to say is approximately equal to 1 and tan h of 2 is approximately equal to 1 as well so that's it's it's not quite true but um from the purposes of keeping the math simple uh this will let us illustrate uh the kind of point here um and most neural networks by the way do have this uh this you know this property with when you use units like tan h where there's a kind of saturation um and that's kind of why uh that's part of the reason why the term neural networks came about is it's sort of motivated from the idea of action potentials and neurons which is that you know you basically get this kind of buildup and then the neuron fires um but it's this kind of binary thing where um you know it it it sort of fires in a discreet way so here it's like once you get enough activation um you get the value of 1 but then kind of increasing it additionally doesn't do anything all right so now z in this case is going to be uh the okay so uh sorry the example that we're going to use here is going to be a simplified version of the good bad example that's going to look like this and now z is going to be tan h of x1 tan h of x2 and tan h of x1 plus x2 okay where did this come from so remember that uh we're multiplying v by uh f of x in this case f of x is two features uh it's x one and x two and so uh and then we're applying g to it right and then that's going to give us z so when we do all that we get now a three coordinate z that has this property okay so now what we're doing is we're transforming this original feature space into the following one so this point at the origin stays at the origin what happens to the pluses that are along these coordinate axes well the plus that has a value of 1 for x1 gets a value of 1 for 0 z1 0 for z2 and 1 for z3 so that plus ends up here um the other plus uh does exactly the same thing and then uh this is the part that's kind of impossible to draw but the there's a minus\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[84].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"in details and where you get the homework and so forth so make sure you're enrolled in that speaking which as I said I will not you know read the entire syllabus and course policy details here but it's definitely obligatory that you do that yourself I don't know they'll take you to ten minutes or twenty minutes or seven minutes or something but please definitely go online to dinner I'll find it and read all details so you know everything there is to know about the course policy and actually ask for homework it's a little bit of a warning we're gonna have six homeworks and the first one's gonna be a little bit short and just for your information it'll be due and you know ten days or something it'll come out on Thursday so it's just to get you ready for action in this course right so as for grades this is how it's going to break down the main component of your grade is going to be from homework there's going to be basically six homeworks but the first one is a little bit short so there are each worth 12% and that adds up to 68 percent there's also going to be a written project which I'll tell you about later in the course the due date for that is like April 3rd or something and for the beginning of the course just focus on doing the homeworks and I'll tell you about how to start doing that project asynchronously later basically the short story is that you'll be writing some a page document about like another topic such as I might have lectured about but didn't actually after those are turned in you'll also be writing a short review of two of your classmates projects your project will not be graded by your classroom classmates but you know will grade the reviews and these will also be taken into account when we raise your projects this is partly done to just have you give you the right mindset about who your audience is you're writing this project for your classmates and another component of that class is seminar attendance you can find more details about that again\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[542].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entropy</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.008065</td>\n",
       "      <td>that you writing this or someone reading it ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>0.008237</td>\n",
       "      <td>square is minus one or like here we're calling...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>0.008506</td>\n",
       "      <td>the existence of all these fields okay the fac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0.009104</td>\n",
       "      <td>turned out to be pretty hard after all yep ah ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0.009132</td>\n",
       "      <td>that but I leave it as an exercise for you not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       entropy                                               text  label\n",
       "277   0.008065  that you writing this or someone reading it ma...      0\n",
       "1064  0.008237  square is minus one or like here we're calling...      1\n",
       "1063  0.008506  the existence of all these fields okay the fac...      1\n",
       "850   0.009104  turned out to be pretty hard after all yep ah ...      1\n",
       "673   0.009132  that but I leave it as an exercise for you not...      1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"entropy\", ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O chunk com menor entropia (que o modelo tem a maior certeza que pertence à classe de NLP), é sobre um texto que fala sobre PoS Tagging e Redes neurais, então faz sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"that you writing this or someone reading it may not even resolve for themselves and so uh how can we expect a model to necessarily come to a decision about one of these tags to use here okay so I'm going to switch gears a little bit and talk about uh a different way of producing part of speech tags which is coming back to using neural Nets here and in particular feed forward neural networks so rather than appeal to part of spe these like hmm taggers we can instead go back to this idea of using something that looks more like a classifier we can take uh these words around uh the word that we're tagging here so we look at the previous word the current word the next word we embed them into vectors we can catenate those into a feature Vector here and then we're going to put that into a neural network uh and so again what we're doing here is we're being very careful to capture this positional position sensitive information we're looking at the previous word the current word and the next word we're not using some kind of bag of words thing or that deep averaging Network where you like add everything together here we're concatenating these three vectors uh and so Yan Batha had all looked at this for a number of NLP tasks and they did something a little bit more clever which was they also uh looked at uh Byram and character byrs and trigrams and had a way of incorporating those into the model as well um and then that all gets kind of mixed together in a hidden layer and then uh the model makes some sort of prediction and this actually turns out to work pretty well across a whole range of languages so they compared it to previous work that was doing recurrent neural networks and found that it was it was better than that work and uh you can see on the right here that uh across a whole bunch of different languages the accuracies are you know at least above 90% in all cases um again the kind of you know the error rates differ a bit and uh this isn't comparable to the other data\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[277].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['high_entropy'] = df['entropy'] > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df['high_entropy']==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(all_vectors_tsne[:len(nlp_embeddings), 0], all_vectors_tsne[:len(nlp_embeddings), 1], label=\"NLP Course\")\n",
    "plt.scatter(all_vectors_tsne[len(nlp_embeddings):, 0], all_vectors_tsne[len(nlp_embeddings):, 1], label=\"CS Theory\")\n",
    "plt.scatter(all_vectors_tsne[df['high_entropy'] == 1, 0], all_vectors_tsne[df['high_entropy'] == 1, 1], label=\"High Entropy\", color=\"black\", marker=\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houveram muitos chunks com alta entropia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_centroid = np.mean(nlp_embeddings, axis=0)\n",
    "cs_centroid = np.mean(cs_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024,), (1024,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_centroid.shape, cs_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks mais próximos ao centroid do curso de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "and feed them into a classifier and so we can we can basically take a labeled data set of part of speech instances treat each position in every sentence as a example and feed that and train on that and produce a classifier all right so what goes wrong the the kind of problem is that we're not making use of the output structure at all so for example we had different possibilities for raises and interest rates right and it turns out that some of the edges that we kind of considered here are not good like for example we're not typically going to have a plural noun followed by a noun we're also not typically going to have a vbz followed by a vp of vbp and so the the predictions of a classifier may be incoherent uh meaning kind of locally each prediction looks reasonable right but the overall structure doesn't kind of add up to be the thing that we want and so this is going to lead us into our idea of sequence modeling and we're going to think about two models hidden markov models and conditional random fields these are going to be two things that we're going to spend some time unpacking throughout the next section of the class and essentially these are going to be two kind of contrasting ideas for how to deal with this hidden markov models are going to be our first example of a generative model and the they're gonna look a little bit different from other things we've done but they have some attractive properties uh conditional random fields are gonna look a little bit more like building off of this classifier view of uh tagging but uh making the output predictions coherent that's the end of this segment you\n",
      "Similarity: 0.7986732096922525\n",
      "\n",
      "Chunk 2:\n",
      "just with k equals 1 we're just kind of extracting these adjacent pairs of words we form this big training set of these word context Pairs and now we want to maximize the probability of the the sum of the log probabilities of The observed pairs so the thing I will say that is a bit different from sentiment is that this is an imp I'm going to say quote unquote impossible problem we're never going to be able to uh uh well I'll say we can't we can't drive the probability to one we're never going to be able to fit this distribution perfectly or I guess we're never going to be able to completely optimize this objective from the standpoint of of getting every Pro prediction to be of probability one um because there's going to be uh many words that occur in the context of a single word X so you're going to have these like conflicting training examples so unlike classification where it's totally reasonable to assume that you can fit the data perfectly um and in many cases a big enough neural network will uh here that's not going to happen and uh the the last thing I'll point out here is that uh we are going to initialize our parameters randomly so again we've talked about for neural networks how important it is to have good initialization here we're just going to kind of throw out some random vectors and then iterate over this data and what happens roughly is that you know the model is going to kind of pull you know similar vectors together over time because they're going to be seen in similar contexts and optimizing this objective is going to give us vectors of the sort that we want so this is the basic skip gram model and Skip gram uh training procedure uh we'll talk more about alternatives to this and understanding it uh in a little bit more detail um but this is the foundation for producing word vectors which are going to be useful for lots of different NLP tasks um and it's a kind of nice technique that builds out of a lot of things we've seen so far that's it for\n",
      "Similarity: 0.7980324019298258\n",
      "\n",
      "Chunk 3:\n",
      "of all these individual phrases did i do that in a way that is uh kind of gives me plausible language in whatever language i'm translating into in that it should have reasonably high probability under one of these language models another application is grammatical error correction so if we want to build a system that can take some text that might have some grammar errors in it and figure out uh let's say you use the wrong choice of articles somewhere what we can do is we can try checking okay do other articles like dramatically increase the language modeling probability so can it help us find text that's ungrammatical but more like the the or something like that right which should have pretty low probability under one of these models uh and the final application is not of engram language modeling per se but of language modeling more general um it's a way to build uh what i'm going to call word to vect plus plus and so language models basically the task of trying to predict the next word in general requires drawing out a lot of cues from text and just like we saw that skip gram is a useful way of learning word embeddings by just saying okay what other words can occur with this one we can take this language modeling task and turn it into something more sophisticated and then use that to uh and then use that to build a more powerful set of word embeddings later so for now we've seen that we can take a corpus estimate these probabilities from it and build a simple engram language model and that's it for this segment you\n",
      "Similarity: 0.7949099847068236\n",
      "\n",
      "Chunk 4:\n",
      "that you writing this or someone reading it may not even resolve for themselves and so uh how can we expect a model to necessarily come to a decision about one of these tags to use here okay so I'm going to switch gears a little bit and talk about uh a different way of producing part of speech tags which is coming back to using neural Nets here and in particular feed forward neural networks so rather than appeal to part of spe these like hmm taggers we can instead go back to this idea of using something that looks more like a classifier we can take uh these words around uh the word that we're tagging here so we look at the previous word the current word the next word we embed them into vectors we can catenate those into a feature Vector here and then we're going to put that into a neural network uh and so again what we're doing here is we're being very careful to capture this positional position sensitive information we're looking at the previous word the current word and the next word we're not using some kind of bag of words thing or that deep averaging Network where you like add everything together here we're concatenating these three vectors uh and so Yan Batha had all looked at this for a number of NLP tasks and they did something a little bit more clever which was they also uh looked at uh Byram and character byrs and trigrams and had a way of incorporating those into the model as well um and then that all gets kind of mixed together in a hidden layer and then uh the model makes some sort of prediction and this actually turns out to work pretty well across a whole range of languages so they compared it to previous work that was doing recurrent neural networks and found that it was it was better than that work and uh you can see on the right here that uh across a whole bunch of different languages the accuracies are you know at least above 90% in all cases um again the kind of you know the error rates differ a bit and uh this isn't comparable to the other data\n",
      "Similarity: 0.7945506727936091\n",
      "\n",
      "Chunk 5:\n",
      "learning kind of which words is this hi representation compatible with and let's put high probability Mass on those from our language model so the uh that's the sort of computation process of the model how do we train this thing so effectively what I showed on the previous slide was talking about taking four words and then just predicting one next word as the output however in reality when we're training we can kind of take a sequence and slice and dice it however we want and what we're going to do is we're going to take every single prefix of that sequence and simultaneously at every Point predict what the next word is going to be so starting with the start of sequence token at the bottom left here we want to First predict that the word I comes next and then uh you know from start of sequence and I we want to predict that saw comes next from starter sequence I saw we want to predict the ETC so we think of this as a model that takes the as sequence of words as inputs and then the target is basically those words shifted over by one so uh you know based on each prefix you're trying to predict the next token so this is nice because it's going to enable us to efficiently train on this entire sequence at once rather than saying okay I'm going to do all this computation and then predict one next word we're going to predict all these next words and then get all this nice gradient signal to update our parameters kind of simultaneously so it's the same idea as batching but I want to be a little bit careful here because when we talk about batching and language models we mean something a little bit different which I'm going to come back to in a minute okay so in terms of actually the kind of mathematics of it the model places a probability distribution over the next word given the context at each point and we can turn that into a loss by Computing the minus log probability of the actual next word so the that happens at each point independently and then we can get a total loss\n",
      "Similarity: 0.7910892880577737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_similarities = [cosine_similarity(embedding, nlp_centroid) for embedding in nlp_embeddings]\n",
    "\n",
    "nlp_most_related_indices = np.argsort(nlp_similarities)[::-1]\n",
    "\n",
    "nlp_top_chunks = [nlp_chunks[i].page_content for i in nlp_most_related_indices[:5]]\n",
    "\n",
    "for i, chunk in enumerate(nlp_top_chunks, 1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\")\n",
    "    print(F\"Similarity: {nlp_similarities[nlp_most_related_indices[i-1]]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks mais próximos dos centroid das aulas de Teoria da Computação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "potentially the case that like this equals by su you know like sometimes at least sometimes um [Music] oh yeah rather maybe not sometimes but like i better say for some neighbors b yeah i guess we want a condition on like whether the bit that was flipped in [Music] going from u to v is in s and then that's how you get the size of s involved yeah exactly so like you'll see indeed like if um you know v and u differ on coordinate i that's ins then what goes inside the brackets uh negation of right you're getting like minus phi s u else like if i is like not an s you're kind of counting plus phi s u yeah nice yeah so the average well the average i guess maybe c is like i mean the key is that it'll be like some multiple of phi s u indeed as you say maybe depending on the size of s and that's good like that's the kind of thing we're seeing where like you know k uh where's my thing k a phi s is value a string u was like some multiple of like phi s u that's what you want for eigen vectors now of course we've been talking about k all along and like oh we originally got talking about um l but i guess you know one can put the pieces together here yeah nice that's good that was helpful to think of k i guess rather than l um yeah that's the thing like somehow uh like k is like a an operator or like a transformation that has like a nice meaning like l is just this thing that facilitates you know its quadratic form being like the quadratic form that we care about i mean of course they're not like you know vastly different it's like just this but like there's no inherent reason in life to like do this move except for the fact that um well like f l f is this nicely interpretable quadratic form um okay another question we could also get back to i suppose 7.3b but we could also do another question all right well perhaps uh 7.3 b are any any thoughts on this yeah so i guess it's like to start off with i just thought he could potentially write out the whole thing like in terms of\n",
      "Similarity: 0.8029903386541616\n",
      "\n",
      "Chunk 2:\n",
      "all right cool i think we're set hello everybody nice to see you um yeah only only two problems to deal with on this week so we can talk about some homework or anything else you got on your mind about the class um yeah what's on your mind did you solve everything i guess can we look at number one number one yes sure thing this is i assume it's not too bad but um yeah i guess i haven't gotten too far yet um yeah i've been trying to do it like algebraically like how you i guess hinted in class that a lot of the proofs go um but it just gets kind of messy um like i had some ideas for how i'm expanding it algebraically but i can't get the uh inequalities that way yeah this one a maybe easier if you know some inequalities and definitions which i did not tell you in class and therefore it's like therefore harder if uh you're trying to prove everything like literally from first principles which and i guess some sense you are um well let's see i guess let's see first of all maybe we should think about it intuitively because this sometimes is helpful yeah i was trying to think about it intuitively as well but um so let's see it's a little bit about this like you might imagine that like okay okay x and y are discrete random variables so um [Music] not necessarily independent so we see something showing up in the problem we have like h of x this is like you know roughly speaking the number of bits you need to generate a draw from x right on average and similarly for h of y and now i guess we kind of have this like kind of computer code that's sort of like okay uh to define z so like you know like do a z kind of goes like this like c gets like a random bit and then if c is one then kind of uh like let's say little z colon equals i might say draw y i call it do a z or whatever i'll call it drawsey else if you know c is zero we'll do z equals draw x and then we'll look return little z and so if we're interested in like the entropy of z it's kind of like saying like okay if i run\n",
      "Similarity: 0.7983294484137308\n",
      "\n",
      "Chunk 3:\n",
      "algorithm to do it so like right now we're not interested in quibbling over factors of two ok so the reason I finally say all of this it's a little bit of like a boring arithmetic aside is if we look at this denominator now it's mu times 1 minus mu where mu is this thing the mean of the indicator or the volume of s and that's you know mu minus mu squared which is actually I claim this is the variance of the indicator of s because the variance of the indicator is expected square of this function which is the same as the expected edition of this function because at zero one valued which is mu minus the square of the expectation ok so I scribbled a lot here and maybe even catch it all but what I'm finally trying to say is this problem sparsa Scott finding the set of vertices with the smallest escape probability or the smallest conductance or like the worst bottleneck seemingly for random walks is so this thing this equality it's like now if I put the variance down here what I'm saying is it's exactly equal to or maybe up to a factor of two this program but when you're only minimizing over zero one functions okay so let me write that more clearly so I'm saying is this combinatorial quantity by G the size of the sparsit's cut or the skate probability of the least conducting set it's basically up to a factor of two given by restricting this minimization problem to zero one valued functions we're doing a minimization problem and saying oh I'm now ruling out some of your allowed possibilities you can only choose a certain kind of F so that means this minimization problems value is going to be smaller than the minimization problem where you restricted to only choose zero one functions so at long last what I'm trying to say is if you're interested in this the sparse is cut value a lower bound for it up to a factor of two is this very easy to compute quantity lambda one so that's great if you have a graph GE you can easily compute using linear algebra its eigenvalues and\n",
      "Similarity: 0.7969945044285692\n",
      "\n",
      "Chunk 4:\n",
      "on i f is the sum or resets s contain i i like to write this s contains i it's the reverse of uh the element sign you can use backslash knee in um the tech for that it's like you know backslash n spelled backwards uh of f hat s squared um okay yeah we want to prove this i suppose it wouldn't hurt to remember the definition of the left-hand side this is uh maybe exactly how i wrote it but like probability uh if you choose x uniformly at random from all the binary strings that f of like x is different from f of x with the i fit flipped i mean i think this is how i wrote it in class um it's all right yeah so i mean uh person who asked the question do you have any any thoughts to start on this yeah i was trying to work like from both sides of it so i guess i did start with it so um i guess the obvious first step is to just use the polynomial definition the f true you want to plug that into here yeah um okay simplifying from there okay so it's the probability that x is drawn from plus or minus one to the n yeah i mean i guess the point uh is that f of x sort of by definition it looks like this it's the sum over all the monomials indexed by s of this coefficient f hat s times um well it's like the monomial also just write like this product i and s of x oops i don't know why i wrote there x i yeah so that's f of x and how should we write this bit well so that's where in the next step um like this will be an if and only if with just checking the subsets that i is uh contained in because they're only not like the terms won't change that i is not contained in since we flip only flip the i bit i think i know what you're saying but can you just yeah a bit more like um okay well i guess i wrote i i just wrote the same exact thing for this step on the right hand side and i just said like it's just like a little bit notationally difficult like it'd be like all right this and now this but like what should i write here yeah um i guess it's probably also a bad form of me like just\n",
      "Similarity: 0.7950806319909228\n",
      "\n",
      "Chunk 5:\n",
      "the you know the volume of s plus the volume of like s complement is always one so one of these two numbers has to be at most a half it's that's true the smaller one is at most half um consider functions instead of um like just taking on values one and zero taking on values of minus one and one then um yeah you could do that so what what do you mean in like uh like your suggestion in the general case like something where instead of like the indicator of s it's like um the two what is it like two times the indicator of s minus one or something where it's like i see what you mean yeah yeah uh okay let's say if we have say s we can even have f star if we have in mind it's like the optimal cut is some subset of the vertices i don't know let's just like uh i don't know let's just define whoops uh i don't know g to be uh as you say like the plus or minus one indicator so plus one if u is let's say ms and minus one if u is not an s right because the example kind of indicated that would give us like the yeah at least for that one edge graph so i mean um there's a couple i mean i guess we there's a lot of quantities floating around like g l g and like opt and um [Music] well i guess these are the a couple of things we have to consider well there's also like you know lambda n minus one [Music] l so should we try like calculating one of these things and see what happens sure uh what should we calculate yeah i guess if you do the quadratic form thing then it's like very similar to the to the thing when we assumed the indicator of s or like because it's it's very close to the same thing it's just like the transformation of that and then you could just like use properties of the quadratic form to solve it or something yeah okay well let's calculate that so that's this thing is this quadratic forms expectation pick a random edge gu minus gv squared um okay so uh i can't make it scroll down anymore that's weird uh okay so yeah what is this quantity equal to um okay i was thinking\n",
      "Similarity: 0.794139835111449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cs_similarities = [cosine_similarity(embedding, cs_centroid) for embedding in cs_embeddings]\n",
    "\n",
    "cs_most_related_indices = np.argsort(cs_similarities)[::-1]\n",
    "\n",
    "cs_top_chunks = [cs_theory_chunks[i].page_content for i in cs_most_related_indices[:5]]\n",
    "\n",
    "for i, chunk in enumerate(cs_top_chunks, 1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\")\n",
    "    print(F\"Similarity: {cs_similarities[cs_most_related_indices[i-1]]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(537, 1024)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = np.concatenate([nlp_embeddings, cs_embeddings], axis=0)\n",
    "all_vectors = np.concatenate([all_vectors, nlp_centroid.reshape(1, -1), cs_centroid.reshape(1, -1)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(all_vectors)\n",
    "\n",
    "nlp_reduced = reduced_vectors[:len(nlp_embeddings)]\n",
    "cs_reduced = reduced_vectors[len(nlp_embeddings):-2]\n",
    "nlp_centroid_reduced, cs_centroid_reduced = reduced_vectors[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGdCAYAAAA8F1jjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD+8UlEQVR4nOydd3gU1ffG39lN2/QGJEFKKAIhFEFKaCJIh4CA2CtiA8EuqPzUL9IEG3YRexcQQjFU6VWQEgIokABCIiWQQvru/v64mWTLlDttdwP38zx5ILuzM7Mle8+c8573cHa73Q4Gg8FgMBgMH8Tk7RNgMBgMBoPBEIMFKgwGg8FgMHwWFqgwGAwGg8HwWVigwmAwGAwGw2dhgQqDwWAwGAyfhQUqDAaDwWAwfBYWqDAYDAaDwfBZWKDCYDAYDAbDZ/Hz9gloxWaz4ezZswgLCwPHcd4+HQaDwWAwGBTY7XYUFhYiISEBJpN43qTWBypnz55FgwYNvH0aDAaDwWAwVHD69Glcd911ovfX+kAlLCwMAHmi4eHhXj4bBoPBYDAYNBQUFKBBgwbV67gYtT5Q4cs94eHhLFBhMBgMBqOWISfbYGJaBoPBYDAYPgsLVBgMBoPBYPgsLFBhMBgMBoPhs7BAhcFgMBgMhs/CAhUGg8FgMBg+CwtUGAwGg8Fg+CwsUGEwGAwGg+GzsECFwWAwGAyGz1LrDd8YDAbjqsZmBU5uA4r+A0LrAY26ASazt8+KwfAYLFBhMBgMXyUzDfj9BaAwp+a2sHhg0JtAUqr3zovB8CCs9MNgMBi+SGYa8Mu9zkEKQH7/5V5yP4NxDcACFQaDwfA1bFZg2UTpbZZNItsxGFc5LFBhMBgMXyNrM1BySXqbkjyyHYNxlcMCFQaDwfAlbFbgr+/ptj25xdhzYTB8ACamZTAYDE8i1cWTmUZKPnLZFB67cafJYPgKLFBhMBgMT5GZBqS/CBScrbktPAHo8ABQehnY8ZGy/SX21PPsGAyfhAUqDAaD4Qky04Bf7oNbGqTgLLBhhvL9WaKBxj10OTUGw5dhGhUGg8EwGpuVZFL0rNUMe48ZvzGuCVigwmAwGEZzcptzuUcLlihgzLfM8I1xzcBKPwwGg2E0rqZtWhj9FdC0t377YzB8HJZRYTAYDKO5cl6f/XBm0iXEYFxDsECFwWAwjCakjj77sVuB0zv12ReDUUtggQqDwWAYTVi8fvvSs4zEYNQCWKDCYDAYRtOoG/FL0QO9ykgMRi2BBSoMBoNhNCYzMHC2PvvSq4zEYNQSWKDCYDAYniApFeg1Wft+9CwjMRi1ABaoMBgMhqfo/QLxQVFLeH3W9cO45mCBCoPBYHiSLo+rfCAHDJzF3GgZ1xzM8I3BYDA8gdBAQlE4ONnth9cnQQqNG21lObB7PnApG4hqDHQaB/gFqDtnBsMHYIEKg8FgGM2hJcCv9yt4QFWQ4h8CdJ8E9HqOLpOyeiqw/QPAbnO47RUgZQLQf5qSM2YwfAZW+mEwGAwjyVgC/PqAusdWXCGTlY+skN929VRg2zznIAUgv2+bR+5nMGohLFBhMBj6YrMCWZuBgwvJvzart8/Ie2SmAQvvh+apyemTpV/HynKSSZFi+4dkOwajlsFKPwwGQz8ylgArnwGKL9bcFp5APETE9BU2K5kuXJgDFJ0DivMAkwlo1ANI7Fl7xaM2K9Gk6EHBGfIaJfZ0P8bJbcBf37pnUlyxW4l2JWW8PufEYHgIFqgwGAx94EsPrhScBX65DxjzjXuwIikwnUNaeYfNoxOR+hont1EKZykp+s/590NLgBXPAsUX6PdxKVu/82EwPAQLVBgMhnYOLREOUqqxA8smAS2H1GRIMtNIACNVFim5BPxyL9D7JSCmKRBaj/iI1IYsi2tgoZXQeuRfmxVY9DBwaLHyfUQ2qvk/n40p+q92va6Maw4WqDAYDG3YrOTKXo6SPGDTXKD3iw5lEUrtxoYZNf+XKyX5CnxgoRmOPOdG3Uhwl/YkUHpZ3a5K8sm/akp0DIaXYGJaBoOhjZPb6MsPOz+uuZJXWxYpyCGZmMw0dY/3FLoMIuTIPwNnkc6fX+5VH6QAwJ+fA6teJgJfxyAFqCnR+frryrjmYIEKg8HQhpISR8mlmnKDaqqyMHKdMN6GdhBhl8eB/tOBlqlAQIjzfeEJRNvTcog+wtySPJnuILvvv66Maw5W+mEwGNpQWuLgNRGasIt3wngbxy6mK+eBLo8Bf30PlBc6b2eJBoa9V1Nq6TZBXDeStVlfYa4UeryuRutfHPcfUgew20lWj2ltrko8FqjMmjULU6ZMwaRJk/Duu+8CAEpLS/Hss8/ip59+QllZGQYMGICPPvoI9erpVdtlMBiG06gbEBxLX/4JrededlCL3oJVrUh1MQXHkJbrOteLt16bzMIBgqefp5bjZaYBv79AAjWesHhg0Jv66F/kRhEwrc1Vh0dKP7t378ann36Ktm3bOt3+9NNPY9myZfj111+xceNGnD17FiNHjvTEKTEYDL0wmYG2Y+i2tUQDDboAq6boc2zdBKs6wHcxiS2gxReBw2lAXFugaW9yG60xnqefp9rjZaYRHY1jkAKQ33+5V7v+Re41Bqq0NveSTjTGVYHhGZWioiLcfffdmD9/Pt54443q2/Pz87FgwQL88MMP6NOnDwDgyy+/RKtWrbBjxw507drV6FNjMBhasFmB7C1kkb1CmSFp0hs4vVOfMkZ4fZLN8QWou5h4DYgNWD3F+XWQygTwwlxPlH+CY9W9rjYrsGyi9DauLepK96+kU+zXBwD7V0DyCOXHYvgUhmdUxo8fjyFDhuCWW25xun3Pnj2oqKhwur1ly5Zo2LAhtm/fLrq/srIyFBQUOP0wGAwPk5kGzGkGfJMKbJ4DHPyZ7nEnNgKZS/Q5h4GzfEeLoKSLqeAM6bpx3Z7PBAhlHWiFuXow+C11r2vWZiKWlqIkD9j+EXDgF2Lpf+AX+jELijvF7OR1Zl1MtR5DMyo//fQT9u7di927d7vdl5ubi4CAAERGRjrdXq9ePeTm5oruc+bMmXj99df1PlUGg0ELn95XQ8lFYPfn2s+h90u+pUHQU0MilnVISgVumgxsnKXfsVxpPkB9BuLkFrrt1rzifhuNrkTta5w+WX0Wh+ETGJZROX36NCZNmoTvv/8eQUFBuu13ypQpyM/Pr/45ffq0bvtmMBgy6Dm/Ri3BMUB0om8NPNRTQ1KSR0pqrlSWA4GhgDlAv2O50u1J9Y/VMndRKpvEo/Y15ruYGLUWwwKVPXv24Ny5c+jQoQP8/Pzg5+eHjRs3Yt68efDz80O9evVQXl6Oy5cvOz3uv//+Q1xcnOh+AwMDER4e7vTDYDA8hN7za9RQfBFYPA74eigw93rfEE026kYCKL3I2uz8++qpwPR6wOpXAKtBE5DValN49GgTXzZJPPhs1I2IsdXga91hDEUYVvrp27cvDh486HTbgw8+iJYtW+LFF19EgwYN4O/vj3Xr1mHUqFEAgKNHj+LUqVNISUkx6rQYDIYWfO0Lv/gC8Ov9wJmJQP9p3jsPkxkY/DbRROgB5/B/sWGPrvgFAc1uARqmACGxZKxBeRH9MZNHk0CU9yFx9YMJjiFBYkgd0m7s6lfSuAfgHwJUXKE/piuOYxZcObKC3K8GX+oOYyjGsEAlLCwMycnJTreFhIQgJiam+vaxY8fimWeeQXR0NMLDw/Hkk08iJSWFdfwwGL6Kr37hb5sHJHT0bodH8gjg8Eh1wwJdaViV2agsl3GSdaCyFDiyHDi5Fej8qLIgBQB2fUJ+whNI0JKxUDp75qorMZmB7pOc5zKpYecnQK/naoIgm5VkmNJUlqUs0b7THcZQhVct9N955x0MHToUo0aNQq9evRAXF4fFi3X4I2cwGMZw5SLA+ejkjWVPKusiMYJRnwOWKO374V/j3fMBu03ZY0suaRPcFpwlgZ9ciU9oNlCv59SXZ3hK8mo0JZlpwLvJwLfDNQxizCPZGEathbPb7VokUF6noKAAERERyM/PZ3oVBsNIeLMtTapJD+FNd1I9XqdRC4A2o4GVzwO7PtPt1PSnarLzUwdrMiB6PX9zgH6fN0s08Pwx1vnjY9Cu3z56acRgMHwKm5XYohsSpHDymyjFmxOWk1LJIEEtmQW+xBbVWJdTMg67e1cN//yDItXv9sI/yszd5BDrpPI2NivxFlr3BrD+DeD4BnIbX+6icS2+BmBDCRkMhjyb5rrbouuGEcGPHQDnPQ+NpFQgZz+wea7CB1ZlKHhNRadxpNNHafnH07iKrPlpz2rLNUZ4xWRtBprcpP9+1ZKZRrqcnATCc4CAUMDs72yed43PL2IZFQaDIU1mmjaBZLeJwJhvyZetI+H1ga5PaDs3SQSu9j2K0kxR1faOjrt+AUDKBF3PyhBcRda+0MbuigGJO9XwpolCXUzlRe4OvzQ+M1cxLKPCYDDEqS75qOSmycDNVQMIWw4hC1jRf2Rha9SN/L7jI33OVQxvtVQn9iTjBWgJTyBBSlJqTWtw0X9A8/4ko7LjI9/MrFiia8oVfIDla23sAJlY7QtoMU3UMiupFsMCFQaD4Q6/UO76TH3JJ7w+cJNDkGMyu5uCVQ/by4FhIl1vtVQ37kEWcSnvj8BwYMhbzr4kh5YQD5TiCzXbWSKBns8DQWHA5VNA2RVg//dGPwM6SvJIV45jeeLiceOPGxAKdHmUBCCLHpKfM3RsTc3Uam+iJdvEa218qYTlAVjph8FgOMO3hH49FDisJtXMkR/XoYGV5WQQ3crnyb+V5S7D9gzIzWt1W9WCyQwMe096m+EfAm3HkADOZCbmbr/e7xykAEDJZWDTbGDz20DjnsCtH5Fymn+oYaevGF7AvHqqdi8VWThgxMdA3/8DmvUBhr4r/5DtHwAZSww+Lwq0ZptcXYuvAVhGhcFg1KBHa2l4AjBgBvETObiQBAt7vqqamuyw39WvEP1F/2mkSyT9Rf11DW3HeDdNnpRKAgrX5xZev6bMw3NwsbwDbUkeeX/GfEMee/1A4M0mQHmhyhM0ASF1gSvig2DpqRIw0xrUqSUwDBj+kfNrRzu+YOWzQNIwYz4TNivJdmRtJjF3ox41AagjWjN8vqS18RAsUGH4BFabHTtOXMT24xcB2JHSJBZdm8bAbLoG/yq9RXXtXGGQwpmBu34FSi+RL+Er591LF0LYbTULc/9pNRqWw2n6eYe0GKzPfrSQlCqsz3FcwA4tARaPpdyhvaabyS8AGPGR+mnWsAHdJwBlRTp12tgBo625Oj/i3v1Cm6UovkDeBz3mEjki2sETAnSb5Oy0y8+FKr6o7li+orXxICxQYXid9IwcTF58EJeLK6pv++CP44gM9seskW0wMDnei2d3DaG2dm63kgWzzWj6uTSObPsA6P0SEGCpWUD0CFTC6/uOdbqQPocnM42Ue5RQcAb4YybRKrQcIpy1MQcC1jL5fV0+BQyeA1QUK3/vvEHjXu63KclS0AQ1jmJmocDSEb6DR4jyK6QMtvNjYNg8EmCZzEDb29WJyANC9Q+yagEsUGF4nPJKG77dno2TecUoLqvEwr1nBLe7XFyBx77bi6dvuR4T+jQTza5YbXbsysrDucJS1A0LQufEaJaJUYOW2nlhDqn/q1robMBb1xO9RlKqg8BWbRlIoM1XK44LV3AswHEkcyS3iNHsV20HyOY55Cc4hgxEfCrDeXHN2Q+sfll+P5ENSbkivh3Q6wVg0xz4rPuwJVp4oW7Ujbwvclk8QD6oyUwTKNWJ+JjQvn8ll0gwM+Zbso8Wg9UFKt0mXnMdPwCz0Gd4mJkrMzF/cxZsCj91ceGBeC21tVt2JT0jB68vy0ROfmn1bfERQXh1WBLLxCglazMR0KohOIZcPVaWym8rClejvZC6SpVDSP+hBaGFy5GweKDjg0BMU+WBi5bX3JWUCcCA6TW/V5YD0+vJtDSbgLB6zp1dQZHqjdrAkSDOqDZqfqEX4tAS+cxUeH1nu39XRDVaVcEv//nkUfr+8ccHiGBdSTB+FY4BYBb6DJ9j5spMfLpJeZACALkFZXj8u71Iz8iB1WbH1n8u4PHv/sRj3+11ClIAIDe/tHpbhgL4TIYatV7xRY1BShXpk2vswqkt6KvOt+sTwP3LyUKgR5BiswIbZpOASWpBKcwh6f1FY8mi9W4yvTGXnn4j2z8AVr1S8zuVWZzNvf28OkhRaViXMgHVnV96ERQlHaQAQOsRJOMgdX4DZpCsk5A1vc1KdCaC2aSq2xw/n4Dy9483IKzudlPwGg1776oKUpTASj8Mj1BeacP8zVma9zNl8UG8uOgA8ksqRbep6j3A68sy0S8pjpWBaOG/PH+5z0snUOUku2kusGEmRMsP/sFET8HjaJSmF4eWAMufAUpUCB55F1G5hRXQ3+Nl+/vA5ZPAjWNJiaT/tKrbP3DJcpiITbuUhsU/GKi4Qn9sx/fhuk4UXVwcADvRJ8U0Jd4re79yfowlCujyuLMYVYr+04CEjsDKZ5zFquH1geRRwKop4vvfNFfa88bR6ZgvP6l5//jghp+JJPc66Z0hrIWw0g/DcKw2O/5v6UF8v/O0x4/947iuSGlK2brIIMiVOozGEiVv3gUQYWHXCUDvF/S90lQjCBaCJlVvs1aVAAwwvLNE1Qg4K8uB3fOBS9lk0GHxJTrX3HZ3Aft/kN+u/3Sg6+POz9VR1yMUhAgtwEpErFK47ufKRWDhAxB9jQMjSPBrqxC+3xF+sjV/nDlNiM8NLfcvd9bZ8OdamEN0T5YYEiCH1HE2ArwKoV2/WUaFYShCHT2e5FyhDuWIawX+C9NaDgz/mLSZLnxAg15BJTRBCkBmomyaBVQW12QOtKJaECwAjYuoUxarKsOgF64CzpTx5PbMNGDVS3T7CIsnP3LuxNveJxolV52O44Lc6zn5IESqO0oJjvvhg0Gp17Ysn37fjlkUkxno8oQyg7vii/oFZNcILFBhGEZ6Rg4e+26vV8+hbliQV49faxDKogTHejhI4YhVPG2gwrNtHuAXBPSerO3L3mYlJQM9oZnYS1sCUMvvL9bMh1HaZcRxRCgstxAX5Tpvw3ciJY+ouU2vIEQpeg5ItES7t7z3eq4qUKM03fv1fvesYXgCMGAmuZ3XzpRdBjgTEN2ETNH2C9DnOdRCWKDC0BW+VTg3vwT/W57ptfPgAMRFkFZlhgxinQ40rZ560+Vxdfbrm94Eds0HUuepr+Wf3KbehEuMC3/Tbcebwu38hD7bQUvh2RpdhdJF+88vZHQbIhRfBBbeD5ydqF+2Sy16Cpa7PCacBUr9gDxfWoSmI0t1LDm6OF+DsK4fhm6kZ+Sgx+z1uHP+Djz9y35c8lK5h+fVYUlMSCuHWjdaveHMwG1fAT2eJpkcNZRWlTpoO25cMWLi7+Gl9OdjMpOFMDxB//Pgn9vRlcoepyZIcWTbPGJM59gp42n0Eiz7h5DsiRDJI2Q6jjTCuzivnmrcMXwYFqgwdCE9IwePC7QK08ABaN8gQrdzibT446lbmqNfUpxu+7xq0TMtrgW7FTh/FJjXTnsmx7WFlBajpiwrOR81bas0hNYj53DgZ333S8PGWcCc5qSTSm9sVlIqEWo35qluu9dI90nSpcX+04Dbvgb8LNqPJQY/zPMagwUqDM1YbXa8vixT9TX5Nw91RuOYEN3O53JJBd5Z+w96zF7PvFTkMCKLoJYNM/QJmvgWUqUoXdASOhhzPkmpJLukV7ASlkCeG3Vpy4AsZMlFUtrQMyPgOOVbysNGj+DPEi2eTQFIgHRiI5CbAcS3VX8cOexW0r11jcECFYZmdmXlqcqkAMRFtluzWCRE6i96ZcZvFNBmEYy8SjQCNQFY9YJGyYWj6s9HLhMQHAPdynGDZpPnRvuaNO2jz3GF2DaPdFZpJWOJsBFfQQ7RW7kGK7xgmdpE0AUps7XMNGBOM+CbVNLyfXqnumPQcinb2P37ICxQYWhGSwtwcv1wmE0cujeto+MZEfiv+deXZWLrPxewdN8ZbD9+EVY11rhXK7RutJUlxLekYTegQTfyf19GbRknKRW4aTLdtuUKzNAcdTdCmYA5TYkLLh+w6JHpskQ7m87RvibN+mo/thQrn9WmWTm0BFj0oMidAg6yNitwfAOQewC48UGg3Z1AEGWp2fU1dIUf9aBVy6OEqMaeO5aPwAzfGJrZeuwC7v5c/VXEo70S8cLAVuj4xhqP+K2wWUAuVHf9AFRX8ZYooP09xAXVF5Gb5yKHzQq82UTf1my+XddkEpklUwVv0nbusLrupxZDgXqtgEY9SJeP42tQWU7Mycok2mgt0cCzR4G3Wxnb9eVqekaLkhlQ9y8n3TXLJrp32fCOtKKOuNFE2CzliGuzkgBTaTu9Fjgz8HLuVdOqzGb9MDyHxlB3/uYsWG12zBrZRp/zkYGVhFzg0+LhlIFbySXfDVIA7VOTTWYgVefnx7frLnkMkn8wvEmbmsm6ANB5HNDnFaBpb+fXIGMJ8HZL6SCFx2QGhryl7vi0qMkYKfWAObqyKtshEEiUXCKBoDkA6P0imTx9/3LiOnv/cuIo3PtF6c/RprmeDVIAYtp3lQQpSmCBCkMzF65IzAuhwGYHvt2ejYHJ8fjkng6ICw90uj8uPBCP9krUdAxH7FU/ry/LZGUgnqRUYoNuhJDSU8il6ZWQlEr2FaZz1o22XKQ2m8MJvH+rp5IgiUZIW5JHRLetRwAtBqs7BxrUlOaUdqjt+Vp+G75ExJvRtRntnokSwmYFdn5Mfy5a4cyk/fka9VFhhm8Mzejh/noyrxhWmx1hQf4YecN1OPDvZVgCzOicGI37uyUiwM+EGxpG6WrHn5Nfil1ZeWwWEEBS6gvF6v4+TkAYCSqa9NLXhpw3YcveAvx6n7J5Lt7iynnn3w8tUT4SoOg/8nk4+rtup+UE34WkFDkbf1doBiq6Dhmk5eQ247IpUU2ApBHMmdYBFqgwNNM5MRrxEUGqO38AoLisUlCjsubwOXy44ThmjWyDgcnx6JcUh7dWHcVHG49rPW0AQG4BmwWkOKXua3R7Uv8ghcdkJotFbQhSAOdMhc0KrHhW+T4Kc4DNb8EwE8DKUuDICuWZL9cgTC/4MpSS+TtGtvVfOgHYK4Gh7xh3jFoGK/0wNGM2cXh1WJLqogHHAQv3nhHNlFwursBjVZoSs4lDoL9+C9LWfwz68qtN+IrpG0CEsPcuBUbOBwbMIP/evxwY/bW4x8mGGcA7rdU70srhS14zUrhmKk5uUy6I5UzErl1NtoCjXE5K8oRbiOUI0b8zEAAJSjLTyGfIsRNL6jNllDkgzzVq7CYGC1QYujAwOR4f39MB8RHKy0AWysDjtbRDWHngLN5dSzk/hYLfD+agvNKm2/5qJb60EA+cRYSgbccQ4WDbMSQtnzyCCB7FWocLc7TZ50uhxNLf5K//8Wnh/VJ41Lyvdg1/C3wD6fUDaDZW7iCst14IIIFx8UXy2XEtLUl9php1I51DRnGNGruJwQIVhm4MTI7Hy4NaKc6sFJfTfVnlFpThlaUZuiakr1TY0HXmumu7A8joq0MaLNEkCMk5AKx/g/heuC5imcuAjTKGbMsm6T9XRkigKoQlBhj5mb7HpjquiIjY4++rHQAH/LOGbnOljr16WeE70n8GsPwp6W2EPlNHVhjf8XMNGruJwTQqDN1Iz8jBhJ/+MvQYeVf091nJu1KOx7/bi4/v6XBteqvwC0BBDjw+nDAwDIhtSXxDNs5yuGNOjadIUmqV2JdiOm1JHnF6bdpbv3Ok1Ua0ux1IHgmc/Uu5gFWO5NHkyt/fAoTGEcMys5+wXwpP9fsqU9ZrcztwUK8ZQPaazAoNSrI+vHOwlA8NLZZo4jYbGC4fcLh+pmxW4PcXtB2fhmvQ2E0MllFh6AI/76c2c822KztZx3uwPdnkT3w9zuwGKorc7+c9RTKWKBP77vlCt1MEQJ+Z4Nt5+eF0aqdAC5GxEDjxB/EG2fMFsPdrIK6tu1+KIzQzbrpNBCIb6neeSlGa9an2/FGRWWl7O9DzeeC+NOKTkpQKnNxC99g/HT5TJ7cp70BSCmcmnT4MACxQYeiElnk/voAdNe3K1yRipm96Lrau2CizYyueUSb2/XuVvuUf2TEDHNE6OApZW48AnvubCIE7P6LfufCU5NFpcsQW9uBYEkz1n+Y965zgWHVtykmpzgZtrSi7h5r3B/q+AjS5qSa4o70uOb5e3/EGclyjxm5isNIPQxe0zPvxJa6W56EK3jfEsUWzMAdY7OUruxKaib8OVJbqW/5xKjlwcF7dqlZ5ITdc3kQMAHYZpF1Jn0zeM6GsSmU5EWReyga6PgHUTSavpWvrbaMeAOYYc35SDJqjvqXc8bUNrQccphBRC2VvEnuSQYJylBfW+K0Yrf1JefKaNXYTg2VUGLqgh+mbGJHB/nioe2PD9u+Ikc+jVuDq0GlEp4UnoE3p0yKWcQpPILdLeYLQDn5Ug5ggdfVUYHo9YNVLJEha/Qrw/UggZ7+7piWxJ+AX6L4PownVqd24/o3qt2uYAvhR/s0fXUn+bdTNuL+Lzo8CA94wZt+1GJZRYegCb/qWm1+qWY7ZsVEkGkQFIyEyCN2b1kHXpjHYlZWHL7Zm63GqosSEBKBzosox8Fcr3hTaasGIUxXKOEmZgvE4ZWQMwLUUsXqqsJjXbqu53fGKfe1rQKW2MRiq0KuEQqtJ2vMFKanwZKYR7VMlZRb1wC9A/zfI+znoTfrhiEpoNUz/fV4FsIwKQxd40zdA+3VjSpNYvHvHDXhhYCt0bx4Ls4lD58RoRFqM9agY3j4BZlMtnnVjBN4S2mpFzWReGpTOhOHhMzJGeG84liIqy4HtH0hv72gmRrO9UehVQqFt43Xcjp8YrkT7VHyhJnvFz4LS7f0U0DkxqmGBCkM3eNO3OBWmb44Izd4xmzg82F2/wYRC9EuKM3T/tRal05W9jX8I+VdvPxWtJKUCo7/Sd5+ui9vu+fKmbY5mYjTbG4HaeT9C0Lbx8ttVj4xQkXZzzAIlpQJD31W+DzckdE4MACxQYejMwOR4bHmxD34c1xW3tlfeQhgV7I+uTYSHBE7o08ywrArHAR0bGeg0WdtJSgUm7gfa3Q2YvaBnUELFFeCbVODdZONs9dWS2FNf0zLXxU1pdsFbpmIdH9BvUe40Tt6+37HdV8vICNdZSqumqNuPIzQ6p2scFqgwdMds4pDSNAZhKoKKmSPbiJZfzCYOs0a10Xp6gtjtwNfbsq9NHxUaDi0B3mwC7P8esHpBz6CGgrPqZsoYiVMpTQNibrRKswveMhWL1jE76hcApEyQ3sax3VetB4olyn2WktYZWZ0eBp46yIIUGVigwjCMRtHB1NvGRwThExdnWKvNju3HL2LpvjPYfvwirDY7BibH45N7OiAyWP/MyvSVh9Fj9vpr205fiNVTgV/vJy2atQ4VM2WMwmYlbdPWcqD3S0CQTAbPEkW2e+U8GdTY63l3wzJXaLILABAar2x7vdF7EnL/acS8zvW5cGZyu6N4WO2xuzyufZaSKw26sHIPBazrh2EY96Y0xvSVhyGVpOAAfPNQZ3RrFuuUSUnPyMHryzKdTOTiI4Lw6rAkDEyOR7+kOOw4cRHbj1/Ev5eKsWSfPtN/c/JLr207fVcyluhvB+9p+BZeowS2jtiswl1BfIeJ4xV4cAzQ6wWSzuMANOxGFtriC+4dRU170/nC8NkFufds0UPEgj8plW57vQkWLu9S4+gRE9WYBFz9pwF9prrf7mqcpubYQVFAr+ecb9NDDFxb2/89DAtUGIYR4GfCuJ6J+HRTlug2j/RKRM/rnf0U0jNy8Ph3e92kbjn5pXjsu73VmZfuzWLRvVks5q46qut520Hs9PslxV3bXUA2K7DyGeP2HxAKlDtY54fXB5JHEbt4xwU9IEx7NufoSuMDFaFgJDyBzOnZ9j7cxJvFF4FNb7pf8Wul/zQS/Gx/X3o73iyOP/b2D5yFtZyZGMWFxxM/Fj1Z+RzgH6yu5LF6qvu5rn6FBFz9pzm3IAuRtVH5Mbs86p750Nq6z7p8qGGlH4ahTBmchEd7JcJ1vTdxwKO9EjFlMGlp5ss8v/11Bi/9dlDyz/7FRQdctCT660quaTt9npPbyGJqFOVFgH8o0LA70ONZYMTHQEJH96F2Zh3KfDs+MlarkrGE+Gq4ahYKcqqyFRKf0W3zyOP15PoBMhvYnc3i+k8DXv4PGDCDWP4PmAG8nEvMx4xwYi0rpBsB4ArvEePaqcR7xKyeKv34ynJ1n4Pzh91v09S6z7EuHwWwjArDcKYMTsKz/Vvi2+3ZOJlXjEbRwbg3pTEC/EicLFTmkSK/pBKTfvoLH9zVAQDxXfngj+O6n/c1bacPeGamSUURcGor+dnylvA2ctNtaZGym1cDX+Y5shzY+anIRpRBdNoEIGmY8LmJlZOkoH3vHLczmcmgw9B65Ic/hpJApesTZGBi+RW67ZW8J5XlVZkpCbZ/SMo/QnNyMpYAy550zuLRws/6cT1PvnXfNZMmBT+5mQloqWGBCsMjBPiZMLZnE7fbxco8ciw/kIPByTkY3DYe+SXl+pykC9kXig3Zb63B6Jkm1OiUMdNTq5KxhJTF9Mo4lRcCm+YCvV2mRItpWwa/DSSPEN8f7Xt38bj4ccITSMag5RAyQLD4gvz+Wgwm7q1LxwP7f5TfXsl7suwpyH4WeI8Y1/KPmFsvLWWF4ufp6lgcUof8f9cnQMnlmu0sUUSQ2+s5lklRCAtUGF7DarPj9WWZqpehqUszcEtSPUxbIZCW1YF31/6NFnGh166otroGr49QWReCY7QFB/zwOpqshBhaFz0xdn7svIjx7qlC2paF9wNnJbQttO/dnq+A2BbAwgfcj8O3d4/5BhjyFun8kiI4tqaLpdktdIEKQJf9sVmJdokGV2+YQ0v0eb+kztNxSCJAJjTf9ILyTBhDEKZRYXiNXVl51OUeIS5eKce327M17UOO15dlXrveKtU1eB8SFA+cBdy/HBi1gLTuhip0E971GfD1UPVmcHotekKUXKrRjNC4p26bB2QsJi3PBxeSf/k2bJMZ6PCA/DELz1YJpsWOU9Xe3WoYEf1KUXwBmNeOvK5KsnE022ZvoffvcfSGsVmBFc/Sn4sUSjOMasctMNxggQrDa+ihAdn0j85+DA7YcQ2JanmPD9cFr9o+38VNNTgWCIr0+GkiLJ586be+lVyh9p8GNFJRyinIUW4Gp+eiJwZ/1U5rJrbwIRJ4LRpL/n2ndc1zimlKd0y5DBVfnuk/DRj9tXR7b8FZIpK9cp6u9Za28yVrs/w2AACuxoEWqBKEU5Ss5GAdOl6FlX4YXkMPDcjeU5e1n4gMV72oNjMNWPk8UJRbc1toHDB4DglUhKYG26zAt8M9e57h9UlpYcNsUibRJLK1A+CUiTm1LHoNugCnd8pvx1+1UwuZXTIhhTkkUBjzrb4aI97NNXkE0HIw8HYr6ddi0VgirJUbeEjb+UKb1LvuRmchrV6C8Gu1Q0eNkNsAWKDC8ApWmx0/7jqlaR/B/iYUllbqdEbi1A3TNmTRp8lMEx5XX5Rbs+AlpbrX4A9S6gX0JL4d8Nb1+nUBObbo0og51S56IxcAybeSEQSll6W35bMbWoOMZZOAZ49q8/lwxNHN9fRO+YDNbiNBSreJwF/fur9nSjtfGvUAMEd+u5tfcf5d6+t4NXfoyAUhmWnA7y84jxwIiwcGvenx14MFKgxNWG127MrKw7nCUtQNC0LnxGgqk7RdWXnILdCWqSiuUDb1Ncrih8ullW42HWJwAOIiyHO6KrFZgSWPSW+z5HHhjIM3OoKOrjRmv7QBiNrnfOU/8voNfY+IYKX49X6A+7aq00aDcLgkjyxCA2dXCXI1EuJgyqgkYMtYBDz7NzmXk1tIvJTYE2jcQ9mVeWJP0jUjFaRaot0DTlpRcVCkcxBpiQa6PHb1dugIdpPFAm3HkM6tKxeFP6uOGTsPBissUGGoRs7mXgpPllPG926KHs3roHNiNN5MPyzplOvKq8OSrl532uMb5P0uyovIds37Ot/uix1BaqENQNQ+50vZZGFYTTlpd9kkEqgMfls+sJHi5BagzyvAbV8BCx90N0lTQkjdmv8rCdgKzpAMDO0IADFMZmDYPOHsH8+w99yDCl4QLtQ9xdNtInDLaz5R4vAIot1kF4gx4o6PIFtr4z+jHnqNDBXTzpw5E506dUJYWBjq1q2LESNG4OhRZ7vz0tJSjB8/HjExMQgNDcWoUaPw338eMJpiaIL3P3HtuMmtmpUjN9iPtpwS6Kf+I8qBBE7P9G+BlKYxMJs4UadcV+Ijgq7+eT8Hf1a/nV5TgL0Kp0wkqfY5Xzwm7ForRkkeEY8mjwCa9Vd+PB5+HQqO0RakAADn8AfToIuyQYZqpxW7kpRKruRdRbphCdJX+FKC8Nu+JiLha6VDh6abDJC/n/+MeghDMyobN27E+PHj0alTJ1RWVuKll15C//79kZmZiZCQEADA008/jRUrVuDXX39FREQEJkyYgJEjR2Lr1q1GnhpDA1L+J1USRdlZOZ0ToxEfEYTc/FLRP4n4iCDYbHb8V0jZlugAf1ShjIijU27WxSvgALS7LhL5JRWIDg1EXDh9CatWU0bpHiq2Hb9wLJtEvrhqI0pFkkmppC16wwz6xxxfr/y8Tm6pyUIcW6388UBNGUQPQamrRkVJ4HPiD1JS0AMhYTdN9kPt42oLQkMahdx5abvJaOA/ox7A0EAlPT3d6fevvvoKdevWxZ49e9CrVy/k5+djwYIF+OGHH9CnTx8AwJdffolWrVphx44d6Nq1q5Gnx1CJnP+JY1tvSlPhVkazicOrw5Lw+Hd7wcE5fufDgzs6NcQ7a/9WdY5xMiUoMafca4pGKcDR5XTbicEvANlbyBUWByAwAljzivhjfIHw+iRIUVNn7/WcDl1HMvB/ECF1JDcTxRJNdCCAPnoix30oDXz2/QA0H0B0IFp0Kjyuwm6jH+fryA1pdESv7BZgxIg1UTyqUcnPzwcAREcTceKePXtQUVGBW265pXqbli1bomHDhti+fbtgoFJWVoayspor7IKCAoPPmuEKrb5EbruByfH4+J4ObjoXPsgoq1SWrp46pBViwwIViXqvaTo/Qr7QJL9xOLKdFCYzceJschP53RsdQTT4WYAbHyRiQc1X0wZ/tvgFlcaLRAhHvYZWPZFreUxN4PPrA3D6nG2eAwSGAakfAK1HqDuvawmb1flioFEP8hlZ83/CLeD8kEagJljJTAPSKXVSNHgw6PNYoGKz2fDUU0+he/fuSE5OBgDk5uYiICAAkZGRTtvWq1cPubm5AnshupfXX3/d6NNlSECrL6HZbmByPPolxQl2Dm0/Tt/xEB8RhAe6J7LgRAl+AUC3J6WdVrs9KZxClsJnZgS5UFkCNEzR/gV7cpuxpS7HbEijbvLdLo4ExwJD33HOFNEISkURmPLbqJuKjiSB45YVki6nMxKjABgkwHArr84BAkLkxfD8kMa/01W+/yIEhNZ8Rj2Ax5xpx48fj4yMDPz000+a9jNlyhTk5+dX/5w+fVqnM2TQwutLxEICXsRK29ZrNnFIaRqD4e3rV4teaY7jyFXdnWMk/aeRrgdXcSRnJrerWUD4K3ilBEUpf4xS0ifXuO6qRc/0uRCO2ZDKcvpJxMGxwDOHhctZUoLSmyYDnR8DAsOd7wuvTx7juj+TmXQk6cW2eWTII8Md3udIKDCm+VzYrcCOT0igo2etpttEj+p7PJJRmTBhApYvX45Nmzbhuuuuq749Li4O5eXluHz5slNW5b///kNcnPAMj8DAQAQGBhp9ygwJpPQlqPpdj8BB7jgAEBnsj1kj21zd3TlG038aueqiEePRoPQKnteL2G3yg++0oscE5SsGjW1w1c0oHX7YfID0/XKC0oEz5MWmvEmYvRKo0wo4r9NA0JXPAknDrh5xqx5Ud+hoZP3/AFuF9v3wWKKJTsuDGBqo2O12PPnkk/jtt9+wYcMGJCYmOt3fsWNH+Pv7Y926dRg1ahQA4OjRozh16hRSUiQEfAyvw+tLJi8+iMvFzn8EkcH+uh/HVccSGeyPB7slYkKfZpoDIqvNjh0nLlaVmuxIaRKLrg6ZnWsCvwAgZbx+++Ov4JdNlC5b9H6p5ktvDuVsGq1o7YJRK3AVwxIJdHnC2VxMzYTm/d8DB34UFlHySAlK5cSmQiZhelF8QXsAebWhV4eOnkEKIOxXYzCGBirjx4/HDz/8gKVLlyIsLKxadxIREQGLxYKIiAiMHTsWzzzzDKKjoxEeHo4nn3wSKSkprOOnlpBf7P5HkF9cgce/21vtQ6LWvZZHSseilfSMHLdg64M/jrNMjR60HEICFSm2zSMLdNZmY7toHNGqoVErcBWjJB/YMBOo24oEeJXl8jNyxBASUeqB2KgFPdFrLk9txVUwq9X7Rm+8OE7A0EDl448/BgD07t3b6fYvv/wSDzzwAADgnXfegclkwqhRo1BWVoYBAwbgo48+MvK0GDpgtdnxWtohWS8Vm82OaSsOq3KvdYTXsejJygNn8cQPfwned7m4Ao99txefXO2mb0ZCE3yUFwEb3yS1dMPhiEZD6xRc3vBMt4Wk6q+IH5C4e772fW/7gJTz1JbvHLFZqzQOBuOrImwh9B7WJyiY9RU44KYXgZte8FppzvDSjxxBQUH48MMP8eGHHxp5Kgyd+WD9MeQWiBux8V4qQoEA716rl/Or0oxNeaUNUxYfwKK9Z2T3/VraIUnjOoYEJ7fQbbfjQ/kWaB5zAGAtV3EyVe+fHlNwlRqe0cLrZy5l67AzG7B8EjDiY+27yt6ifgHtOp68v3IocQj2NK5maqHxwJqXncsy4QlEl6Um2+CJbJUWbvvK6y3kbNYPQzHpGTmqjdgAevda2nN5LS3TacBhXHgQXksVztjMXJmJzzZnUQ8mzC0okzSuY0hA22RQVkjf8WMtBwLCgPJC8W38LET34didE56g3uDNFVUlCjE5uAuFOWQx1IOMRcSnRGtgpsUqvcUgoGFXMvxStEtFoAXa2/AZk23vA/+shux7V3BW3bA+vQSzRsCZgdFfeD1IATzYnsy4OuDt87Xi6F6rlvSMHDz23V63Kcy5BaV4TGDe0MyVmfh0E32QwuPJAYpXFUqEkWH1AP8Qum07yFx9jvwMePoQcP9yYNQC8u9TB/WrrdOWKDo9TDJFA2YAbSgt5K+cJx1XSmbpiFFZJh5kVJYTj42Vz5N/KyWyVFqSiUX/kdd98mnSBh0Q6ny/WAu0N8lMA95NBr4eCvyzCoraepdNUtb+rqelvd6M8o0gBWAZFYZC5OzzlaI2CLDa7Ji8+KDkNlMWH6zO2JRX2jB/M/3UZEdoDe4YLjTuIZ/94LlynpjLbZwlv22LwcS4zbUDxbW916gOkmqn1xwIL2JVWphBb5IsQcZi4OBLdPsOqVPVgTVBedePEELzWJRYrgPEBRVz1B3/4nHyr8kM3DyF6Bx8ed6O2GRhWkrySKmMd2mWQ0l2ruezQP6/wAHKYaK0uOqttIyXMAgWqDAUkZtfouv+1AYBO05cdGuLduVScQV2nLiI7s1i8e32bNhUfPfEhQdSG9cx4C4yHDYPWPSg/ONWvUQyKn5BQKVY8OoghjWZtQ2Zox3iJoSTT4zIpCq+lLHqZWUdPHxHER8wuAYUnJm0kZ/aDvy7W35/rp95sbZnqW6hxJ7K3HEd2fu1c9u1L8/boZ4sLMPuz+kDFSUC4iY3k8949mZ9szD856vrEzqNl9AfFqgwFJF3RY2Q0R0OZKaP2iCA1l5/27EL6N4sFifzilUd57XU1kxIS4uQz4YlCriuM/DvLvnHV0g5bQqIYeUWPbHODKUZBSF4nxi3rI6DFoY/Di2uglJHI768E+S2+p2AiPpk0fruVvl9Or4+NG3PvOW6Y9BmMpOAU43gUw+DPU+hVxnmcBpx2k0eIb8t7RymMIcAfeBsY8S3mUuB/m/4XJACsECFoZB/L+uTUdHuXkt31fPVtmy0uS4CjaKDFe09wuKH2aPastZkWsRS5iWX6IIUOZSKYYWCpvAEIK4d8Pfv7tur8R+RcnpV44UiJCj1CwAiGhBRZ2EOuVoHSOYlIJS0d4vhODMIoGt7tlvJdq7mf0mpRCi67CmgRMmMH9QefxQ9z3Ph/YCJQlhLG3gMml3z2UhKJdmPHTrbePhwUMnEtAxJyittWLD5BP5vaQbmbzqBn3ed0mW/D3VvrCkISGkSS7XdlXIrHv9uL+qFBYI2Jnqqb3PsndqfBSm06JUyd8USA4ycr1wMy7d7ul6lFpwVDlIckROWusJnddqMJv/yi4lSL5TeLwk/P/65uM4XKsyRDlIAdwdR2rZnse2SUoFBFBoiV2qLP4re50k7V4oPAi0C2WVLtHAnkdy4BLWsfsWY/WqEZVQYokxfcQifb87We/kBAPRLEp7lJEe1Z0pRGUIDzSgqo1PYT//9CMb2SJQU1AYHmPH2mHYsQFGKUZ0LJReJyDZ5FH06Wqs5mVhGQSlKvFDC6wvPTrFZ5Z19A0KBwDCXVmwRMSRt27PUdkpdeX3ZH8UVWZG0QpRkKPjsnKMzbaMezsEvT2aa/OdCLTn7gFWvAAPeMGb/KmGBCkOQcd/sxprMc4bsu15YAGx2O3776wzyisoQHRKAuAiLrFGbkGcKDXwrdGmFFWN7JOLLrVlOwlqOA4a0icd7d9zA9ChqMDK1v+olUkKhNdPSYk7Gc/wP7YGKEi8UMQ8RWmffMd8CZn95UXGnceSKWSrTw5nJdmLQaip4fM0fRQonkbROKPnbMJmJCFdKiKvEHK5hd6DH08DicUCpAiH0jg+Bvv+nj6uxTrBA5SqlvNKGb7dn42ReMRpFB+PelMYI8KOr9C3bf9awIAUAzhWV4+7Pd7rdLmWtz3umaOHbHadg4oAHuzdCQkSwqteGIYDRqf2CHLJ40PhtaDEn4zm2hiwIWtozaYICABj9pfhxaJ19T20D+lCk7GnanlPGSy9QtJoKL86F0QTtME1a+L8NPSz3lZrDndoK/LANaNCZuCnTYrfpk1XUERaoXIXMXJmJ+ZudswbTVx7GuJ6JmDI4SfKxVpsdL/0m7U+iFTHDtRwRa30azxRabHZgwZaT6NuyDh7q3gTggN8zcnQddHjNofQqWzFVXsb8LBypL3hd3j7KY0lBFRRMAJJHit9PW31QUqWQa3umERJXC2sFZtMEhALdJjq3JNc2klLJ86DpqhLFoZU+Mw34/QXn8pwlGmh3h7J2YFUlVjsJUvyCAJOfvK6JR5cxDvrBApWrDN591RWbHdW3SwUru7LyUFhaadj5yWGHu7U+jWeKUtYdOY91R8473aZmWCIDLilzIxRNIPulqflrMSdTeiw5tAYFiT2BzRTPRek5OrY9q/GRAZRpKq5VBs4CjqwQzj6V5JGunR0f0c8J0lJirSwFwNHPytJrjINOsEDlKoLGfXX+5iw827+laKnDF+zieWt9fr4OrWeKVvQelnhNUZ0yN3gCrNyXtRZzMqXHokFLUNC4B7nylno9XVuQafEL0J7ap9FUeAM9yiwnt2k7h25PkkBuTlP5bQvO0pU2NZdY7SRTJPf3KadT8gKsMH8VQeO+arOT7cTwFbt454DJqKt0Z/ijvL4sE1Y1NrbXOkmpwPPHSKutJdKYY8h9WfPmZJ44Fi18UDB4jrwGxBGTmeg8pHBtQb7WcZzTs2gs+Xduc2LApgStJcSMRcDxDQoCZrv8nCC+xKqFkjygQRfpbZR8Rj0EC1SuImjdV6W265wYjfgI7wcrjgETrWeKHugxLPGaxmQGer8IPH+iZihgP51aHWlbXXkNBe1EZi3HMhr+ubguUOH1lU/qvdrhTQdddRzFF4kB2+qp9PtqpCJL5UjBGeCgwpk8JXnAprni9/MlVq3U7ygyANRE9EW0hocehAUqVxG07qtS25lNHF4dlqSPJlEl8S7W+p28MGvHF0pgtRpHI7RwncpoHe6nzx4kpQIvHCfZnQDKqcyO+FJbbVIq8FSGcdOgrwZoTAe3zQMOLaHbH19C1EKZ1EgIEba9L204KGUOR8uOj0TGVdiA6zqp36+BsEDlKuLelMay7qsmjmwnxcDkeHx8TwevZFY4uFvr7872fHbDV0pgVwV6lVCiE5Vtz2d3XsgGgimzcpwZGP217wUBYg64DAJtR8zSCaQkc3AhEQGLlVr0KCEGhSt/THkh0bUIBVQ2K3BiI5BzALjxIaDtHdrOTwi58pOXYIHKVUSAnwljezSW3GZcz0Qqz5CByfHY8mIfTB3SSqezkyc+IkhQyKpGTBsfEYR+SXWpbfNdH8smJutIo27ar04B4lKrBr8AYOg7oBIejP6Cbpgcw7egFT6XFwLfDq/Rr7ybTEpGQvDZC1c3XksM3bH2/0jEq0opKwB+dSlVZaYBc5oB36SSTrDNc4EDPynftxwlecDix0gw50MBC+v6uYpIz8jB8gO5gveZOFD5qDhiNnF4oHsi5m8+gdyCMr1Os5r7uzVCw6hgCmdaOmHr8HbxaBAdDIBDStMYdG0Sg/JKG2aszMTu7Es4kltItR9twxIZbpjMQJfHgQ0ztO0npI76x4pNO3aCA87sAVqPUH+c2oQe3TG+gtqsnVzHTVIqYLMBK58hWhdAwVBGO2kHVsu2eUBCR8BkMmZashgZv5AfSxTJKvlAdpGz28Xst2oHBQUFiIiIQH5+PsLDVaTarhLSM3Lw+Hd7RZf0dg0i0LNZneoFnHYhTs/IweTFB3X3MXm0F33QtPWfC7h7gbyzYnRIAPKu1NR3I4P9ATtwuYTu3COD/TFrZBvWmmwENitJaWtpG75/OZ1niNQCbLMCix4GDi0Wf7xSQaEnF/zKcm3+JzxCJmRh8cCgN31iYVKMzQrMakhvaOZKeH2i+xGaq6PVHygwDCiju0hyIziWjEdwHUrpSQwUbdOu3yxQuQqw2uzoMXs9cvLpBKC0C7Jc8KOG0EA/PNwjEYl1QqjdYK02Ozq+sUb3YIknMtgfD3ZLxIQ+zVgmxUiUzClxRWwhcQ0SrlwEVk9xzpo4GmpVlgPT68lY25uAl84CARb588pMc8/S0Bp4KWX1VAHzOBNxuFUSWMm9D70mA3Wa+2aWRSooXDcd2Pym+n27BsI2KykNGea4XEsIjQeeOWTI54AFKtcQ249fxJ3zdyh+3CcSxmZKgx9aODhfm4i5wbrOKqoXFogJP+3T7TxCA/0wbXhrxEVY0LFRFPacvIRzhaXMSt9oNsxWUQLihFPzQkGC2OMBso/802TQoRyB4cDwD6WDDbkFX88r0dVTpe34abNASjNbRgVdahB6vy2RQJcniGV/9hai4VDLqAVErMyTtZnoWBike663gjlDlNCu30yjchWgtpXW1arearNjV1YezhWW4lxBme5BCuCeQBVygxWaVWTigH5JdXHw33wnvUxUsD8uqci0FJVV4nxBGU5eLMbj3+1xKg9FWvzxYPfGmNCnOQtY9KbXc8DOT+nr/OH1SauwUJBCnZ2pmhX0+4tAQnu6h5QVSGsXbFbSISHFsknC84L4rEBhDhEIh9QhZRex7EVlOcmkSLH9Q+KAK1cGopnI7IiSgZBGIvZ+l1wmge/OT8jsHC0U/UfeG5O5psOGQdgwA6jbymufAZZRuQpQm1EBgB/HdUVK0xikZ+Tg9WWZhgQnNMRHBGHLi33wZvphwVlFPON6JqJPy3rV2Y/cglI8/fM+Q86JaVYM4tAS0tUgR6/JQO8XhBf6Oc2MteoHUD1YTqjkdGIj3dX7fWnOFvNSWSCx7MX2D+myQANmCNvi26w1M3myNwOnlX5XSLwOnsBj7zfI80weDWQs9I2ST3h9UupTolEx+QM2A8rkYuVXDdCu36w9+Sqgc2I0ooL9VT32XGFptRbFW0EKQNxg31v7t+ysogVbstCxURSGt6+PlKYxiAs3zu/kcnEFHv9uL9IzvChkuxppPYKUKqToNhHoM0X4SzF7i2cWLcfhhI7YrMDeb+l2kbW55jHrZ5KsgNgCyHeguLbL0k6yFdrOta1VcZACiL4OnkLR+81Bk/99wVlSYvOFIAUcySYOUqi7MSJIAbz6GWCBSi3FarNj+/GLWLrvDD5Yfww2lbNpYkMD8fqyTA9N05Fm3vpjimcVXbqivG3abrfDWpyPyvz/YC3Oh1RSkZ/mzGb/6Ez/acRYLdjFkyI4Frjta2m9Bb/4ewpHj47MNKLxyPiV7rGcw2M2zaJ4gB1In+zsYUE7ydZ1O75coldQp8eQRjUoer/t8NRsMGMxAaO+INk1Pdxo9cJLnwGmUamF6FWmiY8IAuzwaiZFDfysIqvNjmkrDlM/zlZahKKMdSjck47Ky6erb/eLbICwjgMRmtwXpiB3gybXac4MnUgeASQNU97a62nZEO/RoaZriTMrfwx/5cp3oHQaB6x+RbpTyXXibbWlvI7o5TCsFMXvt6tkvzZiA35/HjD71QQrLYcAK58H/lzgvdPy0meAZVRqGXqWaV4dloQLKjIS3oafVbQrK4/6dSg5sQf/fjQWl9Z9icrLKQB+AbAGwC+ovJyCS+u+xL8fjUXJiT2Cj2ezfwxCjTW81oFx1HA1wwltVuI7ogRLNLD3a3WHdrxy9QsgLchSuE68pbWUp4Lz7pBGxe93bQ9Sqii+4FwKNJm92yruxc8AC1RqEVabHZMXH9T8ZxgV7F/dmlzbZto4ziqiDR5KTuzBuYX/g73iJgCnAfwM4DYAt1T9+zOA07BX3IRzC/8nGKzUttfpqiaxJxAQ5plj8cMJj29QbrrV5TH1Rl2uV679pxHdDufylc2ZhVuTdUvRV6UzvDmkUY8BgbUWl1IgbRlQdzivfgZY6acW8dRPe1WZnj3RuwlMHBGZuTrTdk6MRnxEEHLzS2vFdYjjrCKa4MFWWoTzS94E7AMALIX4Rz6O3G8fjvNL3sR1TyyAKSgUHIA4NvvHtzCZgeEf0HUOqYUzk7k/SalVHibvK3u8nwUovazu2JZo4SvX/tNICzKNM61eKfrwBOH2cE/CDwj0pI28L+FYCqQpA+qNJRoY9p5XPwMsUKkFWG12zFv3D5aJzPGRIyo4EA/1SHTzBLHa7Nhx4iLaN4jE7/nq9u1Jhraph3Gd6yI7OxuhoaHo1JgEWVLln6KMdbBXlAFYAPmPux+Az2GvaIiijPUIvzEVdrDZPz5J6xHAmYnSJmhasFuJ0FfOaE2MyhJgx0f6n5dfgHALsiuNupEgQ235xz8EuPNHoHEP33Cm5QWlaU+qDwBrM3yGzC8A6Doe2K4wcKaBMzkHQJYoMqOr13Ne/wywQMXHSc/IwatLD+G/QvVakukrD+PDDcecPEGUzvCJCQlAavsE/PLnaVwp8+xUTV4Eu+j79fjwnuPVtzdt2gK3jL4bZ0qvFxTB2u12FO5JBzASJGNCQzyAW1G453eEdRyGZ/pdz3xUfJX+08jQtrQJZCqu3uSfkTdak8P1y5+GkjxnMa0aTGbiyaIqC8EBt37i7P/iC7QcAiyTaWu/WnHMkF0/QIdAhSO6p+sH1AjZG3QBTu/0ySGVLFDxYdIzcvDYd3t12dfl4go89t1efHJPBwCQ3e+oDvURFx4EjgNSmsSia1NSLuqSGK3bOdFQcmIPzi95E/aKMnDcKAAzAUQBuIQTJxbhszf/h8AgCyKGPYegxI5Oj7WVFFR197yl8KijUXn5V9hKC9E4NkSfJ8IwBr5zaNNcYOdHxKnUFUu0uhbdM7u1p9jVPl6PIXR8FmLZJPfnb4kGbrjH3dhMzAnYF9g0V9tQS61YooAh7wIhMcBf3wIHfvbMcYNjnUuBuuiP7CTYadDZeWyAluDYQFig4qPwwlm9eS3tEGj6/RbtPVP9/4V7zuC1VDKPZ2ByPB7tlehmca+WdtdFYP+/+YL38SJYoi9ZALvdOStit98GIBflZQ/j3MJpqDNqKixNaoIVewVfElIqxCPb28tLmIi2NmAykzkkvZ4Tt6Y/soJyLhBQ7cSqF12fAPb/pCxYunJen2Pzba28My0H0kXDd1jd8prnJj9rwWYlgag3ue3rmixTYY7nApUhbzm/J9T6I4o27d9fFB7z4GOwQMVH2XHioiHTgh3n5NA/ptQpGyNlca+Um1vUFQxUlIhgbbYlAJxFsADA+fNBhtKrMLJ9XGwkE9FqRWrard7wrc5C8As2H8gc3wDs/15gQ4cul/zTAveroMVgoP8bJFhY+xpwliIjGVJHn2MD5HVpcpNwKUfqNVOC0e/zyW3C2TJPEZZA9DrVv+tQDu5GobHqNpHosRyh1h9RXEkWntVeZvQALFDxUbYfpxza5kGmLD4Iq00/tTkH4Oc/TyMkwIwr5c66F60iWAAwWcLhF9kAlZcXgrQh07IQfpENMG1MVyai1UJmGvEecSxj+AeTL96h78kP0NMbx0W57RigxUD3LItjl0t5CbDqZWjy5eC9J0xmstBdPkn3OD0WQk8hNL9I76nLepTCtDBotnPg1agbEVsXq/me5oDRX5Gy5XWdiO7GtaQVEAakfkC2caVaf3QfdPGM8ZbjsAJYoOKz+F6zsJopxVLYIeyKq4cIluM4cByHsI4DcWndlwByKfeVA+A3DLvrRZRbyZiCzonRLGARorLcvVXWZCZXaEdXCne9VBQD+34A9v0IdHtS2irfaByzLK6ZAD7I0vp36Og9cXIb3cLmqknQipHZDjG33oKz5PYx3+oTrOhVChMjIAyAHSgvcr5drDXXZAba3q6us+u2r2qyJPxnMGszcHIL+bgl9pTvtkpKJROtqcuZEnjLcVgBLFDxUVKaxOKDP47Lb6iQuPBAABxyC3zXaVUPEazZQiZxhib3xeVNP8BeMRbSJSQAqATHPQzOPxB/+rfB3p/2ASCjBl4dlsS6fxxZPZV0xDiKRVe9DAQEA+VXKHZgJ2lvuw1o1k/Zl7SeCJU+MtO0X60KLXC0V65tx+gbSBiV7bBZiVBXimWT9NFA6FkKc4ID7vkNaNKL/Cqm5RGixWBlgYqYUNlkBpr2Jj9KSEoFAsOBb4cre5wjYQnecxxWAAtUfJSuTWMQGeyvu07ltdTWAOS7fryJHiJYVAUqpqBQ1BnxQpUodziAz0GyL67kANzDsGMV6ox41andOTefaHQm3twUpy+V4Eq5FZ0aR+H+bjXmc9cUot4idsogxYHtHzi3AG+eAwRGksUtMETa1MwIqmfkaAhSek0Ger/gvsDRXrm2GKz+2I6IBVx6ZTtophqX5JHttLY6G1YKswPnM4FmN5NfxbQ8QlRrRXIg+nkJCAOGvl0j6tY7AC++oO3xA2f6vJAWYBb6PoHjJOTtxy/CarPDbOIwa2Qb3Y4R5GfCR3d1QIQlAGWVNjx9y/WIDPbXbf96olUEywVYnG61NOmIuqP/D5z/RgANAYyB46wf8ntDcH4bUfe2V2FJ7OD0eP4raN4fx/HbvrNYnfkfpq88ghav/I6ZKzMVnmMtp7Jcu7eIHGWXidB112fAqpeA6fVIcOQJtMzICa9PFv8+U4S//PmFTarrTmyeis1KrvQPLiT/2qzu27huv2wSJAOuZZPk9yMF7VRjPaZdV792BnApW93jeK0IANH3dMRHJENGO8dKKVrLNq7Ty30UllHxMkKTkKNDAvDG8GQMbhuPT+7pgNfSDqnq1nEkKMCM/y133k9ceCAGtq6H9EO+JabSKoI1BbnPgbE06YjrnliAooz1KNzzOyov/1p9H5me/BBC2/SFKZDeN8WOmg6oKYOTFJxnLWb3fM/adwPkeHwGx2hNixphoSWKiCPlFiMnEaRr66jETB2h8k1wDDD4bWGxJVDlOUKR7dg0l7R2u0Kja6GVbekh79JbQOqIlvk5YloRJX40SjVEjttbYtT7BAG1QkgLAJzdbvc91aYCCgoKEBERgfz8fISHh3v7dBTBT0IWewMe7ZWIKYOTYLXZsSsrD+cKS/HPf0X44I9juhzfl4ehF/y5tEoEexr0ItiGiOr7UHXXj1jpzG63w1ZaCHt5CbgAC0xBYeA49d+mHICjbwy6NspAK58nmQ5vwJmBl3ONLQNlbQa+Hqr8cfcvp2/xFNSNiCxscnqZlAnAgOnOt9mswKzrgPJi+XOxRAPPH3NeGIXOLywe6PggENO0ZjHN2kynj7h3qXL9hRhC56aVlPHAgBna9qFWsKxUQ6T381fyuTUA2vWbZVS8hNVmx+vLMiUDhU83ZaFNQiSGtk9ASlOSott+/KJugQp/bBMH2O3iQQsHIMLih8sllboclwalIlhwD4PzC0Roch/0aVkH43o2hc1mx90LdrptzXEcEdta9Als7QC+3Z6NsT2b6LI/n8Zr01tB5u/snk8WFqM6WWh0B0IouTKV6jZyhEYvs/0DABww4I2a27I20wUpgLtdv1hgVJgDbHBYzMMTgP4zSTZJyi3WEq3vQpiUCthsQNoTyvVQYuz4BOj7mrYAWI0fjaiGKIfcPuYb52BFrMNKLWJlRh/kGrgE9E12ZeVJDtPjmfDTX1h5oCZ67pwYjegQfbUltqogRSyn8OFdN2DWqLa6ZHB5QgOlFxVeBAtuFYDhIBkTIXKq7l+FOre+CFNQKNYfOY9LV8rQtWlMVZeT8ZzMo1wYajudxpH5Nd7iUjb5wn43mWQ+Fo0l/85tDmQs0b7/at2BwlyjUq0Av7C1GS1eMqLVy2x/Hzi0xOFxW5SdCx9k2az0LdkFOcDCB4AbZBbOYe/pq83ITCPH1StIAUgAvOtT/fZHg2QQWnVb+uQaDRFNh5UrZqnvPk64zOijsEDFS5wrpG8PfuKHv5CeQRZqs4nDG8OTdT+fh7o3RlyEs118eJAfbrq+DnLyS9GnZT080itRt+MVUQw2pBbB+ruLYF9eQsYP8F1OSlATkDWKDlbxqFqIXwApN3iL8iJytem6gBdfBBber4/oNimV2N7TYtSVqZIszdIJNYua0npuaD3y2LQJCozVqg6SsYhYy7t25YQl6OehwqNHR5YY66aRIMhTyAahdqDgDNkOoOuwcsVapUcMcNHshdd3z9b4OKz04wWsNjvOKRTHvr4sE/2S4mA2cRjcNgGP/ntZVyv7vq3qoW/Leth+4gK2HbuAvafzUVBaiY1/n8fGv89j+srDCPL3fPStVgR7qbgSu7LyMDCZCJKf/XU/1dTnp29pjp92n6bKdvFwAO5NaUy9fa2HF7S6+qiAo/NRadoXaH8XyYYowkSs76UWqm3zgPodnW3HhYzp5NL8SjwyjLoyVZKlKS8Efn0A6PyIsoxXeH3gykVgTlMVA/+qFtPgGODpQ8aPStDSkSWHtUxfgzo5aINQfjvVnVMcEBQO3P4daWX25XlOEjAxrYdJz8jB5MUHVfmj/Diua7VWBQBWHsjBK0szkHelvPo2EwenYYHxEUEoqbAiv7hC8OudAxGdBvqZNHcWSdH7+jrY8Lc2d0mlItgR7RMQbvFHo+hg3N6pIXrN+cPptXKEAxAXEYQtL/YBgGrxcvaFYryz9m/J8+JFz9ccQgEAALzdSt7fISweaHMbmUJLu0C2HgkcWiy/nX8w8PwJ4MyfpCzyzxrngIozkayQVAeRzUrKS1JaFc4MjP7CfRaLXtispKSlyqadBo64A8vNm5Fj1ALnCbxGcXChiuBWIeH1gacOGr+Q04q2ebHr+jeATXPUH8/LolkxmJjWB0nPyNFktOZaLhrcNh4DkuOqF9W6YUHo2CgKe05eqv69c2I01mTm4vHv9go2RNqhvzW+EFqDFEC5CHbJvpqrr+krD6Nvq7pYm3kOgPDrMObG6/DOmqMAOKQ0jcHQtgkwmzi0iAvFs7/sd5tHxAF45FoNUoCqMtB459uyNtOZUBXmANveJ3biQZE1zrQXjwGHl7pkakxA836k44SGimJghkSnGE27s2QrcRWjDAxS+HMY/DYpaemNJZpM5V02Ufu+PGXB7onj8OUWoxd1WdF21QRvvqTYqAcADYFKLWlDFoNlVDyE1WZH91nrNVnXu2ZUlCDk18JnW4yY0syTcnI/+hzfjbm97kOZQ7rdbrfDVlKAgJJCvPjnUvzRoju2N27v9Fh+G3tFKTj/IJgs4ZraiAGgX1JdZJwpcHodIoP9UV5pQ7FLIBIZ7I9ZI9tgYHI8rDY7th27gMV7/2XOtFIc+AVYPI5+++BY4o7p6NzJZ2qO/wGc3gmUFRh0sibgpbOAi0GgE4KtuglAh/sAWwVw6RTAcUBkAyDxJvX2/1JdTOlT1M2UEaPdnUDzgUDaePfZNoqoWkw9kYEAHLJcBpV/eDyVIaru+gEEPXUcdSQ2q8ryXBW1PKPCAhUPsf34Rdw5f4fqx8c7lCV2nLhYNV3ZjpQmsehaFbw4Zlb4QXqOHiyxIYGw2e3YmZUHwI4Iiz+mrzyiw7MTpmfWXnyx8HX426zYkNgBj458BSWV5SjKWIfCPekwXz6N3wAMAlAO4M72g7D7JnL1yG9DZv4QiCZlIEKT+zpZ3CvBxAGHXh+IfacvU5d2PrmnA5vzQ8v2D4mbrBocTcz0mLdDQYXJgsXWm3H0vBUtWrXGyEdehH+gs6jcKYi4eBzY+bH4giE2xE4KQUO3WJLx4Ez6+4a0Gk6yVprhPC/K9MTnwpOLumJPHRXtyUJeOT4CC1R8jKX7zmBS1ZA7NXxyD+loEdK3BPqZwAEoraxJl8dHBCG1XTzS9uc4ZQ+4Ks8Uo+GDFJPNBjPssHIcNtRtiqEXz6K0shyBGI7fcAz9cYDcD8AKYKjJH2s4M2CtBJmePBpkhs8lAAsBLAbnH4g6I16ApUlHVec2dUgrjO3ZpCrLtU5Wm8MHiWyCMgVKMypCpEwgWhQDr5x//6cCdywsQbEVqHRIpPmZgeAgC376dREGDRrk/CAlCwWtKNNDAZkTAaEaMylVtBoOdB7neXGmEaZvPJ7SqDiixBMoM420KSvtAPKUSFghLFDxMdRmVKKC/TGzauaPLw8SdMQ1SOGxAliNWNyBrfgJk9Afq2GGzeF+DlbYMRQ3Yg2WQdiRNhfAWIBbhbqj/09VsHJfSiP8b3iyovfk+4e7oHuzWMXHuuZQ6+zqQTp8UoQD58nnblQnYHQXICoEuHQFWLgTWLSbbNe23Q3Yu7fqb85mBd5pTd++S7PgeaqUYTR6TWNWAr+4F+YAV84DxXnA5rna9+ujC7oTleWkDERdEvVwiU4BTEzrY3ROjEZceBCVRiXQbMKgNnEY1eE6dKtaHLvPWm/0KepCysn9gkEKAJgB9EcestEF4ShwClLI/WT75diPgcjEH4KBShyApYB9OM4veRPXPbFAcRmI9zxRohca//1ezBrV5uopAalp2aWhUTdSwjGsU0UbHT4pwr5zNgxsC3zxCBAX6Xz/bV2A3MvAQ58B6fv+QocOHUiwwi+KtNCIMo1stxXDPwSo0NEsDahyUr2XeM+0GOyZDIurE6zNCuz/Qf3rqaZk5y38AoAb7lGgW7J7TiRsED6hBPzwww/RuHFjBAUFoUuXLti1a5e3T0l3zCYOr6XSdYeUWW1Ysu8sHvt+D95f9zd2nLioSYTrSfoc3w1/m9UtSOExwyYYpNTcDwSgAkOwQuIofgA+h72iDEUZygI4E0c8T9IzcvBKlSkcDZdLKvD4d3urjfdqNaunkonEq17Sf0Ix36niEZSV4n7/pwIHzpMgJe1Z9yCFJy6S3D+wLXBg/1/4/fffgaMrlZ+eXKeFNzoxrh9gwE6r/tZ3fFTjEuzolOsJqh2FFZZn294O3JdGNBy1IUjhaTFY+WNqceeP1wOVn3/+Gc888wxeffVV7N27F+3atcOAAQNw7tw5b5+a7vDmY5HBdBb4V8qseHfdMTz0Ze0J3Ob2ug8bEjvAKtGdIxakAIAVJvyOgXgZ00W3IcSD40aicM/vUFK9HNczEeuP/IfHv9tLZQDniB3EeM9qq8XV0tVTSWuu6wRkvmVXj2AleQTQTYe2V1mUvQ93LCwBQDIpfjIX/H5mYEGV1OaO20ap67qRa6f1VFuvI6d2kDKArgMxXCi+CPyqk0uwzQqc2Aise4N4iRzfUOPA6wo/yTg8gX7/Q98DmtzkcyURQWxWUlo9uJD8vYbFQ9H76I3Pm054XaPSpUsXdOrUCR988AEAwGazoUGDBnjyyScxefJk2cfXFo2KI1abHTtOXMTWYxdw5lIx1h45p3jR9GUCK8vx6eI30DP7L5gVfLys4LAaA3ArfkMZguQfgF8A3I7rJv5A/FUkMHEkSHm2f0t0nblO1PiNhqlDWiE2LNDJtya3oBR5RWWIDglAXISluuvKp6gsJ5kT1yDFET0nFB9aAqx4ls5XRQuWaMA/yDntz5mcnmdFRSWCZxdj5I3AzwpiqDHvAb/tAYpftMDfX8GMLVqNytzrjX99XOn9ErBhZtUvBn/9j/6aBK5qEBOOWqKAYfPEMyD8zKLdn8sfY8AMdy8gX0RIQBwQRhyJZWEaFU2Ul5djz549mDJlSvVtJpMJt9xyC7Zv3y74mLKyMpSV1XRpFBQY5bFgHGYTh+7NYtG9WSy2H7+IpfuvgnKCA2V+AXh05CuKghUitO2oIEgBSDcQYC8vETWB6309eZ1bxoVjw9Fz6PjGahSWagsKp604XP1/VydgnviIILw6LMm3NC2750sHKYDzhGKttB4BtBoG7PxEfcsyDSV5wOil5EuY75xo0IV4sFT9vnjRUlRa38LoLsp2PboL8OsuIO1vO0YpGRslZKsv1N0x5C2SffAkMU1J5sGtc0bE2E4LK58FkoYpXyClOqxKLknb3ZvM9GMELmUrOy9vINYZxgcppgDAJnbhVXWxVIsGEArh1UDlwoULsFqtqFfPOSVVr149HDki7O8xc+ZMvP766544PY+gZDihJ9HaxlzmF4Anh7+ILR8/iNCyYkj9iVhhQgFsuAMTFAQpAGlZBjgBwy4+UADgZnSnJ2JVoJz8Ujz+3V587EseLLRfynp+eZvMQJfHyFwgI4WjV84Bbcc43+YgHDxa/AcA0t2jBH77Q+etGEXzADFRpuAVcSjQdQKQdCuQ+ZuyE9NCaD3y2rQcQgKnoyurSlsGZFeKLygXcVYPH5Th9xfJc+AXYMdAUC4g54lqTH9e3oBmEKNokIKqjiwBT5Zahtc1KkqZMmUK8vPzq39Onz4t/yAfpm6YkoUZCA7wTFSstSDoX5SHd755VjZIAXiBLfATJiMQSgKKhfCLbABTUJjbPVOHkCDl8e/2GhakyOFzmhbaL2W9v7yrhY4GckV6REOLFi0AkBZkJfDbt65D8XfX4xlhUSZ/RewaqJUXAZtmAcfWkPlEWmlyMxAYDnHdAuc86dlkJv/PXKL92FIoFXHSdkMVnq2ZLpyZRlq9vx5K5gHRlH04c818Kl9FS2dY65Gk3FPLgxTAy4FKbGwszGYz/vvP+YP833//IS5OeFZHYGAgwsPDnX5qM50ToxEdQl/7nn/vjfj+4S6YcHMzdG4cZeCZqcf2zw7M++gB3Jz3r2yQwkNal3PxGwZTBis5AH5DWMdBgrb601Zk4rW0Q5600RIkJ78UH6z/x8tnUUXHh/TdTglJqaR91SiCpUdLjBw5En5m4pOihIU7ibA29XoKvVFRrnC5R+6KuLyIzCfSSsMUYPiHVb+4nq9ICcATLdJKRZxKApui/8QDQTlSxuujxTISLZ06hxYDh5fpdy5exKuBSkBAADp27Ih169ZV32az2bBu3TqkpKR48cw8h9nE4Y3hyVTbRof441xRGUwch6f7XY+7uzYy+OyUY/tnBz5d/Ab6223UQQoPCVb+wG8YLhOsVALcw+D8AxGa3Edwi5z8UkOnQSvhnbX/eL+t2WYF9nxBt+2ZP405BzUtlbRckRak+vv7IzgkHIt2E58UGnIuAYv/BIIDA+iEtPt+IIumI570Stn7NSmFjPkGCHcpN4YnCNvdG92y6pjBoUVJYBMcKx8IusKZSVea1PRsX0Frp86KZ8W7pGoRXjd8e+aZZ3D//ffjxhtvROfOnfHuu+/iypUrePDBB719ah5jcNsEPPrvZXy6KUtyu7wrFXj6530AiAbjjk4NPXB29NhKi/DUklkYJLOdFSZJH5VBWI3peArP4ROBLXIAPAxgFerc+qrqmT+eZsrig+iXFOe5TiDe0C3vBJCXBZzZC5RSDjQzavGSnRirgWJ5S/GffvoJw4YOxkOfEZ8UqRblSiswdn7V4z54Azj5Bt15pE921k140rui4Ayw5HGg/d3AxP1OYmJREzajW1bViDirPycyAV5YAhHT0QSCnR4mAls9zQ09gVYDRTUaIR/E64HK7bffjvPnz+P//u//kJubi/bt2yM9Pd1NYOuLlFfa8PW2bOzOvojgAL9qJ1k1i9GUwUlod10UXlh0AEVllbLb5+aX4p21fyM4wOw29ddbFGWsw3KbDRPgDzOsgsEIEc6Gi5q+8TN/VmA+gDy4z/r5jcz6ufVVWBI76P4cIoP9MWNEMv45V4R31upXsrlUXIEdJy56xoZ/9VQiXqUVFLpi1OLFa1WqJ8bquW/55PCgQYPQtt0NSN/3F1LfIj4p8QLV05xLJEhJPwDccEMHDLr/GeDdL9wWxPWoxApUYjoCEcSXVgrOANlbAM6E0vzTeHnfAgxBJfp46qv2wM/khxf1yk0B1hI8BoQBzW4B/lnlXrrS4vRa/TmRmas0aLasNqmahimemYisN7yB4kINnWG12OiNx+s+Klrxlo/KzJWZ+GxzlpvoNDjAjLfHtFPd6WG12bHj+EVsP3EBNjvw0+7Tmjw/PIXdbsfZz55A5eUU9MNYLMdQt2DFChNWoz/uwE/4CXcIzPoBrCYzHkx9Ab8XXkThnt8FpicPQmibvjAFSrdvhASYERbkh/8KyhR9/X4/tgu6No1Bj9nrdRfhTri5GZ4b0ELXfbrBG7qpwkN+C5lpxOdCiSW9HPcuBZr2ptq0Q4cOOLD/LwDAyBvdZ/0srqp8tWvZBHsOHa85Z4eFczUqMRTFqOCAgXYzfkNwTbDiH4LSiiLcimKkc1b424HlCEZ/b1wX0syuqW5/BWSDlfqdgCa9Sflkz5fAFYdF0D+EBC6dxgKNe2j/DIn6qDgEQbSzpTw5EdkItPxd+/BzZ0MJDWTmykzZMs0nOrSlqh1k6A2sxfn49/27QUzYbkM/rHYKVvgghfdJCUQpfsOt6I9VVdOTOdhMJjw0+lVsrsqU2O122EoLYS8vARdggSkoTFA4K8Skvs3RKj4Mj1cNcpT7kHMA4qqmJO/KyjPkdZ9wc1M8N6Cl7vuthsbQTZSq11VIx2AENiuw9Elg//fa96VijP3vK1fijpFDUVxpd5+ebAYWjrGgX3I95/1umA1smFEdpFgB2DjAZAf6oyZYKYUdt6IYq2Gtvt8MLwUrwbHAM4flSx20waMlGhjyjvQVvp6D/WxWkqHK2kw+oo16kEXXsSX53WSJjJDvmp0pRo2BojemQSuAdv2ude3J3qa80ob5m6WDFAB4Le0QyittxNBt3xlsP35RcZvqmsxctaepiEA/7boJewWffSC59DXoj6FYDmuVpNYxSAGAMgThVvyG1SDTj60uQQoAcBwHsyUcfhH1YLaEUwcpQX4mTOzbHAOT4/HxPR0QFyHdAs7v9dVhSTCbOMNe95QmOpd9HC21szYDuz5VX+4RE1sahckMDH+fXIVrZdh74l/Erq9RlbBwUMsQ5E8OQ/GLFiwcHYTXb/LHwtFBKH7Rgvwp4ejX1J9cyWdvqdlXr+ewOjjCKUhB1b+rYcWtKEa+S5DC328FMBTFWA35sq6uFF8A3m7lLvJ1JSkVGE4xJqAkD/jtEeltljyun4DTZCYW931fAfq8QrJmju+1U+s7ZadTbaX1COC5v0mGpOsTJAiVhLtqnrvXNSq1jW+3Z4uafDmSW1CG9v9b7aQfUeJWWl5pww+7Tmk5VWrKKrUn1Th/PhioEWyuQX8MRDqGYAVexnQ3MzcSrEzCdNyLXcMnY6dOmpO7ujSs1gkNTI5Hv6Q47MrKw7nCUmRfuIIfd51y6giKc3hfrDY7luzTv0sjOMCMrk2lW2gVIWQgpnbR7z8d6Pq457/QTGag+yRgwwy67YMigdLLNb+HJRCdglhwJfQahSeQhS3nAADSDTSqNcTN3LI2k4XSZsX6He9jaOkZpyCFx8YBq+1WNEYhCkTuhx0YyhUj3R7srFkJCCUtypJwQLcngb++Jc6sSii+IO3kynNqG93+rDLddOVFZCZP877Up6gJfsaP4Htd+83OnOCnRif2BPq/QYSyR1YA+74Hyhxc2sPrX1XPnQUqCjmZR+934CpypXUrTc/IwXO/HkBphcqrYy9gsoTDL7IBKi8vBHBb9e1/oA/+gHALMQCUIQ2TIxsgoXlX3cak9Uty9uCx2uzIPJuPk3nFaBQdjD+euxn7Tl/GucJS1A0Lqp7LY7XZ8dXWLFWaIDkn30A/HZOXYvbiFQrdzHjqJikPUoTs4NUEOr2eA7a9Tzez5LavnW3ypY4pZjtekENub0mhawDI46sCnhUFJ1DB2UT91GwcUGB3D1Ic77cBWJE0BH1i25JTS+xJ9BxHVghY2lfhuOjc8hoJnvb/QESzSlg2ybkjSeCp6sbBnz0XqADkteGddrV+JmsLjkHLgOlX9XNngYpCGkVrc5Dk3UrFWlXTM3Lw+Hd7PW5UFh0SILtAB/iZ0L9VPSw/6F7H5jgOYR0H4tK6LwHkAhA27HOGN217iLqsI0dMSAA6J0ZX/z5zZSbmb85yyoJNX3kY43omYsrgpOrb0jNyNFntyym9LhVXYFdWHlK0ZlVsVrLg6MnJbUAz8WDSDalMhdIrOJMZSP1AvqshvL6zNkEMXtOQ9iSEV147AA74Zw3d+eWfATa/BcCO6QhEpt3qVNZxO7zEx9hkB/pb6mD6iC+BAJfsl+NCW5hDullC6pAJuY6LjslMyh+JPclU4SIFZcqSPGDTXKC3iD19Yk9g8xz6/UlRpjJodkVJQMwv3NciV/lzZxoVEaw2u6C+5N6UxtBqhZGTX4pdWe6+D1abHa8vy9Q1SIkOCUBwgPjbzIGUpN4Yniyb0SivtGH5wRwEB5gRGexugBWa3BecfyCAsYBsLV7etE0N04YnVweAvOjZtVRnswOfbsrCzJWZAGqCQ7VByg0NIqm202WuU/YW9y4IrSj5PPPZHNcr/4Kz5HY5LYQQySOIAZfUCdLU2jOWAHObA9+kOpeI3LADVsr34u+V4AOeIHD4DcHoDzNMCv9IqwW3JaUIeqe18OvELzZtxxDX1LZjxIMzkxm4UYWD8M5PxPUjjXsQsawUtFb/jXQw7HS1xf96KPldzWeMUathgYoA6Rk56D5rPe6cvwOTftqHO+fvQPdZ65GekYMAPxPG9UzUfAyhRWtXVp4uLbExIQF4Z0w7fD+2C96/8wbcKWEMZwcwdUgrDG4bjw/vuoEqCCsut+JycQWevuV6vH1bu+p1zhQUijojXgC4VQCGg2RMhMipun8V6tz6om6mbY/2SsTgtqSkRiN6nr85CyXlVs3B4V+nL1Ntp3SukyBZm7Xvw5VGPei2o8nmLJukTkjZfxop7bgKBMPr04l8V08lWRm1xlhilOY7/aomWHHtCiLTf+/TvuDGNFX+mJK8mvk4bidqJsJkKYZ/CPnIlgM6ywhu5RCzxedLdyxYuaZgpR8X0jNy8FhVS6sjuQWleOy7vRjbvTFuSYqDzQ58vsXdR4UWoUVLr0nK028llvzPLdzvFPiI6SimrTgMk4lDVEgglVCY56fdp7DlxT44+l9Bdbu2pUlH1B39fzi/5E3YKxoCuBWeMG0L9jfhhYGtqn+nET3b7MCMlcZNVnYkJNDsVJJSjd7GtgGh9CljmmwO3ynT5Cbl59J6BNBqmPJa+6ElGrxjJAgIAcrdSxhB4PATgolwVkKTApAghQzcdPBZAQDY3V1shZAqfag15pMyAEtKJaJbt9Keg07m7F/Sr3e3J5U5v7q2IDdIkbDFryrd0bx2jKsGFqg4YLXZMXnxQcltFmzNxoKt2YgLD8T7t7dHTkFZtTNtSIAZ3++Sn+bsb+aQcSYfHRtFIcBBZKn1itvEAR/cSRZ+IZ2LWFCVWyXyfah7Y0XH40tYvNaD14JYmnTEdU8sQFHG+irTtl+rH0NM2x6qNm3zM3Go1GG6cHGFzcn5lVb0nH1Rh2FwFBSXWWG12bVb6DfqAUAnHQFAOjSOrKDTltBmc05sJGWErM3AyS3OolG5hUVprd1mJd4SRpA0nMzvcaEUdtyBYsHuHrfTqxLY3oFiZ1M4gLjYitmb26xET7LzY+cun+AY4lSaPILeat6VkDrS98sJU/kZOa7ux5yZlKyUzNARM3WTxF7jAKwmIGbUOpjhmwNbj13A3Z8rG6/qaOxWXmlDy6m/U2clTBycRJ1Wmx09Zq9Hbn6pqlLER3fdgAHJ8apcVTkAUSH+yLtSoehx793RHsPb1wcAbDx8Dvd/vdvpflrTtiA/E0ortXU5OTq/Lth8AtNWHJZ9zL1dG+LbHZ5pA586pBXG9myibSc2KzCnqfIWVSloTaHWvwFsogiS/IIAk597y21gJFkAA0P0m7lC60yqlIBQ4IUsYF47JzMxVzM3WtzKPzyjFrhbu2emAcsmSr/H/FA9se4mKULjgMFztLeu8vOkLmWrez/Futdo8Q8Bbv3E+BZcrc+TIQozfFPB9uPK69uTFx+sFtoq1a+4ijrNJg6vDiNBi9h3YIiAkDU+Igif3NMBg9smYMeJi6pKGXaQoYdRAiJZKRyzQLtPuV8V0Zi2cZAeEEdPzZc1jejZxAEvDU5CfESQ6OvNAYgK9nd7XSIt/ujcWGBQjARKWttFMZmBYTqXOfgrezlotSyVpcK+IGWXiRPtrs+AVS8RF93VUxWdqhtGzTHpOp4sTnVbgy83qA1SAGdTOKfLENfyDb94ywWi2+YREzveQ0ROBOtIUa564bMjfgEkgzJ4DvlXabknXaT7iJaKK+R5HFqibT9SrJ5KPqerXtL3c8tQBAtUnFCex7hcXIEdDgHOlMFJeLRXoqLOoPmbs1BelU0Qc1ONDPbH07dcjwOvDcCeV/rh24c649b2CeifVA8PdW+MPi3rIT0jB+O/d9fXKKGMMqvBdwt1Toyu7pD6+z850yph7ACKyrR7xjg6v9IEjeN6JsISYBYNDvnfZ45sgz9f6Yenb7kekRYSsFwuqcCubGVZDa2t7dXwOoIwbSManKBZ8Bt1I5kGvbDbyIKr5UvfiAGKfkHApjfJonSMb2O242WUIZ2TDlKkBLY2DkjnrHgZZSDW7vXJa1q9gcLFe9FY4JcHSJbq+WNA75cASyT949UKn/Xg5DblJSsxFj4IZCzWZ1+O8PN1XN2e+c/tqlf0PyZDEFb6cWDrPxdw9wJlpR8AGNE+ATe3rOtkHlZeacO327Pxe0Yu/jwpv6C5lgWsNnu1m6rjfgFhb5Aq40uPwH9Pf3wP0cNo8R/Ri8hgf+x5pZ+bBkTotXItuQHCPiqOTsJa/W1MHHBk2iAnTZJmeKHl4TRytacFucFlQt4pesGZgZdz1aXTq2e9GHBeLqxHJQZyJcSZVuCTwAtnxbQr/MyfdIQQZ1rXbia1ZayAUGDEx2Rfq14Gtn8I6m8DBcMcdeXgQhJo6QlfDtMD2rlZo74E2ozU55jXILTrNxPTOtC1aQxCA/1QVKZsHseSfWerbdcjLf54sHsiJvRphrE9m+BkXjFVoMKXBVwDlKFtE5wWX7GBiJ6MNnnLeUBYtKsWOXdXKWaNbCMoVJ0yOAnP9m+Jb7dnVzvT3pvS2C1gcLXad3Ws1drCPK5nor5BCuAsPNUSqATHOl/Zu6JGB6EEu5WUWVLGK38sP+tFj/PzCwIqy0T30wd+WI4QDOWKAZvdKRjhNSg/IRh3oBir7Va3+/nBhH3CGwrbmx9dqe68y4vI828xSPk+9nzhnUDFiEzYtnlAQkciNFaCUGfVrs/o5mYtehAw+101VvW+CgtUHDCbOLw5qi2e+EF9+eRySQXeWfs3vtyWhVkj21Cn+xtFB8te1ZdX2vAZxUBEo5hwc1N0b1anus22x+z1uixd/OTioW3jqQY+OhJp8cOsUW0lRxIE+JmoRKxmEyfoHKvF30Yoe6M7ars/eAa/5S6k5b+8C3OA9CkwPBS+lK3+sWKzXoIigHZ3AcV5xNJdDpsVcs+zv92E5fYgDEUxYBeenvwbgomWpSpYMcEEs9mE5SmT0T+xL4nIr5wnGRS+m8ZmVW6J74RdXaBzfD05tqfbfLV+ZsVY+SyQNIz++QhlCgPDgAqZeUaOsFZpw2GBiguD28bj0X8TBbMWSrhcXIHHvtuLD+5oDxMHyU4gEwfUCwsUzE7kOswHOp1XojrjIEWkxR+XS+S7fZrXC6teyLcfVyfaFcIOVAdjJg5upRpJOM5tto/e0PrbTLi5GRJjgrHv38sAODSOEc7e6I6WrEK3ie5XoEaWecSIaqzt8VIttcc3UAYqdB1v/eGH5QjGUBTDBvduHsdgJR1WmE1mLL9rOfqXlQJLHiPBH09YPDDoTcASpb9ZHQ1lheIt0kZS/ZnV0PUjRPEF+ucjlikso5g75YhUmzlDF1igIsCUwUlod10kXlmaobhd15WXl2ZgbI9EyUzB2B6JmP77EVF7I4DoQJITtGlwhBjdoT6Ky61YmSE/M+RCYRmW7juDumFByM0v0e0cbrq+TnVGxLFUs+PERaw5fE7ysbyYuXtzuZHn6qH1t+neLBYpTWMw6sYGhp2LKGJZBTGCY4EhbxGTNUeMLvMIwZlJy6dWxDxYEnuKmreppT/8kI5grLBXYjoC4do3FgQOv/V6DS9XXMKQ64egT3EB8KvAPKPCHLJYd31Ct3NTjFGdU3IkpRI34l8fgK6fN5rnUy1c1um43noNrxFYoCLC4LYJGJAcj11ZeViTmYsl+86qmqqbX1KJg2fyMa5nYyzYki0o6uzdop5sySMnvxRNYkMkt1HDwr1nqLd19CWJDtHPR6CXS5DBl2ouFZfLBioAsP3EBUMDlc6J0YiPCBL1t+FLV67Os7ygWkoboyuOWYWjK4EDv5ArTJ7gWDI/psVgYcdXvb+8aaFtbZUbUGezipvMdZsEbJhBcTL0svQ+8COiWCEsMQhq3BNvXTkPZO8ENs6W3tneb6mOaQhSehEtHiKO5UOxIYvBMdD980ajf9Gz64j2mAzVsEBFAl6zkNI0Bi8PSaoWWl4oLKMyE+PZcSIPR3IL8d4dN+BcQanbwvXbX3TBQpuECGxV4fViBJdUBG1CmDjieSIMbY+3crdXJUEE72/z+Hd73ZYx/sivDktyEz3TTG3WHcfR7/3fUGZHr/eXtywmoNsEuk6NQ0uIA61j4BUWD3R8kMy8uXjc3cV18xxiCtZ9EtDjaff7neCIZqL1rcRxVSvWcjIckZbyQuKFUnIJHu3fC08QF1KvnuruPrv6FSCF4j0Ter94HN119c5EuLZ8i6HbcWVeQ4YusECFEkehpdVmx+dbshRpNC4XV2Dij3/h43s6uAk784rohFtF5ZWw+JtQUqHdc0Qren2VSnXDdGpEZ6hGux2PmiCC97dxFTvHOYidHfcvpHHiDf4AGBus8Ci1o/dk+rrdHcCw9+muznk/C1cKc+SzJBVXyDY7PwFuuFdkRk1VgMl34pzaCZzZLbCdAsoV6hwAoHFP0mouGA4bEbzYxadSi73mvIfIhaNAypPCwa/YY3mKL5IBkmcnAs37a3sKrtBM2QZ0yoA4fG6YkNZQWKCiAv4KW2h4oRR2EK1Jv6Q4p6vvyGC6VOp3O08hMtjfJwIVrTh2wzi2ZMeGBgJ24MKVMuzOopv/cfS/QgT4mwU9Z1zREkRItTDz0E5tfrZ/S+NFtjxyJRMeT6av/14N/J0u39aZsVifgYMlecC294l4OGOhy8C9BOd24Sa9tQcqaoi9XlhnxJ/f9QOB5ZOAzKXOehv/EBKQKYYj+3Slslw+q/T3KvITnkBEsfxrl7GE/v3aNg9IuEGf7h9LNJn8TNsm3KgbKYUKZXxocf3cMAyDBSqUuJYK7urSCMEBZhSXK3N25Af5ObbBXi6mL6PkFxNxb0SQH/JLa/xewoPMKCj1ksukAoIDzHj6lua4vxvJpKw8cFazaPndtf+guKLmuUeHBOCN4ckY3Na5ZVmPIEKshZmHdmrzt9uztc/9kTyI1UGr8rNzR4mYkLZBF5KW90T3SUkeEe26mp45krEEWPiQjge1AxmLgIn7gdM7xQO3xJ6kbKSEoGjAVi48OoCWxJ5kyJ7UQMARHwOpHzjfn7MfWP2yigPahf1rds+n8xAByBwk/n1sOQRY+YyyU1j5PGmPX/gAVGeN2t0JDP9QWVbDZCZ/A0ICZyl6v0RKjbSTvdXC5gs5wQIVCoRKBW+sPKy6Vdi13VWJMLVqyDmCA/3w0d0dceFKGeqGBcFmtyseqOgNisutSK4fiQA/k2h2Q/E+K5wDtLwr5Xjih7149F/nUo4nggjaeT66zP0RQ669uPgC+YLOHAkM/4iYfh3/gyzeZQXGnZcbdnEPisw0Uh7Qm4Iz5Hm6lsQcs07BsaRdWHLejks5xl6pLUixRBPhLyBfsnO9v0EXYM1U+uDCESH/GkWeNlWvQfpk4lujNMgtvgCExCjrWHPEEq08SOFpPQI4M5E+A3TTZKC3xvlENGjRBl2lsEBFBlEnWA0l4+wLzotUXIRF0ePtIJkZk4mrnlxstdkRHxHkdSt7Gs4VlmLlgRxdghQpPt2UhXbXRWJw2wQAxgcRVpsdtBMpdJv744qS9uJDi8mPNxHyoNBjYJ0UR1c6H08osLPI6Z5cvTc0BnjD3hNebGnKdn4BZBFTUyIT8q9R42lTcAbYvUD54wDy3NqMrskkFZwB/t0N5GUBx9dJP1bsdaOl/zRSflr4oPR2QVHATS+oPw4tctog4JoMVthQQgloSgVq+Gn3qeqJywBpf40Lp/PqcMQxM8PrZpT3vzjTtr5yr5b7UxoqOm5sSCBeWZqh+DhqeGVpRvVrXUw5GkFNELHywFl0mr4G3+44JbutdKeTBrzVXgwAAWHqH+sq4jW6+2jHRzWTg/lpxa7HK7lM/nV9XpzOX5mWaDJgUqj8lZlG5hh9PZTMxfl6KDCnCbBhtvswwf7TgOYDlB1bzL+m0zh1z/PwUuWPAWq0UXymqN0dpCxz72Ly2ggFjVKvm1KSRwKjv5behgNwZIX2Y0lBow3a/iHZ7hqDBSoS0JQK1MDrVHjMJg53dFJuEuZqRFY9eVlF0MNz4IzyK8NFe89ibHfpScU88RFBAAdVnjSO0AZGeVcqsCsrD1abHZv/OS+7vZogYubKTDzxw1/UOhtD5v4AXmgvriIgDLjhbvWPdxXxOjq3GgJHShWV5cCyiSLbVP3hO3bvBIarK68IYYkieofnj7kvtjYrsH6meAC1YQYwp1lNsMWj1DROzL+Gz9B4AkuMdGtvUirw/HEyPLHX80DP54H70oRfNy0kjyBCazFKLpNMpetrric02iB+LtY1Biv9SGCkjsBVp1KpICISMxgDajpTPlj/D95Z+4/W06SiqKwSn2/NQr+kuliTKW3Q9uqwJKw7rK4N9t6uDcFxHBpFB6PSZsPM349SPe5cIQkM/yuUD44Gt4l3CiKkplhbbXa8t+Zv6hKW4XN/DF/gRSgvJO2/ALkSV7KYCw1ENLxN2k7KC8uektGhuKC1vNN6JClvSAkxM9NI8CR3XiV5JJBxzCpwCvKarUdKlxD4+7a9D0MzdLwhnxQmMxmcaOTwxMy0qucqRpU60Mi5PrTaIC1zsWopLFCRwDAdAYRs2ZV9GbgajDliNnGYdMv1ADi8s/ZvdSeogowzBfjgjhsw+beDbhOoo4L9MXNkG/RLisNzvx5QtF8+MHstNbn6Of+291/qx9cNC6Ke19MvqebqXmpIJABMXnwQl4vpsij3dm2IqUNbG9uSfEU+Y2Q4vEany+PAvu/k56a4DkTMTAM2zaU7lqstflgC0OE+4Nga4Mwe+cdnLqE7jlb8goARn8pP9eXLUEpYNqlm4aR9/y1RwKjP5be75TVg73dAKZ1NgCrsVufhjN6AumRqN3auD602SOtcrFoIC1QkuDelMaavPKxr+UcsG5LSJBYf/HFc9vFRwX6YOVJ6WjBP41jjAi0hcvJLERMWiP2v9seO4xex/cQFAKSdt2uTGJhNHLYeu+AWxEgh5vxaV0F5q2OjKOw5SXflzAeQ6Rk5gkMic/JLFfvnAMCNjaON900JqWPs/qmouvI8nAYM+0C67dR1IKLShbr3FKA039ku32YlnSQ0gYoq7xEVVJbK1yrVCohL8khg1/tFeh+cLo/TBQUntxkbpADAkeXkx9WPxZMoLZlqzfiJjRa44X5g1Uvyj++oZ8t+7YAFKhIE+Jkwrqf2Sco8YosuAHRtGoPIYH/JK/TQQD/sfKkf9YJHO0xPT84VlsJs4tC9eazg/J3tCkcACDm/AlCUgNpz8hL1vJ6OjaKw9dgFvLjogK4Jb4+8FyF1jT8GFfyV52ag2S3ubc9CPi42q4ReRADORFo2efZ/D8S3JyZysqUnrqr92OBF2JEVzwKthokHCFr0RTs/Bno9R7IS4QnE20Ts02uJJtvS4EmnYt6PpfcUz/iUOKL0eWq5IJCyDgikbGQ48+c1N6mZiWllmDI4CY/2SoRrlcXEAY/2SsQn93QgAlEH4iOC8GivRLfb4yKC8PE9HQSzIWYTh1kj20iey9zb2iq6KucXZ08ivyDTLf/9k+rhx3FdseXFPoKv14UrdGMHgJrgiS/ZuF7c8o4YSfFh6DpzLe7+fCfyS+izPnJEh/gL6ol0R4lGwRPs/pyUYcoKAHCkK+X+5cBzf7ubzWVtVqYXcQ1GCs6StmOaIAUAujxGfyw9KL5AghExtAQFJZfIvk1mkpUAIJrCUdLO69FBe3bys2FGTYfT3OZA+hTy2XDtctITpc9TraiWtw4QC0hpNVDX4KRmllGhYMrgJDzbv6XoEDsxW/UXBraStFt3ZWByPB7tlYjPNmW5TfoY0jYeZZU2bD9+UXY/PGYTh9R28Yb7lfDnKCbwdYS2xHV/SmMnB1hXUWtsaCD1ufHBk9i8Ho4j0op1R4zReLwxPJnq/dKMnhqVfm8AG2dpMzFzwg78swqo00L4avDkFp2OIwNve95yCLDzo5o2ZCUExwI3jgU2yUxEdkVqgdEaFPD7TkoVseGvr9zuvTpD44VOMoCYx+34iPwEhAJN+wKdxtIJcJWg9HmWXnYXMsuhp3XANTipmQUqlAT4mUTdSsVs1eXs1l0RM0GzA1h+IAfLD5CujuiQAIxon4B+SXGSQYvVZkfafn06QQYn18PGfy7gSpnwlY0dQIt6ofhqa5bkJGKaEldksD+6Orxu6Rk5eC3tEHILarIo9cICZPcDAHHhgU7Bk+O8nrWZuViw1ZgWdJ5HeyVWG84Zjp5fYPn/6hikOLD9Q6DPVPe2WJsH5ld1ehgY9GbNItflCfmhhkIMeYuUcfZ9q2wRd3x/bFYgewvJFnAAGnYDQuOAolzl5+O676RUaRt+WvgMjVKBrxGUFxGflsNL6eb60M63Asjt8e2VB2S/v0jfAaSXdQDtdOirDBao+AgrD5zFhB//oto270o5vtiajS+2Zld3oQiVR3Zl5eniVBsfEYT37+oIAPhg/T/4cms2Lpe4Bwgb/r6ADX9fkJxEzJe45ASpazJzMTA5HukZOYLb0rQaA8Brqa1htdnx1dYsp2xY58RoPPPLPqp9qIED8P4dN2Boe/kgRaoFWhGyGgWuKn1EERTs/0H58WngfSAc58tkpgF7vzLmeI5wJudFpU4LKJ5M3G1iTdlq4Gx6F2DHBSYzjXTruGlkVGbdwhIAawWw/g1nYbEeOoakVJI5oGmZ9hRCrdmOCOlAgmOAwW8Ld14dWkLKhkopPEvfAaRXuWbAjGtyUjNnp/X89lEKCgoQERGB/Px8hIcrd1X1BcQWY1o4QFD7snTfGUz6aZ+2kwMwukN9BAf6VS/yZhOHXVl5mL/5ONZLlEse7SXuGZKekYMXFx0Q1ILwX9cf3nUDXlqSIZk1CQ4wI8DP5LZNZLA/Zo1sg79OXXKb02TiiF8Kn6Eygo/u6uA2FFEIqRZoms4uN6ot9AG4FRDtQLN+RDcihdapsnJ0fgQYXDX0T4nlv9KgwpUBM0iAZLOSThkl2ZTgWNJKnTTM+Uq9+CKwaorM1TJXM3xRTQuyHAGh7tkvpdOE5agsB2Y1IB1MvkJ4feCpg+7t7VKfp9YjSWs2/xibFZh7vfrP+6gFxP5fjqzNRHejlfvSyODKqwTa9ZsFKl7GarOjx+z1mjIfvD5ky4t9nK7Etx+/iDvn79B0frx+g4c3LXu2f0u0nPq7ZNnExAFHpg0SLANZbXZ0n7XOqZzjdFwAkcF+uFQsL2r99sHOMJm5qo4iO1KaxKJr0xi8mX7YI/ocR/gAiSbIEGuB5t9BMeG1LIKdBbSLPEeEpjs/Vn5cWtrdDdz6EVkk3k32nAbipVwSpFENvzMBPZ8D6raoKR0cWSGg/UgA+s8kg/WOrAD2fe8sinTUhqh+vhzgZwEqXQwo/UPkW6z1spk/vgH4drj2/ejN/ctrMhq0r29AKJD6IcmuaA0gHI8vhV6f9Z7PA31fkd+ulkC7frPSj5fRozzDDynclZXnpImRa8ml2rfLA212MuzvwOl8TZOId2XliQYpAHlONEEKAPyw+yQ+vudGdG9W0w5t1JwmIYL8OYzt0QTdqgIkmrKN1WbH68syBd8XfkL268sy0S8pTnkZiNcobJoLbH2vajGjLE8MnAXkn1Z2PKUcX1+jIaD54g6OBdqOIaJKLWybB2yYCbqAzQY06VWzCIldqRecJV4xY74BBs0CBkwX10ao1inYSZDScijgbwHsHFBZQvxH5NDLSdVTYmelOJZUaF/f8iIymfvsRCC+nfpjhyXQ60Wq9T602UMRfKyxz1OwQMXL0DqmKtmXo+bhjk4NDXGn3ZHlWl8XRmwMwdpMlaJBATb/cxFWm91pQTdqTpMQpRV29GhWR5FwWi5AFQs+qTmyQnlpY+I+InI98Ivy4ymhKKdmMaeh9Qgg4jrtx935ERQtEvz5yXZs2IHlT5OySFi8uHBTq06BJjBxRS8nVV/NuzuKiJW+vtvmAb00TEQeNFtZACjWkaWERj3UPa6WwwIVL6OnEVjdsCBBzYPGyr4gtPsTGkNgtdnx274zup1LUVll9YLOB2kb/vasnbzSgJN2e1WBrBqX0+ILxJgtsSdZbI3mxEb6xXN3ld270jlCrihtReYXQZor9eILwOKqScRiLqveaivVQ8iZ2BPYPEf7fvTEtQNGzeu7eS4QFElajmnRov9x7MjinWkvnyYXByUyZpiW6GvO6I2HBSpehi/P6NGdcz6/BJN+2e8WRCgJUuIjgtCiXig2/K1dTCk2iXhXVh71pGFazhWWCgZpnkJpwEm7vapAVm2JgR9q6An/jM1zgD+/cJ/XI4XqIEWFE21ASM0ieGSFssPxLqu8gJbHW74kegRIjXuQhdIIN9/w+kDyKODgr8oGaw6c5ZzRaNSNdPcUK3C/ttuUBSnJtwEd7iWvh1pMZveAY8B0eYG3ErO+qwzmTOtlHB1TtfJyWobqzMl9KY2qnWB7NqeziE5pEiV5/7ieiYJCWj3LXTzZF4rx+Hd7PR6kcCDBnVLnWT5AFSs5q90vAPVTlNMmAutnkv8PnA3DC+IlF+mDFEc4l89UeH3SNgwOwr7DAGKbKztG+RUSoNisRCCriKq/wvTJzo6qTs6xHkIv3w2TmSyUUoi+ByJYooF7l5LOnf7TgKcPEXHqqAWku+WmyUQwLPQ4IZGwyUxakNVgiaY774xfgW9SSafQoSXqjiWEyUzmNY35FvAXmtHGAf/u1u94tQwWqPgAA5Pj8f6dN2h2QC8sVW8zPSg5HilVQtB7Uxq7jQxwxcQBXz/UVXK8gFhrst5zb6KC/fDjrlMeL6NLzW6SQ87SX+1+Aah3qK0sATbNAmY1JF+Klkh1+zEau420Go9aQBY2fqEb8w0Q7lK2Ck8ARn+h7kv+9xeJKRuttbnzSdboQxzhfUksHhipALhnHbTAn3u4izdQeH1yu9h7IEZJHjk3/vz4TEOb0aQF9+YpwJTTJJjp9TzpeLkvDXj+mHjZJXlEVcCkkJI84CYF5dLiC8Cv9wOrpyo/lhT/7gYqhHR9dqKp0ft4tQRW+vEB0jNyMHVphluHjScQsr6nGcbIZ0vkxgsIoWe5CwBSmsRgZYZx8y9iQgLw+rAkTP/9iNM5iw5MpETM0l/rfjVPUS4vIl+KvkxoPWH/Ctc/IrsN+GeturJR4Vlg9wJ158cjpA/hdQqOzrSNepDFcuGD2o7HExQFpM5zXtCVuLWKIed6y9+/YSawiULTIqefMZmBpr3Jjytiz6f/NKB+R2DJeKBCgbtybHMScCkRu26bByR0FDaSo6W6A+4MsP0D6W3FnJ2vclig4mW0mr3pgdCVO58NETJLc3WdlRovIASfTRDyEFFDkzphAPQPVHgR8n0pjWAFMPe2doCdDETU5CDrgKOlv2ZnWh5PiGG9zUWXeVFi7cOFOWSyslr+WaX+sYC4PsRkJlkDN/MuE7DoQW2iYQDo8qhzkCLkrSMm+pVDSGPhdv9NdIGKWv2MnPts6xFkzMGih4FDi+n2+fdqIKoh8VjhTKQlm+Y5rHyWGAGqyVxJTVMWQsjZ+RqAGb55ETnTM0/w9C3XY9It4vX78kqbomyJEvQQv0aH+OOeLo0wb/0xXc7JkchgfwBwcr3V5BrrKWxWYE5T37E8N4KwBODpDLI4eNo4jhYh51QaDi0hZQUt+IeQsonJLO+Gq5cpnCPlJcCMOPntXsoFAizK9i3nPtttIsmq8GQsJhOZlQR/liig3Z303j20xm+OKHJldsDR2bmWQ7t+M42KF5EzPfMEjWOFhFs18NmS/w1PxtieTUSDFKvNju3HL2LpvjPYfpz4msgxMDkeG5+/GdEh6tOYNzSI1D1IiQz2x1N9myO/uMLNmj83vxSPf7cX6RnG2e9rxmQGujzu7bMwFn7OCqDfwDdXhISc1HDq9SGtRwhrQZRQcYWUlmxWMldIimWTnEW/erDnC32346GZQrxtnrPQNXkkMPpLZccpuaTMYFDprCAt05SjGit/TC2HlX68iBHdL0rRImzlPUvWZuZi8V9ncMlhUY8LD8JrqfKZhz0nLyHvCt2AQVfa1g/HOolZQ2q5XFyBb3acNMY11lP0eo7Y4F/NWRVe36C2y0mOpn2BI2nKH8f7bLQcQjQoajQhQn4bIXUASwzwy91ARYn8PrI2k3/lWopL8khQo+cMmUvZ+m7HQxuUrniWlH7417v1CIBTqD9RwoFfgP5v0L+/aoNrzgx0Gqf8cbUcFqh4Eb27X5QgJKKlxWqzS05RBoDcglI89t1efCIzr0ZtsDY4OQ4rM/Rzt3VFKnjS7BrrCUxmYNg8/Qfg+RK8vkFtl5Mcamzjk24FRi8grc3vtHYOosLigUFv0pdZxLQgzfsDmUvlH39yGxlXQEPWZn0DFdqrfqXZAVrzuuIL7o68jsHfrvnAYYrXkBah40mh1oQvZfw1J6QFWOnHq3ROjEZceKDHj6ul/TU9Iwcd31iDd9b+IxqkODJl8UHJMpCaYC0q2I/awt9IfCEjJolUO2mLwd45J73gzECDLuT/lw2aTaTU3CxlAjDmKxKk/HKve6anMIfcnqkiS+NIx4fotju1FTi7h25bvRODnca5+924HVNFdkCJ+FaoHGMykyyjnkEKT9F/pKSTtRk4uJD8K1ZSUyoi5sxk8nN8O+n9XqWwjIoXMZs4vJba2tCunyA/EwL9TMgvrRnwp7T9lS/xrMnMxRdbsxUd/1JxBXacuOg0MNARNYMT709pjHfX6S+eBfipzf5OZSwxss4raH30FlLtpIeWCLdwWqKBG+4B/vpWeekoNB6wlhnjYOqI3Uos/xt1I46matBqyc8THAsMfot0m9iswDIZH49lk8QHBVaWk66OS9kk29BpnPsVdGJPMgG4XMfPn94zZPwCSOAm1eauJjugxH1WqByjZrwELRePu4u6xTqrql2KcyCpU2naFwitCxxdRbqX+A4m1/3SfG5qMSxQ8TIDk+PxyT0dMHnxQTfhph6UVtrw2E1N0aVJjKr21/SMHLyWloncAvXZg+3HxQMVx1ZlmplEj/ZKRGKdUNXnIgX/ityX0gjvUQRC8zefwJN9r/ddnQqPWAmBMwGWcOdAxT8YaHMb+YLc+63yY934oLJhiFoozAF2fkLS7mqw24BOD9fMEpLCdXHkJzq3GOysPcnaLB/cleSR7Vy9QVZPJT4ajsHT6lfIgu/YxbL2NX2DFKNmyPDn7PqcODMJUhyfEy28++xCiq4ooXKMUcJrU6Dw577gLMmiuXZWOU1TluD4OuHbC87WjGn4dzfd56YWc00EKna7HZWVlbBafTNd1rtZFLY+1xP7T13Gvn8vAeDQtn4ETCYOW46dx9J92v6w0g+cwrjuDWCuT7oYKsrLQBMSbf77HKYty4QZQP0w9e6WwWYrSkvFA53ezaLw2V1t8OEfx3G+SLgLKtLij4l9m+OmFnXx16lLms5HjDqhgRh/c1OUW+3U+9/5Ty46NJIeJeCTHFsPpL8MwAyENnC+L3Ml+fELBUJDAbsd/qUXYLZKBascMPorwF4psY3OpE9RH6TwyJUneAbOIhoTOWHsn5QGcSe3OAcqq6cKZx/stprb+08T304L7e4gC7gaAzg5+k8jBmV6Xu0njwD2D6DzuDm8jPzLv29GCa9tMt2bQlm0pFTgtq+AXx+AurGxdmDJ48JBq+vnppZz1fuolJeXIycnB8XFQrbEvk9ZhRXni9R1xThSJzQAgf70X0J2O5CbXwKrDp8O2mPb7UB5pRVWux3mqnkC/P8D/MzVIwbsduC/glJUUrRA0xJp8UdIoB84TtlrHhbkhwiLv27n4RHsdvKFbaMPKriyAly37y2EXj4svMFtX5POiqzNwNdD9TlPTzBgBrDqJfnteJ8MKXdXJUFEz+eBvq+Q/1eWA9PrSZehODPw4ilgdgN9ylVCqDWA8wZqP2dKhxbqyX1p7oLlExvJ7CCj4MzAy7k+Wwai9VExLKOSnZ2NadOmYf369cjNzUVCQgLuuecevPzyywgIqHnRDhw4gPHjx2P37t2oU6cOnnzySbzwwgu6nIPNZkNWVhbMZjMSEhIQEBAATutAHQ9jt9vhf/4KKm3avpziI4IQbqH/sF4pq0BFMEULpAwmE4dmdUJ1f93rlJbj7GX9xKyOr4/dbofpXBFoYviwIH8kRCo0rPI25VcABa+d3Q6cvxKNf9s/i+abJzhnVgLCgBEf1Sxu3poQrIbw+kBsy6rJymLlGo48n0bdpN1dHa9gaXAsR+yeLx982K3Ab+OMC1IA8anPvoiaScmA94IUQLizim8hN4qrxMnWsEDlyJEjsNls+PTTT9GsWTNkZGRg3LhxuHLlCubOnQuARFP9+/fHLbfcgk8++QQHDx7EQw89hMjISDzyyCOaz6G8vBw2mw0NGjRAcLC0sZkvc10dM05e1JYRCrYEIyiI/u2+XAZwftpLZWFB/rBY9F/Ig4KCYDMF4D8N2hlHXF+fiFAb8im6moItgQgK8l6buSpsxYCfssCxTogZ2YHhqAiKhfnKvzV3jPkWaHZzze/VtXeD2qKDY7WXe3jKrwDfj5TYoOo1GjirqpNHwEW0oKqTJzCM/riWaKCxg3jVKM8RxVS5BKVPFhf7+gomM9D2dmWmbN5G6E/OE9fNhn9ujMew9uSBAwfiyy+/RP/+/dGkSROkpqbiueeew+LFNXMXvv/+e5SXl+OLL75A69atcccdd2DixIl4+22Vo7pFMJlqdxd2hCUAjWKCYRLJSojdzuNvNiEkUOmXjj5llYLSCuTka8/MCBGok5W/0OsTQ+mWGxpYC2VeZuWlquqPmONnzRINNOnlvjHthODw+kDvl4Cez9KdRL83hAcRKqbqOZRelt4sPIFkF1oOqXJ3FbMABFBWSH/4Ye85BwFGeY6oomrq885PjG2BtVlJ2WPdG8D6N4DjG5Qfr7a12At1VundbSXEVeBk69Fv2fz8fERH13x5bd++Hb169XIqBQ0YMACzZ8/GpUuXEBXlLlIsKytDWVmNcKmgQM0I9tpHhCUA4Qn+KCqrxKXiCthsdoQE+iEmNACFpRWSGZeEyCDFpZfySv1SzBcKy1AvPEg2oFKKn07dNkKvT0igH8wmTtIDxmziEFIbA5WAUMDkp0ijIojrguuI64Rg2IDASBIcmEzkCzqxJ3n8ujfojld6SX0rsiP+FqBCIkPpHwLc+SPJepjMwIbZ+rVb937JvazSaRzp0pDTqNw631iNiiOrXgI2vw0MeYtoj/QkYwmQNgEodwzu5pAS3LB59GWn2lRmBIBja9w7vRJ7ypQeNXKVONl6LNVw7NgxvP/++3j00Uerb8vNzUW9es7GN/zvubnCrqMzZ85ERERE9U+DBg0Et7sa4TgOYUH+aBgdjMaxIagTFggTx1VnXPzNzm+nv9mERjHBiFCgTQGAnPwSKjM3WuwA7rrnPowYMUK3ffLs3r4F7RpEoSA/X3Sbpb/8gB6tG1X//vHbszBmQE/4mTjR14fjOFwXJV2yui7KUus0TwBIViTiOvWPD69PN8iOnxAc3xbY/yOwdiqw5S0ykXbp46ScAtCnvy//q01jYIkGbposHaQAZE6O3V4z8HCnTuWF4Fgy2sAV3nNEipTxQFCo/HZ6UnyBDEdcPVW/fa6eSlqLywUyUCWXlBni8WXG2sKOj4lw2hHeQVot9W+Uvv8qcbJVHKhMnjwZHMdJ/hw5csTpMWfOnMHAgQNx2223Ydw4bdHdlClTkJ+fX/1z+rRBrpRe5IEHHgDHcZg1a5bT7UuWLHFaGDds2ACO43D58mVEWALQMi4MTWJD0TA6GE1iQ/HTJ28hMjgQHMfBz88PjRs3xtNPP42iInEPht69eyMhMhjtGkQ5/Uyb8rSm56Rjg041Srt+4sKDUDcsEM89+xxWrV6DVvHhkkGc3gGgT2GJAkLq0m0b1RiIbEQW2hGfkInAtFe9/IRY16teXriZmUaf/o7UEFwBJCtyTqRryRXePv/kNqDksrbj8gx+SzwD1X8amfrr2i7NmZ2nAfefRhxKtRAQAkXiiG3zSBZEK4eW0AmOf3+RvgykscnAo9itJPBb+Tyw/cOaoIUvlYapmMje5Ca6z00tR3He+tlnn8UDDzwguU2TJk2q/3/27FncfPPN6NatGz777DOn7eLi4vDff84zD/jf4+KER4QHBgYiMNCztvO8M6sawzS1BAUFYfbs2Xj00UcFS2BCcByHUAdBKMdxaN26NdauXYvKykps3boVDz30EIqLi/Hpp58K7qPCasOou+7HE89OcT4fjYJYI14uP4XaI5vdjrgICxBB/1wiLAEID/LHlTIrKm02+JmInqVWZlJciagPBAQD+f8Kl4FM/iTzYokEuFKy7XWt6UWWkhNiHYSbE/fJp78t0UDjXsDmt+iOLcbfFN4b/OkBwLb36bYPCCXiXDFtV7eJxP9DClrPkZZDahxK1dAyFTjwo7LHLH2MCIab9pZ//4VauAEyKJAGfjK2nAndoSXAogfp9qkXYQlEJ5WxUF3JydHW39GUzdVBujAXWP2y/P7+/BIY+g5w92LynpZfARp2BTo/elVkUngUZ1Tq1KmDli1bSv7wmpMzZ86gd+/e6NixI7788ks3UWtKSgo2bdqEioqaMsOaNWvQokUL6sXZaNIzctBj9nrcOX8HJv20D3fO34Ees9cjPcMg46AqbrnlFsTFxWHmzJma9uPn54e4uDhcd911uP3223H33XcjLU08tWqzk6Aktm49p5/QMNLjfub0KbRrEIVVy37DAyMHoXOzeNw1pA+yTxxDxr69uHPwzeja4jo8ce9o5F0k3RkcUO2j8vrrr6NOnToIDw/HY489hvLymlSozWbDzJkzkZiYCIvFgnbt2mHhwoVO57dy5Upcf/31sFgsGDaoH/47455RW/rLDxjQJRldmifgqYfvweVLvL6ABBevvfYa2rdvX739Aw88gBEjRmDu3LmIj49HTEwMxo8fX/255DgOhZfO4+7bbkWdqDA0adIEP/zwAxo3box3330XAGlpfu2119CwYUMEBgYiISEBEyfKWKn7ApYooF4yENOMZE3C65N/Y5oB9VqTIEUtsi6gVcLN0zvl09/D3iMLl+vcIqVImtY5kNiTZBFoTMUAEoiM+cb9/IJjiccM7ZWtXwBJ1w+eI562VzonxglOeZACkGnN348E5jSVLs1kphEb+a+HAovGkn/fTQY2zVXWrSU3tC8zjWQn9NTr+Ml077UeCTydQd7LpzKIt87I+cSLp81tyo/Ht7TzpTXeQbrNaKDzI3SdZCV55HX4bgRw4GfgyHLSCfV3uvLz8WEM06jwQUrDhg0xd+5cnD9/Hrm5uU7ak7vuugsBAQEYO3YsDh06hJ9//hnvvfcennnmGaNOSxHpGTl4/Lu9yMl3/nLLzS/F49/tNTRYMZvNmDFjBt5//338+++/8g+gxGKxOAUHrtBmPj5+exbGTXwOP/2+AWY/P0yZMA7vzHgVL7w+E18uWonT2Vn4aC6xlI4NCwQHYN26dTh8+DA2bNiAH3/8EYsXL8brr79evc+ZM2fim2++wSeffIJDhw7h6aefxj333IONGzcCAE6fPo2RI0di2LBh2LdvHx5++GG8N/N1p/M68NefeO35J3HHA+Pw86pN6NStJ+a/T9rhQyU6n/744w8cP34cf/zxB77++mt89dVX+Oqrr6rvv++++3D27Fls2LABixYtwmeffYZz585V379o0SK88847+PTTT/HPP/9gyZIlaNOmDd2L6W04jnwpBkeTuSLB0eR3rVkj2gmxRf+Jp7/DEmr0MNWaBIOzWZZooGEKEXzSEBBGtCdJqTUL2KgF5N/n/tZfjMqLSFWhsQYrpSORKvMpHasgFYzZrEDak8r2R8OIT6XLKLd9WZNN4oOKtmNIQDn8I3qXY1ccy0AAeR3ntVPWSeYIb9uvdfilD2FYy8KaNWtw7NgxHDt2DNdd51xb5o20IiIisHr1aowfPx4dO3ZEbGws/u///k8XDxWtWG12vL4sUyppjdeXZaJfUpxhZaBbb70V7du3x6uvvooFCyituSXYs2cPfvjhB/Tp00d0G3+zCT9/swCLf3Se8zJ11tsYcuuY6t/ve3QCuvfuCwC466FHMXnCw/jsp6W4oVNXAMCIO+5B2q8/ok5YIOKrSi0BAQH44osvEBwcjNatW+N///sfnn/+eUybNg0VFRWYMWMG1q5di5SUFACkhLhlyxZ8+umnuOmmm/Dxxx+jadOmeOstkv5v0aIFDh48iNmzawR1Pyz4BN1798WDj08CADRu0gz7/9yJbRvXSXboREVF4YMPPoDZbEbLli0xZMgQrFu3DuPGjcORI0ewdu1a7N69GzfeSMRrn3/+OZo3b179+FOnTiEuLg633HIL/P390bBhQ3Tu3FnmHbnKCalDt935f8i/UgMUeZJSSebC1XhNT+w2YNNbwoJPITrc676AGYnTnBiRwMMcRJ89UsPvLzp7rciW+RQQllBTLhJi0cPyreVqCImhL7/ZrDUdbRyIzqrrE2TmjlIcTdn4YE8Pewip4Ze1DMMClQceeEBWywIAbdu2xebNBrvzqWBXVp5bJsURO4Cc/FLsyspDStMYw85j9uzZ6NOnD557TqBbgIKDBw8iNDQUVqsV5eXlGDJkCD74QPqPaeRtt+Pex5zFszF1nBedm7veiLphgSi32tGsYX0AQGrvrjAFW1ButaFpw/rIz7tQHaQAQLt27ZyM91JSUlBUVITTp0+jqKgIxcXF6Nevn9NxysvLccMNNwAADh8+jC5dujjdzwc1DaIsyLcBJ479jT4Dna2123XsjO2b1kvqSlq3bg2zueYPOj4+HgcPHgQAHD16FH5+fujQoUP1/c2aNXMqT952221499130aRJEwwcOBCDBw/GsGHD4OdXC9uX9YJ2Osfu+UDvF8gXKs1C7xjQFOaQ0hHNYEFaSi8Dm2bJblaNN/w8xAI2fpJz0jDlegcluOpIsjbrFzgOmi2+uB5aok2fIwWfAeTLb2JkppEgwKllvaq9usVgUnZRWpK6lC0T7KmgJI8EU65uuLWQa/hbVJpzhXRXI7TbqaVXr14YMGAApkyZQhX4udKiRQukpaXBz8+veoyAHPF1YtCxbStcKCxz+pPhUGOEFhFqIcJUACfCSG03MDAAkWFE6BxuCYBNgSKf70RasWIF6tev73QfjXg6IjgAkYHBbkUBf7MJUSH+ssUCf39nEzSO4xSdf4MGDXD06FGsXbsWa9aswRNPPIE5c+Zg48aNbvu+ZqDVJJRcpBNPOuIY0HAmfQMVJQSGSV/9G4lcBop/fWh9apTCL+yZacAyHfRYAaHAiI/FO8psVnpBLk+r4cDhpXTbyml/bFaitRErY5VcImLZ0V+SAPpSNhGqOwpoxYhqbMxkZyHb/loIC1REqBtGZ4tOu50WZs2ahfbt26NFixaKHxsQEIBmzZopflx8hAX1woNwsagc5VYbAswmxIQG4FTlZcX74tm/fz9KSkqqLfV37NiB0NBQNGjQANHR0QgMDMSpU6dw003Cf1itWrVyEwLv2LGj+v8RlgDc0DYZxw/tQ8Po4OoOnf/t+VP1OQMk2KusrMRff/2Fjh07AiC+QJcuOXepWCwWDBs2DMOGDcP48ePRsmVLHDx40CkTc1Xj2u1BW/oB6PUsQmgSl2qk63jlqfXKcv2mCdNkoIyS9ITW07dUUV4EZCwGcg+QtmNeMxUWTwKwk9sUCHKrpnmHxNAFKsGx0gGn0JwnMVa9TES3JjNQXgLMEO5gdaLjQ8CRZfLbKeUqaE4EWKAiSufEaMRHBCE3v1TwT5ADEBdBWpWNpk2bNrj77rsxb55wZ8TBgwcRFlajEOc4Du3atVN9vOLiYifRsxmAX2AgTJy2tvDy8nKMHTsWr7zyCrKzs/Hqq69iwoQJMJlMCAsLw3PPPYenn34aNpsNPXr0QH5+PrZu3Yrw8HDcf//9eOyxx/DWW2/h+eefx8MPP4w9e/Y4CV4BYOLEiejevTs+/2gehg8fjlWrViE9XZsCvmXLlrjlllvwyCOP4OOPP4a/vz+effZZWCw1hm9fffUVrFYrunTpguDgYHz33XewWCxo1KiRzN6vEoS+yMPiScaBRhQoFtQIaQF4R1ueaofSHOiWNqel+1PKtl89legYHEsDjm2qRtCoB4A5+u4zLAFo0IWIPvV8zTN/E749PAFIGkG/n5teJO3gNiude62Ux43SYMyxLHaG8iLpzJ/AlfN02yrBExb9HqB2D8ExELOJw6vDkgC4B6X8768OSzLcT4Xnf//7n2gpolevXrjhhhuqf/irfrXMnz8f8fHxTj933nmnpn0CQN++fdG8eXP06tULt99+O1JTU/Haa69V3z9t2jRMnToVM2fORKtWrTBw4ECsWLECiYmJAICGDRti0aJFWLJkCdq1a4dPPvkEM2Y4p2G7du2K+fPn47333kO7du2wevVqvPLKK5rP/ZtvvkG9evXQq1cv3HrrrRg3bhzCwsKqBxJGRkZi/vz56N69O9q2bYu1a9di2bJliIkxTr/kM4h1exTm0ncu/Pog0R+47ndOM+CbVGDzHOJo++1w9xZZJ4dSD19C7vmCftvVU0k7qqt+wbVNVW94m3Y9GTSbaIM8ZV9fcJZ+AGFAGHDTC+T/NJ1iUh43anUjfIZQSeebkgwkDZZo44XdHoKz08yy92EKCgoQERGB/Px8hIeHO91XWlqKrKwsJCYmqp5wm56Rg9eXZToJa+MjgvDqsCQMTFbhJMi4Kvj333/RoEEDrF27Fn379vX26RiO6N+SzUp8MkQXLI44oZaLuyE7wbtpZqbJT2B2tfFXkp7Xi86PEM8TOSrLgen15Of5vJxrjFEXzetJgyWaeNokpQIHFxKvFI/CQTZoGP21e+Ah9NkIjpWfZZS1mXjBKOX+5SRIoH18j2eJiPtP7d2d1dCMufAyUuu3I6z0I8PA5Hj0S4rzuDMtw7dYv349ioqK0KZNG+Tk5OCFF15A48aN0auXwPTgawkaU7fyIjJjZ/d8+Vk92+YB8e2ANRTZBdcWWUdx6fE/yFwho6GdTLt7vnwniGObqt4kpQLt7wL2/aDu8QGhZOFrclPN6+0VbZBMkCKWHaFpfRdCjXbKsb2atiyp52c1LIFkvHw8SFECC1QoMJs4Q1uQGb5PRUUFXnrpJZw4cQJhYWHo1q0bvv/++2u3o4eH9os8tjkw6gtSupGD9ipdyGqdF5c26kauTjX5bchcvSuZTHspW9/t1NDkZvWBSnkRYPYX0Qb5wPRimuyIGo8bNcGYY3u1k+cNRTZIKwNmAF0euyq8UxxhGhUGg4IBAwYgIyMDxcXF+O+///Dbb79dO0JZKWi/yINjlVmo0yIWKJnMxIBLDcGxJHvQTcb9VMlkWtrMC+12alAz9M6RQhcnbpMZaD1K2z71IDgWeOaw/g7AgIMLMEUG3RItXG7hPW/CPSAVCK131QUpAAtUGAyGFmi/yJc+Dlw8rv/xT213n0bLE9NU+f74RS8plX6iMQ2dxslbrCvJ0KhBk/U+3LtSbFZgv8oMjZ4UXyDCXiOgEWonDQfuXQo8f0y83OI4XqHX84acKgDvtuobCCv9MBgM9VSntmWEmvy8F9p2ZVocjd5c23yDYxXsqGoRGvqOc5aE1lJdDr8Acm7bJIYv8hkaoenDelwl01jvS+HalXJym7zmyFNo8eGRQ8wFOLw+MHBWTXBiswInNoq30vOlJxoDODWE1/ee+aDBsECFwWBoJyBUprOnamG0Wo07B77NFyABhpKhiuEJzouOI3KW6rTwAZSrjwpnJvvnu53cFsQEEmDoIY7UMivJtXRkZHCgFKMzCXJiXDFb/YAwYPgHNWWpzDT6NmulVBQDR1ZcVSJaHtaezGAwZBH9W9Kr7VVP+Dbfw2l0wtyezwM3T/Fcbd/RmTayEVCnFVB6iZTGNsyEaLaj6xNkloweGRbeSO/X+4CSy/Lbh9cHnjrofNwNs5VPRdYdjgRyrufmSWj+BlKeBPq9LtPKr5WqwHzMN7UmWGHtyQwGw1iqzbB8DLsVWDUFaDmMbnvHlltPwGdolPq+7PiI/OiRYTGZyfMe9j5FKYgj2SbH1ygzzQeClCpcz82T0P4NbH+fzP0xtEPKDoAD0idfNVOTeZiYlsFgqMOIIWp6sftzIuC1REFc6Mt5r64v5uZLQ8FZ8lhHd161VHekiIhsw+u7X6H7SoDKmYHbvvJu9kDJ34DYeABdsQMFZ8h5XUWwQIXhxFdffYXIyEhvn4ZPwHEclixZovt+H3jgAYwYMUJym969e+Opp56q/r1x48Z49913dT8XTfiSRkGIghwy0Za/0nSi6ndvXI2rtWV3wk6unG06aH4cO1JGzideHCPnk9+fOugeCPhKgGq3AsFe9rcyShirFV//21QIC1R8lNzcXDz55JNo0qQJAgMD0aBBAwwbNgzr1q2r3mb//v1ITU1F3bp1ERQUhMaNG+P222/HuXPn3PaXnZ0NjuMkf1wH/NUGpJ6X42Tl2s7u3bvxyCOPEBFm0Tkg/zT5V87t1Eh8vhWyKkCxRANhLhNswxO8V8vP3qLPQq/nlTPfkdJ2DClLtR3jPvyRx5cWQW+ei80KHPjFe8eXwuf/NpXBNCo0GNUuKEJ2dja6d++OyMhIzJkzB23atEFFRQVWrVqF8ePH48iRIzh//jz69u2LoUOHYtWqVYiMjER2djbS0tJw5coVt302aNAAOTk1hk1z585Feno61q5dW31bREQEfv75Z8OelxTl5eUICFA/42Tt2rVo3bq1021X00DAOnXqAPlngMsuQWjBGSCkLhBR3/Mn5UvOpKLYSSfGfWnEx8RDf8OiZKYBy2SM5JTgasLmCXxpEfTmuZzcpsHE0ECX2uDYq65NmWVU5MhMI0rtr4eSDoKvh5Lf9agPi/DEE0+A4zjs2rULo0aNwvXXX4/WrVvjmWeeqc4SbN26Ffn5+fj8889xww03IDExETfffDPeeeed6mnDjpjNZsTFxVX/hIaGws/Pz+k2i8VSvf2qVavQqlUrhIaGYuDAgU5BDgB8/vnnaNWqFYKCgtCyZUt89JFzy93BgwfRp08fWCwWxMTE4JFHHkFRUU37Kl/+mD59OhISEtCiRQv873//Q3Jystu5t2/fHlOnSs9+iYmJcXoucXFx1fb2r732Gtq3b48vvvgCDRs2RGhoKJ544glYrVa8+eabiIuLQ926dTF9+nS3/ebk5GDQoEGwWCxo0qQJFi5c6HT/6dOnMWbMGERGRiI6OhrDhw9HdnZ29f1WqxXPPPMMIiMjERMTgxdeeAGujXZXrlzBfffdh9DQUMTHx+Ott9znfjRu2ADvvvtO9e9c/Q74/IffcOvYZxEc1wzNmyYiLc35M5mWlobmzZsjKCgIN998M77++mtwHIfLly8DAE6ePIlhw4YhKioKISEhaN26NVauVJDKdjLD8nGunCcZgjajxTMFRsPrUmi6bGhJn0z2y3t4rHsDWP8GcHyDPmUhIbQax+mFtxdktdmc3i+5u9T6W4S3VUOj7leVkBZggYo0YoK3ghz9xGwu5OXlIT09HePHj0dISIjb/bx+JC4uDpWVlfjtt9/cFj6tFBcXY+7cufj222+xadMmnDp1Cs8991z1/d9//z3+7//+D9OnT8fhw4cxY8YMTJ06FV9//TUAsvAOGDAAUVFR2L17N3799VesXbsWEyZMcDrOunXrcPToUaxZswbLly/HQw89hMOHD2P37t3V2/z11184cOAAHnzwQU3P6fjx4/j999+Rnp6OH3/8EQsWLMCQIUPw77//YuPGjZg9ezZeeeUV7Nzp7HA5depUjBo1Cvv378fdd9+NO+64A4cPHwZA5v8MGDAAYWFh2Lx5M7Zu3Vod2JWXE5fUt956C1999RW++OILbNmyBXl5efjtN2dR3fPPP4+NGzdi6dKlWL16NTZs2IC9e/fWbGC3kXq8C6+//RnGDOuHA2t/wuCbU3D33XcjL4/4OGRlZWH06NEYMWLE/7d33mFRXF8DfheQJh1FQFQQkaLYG9ZYIsTeS4xCRBO7UWKMMcYWLLHEXn4WxHwaY4sSLGjsGqPRCDZE7A2NiQWJImXn+2OzKwsLLLJL877PMw/szJ07586y7JlTiYmJ4dNPP2XixIlq5w8fPpzXr19z9OhRLly4wOzZs7GwsMjbjfXppCgbbmaXt/OUGFu+3Xl5pbCtAPJ0+GUUOn+KfvmPIjV2ZgVY3wmOzYGjcxQ9lea46+eBqqgoqO3nFe4X8tv8TVmVh+afv4kJ6r5G8bPhEN3JVbaq7uYqIgjXT3bkGPCmvzSwa9euIUkSXl5eOY5r1KgRX331FR9++CFDhgyhQYMGtGrVigEDBlCuXP7+KaemprJixQrc3RUlyEeMGMG0adNUxydPnsy8efPo1q0bAG5ubly+fJmVK1cSGBjIxo0bSU5OZv369Spla8mSJXTs2JHZs2er5CtdujSrV69Wc/n4+/sTFhZG/fr1AQgLC6NFixZUrlw5R5kbN26MgYG63p3RgiOXy1m7di2Wlpb4+PjQsmVL4uLi2L17NwYGBnh6ejJ79mwOHTpEw4YNVef17NmTQYMGATB9+nT279/P4sWLWbZsGT/99BNyuZzVq1cj+6+4WFhYGDY2Nhw+fJi2bduyYMECJkyYoLpXK1asICoqSk3GNWvW8H//93+0bt0agPDwcFxcXN4s5F/N5uWgXh3p2yUAgBlfjmDRmh85ffo0AQEBrFy5Ek9PT+bMmQOAp6cnFy9eVLMa3blzh+7du+Pr6wuQ6z3OFmUxrFvH31TllBnCqZWK+iA50WkJ7Jug39oS5vYKF8nNY4Xn8tk26L/AXj2RmtXdy6unCiVGU/+Z/KJUUH8Zpd91ZUfjUfrp7ZMXtO2MrCJTmnfGBomSHI5/r/m0PMvVVDfzFCGERSU7tGlfr4c0sLxYR0JDQ3n48CErVqygWrVqrFixAi8vLy5cuJAvGczNzVVKCoCTk5MqQPfff//l+vXrBAcHY2Fhodq+/fZbrl9X9HKJjY2lZs2aahahJk2aIJfLiYuLU+3z9fXNEpcyePBgfvzxR5KTk0lJSWHjxo0MHDgwV5l/+uknoqOj1baMuLq6Ymn55um9XLly+Pj4qCk35cqVyxKI7Ofnl+W10qISExPDtWvXsLS0VN0HOzs7kpOTuX79Os+fPychIUFN8TEyMqJevXqq19evXyclJUVtjJ2dHZ6enm8umv5a45preHuofi9tboaVlaVK/ri4OJWyp6RBgwZqr0eNGsW3335LkyZNmDx5MufPn9d4Ha1Q1uVo/TW0+lpRQO2L64rYEO/OYJzJOmhur+iR8ugi1Pro7a+bK5IijmD74AJx22rk4g64tL1gr5mRPeP14wby6QQ91ul+3pwwLwM9wvPWZ0lfaNMHSImmNO+MuDbN3SppYglG5jmPMbPLe4foYoCwqGSHtv5HHUede3h4IJPJuHLlilbj7e3t6dmzJz179mTGjBnUrl2buXPnqtwwb4MytkOJTCZTKVBKK8WqVavUvlxBEQeTFzS5tjp27IiJiQk///wzxsbGpKam0qNHj1znqlChAlWqVMn2uKY1adonl2ufRZOUlETdunXZsGFDlmNly5bVcMZbYmiicXepUuof37zKP2jQIPz9/dm1axf79u1j5syZzJs3j5EjdRTsqVReKrd4E5Aetxv+XK9wWVzeqZvr5AWl27agMn7k6bB7rP6vkxMvHijuvT6+wPLbEVtmBFJa7uMqNlEov4VlEcuOnPoA1QlUNMbUJnjbwBA6Lsy5wm3n/+IAcxrTcWHRuj86QlhUskNb/6OOfd92dnb4+/uzdOlSjdk7ykBITRgbG+Pu7q7xPF1Rrlw5nJ2duXHjBlWqVFHblEG83t7exMTEqMlx4sQJlYslJ4yMjAgMDCQsLIywsDD69OmjFuRb0GROcf7999/x9vYGoE6dOsTHx+Pg4JDlXlhbW2NtbY2Tk5Na3EtaWhpnz55VvXZ3d6dUqVJqY54+fcrVq1ffXLS0ts313jzVeXp6cubMGbWjGWN/lFSoUIEhQ4awfft2QkJCWLVqlZbXyiMGhgoXwe/LcukJpG/+s1jqqgZJbhSVxn36SuPN7/8/bZQUAMdqhRcEnRsZ69AoY04+uwDvjc9b8LbSnZY5UNmq/Bv3nXJM5r5Lls76cfEVEYRFJTty9T/+12NCD1HnS5cupUmTJjRo0IBp06ZRo0YN0tLS2L9/P8uXLyc2NpbIyEg2bdpEnz59qFq1KpIk8csvv7B7927CwsJ0LlNGpk6dyqhRo7C2tiYgIIDXr19z5swZnj59ytixY+nXrx+TJ08mMDCQKVOm8PjxY0aOHEn//v21ip8ZNGiQShk4ceKEVjL9888/PHz4UG2fjY1Nvns8bdmyhXr16tG0aVM2bNjA6dOnWbNmDQD9+vVjzpw5dO7cmWnTpuHi4sLt27fZvn07X3zxBS4uLowePZpZs2bh4eGBl5cX8+fPV1M2LSwsCA4OZty4cdjb2+Pg4MDEiRPV421kBoqYj5wo7aD28tNPP2X+/PmMHz+e4OBgoqOjVXVylPE0n332GR988AFVq1bl6dOnHDp0SHXfdYJaT5uK8NtS3c2dLzK4bfVtJi8qNUf0FUxcUCnqtq76nT+/KOvQ5Jfcmh9qO6aEIRSV7FBriZ45512/VS0rV67Mn3/+SWhoKCEhISQkJFC2bFnq1q3L8uXLAfDx8cHc3JyQkBDu3r2LiYkJHh4erF69mv799dskbtCgQZibmzNnzhzGjRtH6dKl8fX1VVVSNTc3JyoqitGjR1O/fn3Mzc3p3r078+fP12p+Dw8PGjduzJMnT7K4l7KjTZs2Wfb9+OOP9OnTR+t1aWLq1Kls2rSJYcOG4eTkxI8//oiPjw+gWOfRo0cZP3483bp148WLF5QvX57WrVurGmwp37/AwEAMDAwYOHAgXbt25fnz56przJkzh6SkJDp27IilpSUhISFqxwHF31l2GTIa6qi4ubmxdetWQkJCWLhwIX5+fkycOJGhQ4diYqJwJaWnpzN8+HDu3buHlZUVAQEBfP+9jgL6or6G35cWbkG63CgIJaKws40AzOz1l8ZrYAjVe7zpWq0PZIZQf7D+5i9qaKP06EoxKiaI7sm5obHtevnsW8IL8o0kSXh4eDBs2DDGji1k/35RQ5IrsoDSXytiV0qXUVhctCA0NJQVK1Zw9+7dPF82T5+lH/sW3dLiGQmM1P8/+7QUCC2nR4VNi8JhPcKhehf9XF6erueOwCgyfIpC8KxA54juybriHTSzFSaPHz9m06ZNPHz4MN+1U0okMgOwcMh9HLBs2TLq16+Pvb09J06cYM6cOVlq2eicC9uLh5JSUM0I757SvZJiXkZR4t6znSL+ZUtg9mMbj9KfkgL67fsjM1SU8xdKyjuPUFS04R0zsxUmDg4OlClThv/973/Y2toWtjjFmvj4eL799luePHlCxYoVCQkJYcKECfq7oDwdIkfrb35doiu3rTxdvX5MpabqwZO6di+995WiYFhG2WU/ZLX6mpdRFETTd60RXa+v7kAwNFLEpNQfDEZv31ZDUHIQioqgSFHMPZFFiu+//153MSfacPs3eJ1YcNd7W977Sjdu28sR8MtoRS8hFXPAzBY6LlJcQ1cxKjm5mwvT6qvrGJzS9oo6PAJBBoSiIhAIdENRyXDJCUtnhUUiv1yOyL6eRcaKsHdPaR6jDSZWUPND8O6gXR2OwrD65rk6ay6I5xSBBkQdFYFAoBu0frqWQZ/NiuZpBc0Hs/NvaVC118iFn4fAySVvf53XL+D0SoXiU1Rj4vJSnVUbhItdoAGhqAgEAt2gbVfdqgGwazTc1q5Gjm4wUGS/6MLlo20Aqab+O3migIvTvS3K6qyZOwKb2SvaJzQbBx/9rHCJ5YSZnaKUfFFCnq6IP7qwVfGzKL8PJRjh+hEIBLpBrfZQNjZ85zpwdU+BigVAjzW6y34pUBdXARanyw/axMl0XFS8yr9rLE3hrPgbF6UpChRhUREIBLpD9XSdybJiYgXd1sDTmwUvU+NRUL2b7uYrjCJuxSH+Rxknk13ZeG1KxBcVlDFImS1niQ8U+wu6seU7jrCoaIEkSfzzzz8kJSVhYWGBvb29qgy5QCDIRHZP1zePKeItCgp9pehWaqzo/lyQPXyKQoVbXVAc6lLJ0xXZXDnxy2jFOoqS3CUYoajkwLNnzwgPD2fx4uVcvx6n2u/u7snIkUMJDAzExsam8AQUCIoqmrJQbh8vmGub2ULPcEW8gz6+SAwMwbcXnFqu+7mzoL+eYoVGUa9Ldet4ppRzDbx6ohhXuUXByPSOI1w/2RAVFYWLSyXGjPmcGzdqAZuB/cBmbtyoxZgxn+PiUomoqCidXlcmk+W4TZkyRafXA3jvvfeQyWRs2rRJbf+CBQtwdXVVvV63bl2OillQUJBKTmNjY6pUqcK0adNIS8vaITUlJYUyZcowa9YsjXNNnz6dcuXKkZqa+lZrUjJlyhRq1aqVrzkEOqKgUk9r91d8gejzadfaRX9zq9BvTzFBNtw8ottxgnwjFBUNREVF0b59B169aoYk3UWSNgE9gTZATyRpE5J0l1evmtG+fQedKisJCQmqbcGCBVhZWant+/xzHdSA0ICpqSlff/11vhWDgIAAEhISiI+PJyQkhClTpjBnzpws44yNjfnoo480dnqWJIl169YxYMAASpUqlS95dEVKSkphi1D8KSirwMVt+s/OeJnLE7cusHJWxPsUpdiNd4Fn93Q7TpBvhKKSiWfPntG9ey8kyR+5fAfgmM1IR+TyHUiSP9279+LZs2c6ub6jo6Nqs7a2RiaTqV47ODgwf/58XFxcMDExoVatWuzdu1d17q1bt1SWkcaNG2Nqakr16tU5ciR3zb9v3748e/aMVatW5Ut+ExMTHB0dqVSpEkOHDqVNmzZERGgOPAsODubq1ascP67uEjhy5Ag3btwgODgYgNWrV+Pt7Y2pqSleXl4sW7ZMbfy9e/fo27cvdnZ2lC5dmnr16nHq1CnWrVvH1KlTiYmJUVl61q1bB8CdO3fo3LkzFhYWWFlZ0atXLx49ehOwqLTErF69Wq0R39atW/H19cXMzAx7e3vatGnDv//mNw31HeByBEQML5hrKbNk9ImBnv91Nh8Hn10QSkphYKOltUzbcYJ8IxSVTISHh/Py5Uvk8tXkHsJjhFy+ipcvX7J+/Xq9y7Zw4ULmzZvH3LlzOX/+PP7+/nTq1In4+Hi1cePGjSMkJIRz587h5+dHx44d+eefnAP/rKysmDhxItOmTdPpF6+ZmVm21ghfX1/q16/P2rVr1faHhYXRuHFjvLy82LBhA9988w2hoaHExsYyY8YMJk2aRHh4OABJSUm0aNGC+/fvExERQUxMDF988QVyuZzevXsTEhJCtWrVVBap3r17I5fL6dy5M0+ePOHIkSPs37+fGzdu0Lt3bzU5rl27xrZt29i+fTvR0dEkJCTQt29fBg4cSGxsLIcPH6Zbt26i7H92yNPhxhHY1F9zBoU+0XeWTCU91/tw1ZA1IygYXJvrdpwg3whFJQOSJLF48XKgO9lbUjLjBHRj0aJlev/Cmjt3LuPHj6dPnz54enoye/ZsatWqxYIFC9TGjRgxgu7du+Pt7c3y5cuxtrZmzZo1uc4/bNgwTE1NmT9/fr5llSSJX3/9laioKFq1apXtuODgYLZs2UJSUhIAL168YOvWrQwcOBCAyZMnM2/ePLp164abmxvdunVjzJgxrFy5EoCNGzfy+PFjduzYQdOmTalSpQq9evXCz88PMzMzLCwsMDIyUlmlzMzMOHDgABcuXGDjxo3UrVuXhg0bsn79eo4cOcIff/yhki0lJYX169dTu3ZtatSoQUJCAmlpaXTr1g1XV1d8fX0ZNmwYFhYW+b5fJY7LETCnCqzvBFe0SOW0dAZjHd5HfWfJuDXLvYBZflBahAq74FhhX78w0Pa9ffVM8/538Z7pGZH1k4F//vnnv+yevLUVl6TuXL++mSdPnmBvb68X2RITE3nw4AFNmqiXHW/SpAkxMTFq+/z8/FS/GxkZUa9ePWJjY3O9homJCdOmTWPkyJEMHTr0reSMjIzEwsKC1NRU5HI5H374YY4BwH379mXMmDFs3ryZgQMH8tNPP2FgYEDv3r35999/uX79OsHBwQwePFh1TlpaGtbW1gBER0dTu3Zt7OzstJYxNjaWChUqUKFCBdU+Hx8fbGxsiI2NpX79+gBUqlSJsmXLqsbUrFmT1q1b4+vri7+/P23btqVHjx6iy3NmcuqDkx11BigaGv6+LPexuWFeRv/xMAaGuRcwyw8nFoEkwR+r4VUGa2hBFhx7VwueGRhChwWwJTDncfu+Ap+Oit9v/wYvEuDGIYjbo56G/y7cMz0jLCoZUD7VQ16/eBTjX7x4oVN5CoOPPvqISpUq8e23377V+S1btiQ6Opr4+HhevXpFeHg4pUuXzna8lZUVPXr0UAXVhoWF0atXLywsLFTvx6pVq4iOjlZtFy9e5PfffwcUriV9kVluQ0ND9u/fz549e/Dx8WHx4sV4enpy82YhFDErqmjbByczR2aBqY1uZKjRq2DcJsoCZpZOuY/NK2kv4ehsdSUF/is4NkD/BccuRyiuo7HgmYbrlzQrgrkWD5yJ9+HoXFhQHcI7wPbBEL0xa62gxISCec9KMEJRycAbE35ei1IpxltaWupUnoxYWVnh7OzMiRPq/VFOnDiBj4+P2j7llzgorA9nz57F29tbq+sYGBgwc+ZMli9fzq1bt/IsZ+nSpalSpQoVK1bEyEg7g11wcDDHjx8nMjKS3377TRVEW65cOZydnblx4wZVqlRR29zc3ACoUaMG0dHRPHmiOQvD2NiY9HT1f5re3t7cvXuXu3fvqvZdvnyZZ8+eZbmXmZHJZDRp0oSpU6dy7tw5jI2N+fnnn7Va5zuBtn1wNPFnuG6+9D3b5X8ObfHpBGMuQWAkeHYooItK+u3/o1I2s3NlS4qCZ8rrX9wBcz0UX9bbghU/F1Qvul/MaSlwcinsHqf4mZaS9fi5H7Sb6/AMLf7ei0nPpiKMcP1kwN7eHnd3T27c2IYk9dT6PJlsG5Ure+bJ/fA2jBs3jsmTJ+Pu7k6tWrUICwsjOjqaDRs2qI1bunQpHh4eeHt78/333/P06VNVzIc2tG/fnoYNG7Jy5UrKlVP39aenpxMdHa22z8TERGtFSBPNmzenSpUqDBgwAC8vLxo3fmO2nzp1KqNGjcLa2pqAgABev37NmTNnePr0KWPHjqVv377MmDGDLl26MHPmTJycnDh37hzOzs74+fnh6urKzZs3iY6OxsXFBUtLS9q0aYOvry/9+vVjwYIFpKWlMWzYMFq0aEG9evWylfPUqVMcOHCAtm3b4uDgwKlTp3j8+HG+1l7iyE8Qa+J9aPGlwrrytsgMoELDtz//bVAWMLt5BOIiC+aa+uz/o42y+eqJwpqQkgS/LdIg33+Wl6KWXr1vkqKjtSTPsO9r8BsBbadrPq4TiknPpiKKsKhkQCaTMXLkUGAb8FDLsxKA7YwaNUzvZfVHjRrF2LFjCQkJwdfXl7179xIREYGHh4fauFmzZjFr1ixq1qzJ8ePHiYiIoEyZMnm61uzZs0lOTs6yPykpidq1a6ttHTt2zNe6ZDIZAwcO1KhQDRo0iNWrVxMWFoavry8tWrRg3bp1KouKsbEx+/btw8HBgXbt2uHr68usWbMwNFSY/rt3705AQAAtW7akbNmy/Pjjj8hkMnbu3ImtrS3NmzenTZs2VK5cmZ9++ilHOa2srDh69Cjt2rWjatWqfP3118ybN48PPvggX+svUeQ3iDU9f3V8kORw91T+5nhb9J0JlBl9ZTZpO+/ROZqVFBV6tvzklX2TFPJmVkIkuWL//1pqPq5LikPPpiKITCrmuZWJiYlYW1vz/PlzrKys1I4lJydz8+ZNtToYufHs2TNcXCrx6lWz/+qo5GR0SsPAoAtmZse4d+92oZfTv3XrFm5ubpw7d05UYxXoFK0/S/J0hdn/bd0/NXrD+ZwVxlzpvkbRGK+gkafDHPeC62cUGKmfp/ObxxTuG12hLzkzI0/PvodQWgqEltOvEqINBXUvigk5fX9nRFhUMmFjY8O2bZuRyaIwMOiCwmKiiQQMDLogk0WxffuWQldSBIIigYGhIsMhz8gUXXStdFBEq7Aa+CkzgQoCfWY2VWqs29TrgrAiXI54E9SqjJOZ66GInwH4Y1XhKylW5UtWz6YCRCgqGvD392fXrkjMzI4hk1VEJutNxl4/MllvZLKKmJkdY/fuXbRt27aQJRYIihDKbBgzbWO2MvS0yW+TN319GWib1ZJdJlCp7DPf3or28/SX2WRgCA3frjyBRkqXzX1MfsguQ+nlP7A1ELZ8DNd+1a8M2iB6Nr01wvWTA8+ePWP9+vUsWrQsS/fkUaOGERgYqKrnIRCUZN7qsyRPV3SYvXEEEu8prCWGpeDc+ky1Ocor/on7dPrPfVIl9+61GpHpJ3jzbeqJZHZD3DgEx+bpRp5q3aBn1h5ZOkWeDrMqQIoOqlRbOsEH3+knqDa/rsYCwwC+fgRGxoUtSJFCW9ePUFS0QJIknjx5wosXL7C0tMTOzk7vgbMCQVFCV58lIOdYAsi9YJzfSIjZqHhiVpJR2dElyqf1LKm6/33+tVWMbhxRVOnNL6a28MX1gnky3xwEl3WVeq8nJVLX8TT6xH8G+BVQv6tigraKikhP1gKZTIa9vb3eqs4KBO8UynTe7FC6T7JYMTIoI+9PzVnZ0QU51hORAJkiq8Wrfe7Xdm2qcIW9laUoA50WFYySIk9XpFvrEm3vVV4oTlk0T28VtgTFlgKJUXn9+jW1atVCJpNlqcFx/vx5mjVrhqmpKRUqVOC7774rCJEEAkFRxqcTfHZRkSXRfY3iZ8Zuwkplx7eH4qc+vrxzrSciad+p2cAQOi7MeUxOAaxW5RXKW0HVJLn9W/6VKjXycK/yQmEFTr8Ntq6FLUGxpUAsKl988QXOzs5ZetIkJibStm1b2rRpw4oVK7hw4QIDBw7ExsaGTz75pCBEEwgERZXcLC/6RtundW3H5WYp8mr/xkpkXgZkMvj3sf4sRjlR2DVatKVSY0W5+4xuwMJAZpBzVpHMEOoPzv64IEf0rqjs2bOHffv2sW3bNvbs2aN2bMOGDaSkpLB27VqMjY2pVq0a0dHRzJ8/XygqAoGgcNH2af3KLu3rtvh0UldIMishRaXGxt/x+plX1xYQA0NoN1+R3VOYVHkf4qOyP+43XATS5gO9un4ePXrE4MGD+eGHHzA3N89y/OTJkzRv3hxj4zdvoL+/P3FxcTx9WkBFk7RAkiT+/vtvbt26xd9//00xjz8WCATaUKkxWDrnPu7Sdri4Xft5C8JtlR8uR+SvjYFGZPpLHa/eBRqP0v28ecGtmSIbi0xJFjJDhWxtpxeKWCUFvSkqkiQRFBTEkCFDsu2f8vDhwyy9ZJSvHz7UXML+9evXJCYmqm364tmzZyxcuBBvLw/Kli2Lm5sbZcuWxdvLg4ULF/Ls2TO9XftdYcqUKaKKrqBocmUXvNby/8vWgW+KixVn5Onwi66/9DPUydGXUtZ2OvQMV7jMCoNj8xQKqzLwupQ5uLeBLsvAo23RaSNQTMmzovLll18ik8ly3K5cucLixYt58eIFEyZM0KnAM2fOxNraWrVVqFBBp/MriYqKolJFFz4PGUMtuxtsHgX7J8DmUVDL7gafh4yhUkUXoqJyMPe9Bbnd2ylTpuj0ekquXbvGxx9/jIuLCyYmJri5udG3b1/OnDmjGnPkyBFatWqFnZ0d5ubmeHh4EBgYSEpKSpb5Dh8+nOtaDh8+rJe16Itt27ZhaGjI/fv3NR738PBg7Nix+b6Oq6srCxYsyPc8gnygTEtOSdLyBEnhfiiqHYO15eax/LcAyKwsWDkXTHPCal3g86tvArC9O+v3ehnJfM9SX8L1X+HnT4t+N+liQJ5jVEJCQggKCspxTOXKlTl48CAnT57ExMRE7Vi9evXo168f4eHhODo68uiRenCV8rWjo6PGuSdMmKD2ZZCYmKhzZSUqKooOHdrj7yuxepCEo4368Z4NJR4+g0GrX9GhQ3siI3fh7++vk2snJLwp2f/TTz/xzTffEBf3ptichYWFTq6TkTNnztC6dWuqV6/OypUr8fLy4sWLF+zcuZOQkBCOHDnC5cuXCQgIYOTIkSxatAgzMzPi4+PZtm0b6elZnxYaN26stpbRo0eTmJhIWNibQlV2dnaFpqykpKSouRy1oVOnTtjb2xMeHs5XX32lduzo0aNcu3aN4OBgXYqZL95mjQJySUvOBX2k4BYkt4/n73yr8jAqWtEYUp+p49mRMQDbohzE7iyY6+ZGUe0mXUzIs0WlbNmyeHl55bgZGxuzaNEiYmJiiI6OJjo6mt27dwOKL9/Q0FAA/Pz8OHr0KKmpbzqm7t+/H09PT2xtNafqmZiYYGVlpbbpkmfPntGrZ3f8fSV2jJFnUVKUONrAjjFy/H0levXsrjM3kKOjo2qztrZGJpOpXjs4ODB//nyV1aNWrVrs3btXde6tW7eQyWRs2rSJxo0bY2pqSvXq1TlyJPt6CEoXnYeHB8eOHaN9+/a4u7tTq1YtJk+ezM6dig/6vn37cHR05LvvvqN69eq4u7sTEBDAqlWrMDMzyzKvsbGx2lrMzMwwMTFR25fxS/SHH37A1dUVa2tr+vTpw4sXL1TH5HI5M2fOxM3NDTMzM2rWrMnWrVvVrnfkyBEaNGiAiYkJTk5OfPnll6SlpamOv/fee4wYMYLPPvuMMmXK4O/vz8CBA+nQQb1YVGpqKg4ODqxZsybLmkqVKkX//v1Zt25dlmNr166lYcOGVKtWjWfPnjFo0CDKli2LlZUVrVq1ypLx9ssvv1C/fn1MTU0pU6YMXbt2Vcl5+/ZtxowZo7I8Kdm2bRvVqlXDxMQEV1dX5s1Tr3Tq6urK9OnTGTBgAFZWVnzyySekpKQwYsQInJycMDU1pVKlSsycOTOL/IIM5JqWnAP6SMEtSPIbfhcwSxE0WhRicCo1VlhzMseNFBoS/DJauIHeAr3FqFSsWJHq1aurtqpVqwLg7u6Oi4ui8diHH36IsbExwcHBXLp0iZ9++omFCxfqxHz+toSHh/Py5UtWD5JjlMvny8gQVgXLefnyJevXr9e7bAsXLmTevHnMnTuX8+fP4+/vT6dOnYiPV4/QHzduHCEhIZw7dw4/Pz86duzIP/9oTt+Ljo7m0qVLhISEYGCQ9c9B2WzR0dGRhIQEjh49qvN1Xb9+nR07dhAZGUlkZCRHjhxh1qw3wXwzZ85k/fr1rFixgkuXLjFmzBg++ugjlQJ2//592rVrR/369YmJiWH58uWsWbOGb7/9Vu064eHhGBsbc+LECVasWMGgQYPYu3evmuUnMjKSly9f0rt3b42yBgcHEx8fr3YfkpKS2Lp1q8qa0rNnT/766y/27NnD2bNnqVOnDq1bt+bJE0Vdil27dtG1a1fatWvHuXPnOHDgAA0aNABg+/btuLi4MG3aNBISElSynT17ll69etGnTx8uXLjAlClTmDRpUhalae7cudSsWZNz584xadIkFi1aREREBJs3byYuLo4NGzbg6ur6Fu/SO4I8XVFFNj/8tlg3shQG+ck6av5F4VoLMvdjAvCfSf61Lx3y6gkcnVvYUhQ7CrUyrbW1Nfv27WP48OHUrVuXMmXK8M033xRaarIkSSxftpju9cnWkpIZJ1voVh+WLV3EyJEj9Vpaf+7cuYwfP54+ffoAMHv2bA4dOsSCBQtYunSpatyIESPo3r07AMuXL2fv3r2sWbOGL774IsucSiXHy8srx2v37NmTqKgoWrRogaOjI40aNaJ169aqp/f8IJfLWbduHZaWlgD079+fAwcOEBoayuvXr5kxYwa//vorfn5+gMK1ePz4cVauXEmLFi1YtmwZFSpUYMmSJchkMry8vHjw4AHjx4/nm2++USlgHh4eWQoKenp68sMPP6juTVhYGD179szWxebj40OjRo1Yu3YtzZs3B2Dz5s1IkkSfPn04fvw4p0+f5q+//lK5PefOncuOHTvYunUrn3zyCaGhofTp04epU6eq5q1ZsyagcIcZGhpiaWmp5v6cP38+rVu3ZtKkSQBUrVqVy5cvM2fOHDVXbKtWrQgJCVG9vnPnDh4eHjRt2hSZTEalSpXy+O68Q2jq6fM2xO+HtJTimY6arwq6hagQaHrvzGwhPTX7cwqLU8uh+efF1z1YCBRY92RXV1ckScqS4VGjRg2OHTtGcnIy9+7dY/z48QUlUhb++ecf4q5ep3uDvH3guteXiLt6XfXErA8SExN58OABTZo0UdvfpEkTYmNj1fYpv9ABjIyMqFevXpYxSrRNtTY0NCQsLIx79+7x3XffUb58eWbMmEG1atXULBJvg6urq0pJAXBycuKvv/4CFEG+L1++5P3338fCwkK1rV+/nuvXrwMQGxuLn5+fmpLYpEkTkpKSuHfvnmpf3bp1s1x70KBBqriZR48esWfPHgYOHJijvAMHDmTr1q0q99TatWvp2bMnlpaWxMTEkJSUhL29vZq8N2/eVMkbHR1N69at83SPYmNjNb738fHxajFCmTPsgoKCiI6OxtPTk1GjRrFv3748XfedIbsOvG+FHP5YpYN5CgFtKuhmR0HrKUoLyt4Jit5Qmd+7V0/zEAxdgLx6Wrzdg4VAgSkqxYGkJMUftW0eO7Irx2eMqyguKF1yV65c0Wp8+fLl6d+/P0uWLOHSpUskJyezYsWKfMlQqlQptdcymQy5XFHlUfme7Nq1SxXvFB0dzeXLl7PEqeRG6dJZ39gBAwZw48YNTp48yf/93//h5uZGs2Y5m7+VFq3NmzcTHx/PiRMnVG6fpKQknJyc1GSNjo4mLi6OcePGAWiM6dEVmddYp04dbt68yfTp03n16hW9evWiRw8ti5O9K+QneDY7inNfF2UFXSstashkpCCL1V2OUGTShHeA35cV3HV1RXHqUVQEEIpKBpTm/qd57GyuHJ/RKqBrrKyscHZ25sSJE2r7T5w4gY+Pj9q+33//XfV7WloaZ8+exdvbW+O8tWrVwsfHh3nz5qmUg4zkFCRsa2uLk5MT//6rg1bw2eDj44OJiQl37tyhSpUqapsy28vb25uTJ0+qWYdOnDiBpaWlKh4qO+zt7enSpQthYWGsW7eOjz/+OFeZLC0t6dmzJ2vXriUsLIyqVauqlJs6derw8OFDjIyMsshbpowibbNGjRocOHAg2/mNjY2zZFJ5e3trfO+rVq2KoWHOJmQrKyt69+7NqlWr+Omnn9i2bZterX/FjvwEz2ZHce/rkrHXUrdVYJzL/zYzO4XbqCDQqfWrkChOPYqKAKJ7cgbs7e3xrOrOttM36NlQ+6erbX/I8KxaGTs7Oz1KpwiSnTx5siorJywsjOjoaDZs2KA2bunSpXh4eODt7c3333/P06dPs3VnyGQywsLCaNOmDc2aNWPixIl4eXmRlJTEL7/8wr59+zhy5AgrV64kOjqarl274u7uTnJyMuvXr+fSpUssXqy/4EFLS0s+//xzxowZg1wup2nTpjx//pwTJ05gZWVFYGAgw4YNY8GCBYwcOZIRI0YQFxfH5MmTGTt2rMYA4cwMGjSIDh06kJ6eTmCgdqW4g4ODadasGbGxsWruyjZt2uDn50eXLl347rvvqFq1Kg8ePFAF0NarV4/JkyfTunVr3N3d6dOnD2lpaezevVs1j6urK0ePHqVPnz6YmJhQpkwZQkJCqF+/PtOnT6d3796cPHmSJUuWsGxZzk+T8+fPx8nJidq1a2NgYMCWLVtwdHRUBUkL0P3TbUnp65Ix1dfIVOFeyY6OCwuuq7OurV8FjaWzfir0lmCEopIBmUzG0GEj+TxkDA+faRdQm/AUtv8B8+aP0msgLcCoUaN4/vw5ISEh/PXXX/j4+BAREYGHh4fauFmzZjFr1iyio6OpUqUKERERqqd5TTRo0IAzZ84QGhrK4MGD+fvvv3FycqJx48aqwmMNGjTg+PHjDBkyhAcPHmBhYUG1atXYsWMHLVq00OeymT59OmXLlmXmzJncuHEDGxsb6tSpo6plUr58eXbv3s24ceOoWbMmdnZ2BAcH8/XXX2s1f5s2bXBycqJatWo4O2tn7m7atCmenp5cu3aNAQMGqPbLZDJ2797NxIkT+fjjj3n8+DGOjo40b95cVXX5vffeY8uWLUyfPp1Zs2ZhZWWlCswFmDZtGp9++inu7u68fv0aSZKoU6cOmzdv5ptvvmH69Ok4OTkxbdq0XGsaWVpa8t133xEfH4+hoSH169dn9+7dWilw7wy6frotiX1dcmuoWJBdnYuzJQWg9gARSJtHZFIxb1yTmJiItbU1z58/z5J9kpyczM2bN3Fzc8PU1FSr+Z49e0alii40q/KKHWNyTlFOS4cu3xtw7JoZt+/cK/Sn1Fu3buHm5sa5c+dEWfo8kJSURPny5QkLC6Nbt26FLU6R5G0+S8UGeTrMrQov/87fPDJDhZJS1Pq6yNOzb4IIigylP1Yp4mpsXRXWoOwUrdzm0je7x8Pp/MXEFTrNxkFr7R6iSjo5fX9nRFhUMmFjY8PmLdvo0KE9Xb43YFWwHCcNtecSnsLgNQZEXZCxa9f2QldSBHlHLpfz999/M2/ePGxsbOjUSVSMfCcxMIT282BLPjrw1uwHHRcUPUuKprRdK2cImK2wguybBCeXgJQhPm3f1+A3QrPCldEdVNDI0yFmY+FcW5cUlfpzxQihqGjA39+fyMhd9OrZnYqjX9KtviIF2ba0InB22x8ytv8B5uZm7Nq1nbZt2xa2yIK34M6dO7i5ueHi4sK6deswMhIfh3eWal3g/ij4bVHezito14e2yNMVhcUOz8h6LDFBEYzq+QHE7c56XJK/uQ9FyTp0+zftm0QWZSoVUNBxCUL8Z84Gf39/bt+5x/r161m2dBGbF19XHfOsWpl580cRGBiItbV1IUqpjrJWjUA7xP0SqNF2OjjXhq25Z34B0OxzaPlV0Ys3yLVw3X9/85qUlIycXAqtJhUdK1F+g54NjECelvs4bZEZQJfl8OQmnF4FrzRX/1bDzK7wLFLFGKGo5ICNjQ2jRo1i5MiRPHnyhBcvXmBpaYmdnZ3eA2cFAkEhUL0bxEdBzKbcx5rbFU0lZfMAdJIVI6UrYlf8hud/Ll2Q36BnXSopoLA8vfwHWk6AFl8oLD5xu3Ou61JQ2VElDBH6rwUymQx7e3tcXV2xt7cXSopAUJJx17JqcOmy+pUjr5T0wnWqJoNFCOX9UcbuBMxUZEdZOqmPs3RW7C9qLsJigrCoCAQCQUYyf8nkd1xBUdIL1xkYKoKAdWUx0gWa7o9PJ/BqX7jZUSUMYVERCASCjGjz5G5VvugV7cot5iSvFMXCdT6doNf6rO+PUSGkzOd0f5QWFt8eip9CSckXQlHJjoMHISQEkpM1H09OVhw/eLBg5RIIBPpF+eSebR6pTJHpU5S+fC5H6L7nTaOhRSeQNiMZy/t3X6P4+dUD6BkO5pkKW1qVh/e+0o8cJbGwXxFFuH40sW8fdOgAqalw+TL8/DNkLHKVnAxdu8LevbB4MURGgkhRFghKDson98KuxKoNqtgUHVM1QPdz6gpN9VyqdQHvjlldLgB/rlOkZevCZVRUC/uVYISikhmlkqJsCrdvn0IpUSorSiVl3z7F8fR0xXihrBQq69at47PPPsuxieKUKVPYsWMH0dHRBSaXoBhTXGIN9FVWXlM6cMYqtjaVoIwn3D2lMD5Valr4bo7sCtKpYltkZFVWZGBoDOmvc5+/ckv4cLOwpBQwwvWTkYxKirKTsFz+Rll5/vyNkpLxuFJZUSov+SQoKAiZTMasWbPU9u/YsUMt4+jw4cPIZLJsv5ynTJmCTCZDJpNhZGSEq6srY8aMISkpKcfrX7t2jY8//hgXFxdMTExwc3Ojb9++nDlzJt9ry4irq6uql1B+6d27N1evXtXJXAKBiuIQa6DrpopKMqcD75sEoeUg6is4/T/YNxE29oBjc+DoHPihM8xxV7ihihqq2JZMAdBW5RX7tbUela8nlJRCQFhUlBw8mFVJUaJUVlxdITFR83FQnL93L7RqlW9xTE1NmT17Np9++im2thpq+GtJtWrV+PXXX0lLS+PEiRMMHDiQly9fsnLlSo3jz5w5Q+vWralevTorV67Ey8uLFy9esHPnTkJCQjhy5Mhby/I2pKenI5PJcm2iZ2ZmhpmZWQFJJRAUITLHZeiCzMHC+yZpV7X31VNFl+WimIqbk4XM1Bpid+Y+hyjWVigIi4qSXbsUMSmZlRAlcrlmJSXj8dRUxTw6oE2bNjg6OjJz5sx8zWNkZISjoyMuLi707t2bfv36ERGh+YlHkiSCgoLw8PDg2LFjtG/fHnd3d2rVqsXkyZPZufPNB/nu3bv06tULGxsb7Ozs6Ny5M7du3VIdDwoKokuXLsydOxcnJyfs7e0ZPnw4qampgKKD8O3btxkzZozK6gMKF46NjQ0RERH4+PhgYmLCnTt3ePr0KQMGDMDW1hZzc3M++OAD4uPjVddTnpeRWbNmUa5cOSwtLQkODiY5u8BogaA4o4+6Tv4zFF/oF7ZC/AFFP6C8sGe8InamqJGdhcy1qaJqbE6Y2SnGCQocoagoCQ2FgADI6ck9OyUFFOcFBCjm0QGGhobMmDGDxYsXc+/ePZ3MCQrLQ0pKisZj0dHRXLp0iZCQEI0WDKUikJqair+/P5aWlhw7dowTJ05gYWFBQECA2tyHDh3i+vXrHDp0iPDwcNatW8e6desA2L59Oy4uLkybNo2EhAQSEhJU5718+ZLZs2ezevVqLl26hIODA0FBQZw5c4aIiAhOnjyJJEm0a9dOpfhkZvPmzUyZMoUZM2Zw5swZnJycWLZMx1kRAkFR4N/HOpxMBt5dYNdYCO8A24JhQzf1poXa8OKBQtEpLhgYKqrG5oSoKltoCEVFiampImC2bduclRVNGBgozsucHZRPunbtqrJm6IKzZ8+yceNGWmXjmlJaKLy8vHKc56effkIul7N69Wp8fX3x9vYmLCyMO3fucPjwYdU4W1tblixZgpeXFx06dKB9+/YcOHAAADs7OwwNDbG0tMTR0RFHR0fVeampqSxbtozGjRvj6enJ/fv3iYiIYPXq1TRr1oyaNWuyYcMG7t+/z44dOzTKuGDBAoKDgwkODsbT05Nvv/0WHx+fPNwtgaCYkN/S8mpIELtDURo+v+grdkZf+HRSuKwy12ixKl80XVnvECJGJSNKZSVzwGxO6ElJUTJ79mxatWrF559//lbnX7hwAQsLC9LT00lJSaF9+/YsWaLZjKttg76YmBiuXbuGpaWl2v7k5GSuX3/TvLFatWoYGr55AnFycuLChQu5zm9sbEyNGjVUr2NjYzEyMqJhw4aqffb29nh6ehIbG6txjtjYWIYMGaK2z8/Pj0OHDuV6fYGgWFGpMZjb60a50CX/XM99TFGjuGR6vWMIRSUzpqawaVP2gbMZMTAAKyvFeD0oKQDNmzfH39+fCRMmEBQUlOfzPT09iYiIwMjICGdnZ4yNs49Yr1q1KgBXrlyhdu3a2Y5LSkqibt26bNiwIcuxsmXf9D8pVaqU2jGZTIZcC+XPzMxM9FMSCLTFwBDazYetgYUtiTqHZ4CDd/GzRGSX4iwoNITrJzPJydCnT+5KCrwJsO3TJ/sKtjpg1qxZ/PLLL5w8eTLP5xobG1OlShVcXV1zVFIAatWqhY+PD/PmzdOoUCjToOvUqUN8fDwODg5UqVJFbbO2ts6TbOnpuQfceXt7k5aWxqlTp1T7/vnnH+Li4rJ153h7e6uNB/j999+1lk0gKFZU7wKNRxW2FFnZ+2XRDKoVFCuEopKRjMXctHH7gHqdFT0pK76+vvTr149FizSnB164cIHo6GjVFhMT81bXkclkhIWFcfXqVZo1a8bu3bu5ceMG58+fJzQ0lM6dOwPQr18/ypQpQ+fOnTl27Bg3b97k8OHDjBo1Kk+Bv66urhw9epT79+/z999/ZzvOw8ODzp07M3jwYI4fP05MTAwfffQR5cuXV8mUmdGjR7N27VrVeiZPnsylS5fydkMEguJE2+mKMvLGlrmPLSgS7xevoFpBkUQoKkreRklRUgDKyrRp07J1mzRv3pzatWurtrp16771dRo0aMCZM2eoUqUKgwcPxtvbm06dOnHp0iVVcTZzc3OOHj1KxYoV6datG97e3qr0Xysrqzyt6datW7i7u6u5jDQRFhZG3bp16dChA35+fkiSxO7du7O4l5T07t2bSZMm8cUXX1C3bl1u377N0KFDtZZNICiWVOsCfbK6ZAuV4hZUKyhyyCRtIyiLKImJiVhbW/P8+fMsX5LJycncvHkTNzc3THOLIQkJgfnzcx5jYJC7EjN2LMybp4XkAkHxIU+fJUHhIk+HuR5FJ7g2MFLEfAg0ktP3d0aERUVJ+/ZQqlT2qcnKwNmcjpcqpZhHIBAICgtlcG1umNmBifYxZW9F5gq3AsFbIBQVJa1aKRoLGhpmVUaUKci3bmmus2JgoDgvMlIn5fMFAoEgX+QaXCuD2h/pP+02YJZI7RXkG6GoZKRt26zKSsY6KdbWWYvCZVRSRPdkgUBQVFAG12buBWRVHhqPhN8Ww6sn+rm2zBB6hBe/1GRBkUTUUcmMUlnp0EERj5K5mFvGonB79wolRSAQFF2qdQHvjuoFzCo0hEU1AT2GJ/ZYq7i2QKADhKKiibZtFUrIrl2K3j2ZgweVysrEiYqYFOHuEQgERZXMBcxuHoPEB/q5llV5hbtHWFIEOkQoKtnRqlXOCoipqcjuEQgExQ99pAs3HwduLUS5eYFeEIqKQCAQvEtkjlnJL1bl4b0JQkER6A0RTJsNB28eJCQqhOQ0zQXcktOSCYkK4eDNgwUsmUAgEOQDXffRqt5dKCkCvSIUFQ3su76PgP8LYP7v8+m6qWsWZSU5LZmum7oy//f5BPxfAPuu7yskSQUCgSCP/PtYt/Nd3Cb6+Qj0ilBUMrHv+j46bOxA+n8fvH039qkpK0olZd8NhXKSLk+nw8YOJUZZWbduHTY2NoUtxjvBe++9x2effZbjGFdXV1XrAoFAJ+ja9SP6+Qj0jFBUMpBRSZGjKJUvl+QqZeV58nOVkiKX/juOXC/KysOHDxk5ciSVK1fGxMSEChUq0LFjRw4cOKAaExMTQ6dOnXBwcMDU1BRXV1d69+7NX3/9lWW+W7duIZPJctzWrVunM/kLmm3btvHee+9hbW2NhYUFNWrUYNq0aTx5ors6EYcPH0Ymk6m6SOeX7du3M336dJ3MJRBoja5dPyD6+Qj0ilBU/uPgzYNZlBQlSmXFdaGrmpKiOp5BWdFFzMqtW7eoW7cuBw8eZM6cOVy4cIG9e/fSsmVLhg8fDsDjx49p3bo1dnZ2REVFERsbS1hYGM7Ozvz7779Z5qxQoQIJCQmqLSQkhGrVqqnt6927d75lf1tSUlLe+tyJEyfSu3dv6tevz549e7h48SLz5s0jJiaGH374QYdSaoe2a7Gzs8PSsgh1uhW8G+ja9QOK+iwCgZ4Qisp/7Lq6i1R5ahYlRYlckpP4OjGLkqI6jpxUeSq7ru7KtyzDhg1DJpNx+vRpunfvTtWqValWrRpjx47l999/B+DEiRM8f/6c1atXU7t2bdzc3GjZsiXff/89bm5uWeY0NDTE0dFRtVlYWGBkZKS2z8zMTDU+KioKb29vLCwsCAgIICEhQW2+1atX4+3tjampKV5eXixbtkzt+IULF2jVqhVmZmbY29vzySefkJSUpDoeFBREly5dCA0NxdnZGU9PT6ZNm0b16tWzyF6rVi0mTZqk8V6dPn2aGTNmMG/ePObMmUPjxo1xdXXl/fffZ9u2bQQGBqrG7ty5kzp16mBqakrlypWZOnUqaWlpquMymYzVq1fTtWtXzM3N8fDwICIiAlAojy1btgTA1tYWmUxGUFAQoHDhjBgxgs8++4wyZcrg7+8PwJEjR2jQoAEmJiY4OTnx5Zdfql0vs+vnr7/+omPHjpiZmeHm5saGDUWsC66gZKBrpUL08xHoGaGo/Edo61AC3AMwkGV/S7JTUgAMZAYEuAcQ2jo0X3I8efKEvXv3Mnz4cEqXLp3luDJ+xNHRkbS0NH7++Wd03QD75cuXzJ07lx9++IGjR49y584dPv/8c9XxDRs28M033xAaGkpsbCwzZsxg0qRJhIeHA/Dvv//i7++Pra0tf/zxB1u2bOHXX39lxIgRatc5cOAAcXFx7N+/n8jISAYOHEhsbCx//PGHasy5c+c4f/48H3/8sUZZN2zYgIWFBcOGDdN4XHm/jh07xoABAxg9ejSXL19m5cqVrFu3jtBQ9fdr6tSp9OrVi/Pnz9OuXTv69evHkydPqFChAtu2bQMgLi6OhIQEFi5cqDovPDwcY2NjTpw4wYoVK7h//z7t2rWjfv36xMTEsHz5ctasWcO3336b7X0PCgri7t27HDp0iK1bt7Js2TKNbjyBIF9Uagzm9rqbT/TzEegZoaj8h6mRKT/3+Zm2ldvmqKxowkBmQNvKbfm5z8+YGpnmfkIOXLt2DUmS8PLyynFco0aN+Oqrr/jwww8pU6YMH3zwAXPmzOHRo/z7ilNTU1mxYgX16tWjTp06jBgxQi02ZvLkycybN49u3brh5uZGt27dGDNmDCtXrgRg48aNJCcns379eqpXr06rVq1YsmQJP/zwg5p8pUuXZvXq1VSrVo1q1arh4uKCv78/YWFhqjFhYWG0aNGCypUra5Q1Pj6eypUrU6pUqRzXNHXqVL788ksCAwOpXLky77//PtOnT1fJrCQoKIi+fftSpUoVZsyYQVJSEqdPn8bQ0BA7OzsAHBwccHR0xNr6TedZDw8PvvvuOzw9PfH09GTZsmVUqFCBJUuW4OXlRZcuXZg6dSrz5s1DLs+q8F69epU9e/awatUqGjVqRN26dVmzZg2vXr3KcV0CQZ7RtrtybpjZQa8fRBVagd4RikoG3kZZ0aWSAuTJOhIaGsrDhw9ZsWIF1apVY8WKFXh5eXHhwoV8yWBubo67u7vqtZOTk+rJ/t9//+X69esEBwdjYWGh2r799luuX78OQGxsLDVr1lSzCDVp0gS5XE5cXJxqn6+vL8bGxmrXHjx4MD/++CPJycmkpKSwceNGBg4cmK2s2t6vmJgYpk2bpibz4MGDSUhI4OXLl6pxNWrUUP1eunRprKystLJq1K1bV+11bGwsfn5+yDIELjZp0oSkpCTu3buX5fzY2FiMjIzU5vHy8hIZWAL9kGt35f9wbQbdVkH/nfDRz4oKtM3GwYAIGHdNKCmCAkFUps2EqZEpm3pswnWha44xKaBQUqxMrNjUY5NOlBRQPJnLZDKuXLmi1Xh7e3t69uxJz549mTFjBrVr12bu3LkqN8zbkNk6IZPJVAqBMs5k1apVNGzYUG2coWHezL+aXFsdO3bExMSEn3/+GWNjY1JTU+nRo0e2c1StWpXjx4+Tmpqao1UlKSmJqVOn0q1btyzHTDP0ctK0dk0WEG3WIhAUaVpNUnRQzqk54e3f4KPtYPTfA0UV0ddMUPAIi0omktOS6bO1T65KCrwJsO2ztU+2FWzzip2dHf7+/ixdulRj9k5OqbHGxsa4u7trPE9XlCtXDmdnZ27cuEGVKlXUNmUQr7e3NzExMWpynDhxAgMDAzw9PXOc38jIiMDAQMLCwggLC6NPnz5qQb6Z+fDDD0lKSsoSzKtEeb/q1KlDXFxcFpmrVKmCgYF2HwOl9Sc9PffiVt7e3pw8eVLN4nPixAksLS1xcXHJMt7Ly4u0tDTOnj2r2hcXF6ezVGiBIAt/rCLXDspS+n/jBILCQygqGchYzC03JUVJxjorulJWli5dSnp6Og0aNGDbtm3Ex8cTGxvLokWL8PPzAyAyMpKPPvqIyMhIrl69SlxcHHPnzmX37t107txZJ3Jkx9SpU5k5cyaLFi3i6tWrXLhwgbCwMObPV/i9+/Xrh6mpKYGBgVy8eJFDhw4xcuRI+vfvT7lyuWccDBo0iIMHD7J3794c3T4ADRs25IsvviAkJIQvvviCkydPcvv2bQ4cOEDPnj1VlqVvvvmG9evXM3XqVC5dukRsbCybNm3i66+/1nrdlSpVQiaTERkZyePHj9WymDIzbNgw7t69y8iRI7ly5Qo7d+5k8uTJjB07VqNi5OnpSUBAAJ9++imnTp3i7NmzDBo0KEclTSDIF09v6XacQKAnhKLyH2+jpCjRtbJSuXJl/vzzT1q2bElISAjVq1fn/fff58CBAyxfvhwAHx8fzM3NCQkJoVatWjRq1IjNmzezevVq+vfvn28ZcmLQoEGsXr2asLAwfH19adGiBevWrVNZVMzNzYmKiuLJkyfUr1+fHj160Lp1a5YsWaLV/B4eHjRu3BgvL68s7iVNzJ49m40bN3Lq1Cn8/f1Vqdw1atRQpSf7+/sTGRnJvn37qF+/Po0aNeL777+nUqVKWq+7fPnyqqDccuXKZcliyjx29+7dnD59mpo1azJkyBCCg4NzVIyUdXBatGhBt27d+OSTT3BwcNBaPoEgT9i66nacQKAnZJKuc1sLmMTERKytrXn+/DlWVlZqx5KTk7l58yZubm5qcQiaCIkKYf7vOUfCG8gMclVixjYayzz/edoJL9CIJEl4eHgwbNgwxo4dW9jiCMjbZ0lQTEhLgdBykNP/NJkhTHz4JkZFINAhOX1/Z0RYVP6jfdX2lDIohUE2t0QZOJtdNpABBpQyKEX7qu31KWaJ5/HjxyxZsoSHDx9mWztFIBDoACNj8MveKgiA33ChpAgKHaGo/Ecrt1ZEfhiJoYFhFmVFmYJ8a/QtjanLBhhgaGBI5IeRtHITUfH5wcHBgWnTpvG///0PW1vbwhZHICjZtJ2uSFPO/AAmM1Tsbyt6UQkKH+H6yUTmxoSZ66RkjmXJqKS0dW+rjyUKBIWOcP2UcNJSFNk9T28pYlLqDxaWFIHeEa6ft6Ste1uVZQXIUswtY1E4QCgpAoGg+GNkrHDztJsj3D2CIoco+KaBtu5t2fvRXnZd3UVo69AsxdyUysrEAxNpX7W9cPcIBAKBQKAn3glFRZvKoplp5dYqRwXE1MhUZPcI3hmKuYdYIBAUY/SqqOzatYtp06Zx/vx5TE1NadGiBTt27FAdv3PnDkOHDuXQoUNYWFgQGBjIzJkzMTLSjVjGxsYYGBjw4MEDypYti7GxsVrvFYFAkDuSJPH48WNkMlmuzR8FAoFA1+hNUdm2bRuDBw9mxowZtGrVirS0NC5evKg6np6eTvv27XF0dOS3334jISGBAQMGUKpUKWbMmKETGQwMDHBzcyMhIYEHDx7oZE6B4F1EJpPh4uKS535OAoFAkF/0kvWTlpaGq6srU6dOJTg4WOOYPXv20KFDBx48eKAqq75ixQrGjx/P48ePs3TVzQ5tooYlSSItLU2rHi0CgSArpUqVEkqKQCDQKdpm/ejFovLnn39y//59DAwMqF27Ng8fPqRWrVrMmTOH6tWrA3Dy5El8fX3Ver/4+/szdOhQLl26RO3atXUmj9JkLczWAoFAIBAUL/SSnnzjxg0ApkyZwtdff01kZCS2tra89957PHnyBICHDx9maVCnfP3w4cNs5379+jWJiYlqm0AgEAgEgpJJnhSVL7/8EplMluN25coVVZbNxIkT6d69O3Xr1iUsLAyZTMaWLVvyJfDMmTOxtrZWbRUqVMjXfAKBQCAQCIoueXL9hISEEBQUlOOYypUrk5CQACg6/CoxMTGhcuXK3LlzBwBHR0dOnz6tdu6jR49Ux7JjwoQJao3qEhMThbIiEAgEAkEJJU+KStmyZSlbtmyu4+rWrYuJiQlxcXE0bdoUgNTUVG7dukWlSpUA8PPzIzQ0lL/++kvVyn7//v1YWVmpKTiZMTExwcTERPVaGQssXEACgUAgEBQflN/bueb0SHpi9OjRUvny5aWoqCjpypUrUnBwsOTg4CA9efJEkiRJSktLk6pXry61bdtWio6Olvbu3SuVLVtWmjBhQp6uc/fuXQkQm9jEJjaxiU1sxXC7e/dujt/zequjMmfOHIyMjOjfvz+vXr2iYcOGHDx4UNUR19DQkMjISIYOHYqfnx+lS5cmMDCQadOm5ek6zs7O3L17F0tLyyJTzE3pjrp7926OKVclAbHWkolYa8lErLVkUlzXKkkSL168wNnZOcdxxb57clFE29zwkoBYa8lErLVkItZaMinpaxXdkwUCgUAgEBRZhKIiEAgEAoGgyCIUFT1gYmLC5MmT1bKTSipirSUTsdaSiVhryaSkr1XEqAgEAoFAICiyCIuKQCAQCASCIotQVAQCgUAgEBRZhKIiEAgEAoGgyCIUFYFAIBAIBEUWoajokKtXr9K5c2fKlCmDlZUVTZs25dChQ2pj7ty5Q/v27TE3N8fBwYFx48aRlpZWSBLnn127dtGwYUPMzMywtbWlS5cuasdL2npfv35NrVq1kMlkREdHqx07f/48zZo1w9TUlAoVKvDdd98VjpD54NatWwQHB+Pm5oaZmRnu7u5MnjyZlJQUtXElYa0AS5cuxdXVFVNTUxo2bJilUWpxZObMmdSvXx9LS0scHBzo0qULcXFxamOSk5MZPnw49vb2WFhY0L17d1VT2OLMrFmzkMlkfPbZZ6p9JWmt9+/f56OPPsLe3h4zMzN8fX05c+aM6rgkSXzzzTc4OTlhZmZGmzZtiI+PL0SJdcRbtPERZIOHh4fUrl07KSYmRrp69ao0bNgwydzcXEpISJAk6U1/ozZt2kjnzp2Tdu/eLZUpUybP/Y2KClu3bpVsbW2l5cuXS3FxcdKlS5ekn376SXW8pK1XkiRp1KhR0gcffCAB0rlz51T7nz9/LpUrV07q16+fdPHiRenHH3+UzMzMpJUrVxaesG/Bnj17pKCgICkqKkq6fv26tHPnTsnBwUEKCQlRjSkpa920aZNkbGwsrV27Vrp06ZI0ePBgycbGRnr06FFhi5Yv/P39pbCwMOnixYtSdHS01K5dO6lixYpSUlKSasyQIUOkChUqSAcOHJDOnDkjNWrUSGrcuHEhSp1/Tp8+Lbm6uko1atSQRo8erdpfUtb65MkTqVKlSlJQUJB06tQp6caNG1JUVJR07do11ZhZs2ZJ1tbW0o4dO6SYmBipU6dOkpubm/Tq1atClDz/CEVFRzx+/FgCpKNHj6r2JSYmSoC0f/9+SZIkaffu3ZKBgYH08OFD1Zjly5dLVlZW0uvXrwtc5vyQmpoqlS9fXlq9enW2Y0rSeiVJsR4vLy/p0qVLWRSVZcuWSba2tmrrGj9+vOTp6VkIkuqW7777TnJzc1O9LilrbdCggTR8+HDV6/T0dMnZ2VmaOXNmIUqle/766y8JkI4cOSJJkiQ9e/ZMKlWqlLRlyxbVmNjYWAmQTp48WVhi5osXL15IHh4e0v79+6UWLVqoFJWStNbx48dLTZs2zfa4XC6XHB0dpTlz5qj2PXv2TDIxMZF+/PHHghBRbwjXj46wt7fH09OT9evX8++//5KWlsbKlStxcHCgbt26AJw8eRJfX1/KlSunOs/f35/ExEQuXbpUWKK/FX/++Sf379/HwMCA2rVr4+TkxAcffMDFixdVY0rSeh89esTgwYP54YcfMDc3z3L85MmTNG/eHGNjY9U+f39/4uLiePr0aUGKqnOeP3+OnZ2d6nVJWGtKSgpnz56lTZs2qn0GBga0adOGkydPFqJkuuf58+cAqvfw7NmzpKamqq3dy8uLihUrFtu1Dx8+nPbt26utCUrWWiMiIqhXrx49e/bEwcGB2rVrs2rVKtXxmzdv8vDhQ7W1Wltb07Bhw2K31swIRUVHyGQyfv31V86dO4elpSWmpqbMnz+fvXv3qjpGP3z4UO1LG1C9fvjwYYHLnB9u3LgBwJQpU/j666+JjIzE1taW9957jydPngAlZ72SJBEUFMSQIUOoV6+exjElZa2ZuXbtGosXL+bTTz9V7SsJa/37779JT0/XuI7isgZtkMvlfPbZZzRp0oTq1asDivfI2NgYGxsbtbHFde2bNm3izz//ZObMmVmOlaS13rhxg+XLl+Ph4UFUVBRDhw5l1KhRhIeHA28+eyXxb1ooKrnw5ZdfIpPJctyuXLmCJEkMHz4cBwcHjh07xunTp+nSpQsdO3YkISGhsJehNdquVy6XAzBx4kS6d+9O3bp1CQsLQyaTsWXLlkJehXZou9bFixfz4sULJkyYUNgivzXarjUj9+/fJyAggJ49ezJ48OBCklyQH4YPH87FixfZtGlTYYuiF+7evcvo0aPZsGEDpqamhS2OXpHL5dSpU4cZM2ZQu3ZtPvnkEwYPHsyKFSsKWzS9Y1TYAhR1QkJCCAoKynFM5cqVOXjwIJGRkTx9+lTVZnvZsmXs37+f8PBwvvzySxwdHbNkFSijzx0dHfUif17Rdr1K5cvHx0e138TEhMqVK3Pnzh2AIr/evLy3J0+ezNJHo169evTr14/w8HAcHR2zZBIUx7UqefDgAS1btqRx48b873//UxtX1NeqDWXKlMHQ0FDjOorLGnJjxIgRREZGcvToUVxcXFT7HR0dSUlJ4dmzZ2qWhuK49rNnz/LXX39Rp04d1b709HSOHj3KkiVLiIqKKjFrdXJyUvt/C+Dt7c22bduAN5+9R48e4eTkpBrz6NEjatWqVWBy6oXCDpIpKUREREgGBgbSixcv1PZXrVpVCg0NlSTpTXBpxqyClStXSlZWVlJycnKByptfnj9/LpmYmKgF06akpEgODg6q7I+Sst7bt29LFy5cUG1RUVESIG3dulW6e/euJElvAkxTUlJU502YMKHYBZhKkiTdu3dP8vDwkPr06SOlpaVlOV5S1tqgQQNpxIgRqtfp6elS+fLli30wrVwul4YPHy45OztLV69ezXJcGWC6detW1b4rV64UywDTxMREtc/mhQsXpHr16kkfffSRdOHChRK11r59+2YJpv3ss88kPz8/SZLeBNPOnTtXdVz5f7q4B9MKRUVHPH78WLK3t5e6desmRUdHS3FxcdLnn38ulSpVSoqOjpYk6U26btu2baXo6Ghp7969UtmyZYttuu7o0aOl8uXLS1FRUdKVK1ek4OBgycHBQXry5IkkSSVvvUpu3ryZJevn2bNnUrly5aT+/ftLFy9elDZt2iSZm5sXu5Tde/fuSVWqVJFat24t3bt3T0pISFBtSkrKWjdt2iSZmJhI69atky5fvix98sknko2NjVqWWnFk6NChkrW1tXT48GG19+/ly5eqMUOGDJEqVqwoHTx4UDpz5ozk5+en+sIr7mTM+pGkkrPW06dPS0ZGRlJoaKgUHx8vbdiwQTI3N5f+7//+TzVm1qxZko2NjbRz507p/PnzUufOnUV6skCdP/74Q2rbtq1kZ2cnWVpaSo0aNZJ2796tNubWrVvSBx98IJmZmUllypSRQkJCpNTU1EKSOH+kpKRIISEhkoODg2RpaSm1adNGunjxotqYkrReJZoUFUmSpJiYGKlp06aSiYmJVL58eWnWrFmFI2A+CAsLkwCNW0ZKwlolSZIWL14sVaxYUTI2NpYaNGgg/f7774UtUr7J7v0LCwtTjXn16pU0bNgwydbWVjI3N5e6du2qpowWZzIrKiVprb/88otUvXp1ycTERPLy8pL+97//qR2Xy+XSpEmTpHLlykkmJiZS69atpbi4uEKSVnfIJEmSCtzfJBAIBAKBQKAFIutHIBAIBAJBkUUoKgKBQCAQCIosQlERCAQCgUBQZBGKikAgEAgEgiKLUFQEAoFAIBAUWYSiIhAIBAKBoMgiFBWBQCAQCARFFqGoCAQCgUAgKLIIRUUgEAgEAkGRRSgqAoFAIBAIiixCUREIBAKBQFBkEYqKQCAQCASCIsv/A3z8EZBBXa1CAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_most_similar = nlp_reduced[nlp_most_related_indices[:5]]\n",
    "\n",
    "corrected_cs_most_indices = []\n",
    "\n",
    "for idx in cs_most_related_indices[:5]:\n",
    "    # Precisei corrigir os índices porque os índices do CS começam depois dos índices do NLP\n",
    "    corrected_cs_most_indices.append(idx + len(nlp_embeddings))\n",
    "\n",
    "cs_most_similar = all_vectors[corrected_cs_most_indices]\n",
    "\n",
    "plt.scatter(nlp_reduced[:, 0], nlp_reduced[:, 1], label=\"NLP Embeddings\")\n",
    "plt.scatter(cs_reduced[:, 0], cs_reduced[:, 1], label=\"CS Theory Embeddings\")\n",
    "\n",
    "plt.scatter(nlp_most_similar[:, 0], nlp_most_similar[:, 1], color='blue', edgecolor='black', s=100, label=\"Top NLP Vectors\")\n",
    "plt.scatter(cs_most_similar[:, 0], cs_most_similar[:, 1], color='orange', edgecolor='black', s=100, label=\"Top CS Theory Vectors\")\n",
    "\n",
    "plt.scatter(*nlp_centroid_reduced, color='red', label=\"NLP Centroid\", s=150, marker=\"X\")\n",
    "plt.scatter(*cs_centroid_reduced, color='green', label=\"CS Theory Centroid\", s=150, marker=\"X\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords (tentativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       to take these systems and apply them to real w...\n",
       "9       is a verb this is a sort of unused unusual usa...\n",
       "15      is that we've got a set of points this is goin...\n",
       "31      vector all right we come to the second example...\n",
       "32      that here we again screw up this first one um ...\n",
       "                              ...                        \n",
       "1946    can talk easily about another concept it's jus...\n",
       "1985    sketch let's assume for a contradiction that t...\n",
       "2061    what this learning parodies with noise problem...\n",
       "2121    ideas by the time dinner approve or theorem uh...\n",
       "2127    operation increases the badness so you're gonn...\n",
       "Name: text, Length: 238, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_entropy_texts = df[df['high_entropy'] == True].text\n",
    "high_entropy_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "rake = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = []\n",
    "\n",
    "for text in high_entropy_texts:\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    keywords = rake.get_ranked_phrases() \n",
    "    keywords_list.append(keywords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['q times k transpose',\n",
       "  'parameter matrices wq wk',\n",
       "  'formula involves q k',\n",
       "  'one extra little step',\n",
       "  'uh inner products'],\n",
       " ['third example iraqi head seeks arms actually',\n",
       "  'higher level syntactic one',\n",
       "  'new dancing taking place',\n",
       "  'last example like',\n",
       "  'unused unusual usage'],\n",
       " ['linear binary classification um one thing',\n",
       "  'break ties somehow right',\n",
       "  'uh academic papers um',\n",
       "  'weight vector w um',\n",
       "  'weight vector leads'],\n",
       " ['get 1 1 minus 1 minus 1',\n",
       "  '0 1 minus 1 minus 1',\n",
       "  '0 0 minus 1 minus 1',\n",
       "  'minus one uh',\n",
       "  'second else line'],\n",
       " ['plus 1 minus 1 minus 1 plus 1',\n",
       "  'one zero zero zero one zero one zero one',\n",
       "  'get 1 1 minus 1 minus 1',\n",
       "  'feature spaces look like um',\n",
       "  'zero one one'],\n",
       " ['minus w transpose f x thing um taking',\n",
       "  'essentially scalars w times times f',\n",
       "  'loss function plus stochastic gradient descent',\n",
       "  'particular weight w uh gives',\n",
       "  'w transpose f'],\n",
       " ['minus w transpose f x thing um taking',\n",
       "  'essentially scalars w times times f',\n",
       "  'um basically two uh two coordinates',\n",
       "  'particular weight w uh gives',\n",
       "  'whole feature vector uh f'],\n",
       " ['know maybe helps connect possibly',\n",
       "  'separating decision boundary',\n",
       "  'error driven procedure',\n",
       "  'uh loss formulation',\n",
       "  'sgd formulation'],\n",
       " ['equals plus 1 given x',\n",
       "  'approaches either plus 1',\n",
       "  'return plus 1',\n",
       "  'equals plus 1',\n",
       "  'w transpose f'],\n",
       " ['given x essentially let',\n",
       "  'train logistic regression',\n",
       "  'return plus one',\n",
       "  'decision rule ends',\n",
       "  'probabilistic interpretation um'],\n",
       " ['negative log likelihood gets abbreviated',\n",
       "  'sgd loop involves picking',\n",
       "  'basic machine learning principles',\n",
       "  'deep learning frameworks um',\n",
       "  'use stochastic gradient descent'],\n",
       " ['w transpose f x thing um',\n",
       "  'uh commute log past sum okay',\n",
       "  'sgd loop involves picking',\n",
       "  'equals plus one um',\n",
       "  'positively labeled example'],\n",
       " ['uh commute log past sum okay',\n",
       "  'equals plus 1 given x',\n",
       "  'sgd update looks like',\n",
       "  'slightly ugly log',\n",
       "  'one plus e'],\n",
       " ['correct step size allowed us',\n",
       "  'whole process pretty tricky',\n",
       "  'weight current weight w',\n",
       "  'step size parameter might',\n",
       "  'w equals minus 1'],\n",
       " ['know one e minus four one e minus five things like',\n",
       "  'need pretty small step sizes',\n",
       "  'correct step size allowed us',\n",
       "  'step size parameter might',\n",
       "  'something like one'],\n",
       " ['second order taylor approximation',\n",
       "  'common feature like uh',\n",
       "  'might reflect different kind',\n",
       "  'common word like',\n",
       "  'terms get used'],\n",
       " ['second order taylor approximation',\n",
       "  'right step size',\n",
       "  'actually fully optimize',\n",
       "  'called adaptive methods',\n",
       "  'big neural models'],\n",
       " ['deep learning things like dropout',\n",
       "  'different optimization techniques exist',\n",
       "  'data without explicitly incorporating',\n",
       "  'deep learning later',\n",
       "  'either early stopping'],\n",
       " ['different features perspective different features',\n",
       "  'one weight vector per class',\n",
       "  'different weights versions version',\n",
       "  'little bit weirder um',\n",
       "  'w health weight vector'],\n",
       " ['uh minus one minus one zero',\n",
       "  'one one zero comma one example',\n",
       "  'last position remains unchanged okay',\n",
       "  'gives us one zero one',\n",
       "  'print weight vector um'],\n",
       " ['binary logistic regression even though',\n",
       "  'binary logistic regression um',\n",
       "  'w transpose uh times',\n",
       "  'binary logistic regression',\n",
       "  'implicit feature vector'],\n",
       " ['binary logistic regression even though',\n",
       "  'add uh alpha times f',\n",
       "  'know gives us zero',\n",
       "  'logistic regression um',\n",
       "  'subtract alpha times'],\n",
       " ['subgroup makes consistent non',\n",
       "  'deploying machine learning models',\n",
       "  'zero prediction errors compared',\n",
       "  'tn cleary defined fairness',\n",
       "  'uh ground truth level'],\n",
       " ['using different criteria across groups',\n",
       "  '60 positive maybe group 1',\n",
       "  'uh ground truth level',\n",
       "  'know zero percent accuracy',\n",
       "  'group two actually tends'],\n",
       " ['choices like sigmoids um various',\n",
       "  'asymptotes um another common one',\n",
       "  'called rectified linear units',\n",
       "  'n dimensional feature vector',\n",
       "  'latent feature vector'],\n",
       " ['g give us useful latent features',\n",
       "  'asymptotes um another common one',\n",
       "  'choices like sigmoids um various',\n",
       "  'also suppose uh tan h',\n",
       "  'use units like tan h'],\n",
       " ['use units like tan h',\n",
       "  'term neural networks came',\n",
       "  'let us illustrate uh',\n",
       "  'get enough activation um',\n",
       "  'x1 plus x2 okay'],\n",
       " ['imagine cutting away like',\n",
       "  'uh one one one',\n",
       "  'little bit hard',\n",
       "  'defined tan h',\n",
       "  '0 z1 0'],\n",
       " ['saw two slides ago',\n",
       "  'latent feature space z',\n",
       "  'whole real line',\n",
       "  'much higher dimensional',\n",
       "  'make deep networks'],\n",
       " ['saw two slides ago',\n",
       "  'loon surface thing uh',\n",
       "  'sophisticated classification models',\n",
       "  'original space gets',\n",
       "  'like totally mashed'],\n",
       " ['basic idea behind frameworks like pytorch',\n",
       "  'equals x times x um',\n",
       "  'use libraries like pi torch',\n",
       "  'capture x times x',\n",
       "  'tool called automatic differentiation'],\n",
       " ['using relatively basic mathematical operations',\n",
       "  'use libraries like pi torch',\n",
       "  'standard neural net tools',\n",
       "  'either implement predefined layers',\n",
       "  'brief crash course uh'],\n",
       " ['get embedded using various word embedding layers',\n",
       "  'using real valued vectors basically',\n",
       "  'w apply soft max',\n",
       "  'little bit careful typically',\n",
       "  'numpy arrays fairly easily'],\n",
       " ['final results uh either',\n",
       "  'whole process uh',\n",
       "  'check uh performance',\n",
       "  'pi torch code',\n",
       "  'make sure um'],\n",
       " ['single epoch takes pretty substantially',\n",
       "  'negative log probabilities across',\n",
       "  'process multiple data points',\n",
       "  'far generalizes seamlessly',\n",
       "  'achieve higher parallelism'],\n",
       " ['seek model um batch size one kind',\n",
       "  'uh first order optimization techniques',\n",
       "  'story changes pretty substantially',\n",
       "  'negative log probabilities across',\n",
       "  'simpler cases like training'],\n",
       " ['consider using rectified linear units',\n",
       "  'hyperbolic tangent gets pretty flat',\n",
       "  'little bit stuck um',\n",
       "  'says okay well even',\n",
       "  'produce larger values um'],\n",
       " ['learning something called batch normalization',\n",
       "  'little bit stuck um',\n",
       "  'negative somehow um',\n",
       "  'single step size',\n",
       "  'single gradient update'],\n",
       " ['surface word context around',\n",
       "  'actual like film reel',\n",
       "  'call selectional preferences',\n",
       "  'actually mean concretely',\n",
       "  'idea behind firth'],\n",
       " ['also xove va saw',\n",
       "  'never see like saw',\n",
       "  'given word equals saw',\n",
       "  'saw given saw um',\n",
       "  'um va saww'],\n",
       " ['say quote unquote impossible problem',\n",
       "  'know word vectors right like vsa',\n",
       "  'given word equals saw',\n",
       "  'saw given saw um',\n",
       "  'window size k'],\n",
       " ['know bit cat later whatever',\n",
       "  'negative sampling exactly corresponds',\n",
       "  'pmi term looks like',\n",
       "  'unigram distribution um meaning',\n",
       "  'following uh values um'],\n",
       " ['negative sampling exactly corresponds',\n",
       "  'unigram distribution um meaning',\n",
       "  'looks like true pairs',\n",
       "  'negative examples uniformly',\n",
       "  'numerator looks like'],\n",
       " ['absolute discounting distribution plus lambda times p',\n",
       "  'called absolute discounting',\n",
       "  'maximum likelihood estimate',\n",
       "  'high level picture',\n",
       "  'etc every combination'],\n",
       " ['three gram uh actually sorry four gram absolute discounting probabilities',\n",
       "  'absolute discounting distribution plus lambda times p',\n",
       "  'three unique contexts',\n",
       "  'sub essentially think',\n",
       "  'maximum likelihood estimate'],\n",
       " ['three gram uh actually sorry four gram absolute discounting probabilities',\n",
       "  'called kinaser nye smoothing kinaserkni smoothing',\n",
       "  'lower order probability distributions depend',\n",
       "  'lambda prime times p',\n",
       "  'absolute discounting'],\n",
       " ['often like minus two minus three things like',\n",
       "  'weird transformation perplexity actually kind',\n",
       "  'language modeling research',\n",
       "  'geometric mean since',\n",
       "  'negative log likelihood'],\n",
       " ['1 0 plus 1 6 times 1 0 plus',\n",
       "  'ultimately gives us half half okay',\n",
       "  'half times zero one plus',\n",
       "  'scores zero zero one zero',\n",
       "  'assume e equals three um'],\n",
       " ['1 0 plus 1 6 times 1 0 plus',\n",
       "  'ultimately gives us half half okay',\n",
       "  'half times zero one plus',\n",
       "  '6 times 1 0',\n",
       "  'three quarters one quarter right'],\n",
       " ['times e times wv',\n",
       "  'rows represent attention scores',\n",
       "  'placing high probability mass',\n",
       "  'distribution uh per row',\n",
       "  'softmax times e'],\n",
       " ['say k equals three',\n",
       "  'generally v continuations uh',\n",
       "  'n uh total sequences',\n",
       "  'v squared hypotheses uh',\n",
       "  'uh highest probability sequence'],\n",
       " ['consider k times v times n hypotheses',\n",
       "  'possible uh possible next words right',\n",
       "  'k times n transformer calls',\n",
       "  'uh k times v',\n",
       "  'running big neural nets'],\n",
       " ['also high de mexico also high probabilities',\n",
       "  'entire sequence looks pretty high right like',\n",
       "  'models called globally normalized models',\n",
       "  'looking quite weird like',\n",
       "  'string actually gets generated'],\n",
       " ['roads towns people civilization etc',\n",
       "  'little bit less likely',\n",
       "  'like even rarer stuff',\n",
       "  'include things like energy',\n",
       "  'like highest probability thing'],\n",
       " ['actually three different tags uh',\n",
       "  'tag nnp um two',\n",
       "  'different interpretations right um',\n",
       "  'know pretty large number',\n",
       "  'verb right um like'],\n",
       " ['like two nouns get like jammed together',\n",
       "  'noun noun compound um',\n",
       "  'running around like',\n",
       "  'two nouns kind',\n",
       "  'someone would say'],\n",
       " ['relatively simple sentence like',\n",
       "  'someone would say',\n",
       "  'later syntactic parsers',\n",
       "  'illustrates two things',\n",
       "  'blah blah blah'],\n",
       " ['log p log p log p',\n",
       "  'big long gnarly complicated thing',\n",
       "  '3 log p plus log',\n",
       "  'probability one minus p um',\n",
       "  'log one minus p'],\n",
       " ['log p log p log p',\n",
       "  '3 log p plus log',\n",
       "  'probability one minus p um',\n",
       "  'log one minus p',\n",
       "  'two positions um n'],\n",
       " ['two positions um n',\n",
       "  'noun verb stop okay',\n",
       "  'like one percent probability',\n",
       "  'b okay tags',\n",
       "  'entire tag set'],\n",
       " ['y1 x1 given y1 y2 given y1 x2 given y2 y3 given y2 x3 given y3',\n",
       "  'stop given y3',\n",
       "  'like added one everywhere',\n",
       "  'like verb given',\n",
       "  'like one percent probability'],\n",
       " ['dynamic program looks like',\n",
       "  'hidden markov models okay',\n",
       "  'first time step uh',\n",
       "  'twiddle plus log p',\n",
       "  'fairly complicated expression'],\n",
       " ['twiddle plus log p',\n",
       "  'first time step uh',\n",
       "  'first time step',\n",
       "  'second time step',\n",
       "  'current time step'],\n",
       " ['works um like recall',\n",
       "  'first couple terms involving',\n",
       "  'direction um etc',\n",
       "  'possible previous tags',\n",
       "  'particular time step'],\n",
       " ['called back pointers uh essentially',\n",
       "  'relatively small dynamic programming chart',\n",
       "  'want although hey wait',\n",
       "  'one previous time step',\n",
       "  'coupling everything together'],\n",
       " ['called back pointers uh essentially',\n",
       "  'viterbi algorithm although',\n",
       "  'stop symbol um',\n",
       "  'real valued scores',\n",
       "  'little bit overloaded'],\n",
       " ['minus four uh taking',\n",
       "  'get minus eight okay',\n",
       "  'also minus one giving',\n",
       "  'two possible places',\n",
       "  'recurrence looks like'],\n",
       " ['possibilities alive even though uh',\n",
       "  'get minus eight okay',\n",
       "  'makes us reevaluate',\n",
       "  'makes sense given',\n",
       "  'computation forward notice'],\n",
       " ['conditional random fields',\n",
       "  'hidden markov models',\n",
       "  'sequence models',\n",
       "  'useful tool',\n",
       "  'stage also'],\n",
       " ['right branching fashion okay let',\n",
       "  'recursively get whole sentences inside',\n",
       "  'typically right branching meaning',\n",
       "  'court estimated fundamentally',\n",
       "  'unary rule uh'],\n",
       " ['something called co',\n",
       "  'correctly necessarily gives',\n",
       "  'tree structure assumption',\n",
       "  'tree structures really',\n",
       "  'court noun phrase'],\n",
       " ['hard time resolving ambiguities like',\n",
       "  'even pretty good automatic systems',\n",
       "  'ambiguities uh modifier scope um',\n",
       "  'know chocolate cake um',\n",
       "  'finally coordination scope'],\n",
       " ['spoon piece tells us something',\n",
       "  'nicely formed constituency structure',\n",
       "  'tells us something',\n",
       "  'constructions like clefting',\n",
       "  'called constituency tests'],\n",
       " ['interesting stuff going forward',\n",
       "  'nicely formed constituency structure',\n",
       "  'tells us something',\n",
       "  'understanding constituency',\n",
       "  'constituency starts'],\n",
       " ['probabilistic context free grammars',\n",
       "  'unary rules involving non',\n",
       "  'four tuple ntsr',\n",
       "  'children ate cake',\n",
       "  'something special okay'],\n",
       " ['probability distribution uh basically p',\n",
       "  'uh probabilistic context free grammar',\n",
       "  'case uh vp goes',\n",
       "  'unary rules involving non',\n",
       "  'pick one etc'],\n",
       " ['uh probabilistic context free grammar',\n",
       "  'see noun phrase rewriting',\n",
       "  'higher order n reproductions',\n",
       "  'get probability three sevenths',\n",
       "  'unary productions rather'],\n",
       " ['still allow unary rules',\n",
       "  'chomsky normal form',\n",
       "  'chomsky normal form',\n",
       "  'three different rules',\n",
       "  'next two rules'],\n",
       " ['intermediate symbols get introduced',\n",
       "  'tree bank',\n",
       "  'take uh',\n",
       "  'really important',\n",
       "  'prepositional phrases'],\n",
       " ['tag versus sentence matrix',\n",
       "  'cell 0 comma 1',\n",
       "  'structure actually looks like',\n",
       "  'actually three dimensions',\n",
       "  '0 comma 4'],\n",
       " ['cell 0 comma 1',\n",
       "  '0 comma 4',\n",
       "  'spatially raising something',\n",
       "  'screen um capturing',\n",
       "  'normalize notice though'],\n",
       " ['represent two possible split points',\n",
       "  'grammar x1 x goes',\n",
       "  'two possible ways',\n",
       "  'dynamic program works',\n",
       "  'chart values associated'],\n",
       " ['one possible split point right',\n",
       "  'give us negative infinity',\n",
       "  'rule uh additionally take',\n",
       "  'k equals one',\n",
       "  'probability negative infinity'],\n",
       " ['n squared chart cells',\n",
       "  'grammar constant times g',\n",
       "  'run time ends',\n",
       "  'many possible values',\n",
       "  'handle unary rules'],\n",
       " ['n squared chart cells',\n",
       "  'know without going kind',\n",
       "  'grammar constant times g',\n",
       "  'run time ends',\n",
       "  'little bit hard'],\n",
       " ['independence assumptions underlying pcfgs',\n",
       "  'little bit stronger',\n",
       "  'fairly simple pre',\n",
       "  'prepositional phrase um',\n",
       "  'two different well'],\n",
       " ['verb phrase uh right',\n",
       "  'like super symbol uh',\n",
       "  'like everything listed',\n",
       "  'two different well',\n",
       "  'statistical efficiency standpoint'],\n",
       " ['like vertical markovization controlled',\n",
       "  'call h equals zero',\n",
       "  'binarization called horizontal markovization',\n",
       "  'generation h equals one',\n",
       "  'would actually always remember'],\n",
       " ['grammatical role um preposition label um object',\n",
       "  'pretty shallow way based',\n",
       "  'one big difference representationally',\n",
       "  'easily determined based',\n",
       "  'could say okay'],\n",
       " ['ternary ternary rule np goes',\n",
       "  'fairly major way several',\n",
       "  'prepositional phrase example ate',\n",
       "  'unquote prepositional phrase',\n",
       "  'cats example um'],\n",
       " ['actually two people involved right um',\n",
       "  'collapsed representation called stanford dependencies',\n",
       "  'ternary rule np goes',\n",
       "  'tasks like relation extraction',\n",
       "  'called standard dependency representation'],\n",
       " ['uh standard constituency representations',\n",
       "  'linear structure um anymore',\n",
       "  'crossing arc look like',\n",
       "  'crossing uh well okay',\n",
       "  'language like check'],\n",
       " ['sigma bar w minus one means',\n",
       "  'w minus two w minus one',\n",
       "  'w minus one uh',\n",
       "  'w minus one',\n",
       "  'uh involve combining items'],\n",
       " ['arc standard transition system uh',\n",
       "  'add w minus two back',\n",
       "  'w minus one uh',\n",
       "  'quote unquote oracle derivation',\n",
       "  'good little thought exercise'],\n",
       " ['good little thought exercise',\n",
       "  'might directly pull things',\n",
       "  'whole spaghetti bolognese thing',\n",
       "  'shift shift left arc',\n",
       "  'know typically use operations'],\n",
       " ['whole spaghetti bolognese thing',\n",
       "  'right arcs essentially um',\n",
       "  'particular reduce operation',\n",
       "  'partially built structures',\n",
       "  'happens next well'],\n",
       " ['check sentence one sentence two sentence three etc',\n",
       "  '17th century fresco even going',\n",
       "  'use large language models',\n",
       "  'framed 13th century icon',\n",
       "  'checked well one approach'],\n",
       " ['say x double prime represents',\n",
       "  'actually say anything useful',\n",
       "  'could draw x primes',\n",
       "  'model overall predicted positive',\n",
       "  'x double prime'],\n",
       " ['know enable large language models',\n",
       "  'data set called strategy qa',\n",
       "  'put together different pieces',\n",
       "  'okay aristotle lived way',\n",
       "  'natural lang language'],\n",
       " ['letter concatenation thing seemed really trivial right like',\n",
       "  'approaches uh program aided language models program',\n",
       "  'bit across three different tasks',\n",
       "  'things like programmatic interpreters',\n",
       "  'bad natural language expressions'],\n",
       " ['uh one way', 'make sure', 'existing tools', 'uh kind', 'go even'],\n",
       " ['know roughly looks something like',\n",
       "  'states border texas etc',\n",
       "  'know food safe temperatures',\n",
       "  'knowledge based question answering',\n",
       "  'lambda calculus expression'],\n",
       " ['specialized cooking question answering system',\n",
       "  'huge domain specific knowledge base',\n",
       "  'franz ferdinand took place',\n",
       "  'world war one rather',\n",
       "  'world war one right'],\n",
       " ['extracting relatively straightforward answers',\n",
       "  'gives us contexts',\n",
       "  'generalizing qa making',\n",
       "  'work better making',\n",
       "  'harder qa problems'],\n",
       " ['open domain qa approach',\n",
       "  'basically span extracts like',\n",
       "  'passage maybe one',\n",
       "  'question answering models',\n",
       "  'also lower um'],\n",
       " ['particularly historical ideas behind chat bots',\n",
       "  'chat bots really goes back quite',\n",
       "  'original idea behind turing',\n",
       "  'third person um',\n",
       "  'called original interpretation'],\n",
       " ['11 year old ukrainian boy',\n",
       "  '11 year old ukrainian boy',\n",
       "  'system would make grammatical errors',\n",
       "  'interesting chat bot systems',\n",
       "  'doctor script would ask'],\n",
       " ['know possibly approaching human',\n",
       "  'doctor script would ask',\n",
       "  'super engaging conversations',\n",
       "  'several different types',\n",
       "  'system works um'],\n",
       " ['user says something cleverbot tries',\n",
       "  'know farming maybe okay',\n",
       "  'system called cleverbot uh',\n",
       "  'cleverbot cleverbot says',\n",
       "  'slightly better attempt'],\n",
       " ['hopefully siri would say something like sushi seki chelsea',\n",
       "  'real data real databases information',\n",
       "  'basically api front ends',\n",
       "  'good sushi restaurant',\n",
       "  'generating random crap'],\n",
       " ['know many many uh many systems',\n",
       "  'real data real databases information',\n",
       "  'oriented dialogue systems work',\n",
       "  'lambda calculus expression uh',\n",
       "  'new york tomorrow morning'],\n",
       " ['know many many uh many systems',\n",
       "  'oriented dialogue systems work',\n",
       "  'system like amazon alexa',\n",
       "  'intent plus slot model',\n",
       "  'lambda calculus expression uh'],\n",
       " ['build really compelling systems',\n",
       "  'work really well',\n",
       "  'big open problems',\n",
       "  'industry players uh',\n",
       "  'know parsing generation'],\n",
       " ['pretty big model however',\n",
       "  'turn dialogue yet even',\n",
       "  'interesting joke explaining example',\n",
       "  'person says yes sing',\n",
       "  'past conversation turns'],\n",
       " ['really understand phenology right',\n",
       "  'pronounced like fun guy',\n",
       "  'interesting joke explaining example',\n",
       "  'past conversation turns',\n",
       "  'helpful conversation partner'],\n",
       " ['english side makes us think',\n",
       "  'translation keska tufay translates',\n",
       "  'maybe je translates',\n",
       "  'word translations however',\n",
       "  'maybe rough assumptions'],\n",
       " ['one gram two gram three gram',\n",
       "  'whole really rich literature',\n",
       "  'extremely good blue score',\n",
       "  'would get 100 precision',\n",
       "  'human level translation quality'],\n",
       " ['model actual actually works uh',\n",
       "  'pretty big simplifying assumption',\n",
       "  'makes ibm model one',\n",
       "  'model fairly easy',\n",
       "  'little bit later'],\n",
       " ['model actual actually works uh',\n",
       "  'ti given sai sort',\n",
       "  'ti given sai',\n",
       "  'sentence pair gem',\n",
       "  'letting ai range'],\n",
       " ['involve implementing things like',\n",
       "  'two things one',\n",
       "  'pretty reasonable constraint',\n",
       "  'marginal log likelihood',\n",
       "  'labeled alignment data'],\n",
       " ['strong earthquake hit large parts',\n",
       "  'uh state tv said',\n",
       "  'biomedical research papers uh',\n",
       "  'got maybe one article',\n",
       "  'summarizing legal documents'],\n",
       " ['help us understand maybe',\n",
       "  'indian express article directly',\n",
       "  'iranian border eight villages',\n",
       "  'across articles tells us',\n",
       "  'little bit sneaky'],\n",
       " ['called maximum marginal relevance',\n",
       "  'two uh two terms',\n",
       "  'say one key fact',\n",
       "  'called query focused summarization',\n",
       "  'topic critically like'],\n",
       " ['cover massive earthquake several times',\n",
       "  'massive earthquake two',\n",
       "  'wi values come',\n",
       "  'integer linear program',\n",
       "  'high scoring things'],\n",
       " ['one document said massive earthquake',\n",
       "  'single document summarization setting',\n",
       "  'someone said large earthquake',\n",
       "  'know uh bad earthquake',\n",
       "  'say massive earthquake'],\n",
       " ['know basically first second third person uh singular versus plural um',\n",
       "  'verb arrive right arrive arrives arrived um',\n",
       "  'singular versus plural',\n",
       "  'things like past tense um',\n",
       "  'two different ways based'],\n",
       " ['chat gpt really understanding stuff',\n",
       "  'deeper grounded understand standing',\n",
       "  'google translate produces translations',\n",
       "  'grammar books things like',\n",
       "  'thought experiment goes'],\n",
       " ['efforts also revolve around documentation',\n",
       "  'value sensitive design whose values',\n",
       "  'propagate certain value systems',\n",
       "  'also downsides right like',\n",
       "  'example large tech companies'],\n",
       " ['value sensitive design whose values',\n",
       "  'efforts also revolve around documentation',\n",
       "  'companies like open ai',\n",
       "  'propagate certain value systems',\n",
       "  'releasing tools widely onto'],\n",
       " ['completely dissimilar form like',\n",
       "  'people would prefer',\n",
       "  'mathematical things especially',\n",
       "  'course 751 okay',\n",
       "  'hundred possible topics'],\n",
       " ['applications spectral graph theory',\n",
       "  'please definitely go online',\n",
       "  'website called dinner oh',\n",
       "  'straight satisfaction programs',\n",
       "  'sdp hierarchies information'],\n",
       " ['please definitely go online',\n",
       "  'project asynchronously later basically',\n",
       "  'like april 3rd',\n",
       "  'like another topic',\n",
       "  'basically six homeworks'],\n",
       " ['acl seminar theory launch seminar',\n",
       "  'regular weekly seminar series',\n",
       "  'class last semester okay',\n",
       "  '3 hours per week',\n",
       "  'three theory talks'],\n",
       " ['know modern people use twitter',\n",
       "  'know like write every day',\n",
       "  'know novelists advice always',\n",
       "  'olden days people used',\n",
       "  'progress every day'],\n",
       " ['know modern people use twitter',\n",
       "  'mean current social network',\n",
       "  'worth 6 percent',\n",
       "  'little bit old',\n",
       "  'days people used'],\n",
       " ['top algorithms conference si si si',\n",
       "  'tcs researchers excluding like',\n",
       "  'bus home watch watch',\n",
       "  'mean current social network',\n",
       "  'really properly learn things'],\n",
       " ['top algorithms conference si si si',\n",
       "  'tcs researchers excluding like',\n",
       "  'free archiving paper service',\n",
       "  'ancillary research type things',\n",
       "  'top complexity conference'],\n",
       " ['days reference management software',\n",
       "  'mean get started download',\n",
       "  'ancillary research type things',\n",
       "  'like one giant file',\n",
       "  'dot bib file'],\n",
       " ['tech tech da stock exchange comm every possible question',\n",
       "  'days reference management software',\n",
       "  'course writing low tech',\n",
       "  'stock exchange',\n",
       "  'mean get started download'],\n",
       " ['still good top version control',\n",
       "  'also need version control even',\n",
       "  'really great uh text editor',\n",
       "  'course writing low tech',\n",
       "  'proper text editor'],\n",
       " ['like one lifetime like stubb tech file',\n",
       "  'still good top version control',\n",
       "  'also need version control even',\n",
       "  'cs adjacent people like us',\n",
       "  'one lifetime style file'],\n",
       " ['like one lifetime like stubb tech file',\n",
       "  'sub tech file looks kind',\n",
       "  'like weird bibtex like bug',\n",
       "  'like slash begin document',\n",
       "  'like people capital letters'],\n",
       " ['get bib file big tech entry',\n",
       "  'like beautiful bib tech entry',\n",
       "  'like weird bibtex like bug',\n",
       "  'indexes computer science proceedings',\n",
       "  'bibliography file please make'],\n",
       " ['name amy sure yes yeah back ticks',\n",
       "  'terrible yes jennifer yeah definitely',\n",
       "  'yeah back ticks yeah',\n",
       "  'name sunny great thank',\n",
       "  'tek dog stock exchange'],\n",
       " ['name hey bailey yes ah yes solet eric',\n",
       "  'say oh yes exactly right yes',\n",
       "  'yeah last one oh yeah',\n",
       "  'says latech never break',\n",
       "  'use infinite wisdom decided'],\n",
       " ['draw wonderful vector graphics arctic sea',\n",
       "  'use proper figure drawing software like inkscape',\n",
       "  'write slashing clue graphics',\n",
       "  'scott like sir tools',\n",
       "  'presentation software like powerpoint'],\n",
       " ['like phd students oh fantastic okay yeah let',\n",
       "  'good anything else yes oh tell',\n",
       "  'correct font matching tier yes',\n",
       "  'draw figures yeah pedro yeah',\n",
       "  'matching font yeah oh'],\n",
       " ['like phd students oh fantastic okay yeah let',\n",
       "  'name hey farmiga oh thanks yeah okay',\n",
       "  'know people enjoy using beamer',\n",
       "  'great okay cool yeah',\n",
       "  'make nice talks using'],\n",
       " ['name hey farmiga oh thanks yeah okay',\n",
       "  'west coast stanford kayvon',\n",
       "  'good like content presentation',\n",
       "  'equation editor yeah',\n",
       "  'good talk okay'],\n",
       " ['getting exactly half coin flips',\n",
       "  'flip end coins um okay',\n",
       "  'course uh okay well',\n",
       "  'right well uh maybe',\n",
       "  'homework yet um'],\n",
       " ['nothing none negative nothing',\n",
       "  'function like k times',\n",
       "  'negative random variable',\n",
       "  'little bit weird',\n",
       "  'advantageous ly later'],\n",
       " ['like secretly underlies like every like algorithms text book',\n",
       "  'need like n log n time',\n",
       "  'something called word ram model',\n",
       "  'use like radix sort',\n",
       "  'probably also knew due'],\n",
       " ['underlies like every like algorithms text book',\n",
       "  'quadratic time algorithm complexity theorist would',\n",
       "  'like every time step depending',\n",
       "  'today well maybe alright',\n",
       "  'least basic algorithmic theory'],\n",
       " ['maybe one might write b',\n",
       "  'inputs x1 x2 x3',\n",
       "  'dags directed acyclic graphs',\n",
       "  'three like reasonable choices',\n",
       "  'things like exactly count'],\n",
       " ['operate something like 64 bit chunks',\n",
       "  'core code standard algorithmic model',\n",
       "  'like model like normal computation like',\n",
       "  'like literally formally described',\n",
       "  'never really exactly want'],\n",
       " ['log n bit number cost like log n time like',\n",
       "  'like one basic step cost one done',\n",
       "  'like model like normal computation like',\n",
       "  'classical undergrad algorithms textbook right',\n",
       "  'really log n bits'],\n",
       " ['missing yeah oh things like cosine',\n",
       "  'know like assembly plus ram instructions',\n",
       "  'classical undergrad algorithms textbook right',\n",
       "  'yeah yeah comparisons see yeah',\n",
       "  'usually look mod two'],\n",
       " ['store total cause total sexual value',\n",
       "  'space yeah excellent uh okay',\n",
       "  'trans dichotomous man model mention',\n",
       "  'two words yeah exact appoint',\n",
       "  'square root n bit numbers'],\n",
       " ['know bitwise shifts adding two numbers subtracting two numbers',\n",
       "  'trans dichotomous man model mention',\n",
       "  'know constant parallel time operation',\n",
       "  'square root n bit numbers',\n",
       "  'time one takes two words'],\n",
       " ['cells named like zero one two three',\n",
       "  'except maybe maybe one time',\n",
       "  'um uh yeah well uh',\n",
       "  'makes sense cool thank',\n",
       "  'graph paper papers yeah'],\n",
       " ['know binary search tree type like data structures',\n",
       "  'word right model right well ah',\n",
       "  'like abstract data types sorting really',\n",
       "  'like things called like fibonacci heaps',\n",
       "  'read instruction like please read'],\n",
       " ['know abstract data type persons would like',\n",
       "  'word right model right well ah',\n",
       "  'away yeah okay like okay let',\n",
       "  'read instruction like please read',\n",
       "  'like memory spot like 643'],\n",
       " ['problem one type stuff called paley sigmund inequality',\n",
       "  'know abstract data type persons would like',\n",
       "  'good hint yeah yeah let',\n",
       "  'fourth moment method thing let',\n",
       "  'know pure system would'],\n",
       " ['digit 1 2 4 8 13 okay',\n",
       "  'know packaging log n bits together',\n",
       "  'foligno hypothetically log n wherefore',\n",
       "  'exactly like log base two',\n",
       "  'know called like bit tricks'],\n",
       " ['interesting work like research work',\n",
       "  'generally use fourier analysis',\n",
       "  'meeting yep great question',\n",
       "  'voting one way',\n",
       "  '4 different p'],\n",
       " ['way voting scheme right',\n",
       "  'maybe either fourier expansions',\n",
       "  'like x2 equals x3',\n",
       "  'saw well see like',\n",
       "  'saw hey presto'],\n",
       " ['wrote like f empty set',\n",
       "  'two two like two',\n",
       "  'um empty set great',\n",
       "  'uh yeah sure 2b',\n",
       "  'help yes yeah yeah'],\n",
       " ['like okay zero zero zero zero zero one one zero zero',\n",
       "  'like zero zero one one zero one zero zero one f',\n",
       "  'find like absolutely sixteen yeah',\n",
       "  'like uh maybe minus 3',\n",
       "  'machine came back online'],\n",
       " ['like 1 0 0 0 0 0 okay',\n",
       "  'transformation right away okay',\n",
       "  'input circuit seat okay',\n",
       "  'really high probability okay great',\n",
       "  'matrix look like hopefully'],\n",
       " ['whole old value different',\n",
       "  'constant coefficient mu negated',\n",
       "  'x minus mu right',\n",
       "  'values f x values',\n",
       "  'half half half'],\n",
       " ['zero zero one zero oh tada',\n",
       "  'case little n equals two',\n",
       "  'one grover move well first',\n",
       "  'would get x star',\n",
       "  'little bit fast'],\n",
       " ['point one well something like root n divided',\n",
       "  'point one times maybe divided',\n",
       "  'probability point one squared',\n",
       "  'like order root n',\n",
       "  'like one percent chance'],\n",
       " ['like literally looks like sorry',\n",
       "  'actually like one two uh',\n",
       "  'mean okay wait uh',\n",
       "  'like one two goes',\n",
       "  'adding like two one'],\n",
       " ['might say like oh surprising like',\n",
       "  'line even though one one',\n",
       "  'know pictures uh aside like',\n",
       "  'course like five minus five',\n",
       "  'direction um five minus five'],\n",
       " ['error involves changing corrupting one symbol',\n",
       "  'symbol gets corrupted change',\n",
       "  'transmitter transmits well',\n",
       "  'somehow racist symbol',\n",
       "  'receiver gets z'],\n",
       " ['good error correcting code encoding function',\n",
       "  'errors could get corrupted okay',\n",
       "  'systematic error correcting code',\n",
       "  'potentially corrupted code word',\n",
       "  'symbol gets corrupted change'],\n",
       " ['100 code words okay',\n",
       "  'one symbol gets changed',\n",
       "  'arbitrary different symbol',\n",
       "  'forth yep yeah',\n",
       "  'said today'],\n",
       " ['specific generating general matrices generator matrices g',\n",
       "  'certain k dimensional subspace',\n",
       "  'k dimensional subspace q',\n",
       "  'strictly speaking true okay',\n",
       "  'actually pretty convenient okay'],\n",
       " ['linear algebra concept actually see perp',\n",
       "  'k dimensional subspace code c',\n",
       "  'certain k dimensional subspace',\n",
       "  'mapping f k q',\n",
       "  'actually pretty convenient okay'],\n",
       " ['know low hamming weight strings z',\n",
       "  'least hamming weight vector z',\n",
       "  'vector well let',\n",
       "  'vector z',\n",
       "  'put z'],\n",
       " ['rows okay like zero one one zero one zero zero one phew okay hopefully',\n",
       "  'chat great zero one one one zero zero zero zero great',\n",
       "  'right okay true okay uh three okay',\n",
       "  'like one zero one cubed',\n",
       "  'one two three okay guess'],\n",
       " ['like distance plus plus okay',\n",
       "  'b h b zero one',\n",
       "  'know like print uh okay',\n",
       "  'distance variable okay cool',\n",
       "  'spot u comma v'],\n",
       " ['two ways like sometimes people think',\n",
       "  'vector space sense yeah',\n",
       "  'like humvee w essentially',\n",
       "  'like linear algebra facts',\n",
       "  '20 different code words'],\n",
       " ['um encoded using inner right',\n",
       "  'field f eight um oops',\n",
       "  'field size two two vectors',\n",
       "  'linear algebra would call',\n",
       "  'like humvee w essentially'],\n",
       " ['one zero zero one zero one one oh assume',\n",
       "  'makes us like really make sure',\n",
       "  'like little n times big n',\n",
       "  'uh well yeah without getting',\n",
       "  'maps four bit strings'],\n",
       " ['rewatch especially regarding um 2b earlier',\n",
       "  'say yeah yeah oh',\n",
       "  'well okay thanks',\n",
       "  'evening okay sure',\n",
       "  'takes like forever'],\n",
       " ['r3 r rd something like',\n",
       "  'spectral graph theory well',\n",
       "  'examples might include maybe',\n",
       "  'physical object maybe f',\n",
       "  'spectral graph theory'],\n",
       " ['probability distribution pi well going back',\n",
       "  'special probability distribution payan vertices',\n",
       "  'uniformly random directed edge okay',\n",
       "  'gray directed graph g',\n",
       "  'vertices neighbors uniformly random'],\n",
       " ['magical thing happens',\n",
       "  'regular graph otherwise',\n",
       "  'probability distribution pie',\n",
       "  'probability distribution pie',\n",
       "  'probability distribution pie'],\n",
       " ['f around used neighbors',\n",
       "  'lira bra sense right',\n",
       "  'plastic linear algebra style',\n",
       "  'normalized adjacency operator let',\n",
       "  'standard random walk'],\n",
       " ['plastic linear algebra style',\n",
       "  'standard random walk starting',\n",
       "  'normalized adjacency operator let',\n",
       "  'standard random walk',\n",
       "  'edge otherwise zero'],\n",
       " ['adjoint ness says',\n",
       "  'symmetry called self',\n",
       "  'new function kf',\n",
       "  'little diffusion operator',\n",
       "  'previous lecture motivated'],\n",
       " ['function f across edges okay',\n",
       "  'case k matrix finally',\n",
       "  'arbitrary probability distribution row',\n",
       "  'row vector times',\n",
       "  'case distribution row'],\n",
       " ['original transition matrix seems pretty believable',\n",
       "  'random step like normal like along',\n",
       "  'worst case starting point',\n",
       "  'great another wonderful property',\n",
       "  'stupid even odd problem'],\n",
       " ['costco run real quick',\n",
       "  'wha ... oh yeah',\n",
       "  'seems pretty legit',\n",
       "  'pretty big deal',\n",
       "  'pretty big deal'],\n",
       " ['get get get strong',\n",
       "  'couch watching netflix',\n",
       "  'busy shopping mall',\n",
       "  'uh ... couch',\n",
       "  'gonna happen ?!'],\n",
       " ['pollen real time poly n time decoding',\n",
       "  'super fast linear time decoding algorithm',\n",
       "  'polynomial time decoding algorithm',\n",
       "  'decoding another great fact',\n",
       "  'super fast algorithm'],\n",
       " ['incorrectly say yes even though x',\n",
       "  'one percent bad strings random strings',\n",
       "  'incorrectly overall say yes okay',\n",
       "  'standard repetition method okay',\n",
       "  'overall output yes otherwise'],\n",
       " ['specific l uh yeah uh generally oh uh',\n",
       "  'f minus k f okay',\n",
       "  'one two three um okay',\n",
       "  'many definitions flying around',\n",
       "  'random walk um sort'],\n",
       " ['guy called ted harris',\n",
       "  'sense come back',\n",
       "  'ted harris',\n",
       "  'soviet union',\n",
       "  'soviet union'],\n",
       " ['maximum weight perfect matching assuming',\n",
       "  'variables represent real numbers',\n",
       "  'maximum weight perfect matching',\n",
       "  'x sub u v',\n",
       "  'integer linear program ilp'],\n",
       " ['edges capacity times lambda u v',\n",
       "  'least mu u minus mu v',\n",
       "  'mu u minus mu b',\n",
       "  'vertex green vertex labels',\n",
       "  'lambda u v'],\n",
       " ['like separating hyper planes',\n",
       "  'whole thing yeah cool',\n",
       "  'whole lp written',\n",
       "  'proof theoretic interpretation',\n",
       "  'little bit trivial'],\n",
       " ['hand side yeah literally look like identical',\n",
       "  'max sum c w x w',\n",
       "  'blue things exactly right right oh',\n",
       "  'blue stuff literally exactly right',\n",
       "  'really well selected right right'],\n",
       " ['sorry minimum word type rated vertex cover like',\n",
       "  'max sum c w x w',\n",
       "  'blue things exactly right right oh',\n",
       "  'blue stuff literally exactly right',\n",
       "  'like find well maybe like'],\n",
       " ['ij equals u pi times u j',\n",
       "  'ij equals ui dot u j',\n",
       "  'positive semi definite matrix capital',\n",
       "  'also basic linear algebra proposition',\n",
       "  'ui dot u j equals'],\n",
       " ['zero one indicator random variable',\n",
       "  'random hyperplane rounding algorithm ok',\n",
       "  'semi definite program namely',\n",
       "  'pick n gaussian x',\n",
       "  'two vectors v vector'],\n",
       " ['associated unit vectors v vector',\n",
       "  'basically like 180 degrees',\n",
       "  'two vectors v vector',\n",
       "  'two vectors v vector',\n",
       "  'w vector associated'],\n",
       " ['angle 90 degrees like b',\n",
       "  'basically like 180 degrees',\n",
       "  'angle 90 degrees see',\n",
       "  'make sense right like',\n",
       "  'orange line would split'],\n",
       " ['slightly darker slightly thinner green curve point 8 7 8 times',\n",
       "  '8 7 8',\n",
       "  'new curve fits underneath',\n",
       "  'green point another way',\n",
       "  '87 8 times'],\n",
       " ['define constraint satisfaction problems',\n",
       "  'say okay yeah uh',\n",
       "  'constraint satisfaction problems',\n",
       "  'c 1 c',\n",
       "  'xn another ingredient'],\n",
       " ['point eight seven eight beta comma beta certification algorithm',\n",
       "  'goins williamson algorithm hyperplane rounding algorithm exists',\n",
       "  'point eight seven eight beta okay',\n",
       "  'least point eight seven eight beta',\n",
       "  'least point eight seven eight beta'],\n",
       " ['case xa plus xb plus xc',\n",
       "  'max independent set problem',\n",
       "  'something called proof lines',\n",
       "  'representing zero one values',\n",
       "  'three edges okay'],\n",
       " ['spare time otama sizable automatize',\n",
       "  'probably called automatize abode',\n",
       "  'generality one line long',\n",
       "  'proof system even though',\n",
       "  'cutting planes proof system'],\n",
       " ['integral distance apart pretty much yeah',\n",
       "  'looks like yes yes exactly one',\n",
       "  'sorry yeah uh okay good yeah',\n",
       "  'positive right okay cool yeah',\n",
       "  'like maybe abusive notation okay'],\n",
       " ['looks like yes yes exactly one zero one yeah okay',\n",
       "  'j transpose j yeah yeah uh okay yeah',\n",
       "  'feasible solution yeah yeah yeah like like like',\n",
       "  'proposed solution yeah like like like basically',\n",
       "  'something like j script j transpose'],\n",
       " ['media patent said either includes',\n",
       "  'children v 1 w 1',\n",
       "  'includes v okay',\n",
       "  'think two tables',\n",
       "  'maximum pennant set'],\n",
       " ['iron borden proscar ski defines',\n",
       "  'equivalent concept also around 1985',\n",
       "  'smallest possible bag size',\n",
       "  'maximum bag size',\n",
       "  'maximum bag size'],\n",
       " ['bags containing v lie',\n",
       "  'one namely forests okay',\n",
       "  'rios key back',\n",
       "  'ah good point',\n",
       "  'somehow measure like'],\n",
       " ['five cycle gd c f e g gd c f eg yeah',\n",
       "  'like recursively gluing together like graphs like',\n",
       "  'ways like tree witzke graphs',\n",
       "  'recursively recursively glue together',\n",
       "  'chordal graph ah next definition'],\n",
       " ['missing one virtus vertex like',\n",
       "  'also linear time okay',\n",
       "  'times n bags okay',\n",
       "  'nice tree decomposition ones',\n",
       "  'great much crazier'],\n",
       " ['missing one virtus vertex like',\n",
       "  'red terrible mouse drawing',\n",
       "  'say v actually wait',\n",
       "  'gonna get always gonna',\n",
       "  'every x sorry c'],\n",
       " ['two red ian h could',\n",
       "  'red terrible mouse drawing',\n",
       "  'would become green let',\n",
       "  'read well actually',\n",
       "  'valid partial covering'],\n",
       " ['original input little x apply',\n",
       "  'communication protocol pi looks like',\n",
       "  'tree mapping capital x201',\n",
       "  '2 0 1 annotating',\n",
       "  'private input little'],\n",
       " ['two maps like basically um wait okay actually',\n",
       "  'empty ly yeah yeah',\n",
       "  'uh one okay',\n",
       "  'satisfying assignment perhaps',\n",
       "  'one um yeah'],\n",
       " ['green red green green red green right yeah',\n",
       "  'yeah red green look like red red',\n",
       "  'like l one three right towards',\n",
       "  'like size four initially right right',\n",
       "  'possible like red red yeah'],\n",
       " ['one three five sorry one yeah one three five one okay',\n",
       "  'right right right right right right',\n",
       "  'like like l one three',\n",
       "  'possible like red red yeah',\n",
       "  'makes sense um oh okay'],\n",
       " ['one three five sorry one yeah one three five one okay',\n",
       "  'l15 yeah one five uh okay',\n",
       "  'different um values right uh',\n",
       "  'one five one five',\n",
       "  'like like l one three'],\n",
       " ['uh l one five one three three five together like kind',\n",
       "  'l15 yeah one five uh okay',\n",
       "  'uh one five well',\n",
       "  'l135 uh one three',\n",
       "  'like l one three'],\n",
       " ['like k possible k plus one possible elements',\n",
       "  'k plus one variables everything looks fine',\n",
       "  'globally satisfying assignment right right',\n",
       "  'locally consistent solutions right',\n",
       "  'global assignment really satisfies'],\n",
       " ['like sub trees become like independent',\n",
       "  'really simple like linear classification',\n",
       "  'everything else becomes disconnected',\n",
       "  'like sample complexity currently',\n",
       "  'know phase networks'],\n",
       " ['cool cool uh okay uh thanks',\n",
       "  'guess like current current formulations kind',\n",
       "  'like many machine learning problems',\n",
       "  'could like give us indication',\n",
       "  'everything else becomes disconnected'],\n",
       " ['pure basic information three things',\n",
       "  'three random variables involved',\n",
       "  'one little x might',\n",
       "  'conditioned random variable okay',\n",
       "  'little x might'],\n",
       " ['like three random variables random bits',\n",
       "  'three random variables involved',\n",
       "  'three random variables',\n",
       "  'get conditional mutual information',\n",
       "  'independent random bits'],\n",
       " ['two problem polynomial time algorithms',\n",
       "  'noticeable non negligible difference',\n",
       "  'another polynomial time',\n",
       "  'two essam ensembles',\n",
       "  'outputting one well'],\n",
       " ['contexts learning theory coding theory',\n",
       "  'know says ok please tell',\n",
       "  'probability 1 minus epsilon',\n",
       "  'probability 1 minus epsilon',\n",
       "  '1 mod 2 okay'],\n",
       " ['like mini pcp step like fixes',\n",
       "  'cs pieces like graphs esps',\n",
       "  'go along well throughout',\n",
       "  'involve exactly two variables',\n",
       "  'standard coloring constraints like'],\n",
       " ['fixed constant epsilon 0',\n",
       "  'constant size increase um',\n",
       "  'fixed c 3',\n",
       "  'universal constants iii',\n",
       "  'fixed constant less']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine todas as palavras-chave em uma única string\n",
    "all_keywords = ' '.join([' '.join(keywords) for keywords in keywords_list])\n",
    "\n",
    "# Gerar a nuvem de palavras\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"black\").generate(all_keywords)\n",
    "\n",
    "# Plotar a nuvem de palavras\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
