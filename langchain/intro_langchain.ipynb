{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fala dev! \n",
    "#### Nesse notebook nós vamos começar a entender como inicializar um projeto, como funciona um ambiente virtual e como instalar as bibliotecas necessárias para o nosso projeto. \n",
    "\n",
    "#### Iremos entender também como usar os tokens e já indicar para você o caminho para a proximas etapas. Então, vamos lá!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes de começar a qualquer projeto, é interessante você criar um ambiente virtual. \n",
    "### Mas o que é um ambiente virtual?\n",
    "Ambiente virtual é um ambiente isolado do seu sistema operacional, onde você pode instalar as bibliotecas necessárias para o seu projeto sem interferir no seu sistema operacional.\n",
    "\n",
    "Olhe o exemplo abaixo de como instalar um ambiente virtual no seu sistema operacional:\n",
    "\n",
    "```bash\n",
    "pip install virtualenv\n",
    "```\n",
    "\n",
    "Feito isso, você pode criar um ambiente virtual com o comando abaixo:\n",
    "\n",
    "```bash\n",
    "virtualenv nome_do_seu_ambiente\n",
    "```\n",
    "Em geral chamamos o ambiente virtual de `env`, ou `venv`, mas você pode chamar do que quiser.\n",
    "\n",
    "Para ativar o ambiente virtual, você pode usar o comando abaixo:\n",
    "\n",
    "```bash\n",
    "cd nome_do_seu_ambiente/Scrips\n",
    ".\\activate\n",
    "```\n",
    "\n",
    "Se você observar o seu terminal, você verá que o nome do seu ambiente virtual aparecerá antes do seu nome de usuário.\n",
    "(Ex: (nome_do_seu_ambiente) C:\\Users\\seu_nome_de_usuario>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "Bem agora que voçê já sabe o que é um ambiente virtual, vamos instalar as bibliotecas necessárias para o nosso projeto.\n",
    "\n",
    "Para isso muitas vezes usamos o arquivo `requirements.txt` que contém todas as bibliotecas necessárias para o nosso projeto.\n",
    "\n",
    "Obs: É importante que na construão do projeto a medida que você for instalando as bibliotecas, você vá atualizando o arquivo `requirements.txt`.\n",
    "\n",
    "Para instalar as bibliotecas necessárias para o nosso projeto, você pode usar o comando abaixo:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai (from -r requirements.txt (line 1))\n",
      "  Using cached langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 2))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting langchain-core<0.3,>=0.2.2 (from langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached langchain_core-0.2.7-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting openai<2.0.0,>=1.26.0 (from langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached openai-1.34.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached langsmith-0.1.77-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\talinho\\desktop\\nero\\nero-summer-24-2\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1)) (24.1)\n",
      "Collecting pydantic<3,>=1 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Downloading tenacity-8.4.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached orjson-3.10.5-cp312-none-win_amd64.whl.metadata (50 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached pydantic_core-2.18.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 1))\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\talinho\\desktop\\nero\\nero-summer-24-2\\.venv\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.26.0->langchain_openai->-r requirements.txt (line 1)) (0.4.6)\n",
      "Using cached langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached langchain_core-0.2.7-py3-none-any.whl (315 kB)\n",
      "Using cached openai-1.34.0-py3-none-any.whl (325 kB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.1.77-py3-none-any.whl (125 kB)\n",
      "Using cached pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Using cached regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tenacity-8.4.1-py3-none-any.whl (27 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached orjson-3.10.5-cp312-none-win_amd64.whl (141 kB)\n",
      "Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.4 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 112.6/121.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 121.4/121.4 kB 1.8 MB/s eta 0:00:00\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, tenacity, sniffio, regex, PyYAML, python-dotenv, orjson, jsonpointer, idna, h11, distro, charset-normalizer, certifi, annotated-types, requests, pydantic-core, jsonpatch, httpcore, anyio, tiktoken, pydantic, httpx, openai, langsmith, langchain-core, langchain_openai\n",
      "Successfully installed PyYAML-6.0.1 annotated-types-0.7.0 anyio-4.4.0 certifi-2024.6.2 charset-normalizer-3.3.2 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.7 langchain_openai-0.1.8 langsmith-0.1.77 openai-1.34.0 orjson-3.10.5 pydantic-2.7.4 pydantic-core-2.18.4 python-dotenv-1.0.1 regex-2024.5.15 requests-2.32.3 sniffio-1.3.1 tenacity-8.4.1 tiktoken-0.7.0 tqdm-4.66.4 typing-extensions-4.12.2 urllib3-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "\n",
    "Para você conseguir acessar a API da OpenAI, você precisa de um token. Para conseguir um token, você precisa se cadastrar no site da OpenAI e solicitar um token, mas provavelmente o grade mestre senhor kaiô ja lhe passou seu token no privado.\n",
    "\n",
    "### Próximos passos\n",
    "Crie um arquivo `.env` e coloque o seu token lá. \n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=seu_token\n",
    "```\n",
    "\n",
    "Agora que você já tem o seu ambiente virtual, já instalou as bibliotecas necessárias e já tem o seu token, você já está pronto para começar a codar... ou não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agora vamos ver como o modelo fuciona via API\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deu errado? Não esqueceu de dar pip install nessa nova biblioteca que você quer utilizar?\n",
    "\n",
    "Já atualizou o requirements.txt hoje irmão?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mais sobre ChatOpenAI, você pode acessar a documentação da OpenAI [aqui](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos fazer uma pergunta\n",
    "question = \"Qual é a capital do Brasil?\"\n",
    "answer = llm.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos orientar resposta do modelo com um modelo de prompt \n",
    "# Esse modelo de prompt é uma string que é usada para orientar a resposta do modelo\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Voçê é um ajudante de aluno e está respondendo perguntas de geografia.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cadeia de comandos\n",
    "Chains of commands, ou cadeia de comandos, é uma sequência de comandos que são executados em sequência.\n",
    "\n",
    "Vamos combinar os comandos anteriores no que chamamos de chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos invocá-lo e fazer a mesma pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"Qual é a capital do Brasil?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída de um ChatModel (e, portanto, desta cadeia) é uma mensagem. No entanto, muitas vezes é muito mais conveniente trabalhar com strings. \n",
    "\n",
    "Vamos adicionar um analisador de saída simples para converter a mensagem de chat em uma string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"Qual é a capital do Brasil?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem, essa é uma aplicação simples de cadeia de comandos. É possível criar cadeias de comandos mais complexas, \n",
    "\n",
    "com várias entradas e saídas, e até mesmo cadeias de comandos que chamam outras cadeias de comandos.\n",
    "\n",
    "Se aventure e crie suas chains pequeno garfanhoto.\n",
    "\n",
    "mais informações sobre chains [aqui](https://python.langchain.com/v0.1/docs/expression_language/cookbook/multiple_chains/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plus \n",
    "E se você quiser streamar a resposta do seu chatbot?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos usar o mesmo prompt para fazer outra pergunta e a resposta queremos que seja stream\n",
    "async for msg in chain.astream({\"input\": \"Qual é a capital do Brasil? Fale brevemente sobre a história do Brasil.\"}):\n",
    "    print(msg, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Então quer dizer que você já está streamando a resposta do seu chatbot?\n",
    "Já pode começar a ir pensando em como você aplicar isso em um projeto real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para melhorar ainda mais\n",
    "\n",
    "Procure sobre LCEL (Language Chain Expression Language) e veja como você pode criar expressões mais complexas para a sua aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
