{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from operator import itemgetter \n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vdb_and_retriever(path=\"./nlp_cs_theory\",\n",
    "                           k=4):\n",
    "    embedding_size = 1536\n",
    "    embedding_model = \"text-embedding-3-small\"\n",
    "    embeddings = OpenAIEmbeddings(model=embedding_model, dimensions=embedding_size)\n",
    "    \n",
    "    vdb = FAISS.load_local(path, \n",
    "                           embeddings, \n",
    "                           allow_dangerous_deserialization=True)\n",
    "    \n",
    "    retriever = vdb.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    return vdb, retriever\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb, retriever = load_vdb_and_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"[Music] in this segment we're going to come back to the task of part of speech tagging now armed with hmm and see how they can actually do on real data and kind of compare them to other approaches so to start off with we're going to talk about the version of uh hmm that actually works well for part of speech tagging so what we've talked so far is what I'm going to call a quotee unquote byr hmm where the states here are just single tags now recall that hmm make this marov assumption which is that the next step depends only on uh the current state and not anything in the past and it turns out that this is a little bit too aggressive of an independence assumption instead what we want to do is we want to be able to condition on stuff that's a little bit farther in the past but kind of modifying the actual hmm uh model and and and algorithm is a little bit uh annoying so instead the way we can do this is we can think about changing our state space to actually be a pair of tags consisting of the current tag and the previous tag so corresponding to this fed raises example above now the state for y1 is going to capture this uh this uh kind of bracket s start of sentence symbol and for Y 2 it's going to remember that an nnp was the previous symbol here so these transition probabilities are now these more complicated things that look at uh you know the look at basically transitioning to a pair of tags from a pair of tags but one of these tags is always shared right like if it's nnp in the kind of second position at the current time step then at the next time step it you know it has to have nnp in the history here so the transitions become more complicated and there's certain optimizations that you might want to think about when uh actually implementing this because only some of these transitions are legal now but uh from the perspective of the rest of the algorithm you know even though we have this more complicated State uh everything else can kind of proceed as before in particular the emissions now condition on both of like on on kind of both of these tags you know the tag and and it's kind of History um and that's useful so we could go farther we could try to incorporate more of a history it turns out trigrams are kind of The Sweet Spot for hmm okay so taking our existing tagger um we can apply this to the pent Tree Bank uh data set which uh is one of the earliest data sets that was created for large scale English part of speech tagging now there are part of speech data sets for a lot of languages now we'll we'll come back to those a little bit at the end of this segment um but the pen treebank data set has 44 tags in it it turns out the Baseline of just assigning each word its most frequent tag seen in the training data actually does very well on the test set it gets around 90% accuracy and the reason is I I think I mentioned before that we have tags for things like period in there and then there's plenty of words like the and a determiners that never show up with any other tag so you're always going to get these right they're just totally unambiguous regardless there are still a number of ambiguities right I mean 10% uh kind of 90% accuracy means that one in every 10 words is wrong and so what that means is if we imagine a 20-word sentence which is a fairly standard length sentence in a news news article we're going to have about two errors in that sentence is what we' expect so our trigram hmm can cut that error rate in half it can take the accuracy to 95% and in particular it can get 55% accuracy on words that don't show up in the training set now these are hard words because uh you know recall that for this Baseline uh it actually wouldn't be able to say anything about those words right but the trigram hmm knows about the context and so even though the emissions uh aren't useful for determining the tag of that word it still can do something reasonable so uh a kind of souped up version of this system due to Taren Brands uh gets you know slightly higher accuracy overall but much higher accuracy on unknown words um Christina tanova and Chris Manning uh created a maximum entropy tagger so again maximum entropy is kind of a synonym for logistic regression and this is going to be more or less doable once we see crfs we're going to understand we're going to look at named entity recognition but it's going to be more or less the same kind of system as they build here um and that does even a bit better and this is actually not too far behind the state-of-the-art if you do kind of crazy stuff with uh you know bstm crfs or Bert you actually don't even get that much higher than this um you get around 97.5% and you get higher performance on unknown words so let's take a look at what some of the errors in these systems look like so this is from the tanova and Manning uh work and what they have here is a confusion Matrix uh showing basically uh which pairs of tags tend to get confused between Golds and predictions um so for example what this cell up here means is that the correct tag was JJ adjective so that's what's on the um you know the kind of left side here and then the column means the erroneous tag that was predicted so in this this case nouns uh and the kind of the the reason that adjectives and nouns can get mixed up is cases like official knowledge um where official can either be a uh an an adjective meaning that uh well it can be an adjective or it can be a noun like for example a government official and you might think okay why why would you ever have a noun in this context well there's actually a lot of noun noun compounds that have a ilar structure for example tax cut Art Gallery um tax and art can't be adjectives um tax can be a verb but uh you know in these cases you would want noun noun as the analysis but here it's the the knowledge has the property of being official it's not like associated with like a government official or something here's another uh example so this is a verb particle error like made up the story uh what is happening here is that up again is not a preposition uh it's a particle and made up is kind of this unit that uh kind of goes together and up does not really have the the spatial semantics that it normally does When It's associated with uh being a preposition and so uh in this case it gets analyzed as a particle but of course uh it's fairly challenging for a model to know that uh and then a final case here is recently sold shares uh there kind of without seeing more context you actually don't know which of these two tags it is um if I said I recently sold shares that's vbd that's uh just the straight up past tense of the verb um but if I said you know he has 50 recently sold shares that means these shares were recently sold we don't know by who and that's a participial usage of sold so again those those so you can see that these errors can get kind of tricky here and so Chris Manning in in 2011 um wrote a paper called part of speech tagging from 97% to 100% is it time for some Linguistics and looked at the remaining errors uh in part of speech data so one of the one of the tricky cases that we've talked about is when you have a word that uh words that aren't seen but another tricky case is where you have a word that has been seen but it shows up with a new tag in the test set right and this is very hard to generalize to because generally you're going to assume that okay we've seen this word before and we've seen it as a verb you know most likely it is going to be a verb so this this accounts accounts for around 5% of the errors um unknown words are also around 5% uh in his analysis around 15% of the things were were uh models were errors that models could get right maybe you need a better model but it's at least doable 20% he said had quote unquote difficult Linguistics and so here's an example of that they set up absurd situations detached from reality uh the tags here differ whether on whether set is past tense or present tense and you don't know without seeing more of the context whether this happened in the past or the present and so uh you know we would have to think about models that go beyond a single sentence that's much tougher and so you know we're not going to do that the and then what he found was that 60% of the remaining errors were things that were either underspecified or unclear in the text or the gold standard was inconsistent or wrong um and so here's an example of the of the kind of unclear case uh a $10 million fourth quarter charge against discontinued operations um this is a case of a participial usage versus an adjective do the operations have the property of being discontinued or are we saying that like these operations were discontinued it's it's almost difficult to describe what the semantic difference between these two cases is because in order for something to be discontinued now it had to have like been discontinued sometime in the past right so this is the kind of ambiguity that you writing this or someone reading it may not even resolve for themselves and so uh how can we expect a model to necessarily come to a decision about one of these tags to use here okay so I'm going to switch gears a little bit and talk about uh a different way of producing part of speech tags which is coming back to using neural Nets here and in particular feed forward neural networks so rather than appeal to part of spe these like hmm taggers we can instead go back to this idea of using something that looks more like a classifier we can take uh these words around uh the word that we're tagging here so we look at the previous word the current word the next word we embed them into vectors we can catenate those into a feature Vector here and then we're going to put that into a neural network uh and so again what we're doing here is we're being very careful to capture this positional position sensitive information we're looking at the previous word the current word and the next word we're not using some kind of bag of words thing or that deep averaging Network where you like add everything together here we're concatenating these three vectors uh and so Yan Batha had all looked at this for a number of NLP tasks and they did something a little bit more clever which was they also uh looked at uh Byram and character byrs and trigrams and had a way of incorporating those into the model as well um and then that all gets kind of mixed together in a hidden layer and then uh the model makes some sort of prediction and this actually turns out to work pretty well across a whole range of languages so they compared it to previous work that was doing recurrent neural networks and found that it was it was better than that work and uh you can see on the right here that uh across a whole bunch of different languages the accuracies are you know at least above 90% in all cases um again the kind of you know the error rates differ a bit and uh this isn't comparable to the other data that we were seeing because it's a it's a new data set in a different part of speech tag set but uh the kind of General principle here is that these kind of models that we've built up so far can be used for lots of different languages and uh you know com kind of using this feed forward neural network version of them can be fairly effective so that gives you a sense of what a few models for doing uh part of speech tagging are that work basically as well as the fancier stuff that we're going to come to later in the course uh things like recurrent neural networks in Bert so uh you know now you kind of understand where the state-of-the-art is in this area and what some of the strong models are doing that's the end of this segment\", metadata={'source': 'wijpAX_LLXo'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to introduce the concept of part of speech tagging so a lot of what we've talked about so far has been uh things like bag of words techniques and particularly for various classification problems so this is a you know fairly effective way to do a lot of classification if we want to uh you know if if we want to predict things like sentiment and do so with uh end end neural networks but if we want to start to build up towards more structured linguistic abstractions we need to start thinking about how to represent uh basically how to represent what words in language mean and how they function in a slightly deeper way and so part of speech tagging is going to be our first step towards doing this we're going to look at language as kind of the sequence of uh syntactic abstractions over the words and see what that allows us to do so the reason we might want to do this is the following let's say we have this word and we want to feed this into a TTS system now what should TTS say so there's two ways to pronounce this it's either record or record um and we can thank the English language for being weird in this regard uh but basically in order to determine this we need to know whether this is a noun or a verb right are we recording something or are we putting on a record to listen to so another another thing we might uh kind of be interested in is let's say we're doing some kind of information extraction um and we're going to take the example of arms so is this a noun or a verb are we talking about arming people or are we talking about you know either our arms or weapons or something like that and so the you know obviously there's kind of deeper questions that we want to be able to answer here for example if we do have a noun form of arms what kind of arms are we talking about but this is a first step towards uh at least getting some idea as to uh what's going on you know deeper than the surface structure deeper than just the sequence of uh characters associated with each of these uh associated with each of these words and the other the the last thing that this is going to really be is it's going to be uh a kind of first stage towards uh thinking about syntactic parsing and parsing is going to let us take large and complicated sentences and uh you know turn them and and basically start to unpack their structure and so this is going to be a building block towards allowing us to do that okay so we have to think about uh what the part of speech tags are and and what they mean and we're going to kind of go through a crash course in that so that we understand the kind of linguistic abstractions we're dealing with going forward so there are going to be two rough categories here um we are going to have what are called open class tags and roughly these are ones where you know you can imagine like you know new words uh can join these categories um and so you know an example of an Open Class uh part of speech tag are the nouns uh so we have both proper nouns like you know IBM Italy and also what we call common nouns um than things like cat or snow and so the reason this is open class is because we're inventing new devices all the time right you know computers didn't exist and then they did um you know sort of new proper nouns associated with companies and things like that there's always new words joining this class um verbs this is another uh another one uh adjectives uh and adverbs um you know things like swiftly that are that are kind of modifying verbs here all right so in terms of Clos class tags we have uh a few more categories that are typically related more to kind of function words and uh these you might be less familiar with so the first one we have here are determiners um which are going to be the some and and other words like this so this includes articles and basically uh these are words that uh can modify nouns as part of noun phrases and so what they allow us to do is say you know the cat versus a cat the choice of article or determiner there is going to change how we uh interpret that noun those each of those noun phrases um whether we think that we're talking about some generic cat or you know a cat that's already in the discourse or something else conjunctions are another one and or um pronouns um and then we have a couple of verbal categories here like auxiliaries um for example had when it's followed by a verb so um you know just saying like I had three apples uh is not an auxiliary but I had gone to the store when blah blah blah that that is an example of it as an auxiliary um and then also modals which uh you know are words like could like I could have gone to the store that implies uh a certain modality of the uh you know about the statement you know your ability to do it in this case so uh these are both uh these are both sort of types of verbs that typically get tagged in a different um in in a different fashion and uh you know they're closed class because we're not we're not always coming up with uh new constructions basically surrounding uh different types of modality or or different ways of expressing tens and and things like that uh and then the last two uh list here are prepositions um things like up into um so for example when you say like hike up a mountain you know that's a preposition there's also a notion of what are called particles um which overlap heavily with the prepositions um but these are going to be uh used in a slightly different way so for example when we say we I I made up the story um this is what's called a verb particle construction and you there's sort of like one way to think about it is there's not really a kind of spatial aspect associated with making up a story right um it's really this uh this word that's kind of combining with the verb in order to get its semantics and uh so in in contrast with the preposition it's it's behaving a little bit differently but the the words that actually get used to overlap there and so this is an example of why things are kind of ambiguous from a part of speech perspective Beyond things we've already seen before okay so these are examples this is kind of a crash course in some of the basic uh syntactic uh part of speech categories that we're going to see now let's look at an example and this is example is going to help us think about what kinds of ambiguities show up and why this task of trying to take a sentence and analyzing what part of speech uh each word falls into might be difficult for uh an automatic system so fed raises interest rates 0.5% so a lot of the examples a lot of the early uh work on uh syntax and NLP for English was done on this data set called the pent Tree Bank taken from the Wall Street Journal so there's a lot of uh a lot of kind of financial text in here so what you could do is take a minute and think about what are the different Poss possible part of speech tags that might be associated with each of the words in this sentence so if you if you look at it and kind of think about each word in isolation you should be able to come up with uh a kind of set of possibilities and so I encourage you to do that you know but now I'm going to tell you what uh what they are so I'm going to use part of speech tags that correspond to the categories on the previous slide but are actually a little bit more refined these are the actual um categories in the pen Tree Bank data that you uh are going to see some throughout this course you're not expected to have uh kind of encyclopedic knowledge of these I'll kind of Define them as we go so fed the you know in the canonical way that we're interpreting the sentences the the sentence um you know the uh you know we are we are using this as a proper noun uh so that that gets the tag nnp um two other possibilities are two different types of verbs so what is the difference between these things um so when I say like I fed someone something that's uh vbd that's just like past tense if you said I had fed there we get this participial usage of it and uh it's vbn in that case Okay so actually for that word which you know the first time I showed you the sentence you probably had no trouble understanding there's actually three different tags uh and three different interpretations for it all right raises um this is a PL this is a plural noun um that's what nns means um but the other possibility is it can be a verb right um like he you know raises up the his hand interest uh again this can be a noun um or two different types of verbs for example uh I interest you in NLP hopefully that's a vbp um or I want to interest you and this is the this is the infinitive form of the verb um so that's that's just this this this kind of bare VB here all right rates again plural noun or verb uh and then 0 five in perc uh each only have one interpretation okay but even setting aside all of the kind of crazy tags that can't show up in the sentence we actually get a whole you know pretty large number of different interpretations right um so the number of paths through this sentence if we think about it uh is 3 * 2 * 3 * 2 which is 36 so we're going to need a model that can look at all of these 36 paths and decide what is actually reasonable now another thing I want you to think about is what are the reasonable paths to this sentence so which one corresponds to the kind of correct intended interpretation and are there others which correspond to some interpretation that that might be true even if it's a little bit weird so the correct interpretation is this one the FED raises uh where the FED fed is a noun raises is a verb and then the interest rates uh is this noun noun compound um now notice you you you know we're often going to see cases where like two nouns get like jammed together into a noun phrase that's okay it doesn't interest is not an adjective in this case um it's okay to have two nouns kind of combining in this way all right there is another interpretation though which I want you to see if you can spot another interpretation that you can get for this sentence is the following so roughly what we have here is we have an idea of fed raises as both nouns and then interest becomes the verb and then rates and and uh you know rates is a noun again so uh you know thinking about this sentence for example um what we what we sort of have going on here is we have this idea of fed raises and they are interesting to the rates they are causing rate to be interested rates in this case are some sort of animate thing that can express interest um and they're they're interesting them a little bit they're they're interesting them half a percent okay so this is a little bit kind of nonsensical here um you know it it sort of doesn't we can't really think of why someone would say this but uh you know if you stretch your brain a little bit this you know the sort of an Alison Wonderland way where there's rates that are kind of running around like you can contort yourself to believe in this interpretation and in particular this is going to be something that's very easy for a part of speech tagger or later syntactic parsers to produce because you know we're really using our our kind of World Knowledge about the fact that like rates are inanimate and so blah blah blah but like syntactically this is completely valid so this kind of illustrates two things here one is it gives you some practice thinking about what part of speech tags we have here how they interact Etc um and it also shows you an example of the kinds of ambiguity that we're going to need to be dealing with and how challenging it is to to part of speech tag even a relatively simple sentence like this one that's the end of this segment\", metadata={'source': 'Llw6qfeAWDs'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about hidden markov models we're just going to set up the basic definitions and see how this model is defined what its parameters are and what it can do so hidden markov models are a generative sequence model that is going to allow us to uh represent uh tasks like well particularly tagging tasks like part of speech tagging in a nice uh kind of modeling framework so uh first we have uh so we we have we have basically two types of objects here um we have uh tags y i which we're gonna say are in this uh this tag set and we have words x i which are in a vocabulary all right so the definition of a hidden markov model is the following well this is this is kind of one way well this is there there are a couple of slight things that that might differ between definitions but uh this is basically the the kind of standard form of these okay so what's going on here so let's look at this graphically uh using uh bayes net notation so we start off by sampling a tag y1 and then based on that tag we sample a word x1 so when we decompose the probability in this particular way one one kind of term that's used for this is the generative story of the model and so the generative story of this model is saying that all right we have some distribution over tags that can start a sentence y1 and then condition on that tag we generate a word which is going to be the first word of the sentence and then we generate a second tag y2 and then the second word x2 conditioned only on that tag y2 and then we kind of continue until we have y n which gives x n and then stop okay so this is a this is a model that makes uh certain independence assumptions and essentially the reason it's called a hidden markov model um is because the y's form a markov process meaning y i depends uh or i'll say this formally is conditionally independent of y1 through y i minus 2 given y i minus 1. so it's not right to say that it's completely independent of the uh the kind of earlier states in the process because the earlier states in the process influence what y i minus 1 is but if we know what y i minus 1 is then kind of that fully determines the distribution over y i and so the kind of idea here is that the part of speech tags themselves are giving us this sort of syntactic skeleton of a sentence right like okay you're going to have a maybe if you have a determiner and then you're going to have a noun and then maybe there's some chance of having a verb after that right and then after a verb doesn't matter that there was a noun before that you know we're probably going to have a noun afterwards or maybe a preposition and then after a preposition okay now we're going to get the object of that preposition uh etc and then the other assumption here is that the words are kind of conditionally independent of each other given the tags so x2 doesn't depend on x1 but instead is only going to depend on y2 so this is not necessarily a good assumption because if we have x1 and x2 as uh for example new york the fact that we have you know a kind of proper noun phrase or maybe san francisco is a slightly better example you know given that we see san there's only you know not that many words in english that are going to come after that and uh but but right now we're just saying okay we have a we have a proper noun and then a proper noun and you know san francisco you know that's possibility that's a possibility um but san york or new francisco these are also possibilities so these are assumptions that the model is making um and the reason you make these kinds of assumptions in generative models is for uh is for tractability basically um so that you're not having to deal with um you know conditioning and thinking about a whole bunch of different variables at the same time okay so what we also want to think about are the parameters of this model so we have p of y1 which is called the initial distribution and so i'm just going to draw that like this we're going to call that this s and this is a this is a t length vector so it's going to it is a probability distribution so unlike a lot of the other models we've seen like bag of words models where the parameters can be any real valued uh or any real number here the parameters have to be real numbers between 0 and 1 that sum to 1 here right because they're probabilities and this is a distribution over what the possible starting tags are so then we have uh these y i given y i minus one terms uh these this is these are called transitions and so the way that's going to be represented is by t um so we're going to use the kind of square t here for the transition matrix um and the you know tau or squiggly t for the uh the set of tags and we could think of that as a t by t matrix here where uh essentially the value of a particular cell is telling us uh what is the probability of why i give it of uh let's say tag uh well actually let me change this a little bit um i'm going to write uh y cur and y next um so it's going to tell us what is the probability of the y next tag given the y-curve tag and so these transitions are independent of the positions in the sentence so there's there's no dependence on i or anything there it is just a matrix of these probabilities defined over um every pair of tags all right and then we have these p x i given y i terms and these are called emissions and the parameters here i'm going to draw a long kind of long matrix here uh we'll call it y cur and x curve and we're going to call this thing e and is going to be t by v again a matrix and again uh it's it's just encoding the probability of given tag say nn um you know i'll draw kind of a slice here it says okay given tag nnn what is the probability of seeing each word in the vocabulary and so we do make an assumption that there is a fixed vocabulary here we're placing a distribution over words in that vocabulary and so given tag nn we need each word to have some probability and those probabilities need to sum to one okay so there's two steps that we need to take in order to use this model the first is parameter estimation you know given data how do we get values for these parameters and the second is inference so given this model uh how do we actually use it to part of speech tag a sentence and so these are the two topics that we're going to talk about next and that's the end of the segment you\", metadata={'source': 'FeLtLLbn4qU'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about how to approach the problem of part of speech tagging we're going to define it as a sequence labeling problem and we're going to talk about how to use basic classifiers like we've seen so far in order to do this so the idea behind sequence labeling is that we have an input uh which we're going to represent as x here and we're going to think of this as a sequence of what in our case is going to be words x1 through xn so previously when we thought about uh you know when we thought about extracting features and things like that uh you know we didn't necessarily think about the words as the sequentially structured object now we're going to think about them in this way and our output y is going to be the same length as the input now so previously we always thought about why as some kind of uh binary or multi-class prediction right some single discrete label associated with a document or sentence or whatever now we're going to think about it as one prediction per per word and so this is an example of what's called structured classification and the reason it's structured is because these y's are not somehow just like a bag of independent predictions but they actually have this sequence associated with them where like y2 comes after y1 and comes you know a certain number of positions before yn all right so what we could try to do i think is instruct it's instructive to think about how to do this with the current tools that we've built up so far so uh so let's try to predict each yi independently with logistic regression so that's going to be something that looks like this now uh over on the left here we have y i equals y so we're going to use y to reflect the kind of uh i guess part of speech tag or whatever that we're thinking about predicting here um and we are going to need to uh and and we're thinking about assigning that to uh position y i and notice we're conditioning on two things on the right side we're conditioning both on the input sequence and that was how we kind of normally did it in logistic regression and also on this index i um and here's why that's going to be important so previously we had bag of words features and we said okay you know when we extract features like f x uh uh actually i'm gonna use the um the different features notation here um this is because this is really where that stuff kind of comes into play here um you know if we are thinking about assigning a uh label y to a you know to a an example that we're gonna associate with a particular bag of words feature set well the the kind of basic bag of words features might look something like you know this where you have you know blah blah blah where you have a one for fed a one for raises um you know and then other uh you know other kind of ones in there as well all right so you know let's say we want to make this uh prediction here we want to say okay what's the probability that uh you know and we're gonna think about let's say i equals three um so we're gonna think about uh kind of making a prediction associated with the third word and you know maybe the tag we're going to think about here is nn then what we get remember is this kind of block structured thing where we have this f of x vector and we have a bunch of different copies of it and we're going to get the ones associated with the nn tag and then we're going to have 0s elsewhere there and we're going to have zeros zeros kind of everywhere else okay the big problem from our perspective is that this is independent of i so this model is go or or rather this set of extracted features just looks at the sentence and the part of speech tag it doesn't look at the word that we're actually trying to tag here and so that's kind of a problem from the perspective of tagging right in that we're not making use of that information we're just going to predict like the same tag distribution everywhere and this is generally not going to work as a tagging model instead what we need is a kind of positional view of our features we need to look at x we need to look at the fact that the label that we're thinking about is nn and we need to think about the position so uh one way to do this one very simple way is to just have a single feature on the current word here let me let me write the example up here fed raises interest rates 0.5 percent so when i say a single feature on the current word what i mean is that uh we're going to have a vector where uh you know again we're going to have all zeros except in this nn segment so zero's everywhere else and we're just gonna have a one associated with interest and everything else is going to be zeros all right so basically what we've done is we've taken this back we've taken our bag of words and sort of masked it out right um we've said okay we're only going to look at the current word and so this is at least a little bit better right in that we can now say okay we're predicting a part of speech associated with interest and so uh you know we're at least gonna get the uh we're at least gonna have some notion of what the word we're currently looking at is and what and what part of speech it might have we could do better than this though um and here's what that's going to look like all right so what we have is we have our big uh block structured feature vector where we've got you know the vbz tag over here and then we're going to leave ourselves plenty of room for the nn tag and then we have all the other tags right okay and then within the nn tag what we could do is we could say okay we're going to have one set of positions that correspond to the current word and then we have one set of positions that correspond to the previous word so in this case the indicator here would be about raises and then we can have the next word as well and this word is rates so the way we can think about each position in this vector space is as a conjunction of several properties uh and we we call this an indicator and in this case it's an indicator that the current word you know which is defined with respect to this position i equals three right current word equals interest and the tag equals nn here all right and so the the tag information remember came from uh you know came from kind of where we are in this big block structure thing the current word bit came from the smaller block structure within that and then um interest was associated with this particular position in the vector space so it turns out we can we can avoid having to deal with this whole big you know block kind of block feature vector idea just by saying okay we are going to treat this as a word in a bag of words space essentially we think about our feature space now as talking about properties of uh the classification instance and in our conventional bag of words the property was just is this word included or in some cases how many times was this word included now we're thinking about this sort of more complex set of properties in this case this position has the associated property ker word equals interest and tag equals nn remember the feature function you know depends on y in this case um and so that kind of word lights up and uh gives us a one in this whole big feature space so we don't have to think we don't have to actually think about kind of managing this block structure we can again just kind of sort of throw these things in a big index and uh access them later okay so if we take uh so we could take these indicators and feed them into a classifier and so we can we can basically take a labeled data set of part of speech instances treat each position in every sentence as a example and feed that and train on that and produce a classifier all right so what goes wrong the the kind of problem is that we're not making use of the output structure at all so for example we had different possibilities for raises and interest rates right and it turns out that some of the edges that we kind of considered here are not good like for example we're not typically going to have a plural noun followed by a noun we're also not typically going to have a vbz followed by a vp of vbp and so the the predictions of a classifier may be incoherent uh meaning kind of locally each prediction looks reasonable right but the overall structure doesn't kind of add up to be the thing that we want and so this is going to lead us into our idea of sequence modeling and we're going to think about two models hidden markov models and conditional random fields these are going to be two things that we're going to spend some time unpacking throughout the next section of the class and essentially these are going to be two kind of contrasting ideas for how to deal with this hidden markov models are going to be our first example of a generative model and the they're gonna look a little bit different from other things we've done but they have some attractive properties uh conditional random fields are gonna look a little bit more like building off of this classifier view of uh tagging but uh making the output predictions coherent that's the end of this segment you\", metadata={'source': 'yQZ0mDW-U3g'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"How does PoS Tagging work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\".join([x.page_content for x in docs])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You're an AI assistant that will answer questions about natural language processing and computer science theory.\n",
    "\n",
    "Besides, here is some extra content about natural language processing and/or computer science theory:\n",
    "\n",
    "[extra content]\n",
    "{extra_content}\n",
    "[End of extra content]\n",
    "\n",
    "--------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"input\": RunnablePassthrough(),\n",
    "        \"extra_content\": retriever | RunnableLambda(format_docs)\n",
    "    }\n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"What is PoS Tagging?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of Speech (PoS) tagging, also known as PoS annotation or PoS labeling, is a fundamental task in natural language processing (NLP) that involves assigning a part of speech tag to each word in a sentence, indicating the word's syntactic category and grammatical function within the sentence. PoS tags can include categories like nouns, verbs, adjectives, adverbs, pronouns, prepositions, conjunctions, and more.\n",
      "\n",
      "PoS tagging is essential for various NLP tasks such as text analysis, information extraction, machine translation, and syntactic parsing. It helps in understanding the structure of sentences, disambiguating word meanings, and enabling machines to process and analyze human language more effectively.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16722 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Big O notation?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2504\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2502\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2503\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2504\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    169\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    460\u001b[0m ]\n\u001b[0;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m         )\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:537\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    536\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 537\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    605\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 16722 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"What is Big O notation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": \"Explain NP Hardness problems\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP-hardness refers to a classification of computational problems in complexity theory. A problem is considered NP-hard if it is at least as hard as the hardest problems in the complexity class NP (nondeterministic polynomial time). In other words, if a problem is NP-hard, it means that it is at least as difficult as any problem in NP.\n",
      "\n",
      "Here are some key points about NP-hardness problems:\n",
      "\n",
      "1. **Definition**: A problem is NP-hard if every problem in NP can be reduced to it in polynomial time. This means that if you can solve an NP-hard problem in polynomial time, you can solve any problem in NP in polynomial time.\n",
      "\n",
      "2. **Complexity Class NP**: NP stands for nondeterministic polynomial time. It includes problems for which a proposed solution can be verified in polynomial time but not necessarily found in polynomial time.\n",
      "\n",
      "3. **NP-Completeness**: Some problems in NP are so hard that they are both NP-hard and in NP. These problems are called NP-complete. If you can solve an NP-complete problem in polynomial time, you can solve all problems in NP in polynomial time.\n",
      "\n",
      "4. **Reduction**: NP-hardness is often established through reduction, where you show that if you can solve problem A in polynomial time, you can also solve problem B in polynomial time.\n",
      "\n",
      "5. **Implications**: NP-hard problems are considered computationally intractable, meaning that there is no known efficient algorithm to solve them. They are crucial in understanding the limits of computation and the classification of problems based on their complexity.\n",
      "\n",
      "In summary, NP-hardness problems are among the most challenging computational problems, and they play a significant role in complexity theory and algorithm design. They help us understand the boundaries of what can be efficiently computed and provide insights into the inherent difficulty of certain computational tasks.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"one famous hardest assumption pitas not equal NP but there's more to life than just that so I want to tell you about some more opportunities you have I think we know it's super hard in complexity theory to prove any impossibility results for algorithms I mean we basically cannot do it at all I mean the one thing that we're great at as researchers is developing algorithms and the one thing we're bad at is proving our rules do not exist efficient algorithms for problems you don't exist so that's why we have assumptions so that we can make progress on the subject of negative results so uh let's get into it when you want to prove hardness results for a certain computational problem way it works this is usually a combination of two things you got a hardness assumption and a reduction and reductions are just algorithms that's kind of nice I mean it reduces in a way the task of proving hardness do you know making let's say one or a limited number of assumptions and then designing algorithms which is the thing that we're good at so in as I've you know mention probably before in this class a lot of complexity theory is actually just algorithm design let's take a look at this here's maybe the most famous or classic kind of example we may take as an assumption that you cannot solve the 3sat problem in polynomial time and as you all learn in an introductory si s Theory course you can deduce from this that let's say the Hamiltonian path problem can also not be solved in polynomial time this is basically just you know the NP completeness of Hamiltonian path but this deduction is by virtue of a reduction algorithm so just to remind you this reduction algorithm are that you designed to prove this result you know it takes us input a 3-cnf formula Phi and its output is a graph G so that's the algorithm and then the correctness property that you prove about the algorithm looks like this you prove that if Phi is the satisfiable 3-cnf formula then the GE output by your reduction has a Hamiltonian path and if Phi is unsatisfiable and G has no Hamiltonian path she use the lighter instead of what the highlighter the laser pointer right so this uh you know of course tells you that a few could solve a Hamiltonian path problem in polynomial time then by composing that algorithm with the reduction you would be able to solve 3sat in polynomial time if you're assuming it's impossible okay so that's an illustration of you know how you can start with a base assumption by designing a reduction algorithm get a new hardness results and in fact even this base assumption can self be thought of as the consequence of a reduction of this type namely you can take the weaker assumption that P does not equal NP which is to say that all the problems in NP there's at least one of them that's not solvable in polynomial time and the reduction that allows you to prove that this fiscal problem 3sat is not in polynomial time is this cook-levin theorem which itself is also just an algorithm for basically transforming a Turing machine into a circuit and thence to a 3sat 3-cnf formula okay so as I said the sort of the gold standard hardness assumption in CS theory since we cannot prove anything we have to do something is you know the assumption that P does not equal NP which as you all know thanks toach 11 theorem is equivalent to the assumption that 3sat doesn't have a polynomial time algorithm no this assumption has been around for I don't know 50 years and you know it's the baseline assumption that we're sort of always willing to make in order to get started and ideally you know if we you know can't prove anything well we can assume one thing and then try to deduce all the other facts about hardness about impossibility of designing efficient algorithms based on this one single assumption that would be the best thing we can do and sort of the thing that you're suggested is the thing that gets done in complexity theory in introductory courses but there are some downsides to just taking this one assumption about three side or about you know and P does not equal int does not equal P as your one and only assumption one is it's an assumption about worst-case hardness so it basically says you know for every candidate polynomial time algorithm there's at least one instance that it gets wrong but you know this is why it's called worst-case you know there may only be like one instance or like a some very small number of instance is a kinetic candidate algorithm could get wrong and what would be better is um if you could have like large classes of instances that were hard and in particular if you had like an efficient randomized algorithm for generating hard instances they might say like why do you say better like are we happier you know when we can solve problems why would we want hard problems well we sort of talked about this last time it's better for cryptography I mean given that there are seemingly are hard problems for efficient algorithms out there in the world we may as well put them to good use and base you know cool cryptographic primitives on them but for cryptography you want typically or you often need more than this worst-case hardness you need for cryptographic applications the ability to like efficiently generate hard puzzles like efficiently generate hard seeming instances of tasks so that's one downside to like just this P does not equal NP assumption and the other downside is it's a little bit vague on like how hard let's say this 3sat problem is I mean okay it conjectures that it's not in polynomial time I mean it is in 2 to the N time but you know it doesn't this conjecture myself does not say much about how hard 3sat is so you might be like ok it's not an opponent of time but could it be in n to the login time or it could it be in 2 to the Routan time or could it be in 1.3 to the end time in fact it is in 1.3 1 to the end time that's uh result of hurdle from 2011 and so you see that already something kind of interesting is going on I mean if you care about small values of n you know 1028 30 then you know these things can make a difference so these are the kinds of things I won't talk about in this lecture like maybe making different assumptions that allow us to get richer hardness results more precise hardness results and hardness results that are better for applications okay\", metadata={'source': 'bYfQrITGyUs'}),\n",
       " Document(page_content=\"I also want to mention an alternate version of it or a variation of it because this will lead us into another important hardness assumption which is the sparse ltn problem so in the sparse LP n problem you fix a very small constant K like maybe three or five and now when the learning algorithm asks for a noise equation you don't pick you know the the secret holder does not pick all a 1 through a and uniformly at random which would have the property that about half of the A's would be one and half the A's would be zero instead they just picked exactly K AI is to be 1 okay so basically you know if K is 3 and they pick three numbers between a 1 and n and they give you a noisy equation let's call those numbers i1 i2 i3 those are chosen randomly between 1 and n and then you learn you know you get an equation that looks like x i1 + x i2 + x i3 equals B mod 2 and you're given the correct right-hand side with probability 1 minus epsilon and the wrong right hand side was probability Epsilon so this definitely makes your task easier because you're getting like a lot more intuitively you're getting a lot more information about the secret string with each noisy equation and in fact let me make that even clearer let's say you're a polynomial time algorithm and we let you get em noisy equations that look like this these sparse equations first of all imagine a meze like noticeably bigger than n to the K so then you're in mind for this discussion fixed K to be 3 and say you're calling real time algorithm and you're allowed to take like way more than n cubed algorithm AGGA cubes equations like n to the 4th equations well there's only n to the K or so many possible left-hand sides like n choose K left-hand sides and choose 3 left-hand side so if you're a lot to take you know way more than n cubed samples you'll get like you'll see the same left-hand side like many many times like you'll see like x1 plus x2 plus x3 equals 1 or 0 like lots and lots of times and you'll be able to figure out what is the correct right hand side because you know like 1 minus Epsilon of the time when you see X 1 plus X 2 plus X 3 equals something you know you'll see the same value for that something be it zero or one and only epsilon at the time when you see the opposite value so you'll basically be able to get rid of the noise yourself and get back to the case of like non noisy equations with high probability and then you can solve them with Gaussian elimination so this shows that you can you can solve the problem if you're given sufficiently large polynomial number of equations like n to the K in fact this one is not as easy to see but it's known that you can even do this if you're allowed to take like n to the K over two samples okay this is you know not as easy this uses a semi definite programming algorithm um kind of based on work of Fagin Ofek and aqua mom so there's that but this is like the best thing that's known and so for example you know if you could fix K to be three and you might say well I'm not going to give you I'm not gonna let you take n to the 1.5 equation it's noisy equations I'm only gonna give you Big O of n equations then it could be hard so this is a possible assumption that if you're a polynomial time algorithm and I'm only giving you like order n equations which is information theoretically enough for you to solve the problem in exponential time as it turns out you can assume that there's no polynomial time algorithm that can solve this task even when K is 3 and in fact uh this test could actually even be way harder for example we don't know how to solve this task in time better than 2 to the N over log n like 2 to the little o of n over log N and uh we don't even know how to solve this problem if like you don't even have to find s you just have to sort of tell the difference like am I really getting like you know ninety-nine point nine nine percent accurate equations about a fixed string s or is you know the person just feeding me pure junk or by pure junk I mean imagine you take epsilon to be a half so that means like the right-hand side is just randomized so it's like you get completely meaningless equations so it's like actually an easier task could just tell the difference between are you getting completely meaningless equations are you actually getting like 99.99% accurate equations even this is not known to be doable and this is pretty cool because it's like this is an example of like a problem where it's really easy like in three lines of computer code you can generate hard seaming puzzles like you just write code that like picks a random string s it picks M equals order n three tuples of indices and you know it writes down the Associated true equation then it flips these right hand side with probably 1% and it produces a bunch of like noisy equations and then you know you give it off to an algorithm and say here's like you know 100 n noisy equations about a random string s each one involves only three variables in the left hand side please try to find s nobody knows how to do this in less less than 2 to the N over login time and you know it's like a five line piece of code for generating like a cool puzzle in fact a cool puzzle where you the code generator or the algorithm that generates this puzzle it knows the secret it picks the secret s itself so that's a very useful primitive and if this will let me transition to talking about worst case hard problems for a while so in fact the worst case version of this farce LP n with K equals three he's actually provably known to be hard well as always in this world we don't know how to prove anything as hard so when I say like provably hard I mean assuming our most basic fifty-year-old assumption and then we always take P does not equal NP so let me transition to talking about that but feel free to add any questions in the chat if you have them right so what's this worst-case version the theorem a problem so in 1999 host approved the following quite famous theorem it's about the NP hardness of a certain CSP task and the CSP is like the 3x or CSP and it's exactly this sort of thing that we've been talking about like there n unknown you know values x1 through xn and you're supposed to assign the value 0 1 mod 2 and each constraint is like a linear equation mod 2 that involves exactly 3 of them that's exactly what we were talking about on the last hand side last time but before we talked about you know picking the solution at random and like the right hand side would be like correct but with some noise and here's just a worst case problem and you're trying to find the solution and the assignment to the X is that satisfies as many of the equations as you can and hostess theorem is the following even if I give you an instance basically it's hard to tell the difference between like a 99% satisfiable Sol system and a merely 51% satisfiable system or like even if I give you a system where there is like a solution that satisfies 99% of the equations it's hard to find a solution satisfying 51% of the equations and that's it'd be hard in the worst case so it's sort of you know definitely hard assuming P does not equal NP and let me just remind you what that looks like I mean host on theorem can be viewed I mean if you put together the entire proof of hosted serum like maybe it's 200 pages but well 100 at the pages it's a very long and P completeness reduction and B hardness reduction like ultimately he creates a polynomial time reduction R takes his input like a CNF 3-cnf formula Phi and it outputs you know a system of equations I and he proves a zero in that like okay this reduction has the property that a Phi is satisfiable then there will be a solution to these equations that satisfies 99% of them and if I is unsatisfiable then every solution satisfies at most you know 51 percent of them just to remind you why there's you know two quantities like ninety nine percent and fifty one percent are relevant you know if you're given a system where a hundred percent of the equations can be satisfied by some assignment then there's a simple polynomial time algorithm that finds such a solution that's solving a system with satisfiable equations it's Gaussian elimination on the other hand there's always like a super simple algorithm that will satisfy 50% of the equations particular you can just look at the right-hand sides and if there's more ones than zeroes and use the assignment that sets all the variables to one and then whenever you have a right-hand side of one you satisfy the assignment and if there's more zeros and ones set all the variables to zero and you satisfy every equation with a zero on the right-hand side so one of these is at least 50% of the equations okay so that's how stars theorem and as I mentioned you know if you really dig into this reduction it's extremely long I mean a built on I don't know something like eight years worth of effort and complexity theory so it really it divides into like several steps like there's a reduction from 3sat to the problem of like slight inapproximability for three SATs we talked about this in lecture twenty if you want to go back and check that out and this first reduction is called the PCP theorem it's a famous theorem and in fact this we'll talk about it in the final lecture and then there's like another famous theorem called the parallel repetition theorem proved by Roz which gets you to hardness for a problem called label cover which we'll talk about later in this class and then Howe starts contribution was this last piece involves like a gadget reduction involves some very cool analysis of wooly functions and so this is you know one of the longer NP hardness results you'll ever hear about okay so you know even though there's like a tremendously long story from three side reducing all the way to post Don's theorem you know for like a practitioner if you're just a person that you know cares about understanding future problems it's really great to just know this theorem is a black box I mean forget how it's proven but just remember that this problem is np-hard because it's a great starting point for further reductions I mean you put that in your pocket this problem is np-hard and you put you know NP hardness reductions on top of it and this is a turns out to be like the great problem for deducing further results about hardness of approximately solving optimization problems for example here's just like three out of a very well a bunch out of a very huge number of results that are known of this flavor so starting from host less theorem and adding NP hardness reductions on top of it you can get the best-known and B hardness of approximation factors for max cut for two sat for the traveling salesperson problem in the metric case it's known that it's and be hard to approximate it to 1 plus 1 over 122 factor she's not too impressive but it's the best thing that's known you can see the best-known np-hardness approximation for a dense case subgraph non-uniform sparse the Scott approximation version is the graph isomorphism problem yeah so this it's it's sort of not really like an assumption because we have proven it well it's it's equivalent to the assumption that P does not equal NP but it's good to mentally think of it this way is like okay this is like a black box I can put in my pocket and try to start driving future hardness results from and just to connect it up to the thing we were talking about before this farce LPM assumption what's cool is the sparse LP an assumption so the host lines theorem of this NP hardness theorem it's like a worst-case result it just shows that you know for every algorithm probably one that purports to be a blue you know tell the difference between 99% and 51% satisfiable instances of this 3x rcsb it's gonna get it wrong on at least one instance this far self-cam assumption gives you like an efficient way like a really efficient simple way to generate hard seaming instances of this problem that like seemingly foil all the you know polynomial time algorithms that we know because it generates you know 1 minus epsilon satisfiable instances of this 3x or CSP where we don't know any polynomial time algorithm that can you know even satisfy 51% of the equations so therefore actually you know what you can do if you like is you can use like the sparse LP n method to randomly generate hard instances of 3x or and then you can feed those instances through these polynomial time reductions and thereby I get polynomial time algorithms that efficiently generate random seemingly hard instances of you know TSP and graph isomorphism and all these other problems now a downside is these are not especially natural instances that you'll get out but you know if your friend has like you know an algorithm that they claim is real good at solving you know approximately solving max cut problems or TSP like in some ways is our best known way to test them spy like you know generating sparse opium instances and passing them through these poly Tom reductions okay now I want to tell you about a related topic quite related which is instead of like random 3x or CSPs we talk about like the more famous CSP three-set now let's talk to you about what's up with 3sat what if you want to try this you know do the same story with 3sat like if I wanted to say okay we know three sounds and be hard but like please write me a computer program that efficiently generates a hard seeming instance of 3sat what would you do well one thing you can do is just like choose a purely random instance and so let me tell you about that so let's say you fix some number of variables and that you're gonna have and you still decide how many clauses you have to have like each Clause is gonna be a random it's gonna be the or of a random collection of three variables or maybe I'll randomly negate them as well to get literals but how many clauses will you choose well let's let this be a parameter let's let C be a parameter and say that you use C times n clauses so before we get to the question of like well how computationally hard will it be to solve this random 3-cnf for satisfiability let's first estimate a simpler or a more primary question which is um well is this instance gonna be satisfiable or unsatisfiable or is it gonna be 50/50 or what like what will the right answer be so ah there's a fact that for this problem there's a so called sharp threshold behavior with respect to the sort of the number of clauses you choose in particular there is a certain constant alpha 3 which numerically is about 4.2 667 but which has like a formal definition it's like it's a very painful formal definition it's like some smallest root of something involving a differential equation and that takes half a page to write down but there is some special number alpha 3 such that the following is true if your clause density C is bigger than alpha 3 then when you choose a random three set formula with you know and variables and C times and clauses it's gonna be unsatisfied probability okay of course the you know the more closes you have the more constrained you know the instance the more likely it is to be unsatisfiable and conversely if C is less than this magic number then with high probability your instance will be satisfiable okay there's a little asterisk here I put it here because this is not a theorem but it is definitely true people incredibly enough in the field of statistical physics I've been like thinking about like random instances of csps for a really long time they have an incredibly thorough understanding of them which is only very hard to analyze mathematically and only lately have been mathematicians been able to make progress on it but so they know this fact to be true and it is true and has recently sort of recently been proven to be true like in a 100 page paper but only for sufficiently large values of three by which I mean for random KSAT there's some number alpha K which comes out of some weird differential equation such that this theorem is true but only for sufficiently large K but it's surely also true for K being three okay so ah that's just the sort of story unlike okay if you okay fix seem to be something and now you generate random 3sat instances like this this sort of will tell you are they gonna be unsatisfiable or they're gonna be satisfiable and it turns on it see as big or small and you know if you're gonna be generating satisfiable instances then the appropriate puzzle to give your friend or hey an algorithm is try to find a satisfying assignment and there are actually reasonably good algorithms at this you know it's easier the fewer constraints there are so the smaller C is but for 3sat like we do know efficient satisfying algorithms that with high probability find satisfying assignments if C is like as large as 3.5 which is not as large as 4.2 but okay it's something on the other hand if you choose like a big value of C so that the random 3:7 sense is likely to be unsatisfiable then the appropriate puzzle is to ask the algorithm to find a certificate of unsatisfiability or perhaps it should write down an LP relaxation for like the max 3sat problem on this instance and it should solve the LP and you know maybe the LP value will be 0.99 and that will be a certificate that it's actually unsatisfiable this seems to be a lot harder so it's known that if you make a super constrained instance where C is at least like root n so we have n variables and n to the 1.5 clauses then there are efficient algorithms known that will find certificates of unsatisfiability and interestingly enough this uses spectral graph theory these algorithms well this is the best thing that's known so if I give you n variables random 3sat instance with n variables and n to the 1.1 clauses even though it's going to be unsatisfied with very high probability we don't know any efficient algorithm that will find a certificate for that clock and so one can take this as potentially a hardness assumption because people have been working on statins for a long time so it's sort of reasonably believable if nobody can solve some problem about random 3sat that it might actually be genuinely hard then you could try to use it for cryptography purposes or some other purposes so this assumption I was codified by refi guy around 2000 and it's called phagocytosis or Fargas are 3sat hypothesis and it says um just receiving any large constant like 100 or 1000 if I choose a random 3sat instance with n variables and C times n clauses there's no polynomial time algorithm that with high probability finds certificates of unsatisfiability okay so focus hypothesis is up there and we just tell you about it this one of my favorite hardest hypotheses by the way it's a great one to keep in mind I mean if you can't prove your problem it's hard assuming P designs on P try to assume faggus hypothesis so it's actually quite similar to learning parries with noise i'm what have a chance to say much more about the connections but they're quite deep connections and it's basically like a little bit stronger than the learning parodies but noise assumption one thing about it is it's know to apply many strong in approximability results that we already know how to prove assuming P does not equal NP but in an easy way so there are many consequences of like host odds theorem about hardness of Max 3x or that are very well proving them is a little bit hard and like you have to rely on host does theorem which has this like 100 page proof but if you assume focus are set are 3sat hypothesis then you can prove a lot of these theorems like in like two paragraphs so that's cool you know the conjecture only says that polynomial time algorithms can't do the job but you might conjecture it's possible to conjecture that even any sub exponential time algorithm must also fail we don't know any like tooth of little o of n time algorithm that solves this task either and it's very powerful algorithmic framework that we talked about in lecture which one do you maybe the sum of squares SDP hierarchy you can actually prove that it cannot you know solve Phi goes three set random tree set problem in anything less than like two to the big Omega of n time that's pretty good evidence and you can generalize this also to like random case at where we know that if I give you random case at instance it was with like n variables and n to the K over 2 random clauses it's know that you can refute satisfiability like output a certificate of in satisfiability with high probability in that case but if you make them a bit smaller like in to the point one k1 might still hypothesis that no poly-time algorithm can solve random case that in this case and this stronger assumption is also known to have like cool consequences especially for hardness of learning so Danielli and cello schwartz showed that under this assumption you can prove that it's hard to learn DNF formulas pack learn them which was pretty notable open problem they also show that it's hard to learn how spaces like linear classifiers with agnostic noise and a hard noise model and so forth so these are examples of like hardness results about learning that we don't not approve just assuming P does not equal NP but we do know how to prove now if you make Phi gus' hypothesis\", metadata={'source': 'Ehk0IRIFTck'}),\n",
       " Document(page_content=\"okay so the last portion of this lecture I want to talk about NP hardness of approximation and some of the stuff I talked about a little bit in lecture 20 about I'll recap it somewhat so here's like another like SAT style syllogism for you um 3sat is - NP hardness as this problem label cover is - in approximability okay in the sense that like 3sat is like the you know a problem that everybody likes to start from when you prove things are and be hard and similarly if you want to prove things are not just and be hard but and be hard to approximate to some factor like the Erb problem that everybody starts from is called label cover what is label cover it's the kind of CSP and actually there's a parameter Q which is the size of the domain so remember domains CSP is you know they have some variables that you're assigned trying to assign values to from some domain and then label cover parentheses Q the domain is just the integers 1 through Q and you're given some constraints and label can cover all the constraints are binary meaning they operated some two variables so then it's good to draw the picture of the could CSP as a graph where you have a vertex for each variable in a edge for each constraint so in the label cover problem you're given a bipartite graph in fact you can even assume it's regular on both sides so it's a very enjoyable graph it's a bipartite graph and each edge represents a constraints and it has like a pretty liberal kinds of constraints so every can schedule a constraint written on it and it's just given by like a truth table kind of it's given by a function PI some UV like you have one of these pies for every written on every edge and the PI is just a function from once your Q to one through Q here just give it as a table or think of Q as a constant I don't know 10 or something so pi of UV is just a list of 10 numbers in that case so as always you're trying to find an assignment of domain elements numbers 1 through Q to all of vertices you in you and B and you want them to satisfy as many constraints as possible and what is the constraint the constraint is like is this kind of it's called projection to constrain from the capital u side to the capital V side and the meaning of this pie is whatever value you give to let's say this vertex like 6 that's a value between 1 and Q pi is a function so it Maps 6 to something maybe 3 and this constraint says you have to give this guy the value 3 right that's what you should give ok so the constraint on edge little you little me is that whatever you assign to you if you pass it through the pie that's written on the UV edge that's what you're supposed to assign to me it's a little hard to get the gist a bit it's some kind of generalized graph coloring problem but this is the problem that's sort of like the great starting point for all hardness of approximation results so here's the theorem about it it's actually a bus stop - on that like bus - are that in the proof of host a theorem that took us from like 3sat hardness PCP theorem God is too slight in approximability for three sad and Roz's theorem which is called parallel repetition theorem got us the hardness of label cover that's this piece here and then host I put some Fourier analysis on top of that to get this hardness of 3x or approximation but here's a rasa's theorem from 1994 about the hardness of label cover and this you know is like unconditional it's like a proven theorem well it's conditioned on P does not equal NP so it's about a problem being np-hard and it's saying the following for every small number Delta there's a sufficiently large Q which is you know polynomial in 1 over Delta such that this label cover problem is not only hard to solve exactly it's even hard to remotely like optimally solve so this notation if you recall Delta comma 1 approximate means basically I give you a label cover instance with domain size Q and even promise you that there's a perfect assignment that satisfies 100% of the constraints it's np-hard for an algorithm to find a solution that satisfies a delta fraction of the constraints so you can set Delta to be you know 0.1% and it says any algorithm that can find up an assignment satisfying point 1% of the constraints you know perfectly satisfiable label cover in sense can be used to solve sad in polynomial time so that's Roz's theorem rewritten up there and as I said like this is like the starting point even more so than host odds theorem is the starting point for many many optimal inapproximability results so this is Hostos theorem we already talked about it but it's a reduction from Roz's theorem that 3x or it's hard to 1/2 plus epsilon comma 1 minus epsilon approximately close that also showed an even better result for max 3sat it's better because this thing is 1 we talked about this I think in lecture 20 so it's known that even if I give you a perfectly satisfiable 3sat instance it's np-hard to find an assignment that satisfies more than 7/8 of the clauses and that's tight because there is an efficient algorithm that satisfies 7/8 of the clauses and there's also an extremely strong hardness results for max independent sets starting from hardness of label cover it says I give you a graph where there's an independent set of size n to the point 99 if P does not equal NP then there's no polynomial time algorithm that can find an independent set of size n to the point O one so there's an enormous independence sense like almost all the graph well it's n to the point 99 vertices you can't even find an independent set of size n to the point O 1 and by the way this problem is np-hard but it can be stalled in time basically 2 to the n to the epsilon 2 to the N to the point O 1 so that's an example like I've seen before of an np-hard problem that can be solved in 2 to the N to the point O one time there's one outside of rasa's theorem which is related to the blow-up size of the reduction and the problem is of you said like Delta to be you know some constant then Roz's theorem you know takes a 3-sided sense and produces a label cover in sense of size n to some constant so the blob is really eats like it's exponential kind of in in Delta so if you said you don't also to be 1% then you know the blow-up here in mind this thing might be n to the 1 million size and that's not so great because then if you want to say like oh now I'm going to use a ETH to conclude you know that it takes a really long time to solve some of these problems not just that they can't be done in polynomial time which I get out of P does not equal NP but I want to use ETH to show that they take a really long time so that's so great like you can only conclude that because the palm the blow-up is like into some huge polynomial you can only conclude that let's say 0.51 comma point nine nine approximating max 3x or requires time to to the end to the like some really small constant which is still exponential but on the own hand you know if this is like time for for any value and that you've ever encounter in real life like and it's smaller than 2 to the power of 100 then this is at most 4 okay there B goes in there but you get my message but I've got some good news for you which is that uh Ono had some good news for you but I accidentally deleted the slide ok let me tell you the good news in words so I'll put a smile on your face because later with Moskovitz I won't write it up with a ton of Moskovitz this is like 2010 they got a reduction that for any constant Delta this was like n to the 1 plus little of 1 okay so for Delta even as small as like I think 1 over log log n the reduction in had almost linear blow-up and therefore actually you can conclude that this requires time 2 to the n to the 1 minus little o of 1 and so basically two to the end time ok so that's great ok so last thing I want to do is contrast this with the unique games problem which I also mentioned in lecture 20 and I'll just tell you a little bit more about it so the unique games conjecture made by coda in 2002 was really motivated by this roz serum you know this Roskam gave this like really strong hardness of approximation result for this label cover problem and was used to prove many many many other optimal inapproximability results but not everything you wanted so there's some problems we couldn't show her and be hard based on label cover and coat notice that if you made like one small tweak to label cover and assume that didn't change the fact that is np-hard then you could get all sorts of additional cool and B hardness results but didn't know how to prove it so it was made up conjecture and it's about label cover but all the PI's are now required to be by diction's so we talked about 34 we call this CSP like by action parentheses Q so like in the bipartite graph picture like not only does the you said when you choose a candidate assignment for this for text does it force what I mean you're supposed to give to the neighbor vertex also vice versa so that makes the task certainly a lot easier and in fact if I give you a hundred percent satisfiable instance it's easy to find that satisfying assignment what if I only give you a ninety nine satisfied percent satisfiable instance it's not so clear how do solve it well and this is a conjecture that for all Delta if you make you big enough the domain size big enough then even on instances where it's possible to satisfy you know one minus Delta fraction of the constraints its n be hard to satisfy a delta function okay and again many more optimal in approximately results were able to be deduced assuming this unique games conjecture for example we saw that there's this semi definite programming algorithm for max cuts that in polynomial time always finds a cut that's within a factor of 0.8 7/8 of the maximum cuts just proved by Gomez and Williamson and it was shown that if you assume this unique games conjecture then this is tight you cannot get 0.87 eighth plus epsilon we're just kind of funny because point eight seven eight is like a strange number it's actually this presumably irrational or transcendental number the solution of some trigonometric equation in fact raghavendran 2009 proved an amazing generalization of this he basically showed that assuming the unique games conjecture we know the optimal approximation and algorithm efficient approximation on them for every CSP we should take any CSP of air DK and consider this degree k SOS semi-different programming based algorithm then basically whatever approximation or certification it achieves it's np-hard to do better than that by Epsilon and so sort of saying you know that this is the best polynomial time algorithm this SOS algorithm for approximating CSPs but only assuming this unique games conjecture do we know that so that's a super amazing result if unique games conjecture is true it really close the book on the theory of approximating approximately solving csps in polynomial time but let me just end this lecture by asking you know well is it true is it false like indistinguishability obfuscation it's one of these hardness hypotheses that it's kind of more controversial or people are not sure whether they should believe it or not things like ETH I think most people believe faggus hypothesis I think most people believe even as Seth but we'll see we just briefly mention that it's known that this is equivalent to getting half of the constraints correct on instances that are 1 minus stuff is satisfiable so uh here's some evidence that it could be true I mentioned this before just two years ago code min zurafa approved a variation of the unique games conjecture called the 2 to 2 version where instead of by Chechens PI you allow 2 to 2 maps which makes the CSP more complicated and therefore harder to potentially solve and they showed that you do have basically Delta versus 1 minus Delta approximation hardness for this and as a consequence you get Delta versus 1/2 approximation hardness for ujc okay so unique games conjecture is a cool into saying that if I give you a 99% satisfiable instance and be hard to get half they show that if I give you a half satisfiable incidence it's hard to get 1% well they don't have maybe the UGC is false so in 2010 it was shown by Aurora at all that you can solve the task of getting a half of the constraints correct on one of my stuff to satisfiable incense in time that's like to to the end to the Delta to the one-third it's pretty funny but for any fixed constant Delta or for like you know really small constant Delta this is like a really small sub exponential quantity like to to the end to like something tiny so that's kind of close to being polynomial time and as we saw before like if you assume ETA age which is less controversial this is sort of like the easiest an np-hard problem could be like under ETH you can solve NP hard problems in time 2 to the n to the epsilon but not better than that so we kind of know that you can solve the unity games problem and time to to the end of the epsilon but it's still possible that it's np-hard uh so we really don't know and let me just say that you know there's a third strange thing here that I sometimes like to think about which is that maybe it's not clearly important whether it's np-hard or not it seems to be de-facto easy so the unique games conjecture has an extremely strange property that even though it's conjectured to be np-hard we don't know any hard instances we have no way of writing a computer program to generate unique games instances maybe at random where we don't also know a polynomial time algorithm for solving those instances this is quite in contrast to like you know the max 3 X 3 X or CSB and like we don't know any explicit family of instances which are not already solved by this polynomial time SOS degree 4 algorithm so it's a problem where like it's conjecture to be hard but like we don't even know how to find any hard instances so maybe in practice it's easy ok so that's a bit of a philosophical note to end on I guess I shall end the recording here but as always I'll stick around to answer any questions and otherwise I'll see you at the last lecture on Thursday which will be about a sketch of the proof of the PCP theorem\", metadata={'source': 'pp-oaeOuGDY'}),\n",
       " Document(page_content=\"let me move on now to talking about CSP optimization which is like the second task so uh in contrast to CSP satisfiability if I ask you to find you know the optimum assignment the one that satisfies the most constraints even if that's not a hundred percent of the constraints this is really hard it's empty hard for all but the most simple CSP I mean it's already NP hard for max cuts as you well know so uh in light of that when things aren't be hard you have to try some mitigation strategies and one of them is what's called approximation so let's see mitigation strategy I'll talk about in this lecture so let me make a definition let's say an efficient algorithm a for a particular CSP is an alpha-beta approximation algorithm if it has the following property for all instances where the optimum is at least beta huh the algorithm a is guaranteed to find an assignment whose value is at least alpha okay that's alpha-beta and had this little mnemonic I think of beta is standing for best like that's the best assignment achieves beta and like for me alpha stands for algorithm it's like what the algorithm is promised to achieve in this case okay let me put a little pause here because there's a question I was asked even if it is np-hard presuming uh-ah even as np-hard presumably meaning this met a problem of deciding whether predicate at least a p-problem or np-complete problem is it in exponential time it seems like the description of a CSP is usually very short for the problem we care about right yeah so it is an exponential time it's probably in P space and then some version of it if like there's some simple property of the CSP there's like a simple and natural property of CSP s called being a core some algebra thing which I forget which if it happens then the meta problem is in P but yeah it is true that like for the CSP s like we actually care about you know they have their descriptions like finite size you know they're like 14 predicate it's the domain is of size 3 so like you don't really worry about you can you practice decide if it's a p-type or an np-complete type of CSP yeah so this is a meta problem is sort of a theoretical interest only ok let's go back to CSP optimization so this is my kind of slightly complicated definition of what is an approximation algorithm and I built efficiency like pong real-time into the definition just so I can stop saying oh and efficient approximation however an efficient approximation I'll just assume when I say an approximation algorithm it means that follow meal plan 1 and also I'm going to allow randomized algorithms in which case the guarantee should be that it finds a value an assignment of value at least alpha and expectation and one thing I want to mention is that one algorithm a can simultaneously be an alpha beta or proximation algorithm for many betas and let me give a clarifying example what is the what do we prove the first part of this lecture we proved that thus Komen's williamson algorithm for maxcut when we prove is at some point eight seven eight beta betta approximation algorithm for every beta right when we prove there is that whatever the best value is beta the Goans williamson algorithm is guaranteed to get you at least point eight seven eight times beta okay that like a point eight seven eight ratio approximation algorithm means this that you're like point eight seven eight beta comma beta for every beta but it can be interesting to look about you know guarantees that are not just of this type like ratio factor x beta comma beta for every beta okay so let me give a couple examples there's simpler example or algorithms for maxcut than governs Williamson one for example just randomly partitioning the vertices or just like doing local search in a greedy way going through vertices one by one and putting them on the better side and then being done and as we saw before at least a random partition these are like a half comma beta approximation algorithms for every beta I should add here like for all beta because they're actually guaranteed to cut at least half of the edges no matter what the optimum is okay and here's another area like example using this terminology there exists 1 comma 1 approximation algorithms for too sad but what is a 1 comma 1 approximation algorithm it's an algorithm that's guaranteed to find you an assignment of value 1 100% whenever the optimum is 1 which it means the satisfiability algorithm or satisfy our gonna find satisfying assignments when they exist should mention that alpha will always be at most beta and in fact one it's not often stated this way in my undergraduate classes but one algorithm that is polynomial time satisfiability algorithm for too sad is to write down the natural LP relaxation and just check whether the optimum value is 1 it turns out that LP out there is 1 if and only if the two side instance is satisfiable ok all I want to say about optimization for now any questions if not I'll talk about this third task the certification task which you might have heard about a little bit less so here's my definition again I'll say a certification algorithm for a kind of CSP he's any algorithm that give it an instance I it outputs a correct statement like a true statement of the form optimum of AI is at most beta star so it outputs a number beta star which it guarantees is an upper bound on the optimum value and ideally beta star should be as small as possible and now we can make a similar definition we say that a CSP certification algorithm is an alpha beta certification algorithm if the following holds whenever the optimal value is less than alpha the certificate that the algorithm is guaranteed output will be less than beta okay it's always good when the certificate value is smaller so we get some algorithms that explain this and the most canonical example of a certificate certification algorithm let's forget this alpha beta stuff for a moment most canonical example of a certification algorithm it's just an algorithm that like takes a CSP considers it to be an integer linear program relaxes it to a linear program or semi-definite program and outputs the like LP optimum value right we're always talking about maximization problems so the LP optimum value is always an upper bound on the true optimum value and you can compute it in polynomial time and so like that's a great notion of a certification algorithm like just outputting the LP relaxation value and even better would be you know with LPS they have a dual and the dual is like sort of a very explicit certificate of a statement like this it's really just like it's some multipliers for the constraints in the LP which when you add up the constraints with multipliers turns into the inequality optimum is at most some number good and just to get a little warmed up for this alpha-beta notion let's think about what is a 1 comma 1 certification algorithm it's nothing more than a satisfiability algorithm why because what is a 1 comma 1 certification however them supposed to be an efficient algorithm that has the following property whenever the optimum value is strictly less than 1 so whenever it's unsatisfiable the algorithm outputs like beta star less than 1 so output a number less than 1 together with the guarantee that the optimum is less than 1 so it's an algorithm that whenever the instance is unsatisfiable like is guaranteed to like say i certify this instance is unsatisfiable ok and so in that way it can be used as a satisfiability algorithm you run the algorithm if it satisfies unsatisfiability then you're like great it's unsatisfiable and if it fails to if it outputs like 1 as its number then you're like well it must be satisfiable ok so now uh in the reader lecture I want to talk about this kind of confusing point like what is the difference and what's the deal with approximation versus certification because turns out they're both very important problems in the study of CSPs and algorithms and they're different problems but like the difference is kind of subtle so let's talk about this now an alpha-beta approximation algorithm searches for an assignment it's like a search problem for an assignment ok and just to remind you the guarantee is that when the instance happens to have value optimum value at least beta the algorithm will give you back a solution of value at least alpha an alpha beta certification algorithm also kind of searches for things but it searches for like a tight upper bound on opt ok so again it has a property that when the optimum is strictly less than alpha it certifies the statement like optimum is less than beta okay now let me try to really clarify the difference here for the max cut problem the goldmans williamson an algorithm I first saw the SDP II get the vectors then do this random hyperplane rounding to convert them to like an actual cuts and output that cut that's an approximation algorithm right it finds good solution and in particular like whenever the optimum cut is beta it guarantees to find a solution that's at least point 870 times beta on the other hand there's like a lesser thing you could do you could just solve the SDP don't you know maybe you didn't even figure out this random hyperplane thing you just figured out oh I can write down this SDP which is a relaxation and therefore the SDP optimum value is an upper bound on the true optimum just solving an SDP and outputting the STP OP number is I claim a certification algorithm actually that's clear that it's a certification algorithm it's just because the SDP op is always at most R or yeah so I can upper bound on the true ops I also claim though that it's a certification algorithm that's like point eight seven eight beta comma beta and let's really understand why this is the case so let me dive into this question I claim that outputting the STP value is at point eight seven eight beta comma beta certification algorithm for every value of beta so why is this true well fix some number beta fix a number beta and suppose you have a graph G whose maximum cuts is strictly less than point eight seven eight beta could it be that when you write down the SDP for this graph and compute its optimum the optimum is at least beta I claim no and if my claim is true then I have shown that this outputting SDP opt is a point eight seven eight beta comma beta a certification algorithm because I've shown that like whenever the opt is less than point eight seven eight beta the TP op will be less than beta and therefore the certified amount that you'll output will be less than beta so why is it true that if the true office is less than point eight seven eight beta the SDP opt will also be less that it could not be atleast beta the reason is we happen to know the Goins Williamson algorithm hyperplane rounding algorithm exists and we know that the hyper prime rounding algorithm has the property left the SDP opt is at least beta it would find a cut achieving value at least point eight seven eight beta but that's impossible because we're assuming the graphs max cut is less than point eight seven eight beta okay so therefore the existence is hyper priming rounding and it's guarantees tell you that when the opt is less than point eight seven eight beta the SDP op will be at most beta so just outputting the SDP opt is at most is a point eight seven eight beta comma beta certification algorithm so probably like you wouldn't know this fact about the algorithm that just outputs the SDP value unless you figure it out this hyperplane rounding stuff but nevertheless that's the case and in general the same phenomenon always holds and you can you know check for yourself that this is true in general whenever you have an alpha beta approximation algorithm it's also an alpha comma beta certification album [Music] good so the approximate trying to conclude here is that the approximation task is strictly well it's at least as hard as a certification task I got a question here which said do we have to talk about the D randomized version of the hyperplane rounding instead of just the randomize rounding good question I've kind of been brushing the difference under the rug actually for making this deduction we don't because actually even if you just know that the hyperplane rounding has the property that when the sdp opt is beta the expected value of the cut is 0.87 at least point eight seven eight beta then you can say oh by the probabilistic method if the expectation is at least point eight seven eight beta there must exist a cut yeah whose values at least point eight seven eight beta which contradicts opt to less than point eight seventy beta so yeah so you don't need it to make this deduction and one reason I'm glossing over it is like basically um whenever you have a randomized approximation algorithm like this for CSP you can convert it to like a deep randomized deterministic one um very easily in particular like you can run it like n times and pick the best approximate of argument that I'll let you think about shows that your with high probability you'll find a solution whose value is at least the expected value minus one over N so I'm always glossing over like I don't mind if the algorithms are randomized and we only care about the expectation okay so uh let me conclude with a one long slide it's a tale of the gums Williamston algorithm one two graphs I should I should mention that uh you know I only have like two minutes left so I'll give this slide and then I have a little bit more content like one or two more slides that I'll add to this video at the end in which I you know tell you what is the PCP theorem and what is the unique games conjecture among other things but let me end just this portion by talking about the government's Williamson algorithm on two graphs because I think it will assess difference between certification and well it'll illustrate like another confusing point potentially competing point so the first graph I want to talk about is looks kind of like this the vertices are the corners of a cube centered at the origin but I don't put in the normal edges think of this cube is sitting in you know high dimensional space I connect two vertices by edges if like the origin is here and I consider connecting two vertices if the angle between the vectors going to those corners is at least three-quarters pie or 135 degrees okay so this vertex you know it's connected to like the vertices that are kind of like sufficiently far away from it so the dotted things are not the edges the yellow things are the edges and you have the same picture for every vertex so I'm not I'm just going to tell you some facts but are not necessarily easy to prove but they're true so the first question is like what is the maximum cut in this graph the maximum cut in this graph you might guess it if you actually take any axis parallel hyperplane it turns out you can prove that this is the maximum cuts and once you know that it's not hard to show that the actual optimum value a fraction of edges cut is basically this quantity 1/2 minus 1/2 coasts 135 degrees also known as 1/2 plus 1 over 2 root 2 which is about 0.85 so in this graph the best at least in high dimensions the best cut cuts about 85% of the edges now what about the SDP algorithm well remember it in the SDP algorithm you're trying to assign a unit vector to each vertex and our graph is already kind of geometrical in fact it turns out that the optimal SDP assignment of vectors of vertices is like the identity mapping that just assigns each vertex to like where it really is in space and you can show that the SDP opts value is equal to the true up the SDP opt is 85% so here like the SDP has done a perfect job of certifying the solution it says it's at most a 5% and indeed the OP is 85% but if you do the goldmans Williams some random hyperplane rounding it will not produce an axis parallel hyperplanes it'll produce some like you know random one and you can show that it an expectation will give a cut that only cuts on a 3/4 of the edges it's the same 3/4 so it only cut 0.75 of the edges and by the way this point seven five over point eight five is really close to this point eight eight value point eight seven eight value so this is a case where the optimum SDP ops value gives a perfect certification but the algorithm only gets the point eight seven eight factor that's like a search optimization problem on the other hand there's the opposite occurrence for a different graph it's an interesting so you can take another kind of geometric graph where you take a smear in high dimensions the unit sphere and you set the vertices to be quote-unquote all points on the surface of the sphere okay you have there like to make it a fine a graph you have to discretize this instance but just imagine like you put in all points and you use the exact same edges for every two points in the sphere you connect them if their angle is at least 135 degrees so each point is connected to like a bunch of points that are like quite far away for this graph some other interesting things happen it's perfectly rotationally symmetric so the best cut in this graph this is hard to prove but it's true the best cut in this graph is any hyper plane through the origin cuts and any hyper plane through the origin cuts you can show cuts about 3/4 of the edges so opt is like 3/4 if you give this to SDP it turns out again that the best SDP solution for associating a unit vector to each vertex is like the identity mapping you just let the vertices be themselves and then you can show the SDP value is bigger it's this 85% number so this is a funny case where the SDP is like a relaxation it's like as far away from the optimum it's like we need 7 a weight 8 away from the optimum it's bigger than this 0.75 but ironically if you do a random hyperplane rounding on the vectors you get back you know you get a hyper plane through the origin and those are optimal so this is a case where the random rounding algorithm actually will find you an essentially optimal cut a value 3/4 even though the SDP opt value is off ok so this is a situation is a good pair of situations to keep in mind when you're trying to understand the difference between sort of certification algorithms and approximation algorithms ok so let me end the lecture there as I mentioned I'll add like five minutes on to this lecture later I'm going to stop the recording now but you can feel free to stick around and ask me any questions here's the last bit I wanted to add it to the okay so I want to just tell you some known complexity results for alpha comma beta approximating various simple csps so let me start with the 3sat problem or maybe more accurately the ìiî sap problem where every clause has to have exactly three literals per or and we can think this is an optimization problem by also imagining the version where you're trying to find a truth assignment that satisfies as many clauses as possible okay so the first result here is that 1 comma 1 approximation is np-complete and just remember that 1 comma 1 approximation means that when the best truth assignment a satisfied 1% of the clauses the algorithm should find a truth assignment satisfying Honda decided that clauses in other words it's the satisfiability problem you have to find satisfying assignments when the exists or else decide that they're non-existent and so this is just the fact that you know the 3sat problem with a three-set problem is np-complete that's been known since time immemorial but there's still an NP hardness result for this even when you're not so ambitious as to try to find perfectly satisfying assignments so imagine that you're given a perfectly satisfiable III Sat instance and your only goal is to try to find an assignment that satisfies 0.99 nine nine nine nine nine nine nine nine a fraction of assignments okay some 1 minus epsilon fraction of the assignments for constant epsilon turns out that this 2 is np-hard and this is exactly equivalent to the theorem known as the and the PCP theorem very very famous theorem in theoretical computer science first fully proven by a LMSs that's a lone sorry Aurora loomed well twonny sudan and ii ii and 1992 and i was presented for the first time in 1992 at the fox conference in pittsburgh so it's a great result and subsequent to this result people try to make this point 99999 etc um smaller to show that even trying to get let's say 99% of the clauses satisfied is np-hard or 95% of the clauses is an hard and the ultimate result along these lines is obtained by hosted in 2001 but I'll tell you about that in just a moment let me first I'm gonna tell you what the limit to this kind of result is a positive result if I'll give you a satisfy a bull instance of III Sat and I ask you to find us an assignment that satisfies at least seven eighths of the clauses this is super easy you can do this in polynomial time and one way to do this is to simply pick a random truth assignment now this will be a randomized algorithm so it's not literally in P but if you imagine picking a random truth assignment then for every clause which is the or of three literals the chance that that Clause is satisfied by random trees assignment is 7/8 and therefore an expectation this randomized algorithm which doesn't even look at the input actually will satisfy 7/8 of the constraints and it's not too hard to do randomized this algorithm and therefore this 7 7/8 comma 1 approximation is in P and this randomized algorithm idea was due to Johnson in 72 so that's the positive result and then as I mentioned the ultimate hardness resolved which matches this was proven by hosted well around the turn of the millennium and he showed that for every positive epsilon constant trying to get 7 eighths plus epsilon fraction of constraints satisfied on instances that are fully satisfiable is np-hard ok so taken together these two results are sort of a perfect understanding give a perfect understanding of the complexity of approximating the ìiî Sat problem unsatisfiable instances you can get if I give you a satisfiable instance you can get seven eight so the clause is satisfied in the worst case in polynomial time but also in the worst case is np-hard to get seven eighths plus epsilon fraction of the clauses just aside on the topic of the 3sat problem where it's the slightly more usual version of the 3sat problem where you also allow clauses of length one and two actually so the hardness results still stand for this because 3sat is more general than III sad so since the 7/8 plus epsilon is hard for III sad it's also hard for three sad what about the algorithm well for example to pick a random assignment algorithm doesn't quite work anymore because actually clauses of length one and two are only satisfied by a random assignment with probabilities 1/2 and 3/4 which is less than 7/8 so that doesn't quite work it's funny you think that like clauses of length one and two should make the problem kind of easier but formally they don't it is known that you would get 7/8 also in the 3sat case when clauses of length one and two are allowed in polynomial time but this turns out to be extremely extremely difficult it's an extremely difficult analysis of a semi-definite programming based algorithm due to Karloff in Zwick from 1997 in fact they didn't even quite prove it they just gave extremely good numerical evidence that was true and only subsequently Zwick was able to give a computer-assisted proof that the algorithm was correct and that it gave a 7/8 comma 1 approximation for the 3sat problem ok up about 3sat let me move on to maxcut which is an even simpler in some sense well even simpler CSP so Mexico as we all know is np-hard or at least that's something you learned early on in complexity theory but that really means is finding the optimum cuts is np-hard when the out phone cut is a hundred percent of the edges the task is actually not at be hard that's testing if a graph is bipartite but if you inspect the proof that you know a classical text book proof that machs cut is np-complete what it shows is that for some specific constant you know between 1/2 and 1 let's say 3/4 if I give you a graph or the maximum cut cuts a 3/4 structure in the edges it's np-complete to find or I'd be hard to find such an optimum cuts so now let me tell you about a few more results that are known on the subject of trying to find a cut that cuts 3/4 of the edges in a graph so the first thing I'll tell you is that this is sometimes hard if I even if I give you a graph for the optimum cut is essentially 4/5 of the edges so for any epsilon greater than 0 if I give you a cut we're 4/5 minus epsilon with the edges can be caught so seventy nine point nine nine nine percent even in that case it's still hard to find a cut that cuts three-quarters of the edges that's a result building on host odds work due to Travis ons Sorkin Sudan and Williamson on the other hand if this is a little bit of an exercise but you can use the Commons Williamson algorithm to get that in graphs where the optimum cut is 1/2 plus 1 over 2 root 2 fraction of the edges or this point 8 5 fraction of the edges that we've seen before then it is possible in polynomial time using Goines Williamson to cut find a cut that's out of cuts 3/4 of the edges okay so if the graph is like 85% cuttable you can get 3/4 but if it's only 80% cut a bowl or 79.9% cuttable you cannot get 3/4 unless P equals NP what about in between let's say between 80% and 85% there's 83% or 5/6 and here's a fact of life is it possible for 3/4 comma 5/6 approximates max cut this is unknown it's a gap in our knowledge I give me a graph where it's possible to cut 5/6 of the edges can you find efficiently a cut that cuts 3/4 of the edges this is open and it's a problem just like factoring or grass up graph isomorphism you know one of these mystery status problems where we don't know this in polynomial time and we don't know that it's n be hard so that's still a great open mystery about max cut so let me tell you about one more problem is this problem I mentioned before called max by ejection it's the CSP where all the constraints involve two variables there to be assigned values between 1 and Q where Q is you think of as maybe a large number and they have this project of property that constrains that whenever you set one variable to a certain value and participating in a constraint was another variable there will be exactly one unique value to set this other variable to that will make this constraint satisfied so as we mentioned before if I give you a 100% satisfiable instances instance this Maxima ejection problem then in polynomial time it's easy to find a 100% satisfying assignment you notice this is a propagation algorithm I mentioned on the other hand it's np-hard in general to find the optimum assignment and a max bisection problem really this actually follows from the fact that max cut is np-hard in Mexico does a special case of this max by ejection problem when Q is 2 but in case it's true that for every epsilon greater than zero it simply hard if I give you a 1 minus epsilon satisfiable instance to find a 1 minus epsilon satisfying assignment now I'm not sure this is a very interesting problem it's connected to this thing called the unique games conjecture so let me tell you a couple of facts along this line so this is not the unique games conjecture but the first thing I wrote here in white if I give you a 1 minus epsilon satisfiable assignment and I ask you to let me just take this back to find an assignment that satisfies half of the constraints so it's almost perfectly satisfiable but I just want you to find an assignment satisfying half of the constraints you can do this in polynomial time if Q is 2 but the problem gets harder the larger Q is and if Q is 3 or higher than 3 it's not known if you can do this task getting 50% of the constraints satisfied even when you promise that 99.99% of them are satisfiable and finally unique games conjecture so asking about this exactly when Q is large so the unique games conjecture was initially opposed by Cote in 2002 was the conjecture that for all epsilon greater than 0 there's a large enough Q depending on epsilon such that even if I give you a 1 minus epsilon satisfiable instance of this max by ejection problem on w of size Q it's NP hard even to find an assignment that satisfies an epsilon fraction of the constraints so this is an extremely strong conjecture and it was really not clear if there was evidence that you know such an easy constraint satisfaction problem could indeed be so and be hard and thoughts of about whether it's true or not back and forth over the years let me mention that it's it's known that this is equivalent to the harder task of finding an assignment that satisfies half of the constraints on instances which are one minus Epsilon satisfied well that's requires a theorem but if the getting half is np-hard then even getting epsilon is also np-hard that's known as I said you know opinions went back and forth over this over the years at first maybe people thought okay sure why not Subash code knows he's talking about maybe it's gonna be hard then people thought some more about algorithms and they got the idea that maybe it's not gonna be hard maybe it's in polynomial time but the latest salvo in these wars came just recently and it swung the tide in favor of maybe hardness so this was not quite proven but sort of a version where the half is in the other place was shown to be np-hard so building on several previous works over the last few years code mints are in Saffron 2018 showed that if I give you a one of these max bisection instances where the optimum solution satisfies at least half of the constraints so it's not like almost satisfiable is not 99.9 percent satisfied well but it's half satisfiable then indeed it is np-hard to find even an epsilon satisfying assignment assuming the domain size Q is large enough so this is sort of considered a half the unique games conjecture this is a consequence of the two-to-one conjecture being proven true by Cote mincer in Safra\", metadata={'source': '9nMwIr6y8NM'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Explain NP Hardness problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"[Music] thank you in this segment we're going to talk about beam search and more generally other decoding strategies for producing language out of language models so the decoding strategies we're going to talk about are going to apply the two types of models that we think about in this class the first is language models uh just placing distributions over the next word and the second are sequence to sequence models placing next word distributions over an output y conditioned on an input X so for in both of these cases at the end of the day we have this model and we give the system some input either a prefix of y's or an X or you know we just wanted to straight up generate a story or generate something for us so how do we actually do this generation so one option is just greedy search so we just take the most likely next word at each step and we kind of repeat this and just crank out a sequence that's one approach uh it's going to be the sort of worst of all possible worlds but it actually works pretty well in practice so uh it does get used um the one we're going to talk about right now is using beam search which is going to be a kind of improved version of greedy search that's going to allocate a little bit more computation but ideally find something with even higher probability and the third option which we're going to talk about a little bit later is drawing samples from the model so rather than getting the most likely thing instead draw a random thing so let's dive into beam search so uh we're gonna assume that we have uh vocabulary of size V and a sequence of length n that we're going to generate so at the first time step we can think about basically the probability distribution over the first word and I'm just going to you know make up some words and some probabilities here blah blah blah and uh each of these things you know has a pretty low probability the model doesn't really know what it's supposed to say um Etc so they're V of these so there are a lot of different words that we have to consider here and then if we think about what happens at the second time step each of these has a number of different things that can kind of produce right so we could say the dog the cat the fish Etc right and each of these gets a probability and these are going to be even smaller now because we're sort of multiplying you know small number after small number together and there are generally V continuations uh of each of these and so we end up with v squared hypotheses here or size of V squared hypotheses uh and at the end of the day we are going to end up with size of e to the N uh total sequences so I mean uh I think we're kind of familiar with exponentials being bad so the fact that this is exponential in n and you might want to generate something potentially very long is a problem so what this is this is the kind of exhaustive search mechanism that's going to enable us to find a uh highest probability sequence but we're going to do something better with an approximation so beam search is going to approximate exhaustive search but with less compute and basically the idea is keep the top k ypotheses at each step so if we kind of go back to the uh this lattice of options we were building above we're going to kind of revisit it and let's say let's say k equals three then what we're going to do is we're going to cut the model off here and ignore all of the rest of the options that come afterwards all right and then each of these still generates a whole bunch of possible uh possible next words right so we get the dog um you know all these all these things and we generally end up with uh K times V of these okay but once again we're going to chop off most of that we're only going to keep around the top things which are maybe the dog the cat and let's say uh cat or something like that so we end up with a relatively small number of options at each time step and then uh you know there's an expensive process of checking all the stuff that comes next but then we always prune back down to this small set so the run time here we can kind of think about it in terms of several uh different mechanisms we're not really going to talk about runtime of data structures like you need to maintain some kind of Heap or priority queue or something like that to actually manage all these items but like that overhead is so small compared to the overhead of running big neural Nets that we're not even going to think about it instead the kind of relevant things are that uh we're going to have K times n Transformer calls in the sense that we think about just how many times do we need to run the Transformer and get a distribution over the next words well it happens kind of once at the first time step and then it happens three times or k times for each subsequent time step because you need to consider each of the things in your Beam run the Transformer over it get the distribution over the next words um and we consider K times V times n hypotheses ultimately so it's much smaller than uh V to the N um you know we're just not thinking about that many items but it still allows us to explore the space in a way that kind of scales with K essentially okay so uh beam search you're going to see used most frequently in sequence to sequence conditional generation settings like machine translation uh where you really want to get a high probability thing and you know that okay greedy is going to find me something but then beam search is going to find something of even higher probability than greedy will um greedy can get kind of derailed down some path and pick some word that's then going to lead to some low probability stuff later and veeam search might avoid that that can lead to like better translations um but we'll see later that for certain types of applications beam search is not what you want and it's also the case that once you get bigger and bigger Transformer models the beam search becomes less crucial because these models already do a little bit of kind of not explicit planning ahead but the modeling of the distribution of the next word already accounts for the fact that more stuff is going to be generated later so um you know there's less of a need to use this with the kind of latest and greatest models and it's also very expensive which when you have a model that's already kind of Breaking the Bank in terms of compute you probably don't want to use this however it's an important search technique to be aware of and uh sort of an important way of producing hypotheses out of these models that's the end of this segment\", metadata={'source': 'wltqDbhlcJ0'}),\n",
       " Document(page_content=\"[Music] thank you in this segment we're going to talk about sampling strategies for generating text from language models so previously we talked about how there are some strategies around taking the most likely next token and then subsequently we looked at using beam search to find an even higher probability sequence but the third option that we outlined before was to draw a random sample from the model now you might think why would you want to do that well for an application like machine translation where you really want a like high quality translation you probably want to use something like beam search but if you're using a language model for some sort of open-ended generation task like let's say you're trying to generate a story there may not be one right answer and you may be okay with more variety so this was one of the capabilities that started to arise with the GPT series of models and particularly gpt2 so uh one of the kind of headline examples that they had back when they released that work was the ability to continue stories like this one in a shocking finding scientists discovered a herd of unicorns living in a remote previously unexplored Valley in the Andes mountains even more surprising to the researchers was the fact that the Unicorn spoke perfect English so then we might say okay what does the language model like to sort of say next can it continue this story and beam search eventually gets stuck in this very interesting Loop where it just keeps repeating the name of this University again and again so this is exactly the kind of case where uh you know we might say okay this is somehow not random enough right so then we can consider sampling and if you do sampling from the model's distribution you end up with stuff that ends up looking quite weird like they live in a remote desert under inner uninterrupted by town and there's lots of grammatical errors in this so what's going on here so this phenomenon with beam search is called degeneration and this arises from the structure of the language model that we're using so let's like think about how this string actually gets generated here so first we uh let's say we kind of work going along and then we ran into University dad as a token and it might be the case that the probability of Nacional given Universidad is high that's fine and then autonomous maybe coming after that is also High de Mexico also High probabilities and then the model is kind of at this Choice point where it then start sort of puts a slash and then keeps continuing things these might be low probability things but once you start looping here ultimately the probability of this entire sequence looks pretty high right like it's found this uh sort of name that it can generate with high probability and then it's just going to keep looping this and beam search is going to think this is great so these words are likely given the previous words but the whole sequence is unlikely there's a separate set of models called globally normalized models which include things like energy-based models that can be used to judge this we're not really going to talk about them in this class so much they're sort of computationally hard to use but basically I you know we want to do something more random than beam search we don't want to just find this like highest probability thing that's kind of degenerate okay but then this sort of Goldilocks style sampling is then too far in the other direction it's too random so what happens here if we say the distribution over the next word given they live in a remote desert uninterrupted by blank well there might be a whole bunch of different options here uninterrupted by roads towns people civilization Etc and maybe town is here but it's a little bit less likely so in general there's going to be a lot of options and maybe the top you know most of the probability Mass uh accounts for these but then there's going to be a long tail and let's assume that this long tail has ten percent of the mass well then what happens is that on average every 10 words you sample you're going to get something from this 10 tail and if you think about if you're going to sample 100 words in expectation you'll get something from the one percent tail so like even rarer stuff and if you're going to generate a story you don't really want it like going completely off the rails every 10 words or every 100 words so nucleus sampling is this idea to correct for this and what it is is basically chopping the distribution truncating it after P percent of the probability Mass if you sort the tokens from high probability to low probability then you just renormalize and sample from the distribution that's left so it's a relatively simple technique to implement all you need to do is get have your options and sort them and then truncate them and it turns out it works pretty well so in the paper by Ari Holtzman at all that presented this they compared the uh stories generated through various different methods across several metrics first is perplexity we see that nucleus sampling achieves like decent perplexity not as low as greeder greedy or beam search but greedy and beam search achieve very high self-blue and very high repetition which basically means that they're falling into these loops and repeating themselves and then this last column Hues is this kind of human in the loop scoring function that nuclear sampling also does quite well on so it seems to be able to balance uh the sort of naturalness of the story while still maintaining uh you know quality and reasonably uh good likelihood from the perspective of perplexity you see it's much lower than the perplexity from peer sampling so when we talk about decoding strategies from language models we're often going to think about these three uh Max decoding or greedy decoding beam search and then nucleus sampling and if you use a large language model API like chat gbt uh the outputs are very often come from an approach like nucleus sampling that's the end of the segment\", metadata={'source': 'JETxaSaj6_k'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about phrase-based machine translation now that we've set up our sort of primitive or let's call it pre-processing of word alignment we can understand how to actually take that piece and build it into a real translation system so the first step we need to do is extract what we call a phrase table and we're not going to go through this procedure in too much detail what i'll say about it is that we have a set of sentences from our by text and we can draw a kind of grid and what word alignment allows us to do is it allows us to draw in uh associations between words in this grid and so like i said in this example it's it can be a little bit hard to actually uh align some of these words um so you know we won't get necessarily a perfect phrasal correspondence between these two uh between these two sentences but what it does tell us is that okay you know two and u are likely translations of each other and it also tells us something more it tells us that okay we have kind of two words here that align in this block so tufay and you doing seem like they should be translations of each other this is an okay assumption in this case it's correct in this example but in general tufay means you do or you are doing and you doing just as a single unit doesn't necessarily work well but regardless what we're going to do is we're basically going to go through our data and draw these kind of boxes in our alignment grids and this will give us a set of phrase translations which we can associate with particular scores based on how often they show up there's a lot of heuristics involved in this procedure and so we're not going to again discuss it in a ton of detail but this is the basics of how we go from words to what we call a phrase table all right and now the given a phrase table we can build our phrase-based machine translation system by combining that with a language model this is just going to be an engram language model like we were seeing before and given these two pieces we are going to exploit an idea called the noisy channel which is the the probability of an english sentence conditioned on a french sentence assuming we're translating into english from french is proportional to the probability of that english sentence times the probability of the french sentence conditioned on the english sentence so basically it's like generate the english sentence and then translate into french as a way of sort of inverting the process i mean this is again similar to hmms for part of speech tagging we like generated the tags and then the words based on the tags even though what we cared about was inferring the tags from the words so it's the same kind of idea here and what this is going to allow us to do is it's going to allow us to incorporate two constraints here the first is from p of e which is that this has to have high probability under a language model and the second is from p of f given e which is that these things have to basically associate with each other we have to be able to translate reproduce the original french sentence given the english sentence all right so that's the idea behind this noisy channel approach to mt and we have these two pieces called the translation model and the language model and again if you actually write out bayes rule here you have a p of f term we're just going to ignore that because that's really what we condition on from the standpoint of this inference all right so we need a language model we're going to use an n-gram model for this and we need a phrase table which we talked about how to get and again we're going to get these phrase pairs and associate them with some probabilities that are based on basically counting what occurred what what phrases we see co-occur and then what we want is we want to actually produce a high scoring english translation of this french sentence that is produced by a series of phrase by phrase operations so the space of ease is much much larger than anything else we've dealt with so far really in tasks like hidden uh hmms for part of speech tagging uh this was exponential in the length of the number of tags but it was like 40 to that length and now we're going to have something like let's say 20 000 to that length if we have a pretty large vocabulary so the way we produce these translations is we form what's called a phrase lattice what we do is we take our input sentence here maria no dio une buffetta a la bruja verde and we look at each possible phrase match with our phrase table from this sentence which uh is now going to be spanish not french so on the spanish side une bo fatada for example can translate as a slap and so that's something that we would have in our phrase table here and so we can produce this translation option in this lattice so we find all these possible translations and now the what we're going to start with and what we'll focus most on is the problem of monotonic translation how do we walk through paths in this lattice to translate every word from spanish exactly once not skipping any words and stitch together a translation into english that looks good so this doesn't look so dissimilar from the viterbi algorithm but there's a couple of things that make it more complicated and so what we're going to end up doing is is beam search here so we have to think about what state we need to keep essentially in the beam in order to score things and so remember that in part of speech tagging the state was just what is the previous part of speech tag because that was the only thing we needed to score the transition function here we need uh the score of the hypothesis so far that was also something that we needed in in the hmm case just as the key for the uh the the item in the beam and in this case that score is a product of two things it is the product over all the phrasal translations we've seen so far that's the first term and then the language modeling probability the probability of each word conditioned on in this case the prior two english words if we're using a three gram model we also need to know where we are in the sentence and what words we've produced so far so this is the kind of trickiest thing is that we need to remember the past word or two of english context in order to score uh in order to basically score future words that we put down using a three gram language model all right so for example to start off with what we do is we populate this beam after maria which only contains one possible translation which is mary and so let's say the the phrase translation probability there we're going to use log probabilities let's say the log probability is minus 1.1 all right that's a that's a valid beam state we have the word the word so far where we are in the sentence i'm just writing that as the index here and the score then what we do is we say all right for the beam at index 2 after mariano how do we get there so there's a couple of different options now for paths that we can take that end at this particular time step so we can get mary not we can get mary did not and mary no and so notice that in the case of mary did not we dropped information about mary we can fold that into the scoring from the language model but we don't actually need to remember mary because going forward the language model only needs to look at the prior two words of context and so now we have three hypotheses that end here with associated scores and we're going to keep extending these forward in the sentence and again so the score here has to account for both the language model probabilities which are based on the number of english words you've generated so far and each one has to condition on the prior couple of english words and we also need to incorporate the translation model which is just the score of each of these phrase translation options that we picked up basically the score of each of these underlined segments in the lattice so the score here is a product or let's call it a sum of log probabilities and the in in reality you have to weight these things differently so there's some extra parameters here and real translation models actually use several different features that we then have additional weights on all right so one kind of crucial thing is that once we go a little bit into the sentence there's actually multiple ways of segmenting it and translating it so in the third beam here we're going to have two types of hypotheses ones that come from using three individual translations mary no or maria no dio or ones that translate maria and then or uh maria and then no deal uh so the slide is is wrong here the plus should be before no the beam in this case contains alternatives from multiple segmentations of the input and so we're thinking about multiple paths and they're all kind of competing with each other to get to this time step but they've all translated everything up until this this point all right so that's basically how monotonic translation works you go through the uh you go through the sentence until you get to the end you do beam search and then your top thing in the beam is going to be ideally the most likely translation under the model unless you had uh you know an insufficiently large beam and somehow you uh screwed up and only found a kind of approximation of the best translation we cannot there are also models that visit words out of order these are significantly more complicated and so we're not going to talk about them very much the state in this case also needs to track additional information about what words you have and haven't translated so that you can kind of go back and pick up the words that you haven't translated so far uh and so the you get this extra kind of bit vector thing in the state and generally this is very involved to deal with and in most cases it's not actually necessary because if you have a big enough model that has seen a lot of phrases those big phrases actually already capture big reorderings so even though reordering might be very important to get things like la bruja verde translating into the green witch in this case we actually have a phrase green witch which tells us that uh which which tells us that reordering and so we can get that reordering even operating within the monotonic translation framework all right so the last main thing i'll say is that we talked about how the uh the the language model and the translation model have these weights associated with them um setting these weights is a very complicated process uh there's a procedure called mert for minimum error rate training from franzak where you basically form a small set of translation alternatives and then you're not doing anything like gradient descent you're instead using a kind of line search on your parameters to figure out settings of the parameters which make the correct translations rank highly on in those k best lists or thousand best lists in this case and so it's a very weird paradigm for training is again not something that looks much like other things that we look at in this course and so we're not going to talk about it much more but generally it was very hard we didn't have good gradient based optimization techniques for how to train these models and so there were a lot of sort of crazy procedures that looked like this all right so what i'll the last thing i'll say about phrase-based translation is that there's a toolkit called moses that rolls up all this stuff including this uh so-called phrase-based decoder called pharaoh that we've seen in these slides here and it has a whole bunch of different models for doing uh translation including this phrase based technique uh syntactic models etc and for a long period of time you could build basically things that were on par with google translate if you had enough data and so this is a very nice tool if you want to explore translation systems that look like this and is i think still maintained pretty actively so we've seen here how to actually put these pieces together to build a phrase-based machine translation system we'll talk now about alternatives to this so particularly syntactic translation and then also neural machine translation that's the end of the segment you\", metadata={'source': '0k8b5jGk-h4'}),\n",
       " Document(page_content=\"[Music] in this segment we're going to talk about model probing so probing is a different form of explanation than we've been looking at in that it's more a way of analyzing the kinds of representations that our models are using which gives us some idea maybe of why predictions are arising but it's a little bit less explicit than some of the other techniques we've been looking at so essentially our goal is to figure out what kinds of information are being preserved in a neural network and the way we can do that is we can take intermediate representations from the network and then try to predict that information out of those so what we're seeing on the right here is a pre-trained encoder model producing these blue vectors and we can aggregate these into representation well we can either grab token representations or aggregate these into span representations and put these through a very simple classifier so we might use a neural net with one hidden layer for example and then we'll try to predict something like say part of speech tags or dependency relations or something like that and the idea behind this is that by using a simple classifier we are forcing most of the heavy lifting to happen in this pre-trained encoder and if the model is able to do well it indicates that these blue vectors being passed in are pretty good now one critical thing here is that we're going to keep this pre-trained encoder and these blue vectors frozen we're not going to like back propagate into them as we would if we were trying to build a good model for this task because what we're saying is all right given this existing model how well can we predict this information from it and so kind of changing the model to make it predict better is a little bit defeating the purpose of this experiment so in some work from uh the one of the johns hopkins university summer basically summer kind of research series they looked at having a set of tasks that they would put through that they would use to probe the representations of a bunch of different pre-trained models and so here are the results for bert base so what they have on the left side is a so-called lexical baseline which just uses glove embeddings or essentially context-independent representations from uh the model from the burp model itself then they look at how much of a gain there is from the actual contextualization and so that's what's shown in the next two columns here and the conclusion here if you look at the average for these different tasks is that contextualizing these vectors with bert leads to a very large performance increase across these different tasks and so we can also understand certain things like for example if we look at the part of speech row on the top the kind of individual predictions actually aren't very good these uh you know we can't tell a lot about a word part of speech in isolation but then the predictions from the contextualized representations are quite good around 97. and so the takeaway here is that this model seems to preserve part of speech information and because we can read it out from the representations that we get using a fairly simple classifier and then you know we can look at other aspects of this like for example constituency and see that the results are a little bit lower this might indicate that that trying to kind of aggregate over spans is a little bit less reliable and the model has a bit less of an idea of what the kind of relation between these different units is even though it's still doing quite well uh you know given that we're essentially taking some off-the-shelf representations and feeding them into a fairly shallow model so we can look at these results and and again try to kind of decode what's going on and and it lets us compare different pre-trained models and assess their strengths and weaknesses so some some other work from google showed that the basically this kind of idea can also be used to understand how bert relates to the conventional nlp pipeline where you know we might think that okay the first thing we run is a part of speech attacker then we run a parser then we extract entities and then we you know figure out the relations between those entities and co-reference um and so what they did was they looked at all the different layers of bert and they probed those layers for which basically for being able to solve each of these tasks and what they see here in purple they have the kind of delta from the mean representation using representations from that particular layer and so in part of speech at the top for example we see this very high purple bar to start with which then drops off and what that shows is that early on in the network the representations are much much better at predicting part of speech than they are later what this says is that you know early on the the model form some representations that have a lot of information about part of speech in them and then those representations gradually become less and less aware of part of speech but other things kind of pick up after a while for example the second to last one here semantic role labeling does well pretty close to the input but then also kind of in the middle of the network and then uh co-reference resolution at the bottom is sort of doing okay throughout and so they the title of their work is bert rediscovers the classical nlp pipeline and so their argument is that there's a little bit of the same progression of tasks going on inside the model in that it's you know first learning to do simple low-level things and then building up more and more complexity so while this is not an explanation in the sense that it doesn't tell us why the model made a particular prediction it does give us some insight into what bert is doing and we can start to think about like okay we think bert has the capability to do co-reference resolution for example so when we look at a task like multi-hop reasoning that requires resolving references we can think okay well bert is able to do co-ref and so this task involves ko-ref and so i think that bert should be a good model that works well here and my understanding when it gets the right answer is that it might be doing that due to co-reference this reasoning is not you know it's it's a little bit hard to apply and there's reasons why you can't just uh take all these conclusions at face value um there are some issues with probing where you know if you start building very sophisticated probes suddenly those have a lot of parameters in them and it kind of complicates the story but in general this is another useful tool in our toolbox for trying to understand what these models are doing and giving us a sense of uh just what's going on inside these super complicated pre-trained models that's the end of this segment you\", metadata={'source': 'a6u6WM5wcLQ'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is beam search?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": \"What is beam search?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search is a decoding algorithm commonly used in natural language processing and machine learning for generating sequences of words, such as in language modeling or machine translation tasks. It is an improvement over the greedy search approach and aims to find a sequence with higher probability by considering multiple candidate sequences simultaneously.\n",
      "\n",
      "In beam search, at each step of generating a sequence, the algorithm keeps track of a fixed number (beam width) of the most promising candidate sequences based on their probabilities. These candidate sequences are expanded by considering possible next words or tokens, and the top candidates are retained while others are pruned. This process continues until the sequence is complete or a predefined stopping criterion is met.\n",
      "\n",
      "By maintaining a beam of candidate sequences, beam search explores the search space more effectively than greedy search, which simply selects the most likely next word at each step. Beam search strikes a balance between exploration and exploitation, leading to better overall sequence generation compared to greedy search.\n",
      "\n",
      "Overall, beam search is a widely used technique in sequence generation tasks to find high-probability sequences efficiently while considering multiple alternatives.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov Chains are mathematical systems that undergo transitions from one state to another in a probabilistic manner. These transitions are based on certain probabilities, and the next state only depends on the current state, not on the sequence of events that preceded it. In other words, Markov Chains exhibit the Markov property, which is the memoryless property where the future state depends only on the present state, not on the sequence of events that led to the present state.\n",
      "\n",
      "Markov Chains are widely used in various fields such as statistics, physics, computer science, and economics to model random processes and systems. They are essential in understanding and analyzing systems that involve stochastic (random) behavior and have applications in areas like modeling weather patterns, stock market fluctuations, text generation, and more."
     ]
    }
   ],
   "source": [
    "async for msg in chain.astream(\"What is Markov Chains?\"):\n",
    "    print(msg, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_msg = \"\"\n",
    "\n",
    "async for msg in chain.astream(\"What is Markov Chains?\"):\n",
    "    final_msg += msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Markov Chains are mathematical systems that undergo transitions from one state to another in a probabilistic manner. These transitions depend only on the current state and not on the sequence of events that preceded it, making them memoryless or \"Markovian.\" In a Markov Chain, the future state of the system is predicted based solely on its current state and the probabilities of transitioning to other states.\\n\\nMarkov Chains are widely used in various fields such as physics, biology, economics, and computer science. They are fundamental in modeling stochastic processes, analyzing random systems, and predicting future outcomes based on probabilistic transitions between states.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov Chains are mathematical systems that undergo transitions from one state to another in a probabilistic manner. These transitions depend only on the current state and not on the sequence of events that preceded it, following the Markov property. In essence, the future state of the system is solely determined by its current state.\n",
      "\n",
      "Markov Chains are widely used in various fields such as statistics, physics, biology, finance, and computer science. They are fundamental in modeling stochastic processes, predicting future events, and analyzing systems with random behavior."
     ]
    }
   ],
   "source": [
    "for msg in chain.stream(\"What is Markov Chains?\"):\n",
    "    print(msg, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
